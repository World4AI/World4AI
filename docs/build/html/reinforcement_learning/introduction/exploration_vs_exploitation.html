
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Exploration vs Exploitation &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Agent and Environment Components" href="components.html" />
    <link rel="prev" title="States, Actions, Rewards" href="interaction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mathematics of Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../math/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../value_based_approximation/online_td_learning.html">
   Online TD Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="exploration-vs-exploitation">
<h1>Exploration vs Exploitation<a class="headerlink" href="#exploration-vs-exploitation" title="Permalink to this headline">Â¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>Exploitation:</em></p>
<p>The agent exploits his knowledge about the environment to get a high sum of rewards.</p>
<p><em>Exploration:</em></p>
<p>The agent explores the environment to gather new experience and knowledge about the environment.</p>
</div>
<p>The main idea of exploration vs exploitation is that the agent has to decide at each time step if he wants to utilize the knowledge he already possesses to get a relatively high sum of rewards or does he want to explore the environment to maybe find a better path with a higher sum of rewards.</p>
<div class="figure align-center" id="id1">
<img alt="../../_images/exploit_explore.svg" src="../../_images/exploit_explore.svg" /><p class="caption"><span class="caption-text">Exploration vs Exploitation</span><a class="headerlink" href="#id1" title="Permalink to this image">Â¶</a></p>
</div>
<p>The above image depicts an agent who has discovered the path as indicated by the arrows. Generally the agent might keep taking the same path to reach the triangle, but if he kept exploring the environment he could discover that there is actually a big reward in the bottom right corner. Exploration would enable the agent to learn a strategy with a higher sum of rewards.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>Deterministic Environment</em>:</p>
<p>Given the same state by the environment and the same action by the agent the next state and the corresponding reward are always the same.</p>
<p><em>Stochastic Environment</em>:</p>
<p>Given the same state by the environment and the same action by the agent the next state and the corresponding reward are calculated using a probability distribution of the environment.</p>
</div>
<p>The grid environment I covered so far was deterministic. I assumed that there is no uncertainty and given the same circumstances the outcome would be the same.</p>
<div class="figure align-center" id="id2">
<img alt="../../_images/deterministic.svg" src="../../_images/deterministic.svg" /><p class="caption"><span class="caption-text">An example of a deterministic environment</span><a class="headerlink" href="#id2" title="Permalink to this image">Â¶</a></p>
</div>
<p>For example whenever the agent chose the action to go right in the first state for example the environment transitioned in such a way that the circle moved right. Each and every single time. No matter how often the action was repeated.</p>
<p>Most of the environments or the real world for that matter are not that simple, they are not deterministic but stochastic. Which means that there is a probability distribution of how the environment will react given a certain state and the action of the agent.</p>
<div class="figure align-center" id="id3">
<img alt="../../_images/stochastic.svg" src="../../_images/stochastic.svg" /><p class="caption"><span class="caption-text">An example of a stochastic environment</span><a class="headerlink" href="#id3" title="Permalink to this image">Â¶</a></p>
</div>
<p>The image above shows the simplest imaginable stochastic gridworld, where the game ends after just one time step. The agent can choose to go either left or right. If he chooses the left action there is a 100% chance to get a reward of 1. If he chooses the right action there is a 99% chance to get a reward of 0 and a 1% chance to get a reward of 1000000. After trying out each action once the agent receives a reward of 1 for the left action and (most likely) a reward of 0 for the right action. In order to exploit his knowledge the agent would have to take the left action. On average the right action is the one that produces a better reward, but in order to discover that the agent has to keep exploring.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The goal of the agent is to maximize the expected sum of rewards.</p>
</div>
<p>In stochastic environments the agent has to maximize the expected sum of rewards. Intuitively speaking that means that the agent has to choose the strategy that would give him the largest sum of rewards if he played an infinite number of games. The above definition for the goal of the agent is going to be used for the rest of the course, as in deterministic environments the expected sum of rewards would equal the sum of rewards.</p>
<p>The problem that the agent faces is that he does not know exactly how the distribution of the environment looks like. Therefore the agent has to use trial and error to determine the path that leads to the highest sum of rewards in expectation. The common problem that  the agent faces in that situation is the so-called âexploration exploitation dilemmaâ. On the one hand the agent wants to get the highest reward based on his current knowledge. So he wants to exploit the knowledge he possesses. On the other hand in order to find a better path he needs to explore the environment. And the dilemma that he faces is the fact that he can not do both at the same time. At each single step the agent either explores or exploits.</p>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="interaction.html" title="previous page">States, Actions, Rewards</a>
    <a class='right-next' id="next-link" href="components.html" title="next page">Agent and Environment Components</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>