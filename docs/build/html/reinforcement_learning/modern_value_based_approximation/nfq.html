
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Fitted Q Iteration &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Deep Q-Network (DQN)" href="dqn.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Intuition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/components.html">
   Agent and Environment Components
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/terminology.html">
   Reinforcement Learning Terminology
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/bias_variance_tradeoff.html">
   Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/double_q_learning.html">
   Double Q-Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/convergence_optimality.html">
   Convergence And Optimality
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prioritized_experience_replay.html">
   Prioritized Experience Replay
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="additional_dqn_improvements.html">
   Additional DQN Improvements
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algorithm">
   Algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytorch-implementation">
   PyTorch Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sources">
   Sources
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="neural-fitted-q-iteration">
<h1>Neural Fitted Q Iteration<a class="headerlink" href="#neural-fitted-q-iteration" title="Permalink to this headline">¶</a></h1>
<p>The naive implementation of Q-Learning with neural networks from a previous chapter was an online learning implementation. The algorithm used an experience once and threw the information away.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An algorithm <strong>A</strong> is said to be more sample efficient than algorithm <strong>B</strong> if algorithm <strong>A</strong> requires less samples to learn a task.</p>
</div>
<p>In that sense approximate online q-learning was not particularly sample efficient. Neural Fitted Q Iteration (NFQ) takes a different approach <a class="footnote-reference brackets" href="#id2" id="id1">1</a>. Neural Fitted Q Iteration (NFQ) takes a different approach. The experience is collected in a sample set <img class="math" src="../../_images/math/da7c889d702c8a3e5f76fa70f2afa7610b124db5.svg" alt="\mathcal{D}"/> and the whole set is used to reduce the mean squared error. The training is similar to supervised learning, where the same data set is iterated over and over again, until the desired convergence is achieved.</p>
<p>There are several variants regarding how the collection of experiences can be handled. The first variant, that is also going to be used in our implementation below, samples the whole experience set by performing random actions before any training is done. An additional and probably a better variant is to fill the sample set gradually, by iterating between collection and training. This variant makes especially a lot of sense when certain observations can not be reached through random actions.</p>
<p>If you study the paper by Riedmiller, you will notice that the algorithms we are using are deviating from the source material. The original paper uses cost (negative reward), instead of reward. This is less common in modern literature and we are going to stick to rewards. Keep this in mind when you notice expressions where q-values need to be minimized and not maximized.</p>
<div class="section" id="algorithm">
<h2>Algorithm<a class="headerlink" href="#algorithm" title="Permalink to this headline">¶</a></h2>
<div class="math">
<p><img src="../../_images/math/a713706df64e9817cb4bfaaddf61b88bea47c506.svg" alt="\begin{algorithm}[H]
    \caption{Neural Fitted Q Iteration}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: Environment for interaction, $env$
    \STATE Input: Neural Network initialized randomly (q-function) $\hat{q}$
    \STATE Input: Number of episodes for data collection
    \STATE Input: Number of epochs
    \STATE Input: Batch Size
    \STATE Input: Learning Rate alpha $\alpha$
    \STATE Input: Discount Rate gamma $\gamma$
    \FOR{$episode=0$ to number of episodes}
      \STATE Initialize $env$ and get initial state $S$
      \REPEAT
        \STATE Select action $A$ based on $S$ using $\epsilon$-greedy action selection
        \STATE Observe reward $R$ and state $S'$
        \STATE Add $S, A, R, S'$ to $\mathcal{D}$
        \STATE $S \leftarrow S'$
      \UNTIL{State $S'$ is terminal}
    \ENDFOR
    \FOR{$episode=0$ to number of epochs}
        \FOR{batch in $\mathcal{D}$}
            \STATE Update weights $\mathbf{w}$ for a whole batch
            \STATE $\mathbf{w} \leftarrow \mathbf{w} + \alpha[R + \gamma\max_a\hat{q}(S',a, \mathbf{w}) - \hat{q}(S, A, \mathbf{w})]\nabla\hat{q}(S, A, \mathbf{w})$
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}"/></p>
</div></div>
<div class="section" id="pytorch-implementation">
<h2>PyTorch Implementation<a class="headerlink" href="#pytorch-implementation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Q</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating a dataset</span>
<span class="k">class</span> <span class="nc">TrainDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obss</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_obss</span><span class="p">,</span> <span class="n">dones</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obss</span> <span class="o">=</span> <span class="n">obss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_obss</span> <span class="o">=</span> <span class="n">next_obss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dones</span> <span class="o">=</span> <span class="n">dones</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">obss</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">next_obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_obss</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">action</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">reward</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">next_obs</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">obss</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">q_function</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_function</span> <span class="o">=</span> <span class="n">q_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="n">num_epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">q_function</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

        <span class="c1"># Collect Experiences</span>
        <span class="n">obss</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_obss</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_experiences</span><span class="p">()</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TrainDataset</span><span class="p">(</span><span class="n">obss</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_obss</span><span class="p">,</span> <span class="n">dones</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_batches</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="c1">#SOME DATA STRUCTURES FOR LOGGING</span>
        <span class="c1"># save sum of rewards after each evaluation episode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># save average reward for each 100 episodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg_rewards_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># max sum of rewards seen so far in a single game</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># max average sum of rewards over 100 games seen so far</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_avg_rewards</span> <span class="o">=</span> <span class="mi">0</span>


    <span class="k">def</span> <span class="nf">collect_experiences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">obss</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">next_obss</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_episodes</span><span class="p">):</span>
            <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                <span class="n">next_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

                <span class="n">obss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
                <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
                <span class="n">next_obss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_obs</span><span class="p">)</span>
                <span class="n">dones</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>

                <span class="n">obs</span> <span class="o">=</span> <span class="n">next_obs</span>

        <span class="c1"># transform the list into numpy arrays</span>
        <span class="n">obss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">obss</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">next_obss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">next_obss</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dones</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># add additional axes e.g. (5, ) to (5, 1)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">dones</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">obss</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_obss</span><span class="p">,</span> <span class="n">dones</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">reward_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
                <span class="n">action_left</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
                <span class="n">action_right</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_function</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">action_left</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_function</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">action_right</span><span class="p">):</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">next_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">next_obs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rewards_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reward_sum</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_reward</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_reward</span> <span class="o">=</span> <span class="n">reward_sum</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards_list</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">:</span>
            <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards_list</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
            <span class="k">if</span> <span class="n">avg_reward</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_avg_rewards</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_avg_rewards</span> <span class="o">=</span> <span class="n">avg_reward</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">avg_rewards_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_reward</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--------------------------------&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Episode: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Episode: </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_batches</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Reward Sum: </span><span class="si">{</span><span class="n">reward_sum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Max Reward Sum: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_reward</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Avg. Reward Sum: </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Max Avg. Reward Sum: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_avg_rewards</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">obss</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_obss</span><span class="p">,</span> <span class="n">dones</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">):</span>
                <span class="n">obss</span> <span class="o">=</span> <span class="n">obss</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
                <span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
                <span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
                <span class="n">next_obss</span> <span class="o">=</span> <span class="n">next_obss</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
                <span class="n">dones</span> <span class="o">=</span> <span class="n">dones</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">action_left</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">rewards</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
                    <span class="n">action_right</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">rewards</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

                    <span class="n">target_left</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_function</span><span class="p">(</span><span class="n">next_obss</span><span class="p">,</span> <span class="n">action_left</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">)</span>
                    <span class="n">target_right</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_function</span><span class="p">(</span><span class="n">next_obss</span><span class="p">,</span> <span class="n">action_right</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">)</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">target_left</span><span class="p">,</span> <span class="n">target_right</span><span class="p">)</span>

                <span class="n">online</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_function</span><span class="p">(</span><span class="n">obss</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">online</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CONSTANTS</span>
<span class="n">ENV</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>
<span class="n">NUM_EPISODES</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">Q_FUNCTION</span> <span class="o">=</span> <span class="n">Q</span><span class="p">()</span>
<span class="n">Q_FUNCTION</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">ENV</span><span class="p">,</span>
          <span class="n">q_function</span><span class="o">=</span><span class="n">Q_FUNCTION</span><span class="p">,</span>
          <span class="n">num_episodes</span><span class="o">=</span><span class="n">NUM_EPISODES</span><span class="p">,</span>
          <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
          <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">,</span>
          <span class="n">gamma</span><span class="o">=</span><span class="n">GAMMA</span><span class="p">)</span>

<span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="sources">
<h2>Sources<a class="headerlink" href="#sources" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Riedmiller M. (2005) Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method. In: Gama J., Camacho R., Brazdil P.B., Jorge A.M., Torgo L. (eds) Machine Learning: ECML 2005. ECML 2005. Lecture Notes in Computer Science, vol 3720. Springer, Berlin, Heidelberg. <a class="reference external" href="https://doi.org/10.1007/11564096_32">https://doi.org/10.1007/11564096_32</a></p>
</dd>
</dl>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="dqn.html" title="next page">Deep Q-Network (DQN)</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>