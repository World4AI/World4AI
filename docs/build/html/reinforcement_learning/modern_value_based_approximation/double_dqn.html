
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Double DQN &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Prioritized Experience Replay" href="prioritized_experience_replay.html" />
    <link rel="prev" title="Deep Q-Network (DQN)" href="dqn.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Intuition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/components.html">
   Agent and Environment Components
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/terminology.html">
   Reinforcement Learning Terminology
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/bias_variance_tradeoff.html">
   Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/double_q_learning.html">
   Double Q-Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/convergence_optimality.html">
   Convergence And Optimality
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prioritized_experience_replay.html">
   Prioritized Experience Replay
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="additional_dqn_improvements.html">
   Additional DQN Improvements
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Policy Gradient Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/policy_gradient_derivation.html">
   Policy Gradient Derivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce.html">
   REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce_with_baseline_vpg.html">
   REINFORCE With Baseline | Vanilla Policy Gradient  (VPG)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods I
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/baseline_vs_actor_critic.html">
   Baseline Methods vs Actor-Critic-Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/asynchronous_advantage_actor_critic_a3c.html">
   Asynchronous Advantage Actor-Critic (A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/generalized_advantage_estimation_gae.html">
   Generalized Advantage Estimation (GAE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/advantage_actor_critic_a2c.html">
   Advantage Actor-Critic (A2C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/trust_region_policy_optimization_trpo.html">
   Trust Region Policy Optimization (TRPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/proximal_policy_optimization_ppo.html">
   Proximal Policy Optimization (PPO)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods II
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/introduction.html">
   Intoduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/deep_deterministic_policy_gradient_ddpg.html">
   Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/twin_delayed_ddpg_td3.html">
   Twin Delayed DDPG (TD3)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/soft_actor_critic_sac.html">
   Soft Actor-Critc (SAC)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory">
   Theory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sources">
   Sources
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="double-dqn">
<h1>Double DQN<a class="headerlink" href="#double-dqn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>“We first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation <a class="footnote-reference brackets" href="#id2" id="id1">1</a>.”</p>
</div>
<div class="figure align-center" id="id3">
<img alt="../../_images/overestimation_nn.svg" src="../../_images/overestimation_nn.svg" /><p class="caption"><span class="caption-text">Neural Network Maximization Bias.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>The DQN algorithm suffers from a similar maximization bias that is present in tabular Q-Learning. The output of Q-functions is an estimate that might contain some noise. The noise that produces the highest number will be preferred in a max operation, even if the true action values are equal. The researchers at DeepMind showed that applying double learning to the DQN algorithms improves the performance of the agent for Atari games. In this section we are going to focus on simple environments like Lunar Lander in order to account for slower hardware of some readers.</p>
</div>
<div class="section" id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h2>
<p>In the DQN algorithm the target value is calculated by utilizing the neural network with frozen weights <img class="math" src="../../_images/math/74333dc62c746cb3d7bf15305ffb306c99d6597f.svg" alt="\theta^-"/>.</p>
<div class="math">
<p><img src="../../_images/math/117a7c98187493fd94f889ccc1d74e1a12d3943d.svg" alt="target = r + \gamma \max_{a'} Q(s', a', \theta^-)"/></p>
</div><p>It is noticeble that the same Q-function <img class="math" src="../../_images/math/d7ca69a49d75f8b48db36ce5b83e1b1ab20179da.svg" alt="Q(s, a, \theta^-)"/> is used to select the next action <img class="math" src="../../_images/math/1d9de7846b37e74d10486bb30a972b022a9aee27.svg" alt="a'"/> and to calculate the ation-value. This is consistent with the classical definition of Q-Learning. In double Q-Learning two separate acion value functions are used. One is used for action selection while the other is used for the calculation of the target value. Using the same approach in DQN would not be efficient, as it would require the training of two action value function. However the original DQN algorithm already uses two action value funcitons. This will allow us to disentangle the calculate of the target value and to separate action selection and action value calculation.</p>
<div class="math">
<p><img src="../../_images/math/0cd23bcc493acd2b3723dbfb2884578139cff995.svg" alt="\begin{align*}
&amp; a' = \max_{a}Q(s', a', \theta) \\
&amp; target = r + \gamma Q(s', a', \theta^-)
\end{align*}"/></p>
</div><p>The action in the next state is selected by utilizing the action value function <img class="math" src="../../_images/math/c794fabba9885b80037bdb908b54f17a339f7320.svg" alt="Q(s, a, \theta)"/>, while the calculation of the action-value is performed using the frozen weights <img class="math" src="../../_images/math/74333dc62c746cb3d7bf15305ffb306c99d6597f.svg" alt="\theta^-"/>.</p>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>The adjustments to DQN that need to be made to implement double learning are minimal.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># replace the below code</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">(</span><span class="n">next_obs</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>

<span class="c1"># with the following code</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">next_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">online_network</span><span class="p">(</span><span class="n">next_obs</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">(</span><span class="n">next_obs</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">next_action</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="sources">
<h2>Sources<a class="headerlink" href="#sources" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>van Hasselt H., Guez A., Silver D.. Deep Reinforcement Learning with Double Q-learning. <a class="reference external" href="https://arxiv.org/abs/1509.06461">https://arxiv.org/abs/1509.06461</a></p>
</dd>
</dl>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="dqn.html" title="previous page">Deep Q-Network (DQN)</a>
    <a class='right-next' id="next-link" href="prioritized_experience_replay.html" title="next page">Prioritized Experience Replay</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>