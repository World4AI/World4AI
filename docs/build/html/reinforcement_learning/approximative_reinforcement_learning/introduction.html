
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Cart Pole Environment" href="cart_pole.html" />
    <link rel="prev" title="Temporal Difference Learning" href="../tabular_reinforcement_learning/td_learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convergence_optimality.html">
   Convergence And Optimality
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>So far we dealt with tabular solution methods for finite MDPs.</p>
<table class="colwidths-given table" id="id1">
<caption><span class="caption-text">Q-Table</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>State</p></th>
<th class="head"><p>Value Action 1</p></th>
<th class="head"><p>Value Action 2</p></th>
<th class="head"><p>Value Action 3</p></th>
<th class="head"><p>Value Action 4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>2</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>4</p></td>
</tr>
</tbody>
</table>
<p>The number of rows (4 states) and columns (4 actions) in the Q-Table was finite. This allowed us to loop over all states and actions and use Monte Carlo or TD methods. Given enough iterations we were guaranteed to find the optimal value functions and thus policies.</p>
<p>Most interesting reinforcement learning problems do not have such nice properties. In case state or action sets are infinite or extremely large it becomes impossible to store the value function as a table.</p>
<table class="colwidths-given table" id="id2">
<caption><span class="caption-text">Infinite Q-Table</span><a class="headerlink" href="#id2" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>State</p></th>
<th class="head"><p>Value Action 1</p></th>
<th class="head"><p>Value Action 2</p></th>
<th class="head"><p>Value Action 3</p></th>
<th class="head"><p>Value Action 4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>2</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p>.</p></td>
<td><p>.</p></td>
<td><p>.</p></td>
<td><p>.</p></td>
<td><p>.</p></td>
</tr>
<tr class="row-odd"><td><p>1,000,000,000</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
</tr>
</tbody>
</table>
<p>The above table shows action-values for 1,000,000,000 discrete states. Even if we possessed a computer which could efficiently store a high amount of states, we still need to loop over all these states and thus convergence would be extremely slow.</p>
<p>Q-Tables become almost impossible when the agent has to deal with continuous variables. When the agent sees a continuous observation for example there is a chance that the same exact value will not be seen again. Yet the agent will need to learn how to deal with future unseen observations. We expect from the agent to find a policy that is “good”  across many different observations. The key word that we are looking for is generalization.</p>
<table class="colwidths-given table" id="id3">
<caption><span class="caption-text">Q-Table</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>State Representation</p></th>
<th class="head"><p>Action Value 1</p></th>
<th class="head"><p>Action Value 2</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1.1</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>1.3</p></td>
<td><p>1.2</p></td>
<td><p>1.8</p></td>
</tr>
<tr class="row-even"><td><p>1.5</p></td>
<td><p>1.5</p></td>
<td><p>1.2</p></td>
</tr>
<tr class="row-odd"><td><p><strong>1.7</strong></p></td>
<td><p><strong>?</strong></p></td>
<td><p><strong>?</strong></p></td>
</tr>
<tr class="row-even"><td><p>2.1</p></td>
<td><p>1.8</p></td>
<td><p>1.3</p></td>
</tr>
<tr class="row-odd"><td><p>2.2</p></td>
<td><p>1.7</p></td>
<td><p>1.8</p></td>
</tr>
<tr class="row-even"><td><p>2.5</p></td>
<td><p>1.5</p></td>
<td><p>2</p></td>
</tr>
</tbody>
</table>
<p>The example above shows how generalization might look like. The state is represented by 1 single continuous variable and there are only 2 discrete actions available (left and right). If you look at the state representation with the value 1.7 could you approximate the action-values and determine which action has the higher value? You will probably not get the exact correct answer but the value for the left action should probably be somewhere between 1.5 and 1.8 and therefore larger than the value between 1.2 and 1.3. Real reinforcement learning tasks might be a lot more complex, but I think it is a good mental model to imagine the agent interpolating between the states he has already seen and learned from.</p>
<p>In the case when the state/action sets are extremely large and/or continous, lookup tables for each state-action pair become impossible. As the number of states and/or actions is possibly infinite, the function has to generalize and we can assume that it is impossible to create a function that generates optimal values for each state-action pair.  Thus it becomes necessary to create value functions that are not exact, but approximative, meaning that the value function does not return the true value of a policy but a value that is hopefully close enough. Finding the optimal policy and value function is often not possible, but the general idea is to find a policy that still performs in a way that generates a relatively high expected sum of rewards.</p>
<p>We can look at it this way. Humans most definitely don’t have optimal policies for complex tasks they have to perform. If they did, then chess for example would be extremely boring and would not have survived for many hundred years. Still you can appreciate the complexity of the game and the extremely high level of professional players. We are going to have similar expectations for our agents. Even though we will not be able to find optimal value functions for some of the environments we are going to generate extremely impressive results.</p>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="../tabular_reinforcement_learning/td_learning.html" title="previous page">Temporal Difference Learning</a>
    <a class='right-next' id="next-link" href="cart_pole.html" title="next page">Cart Pole Environment</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>