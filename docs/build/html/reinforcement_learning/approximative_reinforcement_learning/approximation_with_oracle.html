
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Value Approximation With An Oracle &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Introduction" href="../modern_value_based_approximation/introduction.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Value Approximation With An Oracle
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="value-approximation-with-an-oracle">
<h1>Value Approximation With An Oracle<a class="headerlink" href="#value-approximation-with-an-oracle" title="Permalink to this headline">¶</a></h1>
<p>Just as in the tabular case of general policy iteration we need to run prediction and improvement sequentially.</p>
<p>For prediction it is sufficient to approximate the state-value function.</p>
<div class="math">
<p><img src="../../_images/math/12858a62406df41677a7fb7a804c5525e8894fd0.svg" alt="\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)"/></p>
</div><p>For improvement we will need to approximate the action-value function.</p>
<div class="math">
<p><img src="../../_images/math/455207cac19c2107945c5ceb301499c7990db69c.svg" alt="\hat{q}(s, a, \mathbf{w}) \approx q_{\pi}(s, a)"/></p>
</div><p><img class="math" src="../../_images/math/129d8f9483020a3917ebfed4f6ed8bc17e7c467e.svg" alt="\hat{v}(s, \mathbf{w})"/> and <img class="math" src="../../_images/math/1d0b71b43fe2db56a1c530879e699638368ec66c.svg" alt="\hat{q}(s, a, \mathbf{w})"/> are both parameterized functions where <img class="math" src="../../_images/math/8cdbc202ab98ecf4d1d1d5fd289844cc850c67cd.svg" alt="\mathbf{w} \in \mathbb{R}^d"/> is a weight vector.  That essentially means that in order for the function to calculate the value of the state or state-action pair a vector <img class="math" src="../../_images/math/78d892421daff9c2c4d06acbe122ccaf9d482e40.svg" alt="\mathbf{w}"/> (the parameters of the function) is needed for the calculation. Better policy evaluation and policy improvement involve adjusting this weight vector. The novelty is also that the same vector is used for all states (or state-action pairs). That means that adjusting the weights to make better predictions for a certain state affects the predictions of all other states. Finding a balance for different states is going to be of great concern to us. Tweaking the weights for seen states we would like our solution to be general for many other, even unforeseen states.</p>
<p>The state representation is also different from the tabular case. In the tabular case each state was represented by a single number, which was used as an address in the lookup table.</p>
<p>With function approximators that approach is not sustainable, as for most interesting problems the number of states is larger than the number of atoms in the observable universe. For that purpose the state is represented by a so-called feature vector. Each number in the vector gives some information about the state. The whole vector is the available representation of the state.</p>
<div class="math">
<p><img src="../../_images/math/f491f5496bae27da0ed07fc2c1e9668221edeb51.svg" alt="\mathbf{x} \doteq (x_1(s), x_2(s), ... , x_d(s))^T"/></p>
</div><p>In many cases the representation is only partial, therefore in approximative methods we are going to use the word observation instead of state to show the possible limitations of state representations.</p>
<p>Pure value methods that use action-value functions to determine the policy are still limited to discrete actions spaces. To determine the action the agent needs to take the max over available options and that gets problematic with continuous action spaces. In case of a high number of possible actions it might take a long time to calculate the max and the performance would suffer. For now it is sufficient to know that there are other approximation methods that can deal with these sorts of problems.</p>
<p>There are many different types of function approximators:</p>
<ul class="simple">
<li><p>Linear Function Approximators</p></li>
<li><p>Neural Networks (Non-Linear Function Approximators)</p></li>
<li><p>Decision Trees</p></li>
<li><p>…</p></li>
</ul>
<p>Depending on the function approximators the weight vector might play a different role in the calculation of the value function. At the moment of writing most modern reinforcement learning function approximators are neural networks. Linear function approximators are especially useful to introduce the topic of function approximators, as those are easiest to grasp and show some useful mathematical properties. Linear functions and neural networks are differentiable, decision Trees are not differentiable functions. That means that for linear functions (and neural networks) it is easy to determine how to adjust the weight vector w so as to reduce some value of a function.</p>
<p>In linear function approximators each of the features is “weighted” by the corresponding weight. The individual weighted features are summed up to produce the value.</p>
<div class="math">
<p><img src="../../_images/math/82ed720a06f3d9cba972bd3616ec95e20b8b5161.svg" alt="\hat{v}(s, \mathbf{w}) \doteq \mathbf{w}^T\mathbf{x}(s) \doteq \sum_{i=1}^d w_i x_i(s)"/></p>
</div><p>Neural networks are non-linear function approximators, where each neuron in itself is a non-linear function. The calculation for each neuron is similar to that of the linear function, but the result of the weighted sum is used as an input to a non-linear function <img class="math" src="../../_images/math/1777507f050ed24127289d8e5564a88e8a386eea.svg" alt="f()"/>.</p>
<div class="math">
<p><img src="../../_images/math/69049a4d9209b12787ff288b22649eb4b57fc6ce.svg" alt="\hat{v}(s, \mathbf{w}) \doteq f(\mathbf{w}^T\mathbf{x}(s)) \doteq f(\sum_{i=1}^d w_i x_i(s))"/></p>
</div><p>To build the theory that is going to be used throughout the rest of the book it is convenient to start the discussion by assuming that we are in a supervised learning setting and that there is an oracle who tells us what the true state-value <img class="math" src="../../_images/math/fc4a32e1a50cc2d8e5b50909ae74cdb02b135704.svg" alt="v_{\pi}(s)"/> for the given policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> and state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> is. Later the discussion can be extended to more complex reinforcement learning settings.</p>
<p>In supervised learning the goal is to find a weight vector w that produces a function that fits the training data as close as possible. That means that we want weights that reduce the difference between the true state-value and our approximation as much as possible. In reinforcement learning Mean Squared Error (MSE) is used to define the difference between the true value function and the approximate value function.</p>
<div class="math">
<p><img src="../../_images/math/5b090cd90e2e23d748b7ac95c993f8ca4de588fc.svg" alt="MSE \doteq \mathbb{E_{\pi}}[(v_{\pi} - \hat{v}(s, \mathbf{w}))^2]"/></p>
</div><p>Stochastic gradient descent in a setting with an oracle would work as follows. Let us assume we interact with the environment using the policy pi. For each of the observations the oracle tells us the true value of the state v_pi(s), which we can use in stochastic gradient descent to update the weight vector w. <strong>Stochastic</strong> gradient descent means that we use gradient descent at each single observation and do not wait to collect these into a batch.</p>
<p>The update rule is as follows.</p>
<div class="math">
<p><img src="../../_images/math/72c85dfc2db0f3f4b830bd2b0852311fb2266147.svg" alt="\begin {align*}
w_{t+1} &amp; \doteq w_t - \frac{1}{2}\alpha\nabla[v_{\pi}(S_t) - \hat{v}(S_t,\mathbf{w}_t)]^2 \\
&amp; = w_t + \alpha[v_{\pi}(S_t) - \hat{v}(S_t, \mathbf{w}_t)]\nabla\hat{v}(S_t, \mathbf{w}_t)
\end {align*}"/></p>
</div><p>The gradient <img class="math" src="../../_images/math/2e83d7c60460e86b76e6667fcdbc52757027114a.svg" alt="\nabla\hat{v}(S_t, \mathbf{w}_t)"/> is a vector that contains partial derivations of the approximative value function with respect to individual weights. We reduce the weights into the direction of the gradient.</p>
<div class="math">
<p><img src="../../_images/math/0087d3ade2732557045a0513da1c55f2c54d87d3.svg" alt="\nabla \hat{v}(s, \mathbf{w}) \doteq (\frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, ... , \frac{\partial f(\mathbf{w})}{\partial w_d})^T"/></p>
</div></div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="../modern_value_based_approximation/introduction.html" title="next page">Introduction</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>