
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Value Iteration &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Introduction" href="../exploration_exploitation_tradeoff/introduction.html" />
    <link rel="prev" title="Policy Iteration" href="policy_iteration.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mathematics of Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../math/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Value Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_methods/online_td_learning.html">
   Online TD Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_methods/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="value-iteration">
<h1>Value Iteration<a class="headerlink" href="#value-iteration" title="Permalink to this headline">¶</a></h1>
<p>When we consider policy iteration again, we remember that there are two distinct steps, policy evaluation and policy improvement. The policy improvement step is a single step, where the new policy is derived by acting greedily. The policy evaluation on the other hand is a longer iterative process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A policy evaluation algorithm that is stopped  early is called a “truncated” policy evaluation.</p>
</div>
<p>It turns out that it is not necessary to wait for the policy evaluation algorithm to finish. A truncated policy evaluation algorithm is sufficient to generate an optimal policy. With the value iteration algorithm the truncation happens after a single step of policy evaluation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Value Iteration is the Bellman optimality equation, transformed from equation to an update step.</p>
</div>
<div class="math">
<p><img src="../../_images/math/5debc259bdb13224f7a5d5fcf4d0bbf97d9ef4c9.svg" alt="\begin{align*}
v_{k+1}(s) &amp; \doteq \max_a \mathbb{E}[R_{t+1} + \gamma v_k (S_{t+1}) \mid S_t = s, A_t = a] \\
&amp; = \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_k (s')]
\end{align*}"/></p>
</div><p>Although the update step looks like a single step it actually combines truncated policy evaluation and policy improvement in a single step.</p>
<ol class="arabic simple">
<li><p><img class="math" src="../../_images/math/cf394fbd06146b1e2ee4a1218de3eb74e85b2cb2.svg" alt="q_{k+1}(s, a) = \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_k (s')]"/></p></li>
<li><p><img class="math" src="../../_images/math/dc304bcca635949431b0c7254891141ef49641af.svg" alt="v_{k+1} = \max_a q_{k+1}(s, a)"/></p></li>
</ol>
<p>In the first step the action-value function is calculated based on the old state-value function and the model of the MDP. In the second step a max over the action-value function is taken in order to generate the new state-value function. That implicitly generates a new policy as a value function is always calculated for a particular policy.</p>
<p>The combination of both is the value iteration algorithm. The iterative process continues until the difference between the old and the new value function is smaller than some parameter theta. As a final step the optimal policy can be deduced using the argmax over the optimal action-value function.</p>
<div class="math">
<p><img src="../../_images/math/fa7e2a1dcb499a2479c0c24251703aef7ebdcf25.svg" alt="\begin{algorithm}[H]
    \caption{Value Iteration}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: model $p$, state set $\mathcal{S}$, action set $\mathcal{A}$, stop criterion $\theta &gt; 0$, discount factor $\gamma$
    \STATE Initialize:
    \STATE $V(s)$ and $V_{old}(s)$, for all $s \in \mathcal{S}$ with zeros
    \REPEAT
        \STATE $\Delta \leftarrow 0$
        \STATE $V_{old}(s) = V(s)$ for all $s \in \mathcal{S}$
        \FORALL{$s \in \mathcal{S}$}
            \STATE $Q(s, a) \leftarrow \sum_{s', r}p(s', r \mid s, a)[r + \gamma V_{old}(s')]$
            \STATE $V(s) \leftarrow \max_a Q(s, a)$
            \STATE $\Delta \leftarrow \max(\Delta,|V_{old}(s) - V(s)|)$
        \ENDFOR
    \UNTIL{$\Delta &lt; \theta$}
    \STATE Output: value function $V(s)$ and policy $\mu(s)$
    \STATE $\mu(s) = \arg\max_a Q(s, a)$
\end{algorithmic}
\end{algorithm}"/></p>
</div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
    <span class="n">V_old</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
        <span class="n">V_old</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1">#truncated policy evaluation</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
                    <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">V_old</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done</span><span class="p">))</span>


        <span class="c1">#policy improvement</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1">#check for stop criterion</span>
        <span class="n">max_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_old</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">max_diff</span> <span class="o">&lt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="n">policy_mapping</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">policy_mapping</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">V</span>
</pre></div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="policy_iteration.html" title="previous page">Policy Iteration</a>
    <a class='right-next' id="next-link" href="../exploration_exploitation_tradeoff/introduction.html" title="next page">Introduction</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>