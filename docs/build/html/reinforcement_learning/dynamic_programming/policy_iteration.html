
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Policy Iteration &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Value Iteration" href="value_iteration.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mathematics of Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../math/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="value_iteration.html">
   Value Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_methods/online_td_learning.html">
   Online TD Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-evaluation">
   Policy Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-improvement">
   Policy Improvement
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solving-an-mdp">
   Solving An MDP
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="policy-iteration">
<h1>Policy Iteration<a class="headerlink" href="#policy-iteration" title="Permalink to this headline">¶</a></h1>
<p>The policy iteration algorithm is an iterative method. Iterative methods start with initial (usually random or 0) values as approximations and improve the subsequent approximations with each iteration using the previous approximations as input. The policy iteration algorithm consists of two steps. The policy evaluation step calculates the value function for a given policy. The policy improvement step improves the given policy. Both steps run after each other to form the policy iteration algorithm.</p>
<div class="section" id="policy-evaluation">
<h2>Policy Evaluation<a class="headerlink" href="#policy-evaluation" title="Permalink to this headline">¶</a></h2>
<p>The goal of policy evaluation is to find the true value function <img class="math" src="../../_images/math/fca07cf65356d5ea39189ade564697a31977f3be.svg" alt="v_{\pi}"/> of the policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/>.</p>
<div class="math">
<p><img src="../../_images/math/12697051e8f3cf7c447d156cf7afb5d51d6c9411.svg" alt="\begin{align*}
v_{\pi}(s) &amp; \doteq \mathbb{E}_{\pi}[G_t \mid S_t = s] \\
&amp; = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
&amp; = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s] \\
&amp; = \sum_{a}\pi(a \mid s)\sum_{s', r}p(s', r \mid s, a)[r + \gamma v_{\pi}(s')]
\end{align*}"/></p>
</div><p>The algorithm uses the mathematical definition of the value function and turns it into an iterative algorithm.</p>
<div class="math">
<p><img src="../../_images/math/4fc81de30f52390b61469c94ad75583f7ea9e15c.svg" alt="v_{k+1}(s) \doteq \sum_{a}\pi(a \mid s)\sum_{s', r}p(s', r \mid s, a)[r + \gamma v_{k}(s')]"/></p>
</div><p>At each iteration the approximate value for each state is calculated using the old value from in the Bellman equation. The old value is then substituted by the new value. At this point it should become apparent why the Bellman equation is useful. Only the reward from the next time step is required to improve the approximation, as all subsequent rewards are already condensed into the value function from the next time step. That allows the algorithm to use the model to look only one step into the future for the reward and use the approximated value function for the next time step. By repeating the update step over and over again the rewards are getting embedded into the value function and the approximation gets better and better. It can be shown mathematically that if the update step is repeated an unlimited number of times, the approximate value function approaches the true value function of the policy. In practice the improvement is done as long as the value function between two iterations is large enough.</p>
<div class="math">
<p><img src="../../_images/math/c77022ffe1bd73c06ce3478479b920110f3fe913.svg" alt="\begin{algorithm}[H]
    \caption{Iterative Policy Evaluation}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: policy $\mu$, model $p$, state set $\mathcal{S}$, stop criterion $\theta &gt; 0$, discount factor $\gamma$
    \STATE Initialize: $V(s)$ and $V_{old}(s)$, for all $s \in \mathcal{S}$ with zeros
    \REPEAT
        \STATE $\Delta \leftarrow 0$
        \STATE $V_{old}(s) = V(s)$ for all $s \in \mathcal{S}$
        \FORALL{$s \in \mathcal{S}$}
            \STATE $V(s) \leftarrow \sum_{a}\pi(a \mid s)\sum_{s', r}p(s', r \mid s, a)[r + \gamma V_{old}(s')]$
            \STATE $\Delta \leftarrow \max(\Delta,|V_{old}(s) - V(s)|)$
        \ENDFOR
    \UNTIL{$\Delta &lt; \theta$}
\end{algorithmic}
\end{algorithm}"/></p>
</div><p>In order to calculate the value function the algorithm needs 4 inputs.</p>
<ul class="simple">
<li><p>The deterministic policy <img class="math" src="../../_images/math/809c91383d7aa79aab7471903435bc0ede29325b.svg" alt="mu"/> is a function that gets a state as an input and generates an action as an output. We are not going to deal with stochastic agents yet, therefore <img class="math" src="../../_images/math/337206c5434d221d9bada59c52ddcb07e3d471fe.svg" alt="\pi(a \mid s) = 1"/> in the update step. <img class="math" src="../../_images/math/a9447568f89c9b36e179357bd8fe22dde9e188f0.svg" alt="a = \mu (s)"/></p></li>
<li><p>The model p will take the current state and action as input and return the next state, the next reward, the terminal flag and the corresponding probability. <img class="math" src="../../_images/math/7139c06a93da1d4d2293f797d6c6b071d3356ebf.svg" alt="probability, next state, reward, terminal = p(s, a)"/></p></li>
<li><p><img class="math" src="../../_images/math/3f98591dff3521c50335f61964d0bab6fb82df24.svg" alt="\mathcal{S}"/> is the state set of the MDP</p></li>
<li><p>Theta <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/>  is the criterion to stop the algorithm once the difference between the old value function <img class="math" src="../../_images/math/40aef5248c1a6b97e9ba7eca2071542ba5068d25.svg" alt="V_{old}"/> and new value function <img class="math" src="../../_images/math/caf39627574b86c3d37285b2a9154f85d78dc6d9.svg" alt="V"/> is less than <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/></p></li>
<li><p>Gamma <img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/> is the discount factor for the update step in the Bellman equation</p></li>
</ul>
<p>We are going to keep two versions of the value function, <img class="math" src="../../_images/math/40aef5248c1a6b97e9ba7eca2071542ba5068d25.svg" alt="V_{old}"/> and <img class="math" src="../../_images/math/caf39627574b86c3d37285b2a9154f85d78dc6d9.svg" alt="V"/>. During an update iteration we move through the states one at a time and it is more intuitive in my opinion to update all the states with old values before updating the value function as a whole. If we used only one value function we would update some of the states with the already updated values and some with old values. Both versions of updates are valid, but the “inplace” version is not going to be used in this chapter.</p>
<p>The imports consist only of Gym and NumPy. Gym for the MDP and NumPy to make calculations more efficient.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
<p>We are going to calculate the value function for the Frozen Lake environment, but the algorithm is general and can be applied to many different MDPs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The policy function contains internally a mapping table that maps states to actions deterministically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="c1">#     LEFT = 0</span>
<span class="c1">#     DOWN = 1</span>
<span class="c1">#     RIGHT = 2</span>
<span class="c1">#     UP = 3</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="mi">4</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">5</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">6</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">7</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">8</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">9</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">10</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">11</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">12</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">13</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">14</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">15</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">mu</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
</pre></div>
</div>
<p>The model returns the list of possible next states, rewards and corresponding probabilities given the current state and action.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
</pre></div>
</div>
<p>The below code covers the actual policy evaluation algorithm.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="c1"># initialize value functions with zeros</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
    <span class="n">V_old</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">V_old</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
            <span class="c1"># we avoid the loop over the actions as the policy is deterministic</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

            <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
                <span class="n">value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V_old</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done</span><span class="p">))</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># check for stop criterion and break if necessary</span>
        <span class="n">max_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_old</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">max_diff</span> <span class="o">&lt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">V</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="policy-improvement">
<h2>Policy Improvement<a class="headerlink" href="#policy-improvement" title="Permalink to this headline">¶</a></h2>
<p>Remember what it means to solve an MDP?</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To solve an MDP is to find an optimal policy.</p>
</div>
<p>What do we need to find an optimal policy for a finite MDP?</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To find an optimal policy for a finite MDP the optimal value-function is required.</p>
</div>
<p>The policy evaluation step is a prerequisite to compare two policies and to determine which is better, but an additional step, policy improvement, is required to take an existing policy and to make it better.</p>
<p>There is a line of arguments that has to be made in order to show that the policy improvement step is a valid approach.</p>
<p>Let us assume that we have a policy <img class="math" src="../../_images/math/cd89edb8d677e76ff2422b13ab39ad709e93db2e.svg" alt="\mu(s)"/> and contemplate if instead of following the policy strictly, in the current state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> just once we would like to take a different action <img class="math" src="../../_images/math/70b0f96cf249bacc26d34b1f5438fefccf1bde1e.svg" alt="a \neq \mu(s)"/>. After that action we will stick to the old policy <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/> and follow it until the terminal state <img class="math" src="../../_images/math/573c03b1afb9413354760f358c959354ca9577f0.svg" alt="T"/>. The state-value function of the original policy <img class="math" src="../../_images/math/809c91383d7aa79aab7471903435bc0ede29325b.svg" alt="mu"/> is calculated using the above policy evaluation step.</p>
<p>Using the action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> at state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and then following the policy <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/> is essentially the definition of the action-value function.</p>
<div class="math">
<p><img src="../../_images/math/e4f911298244abbb55f2c4347e5940fec44f5e45.svg" alt="q_{\mu}(s, a) \doteq \mathbb{E}[R_{t+1} + \gamma v_{\mu}(S_{t+1}) \mid S_t = s, A_t = a]"/></p>
</div><p>What if we compare v and q and we find out that taking a and then following mu generates an advantage?</p>
<div class="math">
<p><img src="../../_images/math/35bca49d28752145bc050be725dfa690dbc9b175.svg" alt="q_{\mu}(s, a) &gt; v_{\mu}(s)"/></p>
</div><p>Does that suggest that we should always take the new <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> when we face the state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and thus adjust the policy to create a new policy <img class="math" src="../../_images/math/28c5045d9588b9bfe005b156f73242676055a41b.svg" alt="\mu'"/> or should we stick to the old policy <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/>? The policy improvement theorem suggests the former.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Policy Improvement theorem</p>
<div class="math">
<p><img src="../../_images/math/34fb16271e67e2ee4509e094b0617aaf25783669.svg" alt="q_{\mu}(s, \mu'(s)) \geq v_{\mu}(s) \Rightarrow v_{\mu'}(s) \geq v_{\mu}(s), \forall s \in \mathcal{S}"/></p>
</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Policy Improvement Theorem Proof</p>
<div class="math">
<p><img src="../../_images/math/2fae920e2be953d907fbc3743e8e070a85c5d072.svg" alt="\begin{align*}
v_{\mu}(s) &amp; \leq q_{\mu}(s, \mu'(s)) \\
&amp; = \mathbb{E}[R_{t+1} + \gamma v_{\mu}(S_{t+1}) \mid S_t = s, A_t = \mu'(s)] \\
&amp; \leq \mathbb{E}[R_{t+1} + \gamma q_{\mu}(S_{t+1}, \mu'(S_{t+1})) \mid S_t = s, A_t = \mu'(s)] \\
&amp; = \mathbb{E}[R_{t+1} + \gamma \mathbb{E}[R_{t+2} + \gamma v_{\mu}(S_{t+2}) \mid S_{t+1}, A_{t+1} = \mu' (S_{t+1})] \mid S_t = s, A_t = \mu'(s)] \\
&amp; \vdots \\
&amp; \leq \mathbb{E}_{\mu'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \mid S_t = s] \\
&amp; = v_{\mu'}(s)
\end{align*}"/></p>
</div></div>
<p>How can we implement the proof of the policy improvement theorem into an algorithm? At each iteration step for at least one state we have to find an action that would create a higher value. If we find such an action we create a new policy <img class="math" src="../../_images/math/ac7faf0a0b7000fb64cbed3b6ce2c2a8f7930a0d.svg" alt="\mu’"/> that always takes the new action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> at state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/>. The question still remains: how do you find such an action? The simplest strategy would be to look at all the actions at state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and choose the one that generates the highest value. The approach is undertaken for all states <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/>.</p>
<div class="math">
<p><img src="../../_images/math/4e41e23d4a658722c2297c9810f1eb9e58393f62.svg" alt="\mu'(s) = \arg\max_a q_{\mu}(s, a)"/></p>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Greedy means acting short-sighted by maximizing the short term gain.</p>
</div>
<p>By creating <img class="math" src="../../_images/math/28c5045d9588b9bfe005b156f73242676055a41b.svg" alt="\mu'"/> we create a so-called greedy policy, but acting greedily means acting according to the policy improvement theorem, which guarantees an overall better policy.</p>
<p>Once the new policy and the old policy are exactly the same then we have reached the optimal policy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_improvement</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>

    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">A</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
                <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done</span><span class="p">))</span>


    <span class="n">policy_mapping</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">policy_mapping</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">policy</span>
</pre></div>
</div>
</div>
<div class="section" id="solving-an-mdp">
<h2>Solving An MDP<a class="headerlink" href="#solving-an-mdp" title="Permalink to this headline">¶</a></h2>
<p>The idea of policy iteration is to alternate between policy evaluation and policy improvement until the optimal policy has been reached.</p>
<div class="math">
<p><img src="../../_images/math/c9ee88a5249ffa937d71fccbb4af31f5d68cd141.svg" alt="\begin{algorithm}[H]
    \caption{Policy Improvement}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: model $p$, state set $\mathcal{S}$, action set $\mathcal{A}$, stop criterion $\theta &gt; 0$, discount factor $\gamma$
    \STATE Initialize:
    \STATE $V(s)$ and $V_{old}(s)$, for all $s \in \mathcal{S}$ with zeros
    \STATE $\mu(s) \in \mathcal{A}(s)$ randomly
    \STATE Policy Iteration
    \REPEAT
        \STATE Policy Evaluation
        \REPEAT
            \STATE $\Delta \leftarrow 0$
            \STATE $V_{old}(s) = V(s)$ for all $s \in \mathcal{S}$
            \FORALL{$s \in \mathcal{S}$}
                \STATE $V(s) \leftarrow \sum_{s', r}p(s', r \mid s, \mu(a))[r + \gamma V_{old}(s')]$
                \STATE $\Delta \leftarrow \max(\Delta,|V_{old}(s) - V(s)|)$
            \ENDFOR
        \UNTIL{$\Delta &lt; \theta$}
        \STATE Policy Improvement
        \STATE policy-stable $\leftarrow$ true
        \FORALL{$s \in \mathcal{S}$}
            \STATE old-action $\leftarrow \mu(s)$
            \STATE $\mu(s) \leftarrow \arg\max_a \sum_{s', r}p(s', r \mid s, \mu(a))[r + \gamma V_{old}(s')]$
            \IF{old-action $\neq \mu(s)$}
                \STATE policy-stable $\leftarrow$ false
            \ENDIF
        \ENDFOR
    \UNTIL policy-stable
\end{algorithmic}
\end{algorithm}"/></p>
</div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_policy</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
    <span class="n">policy_mapping</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">policy_mapping</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policies_equal</span><span class="p">(</span><span class="n">policy_1</span><span class="p">,</span> <span class="n">policy_2</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
    <span class="n">equal</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">policy_1</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">!=</span> <span class="n">policy_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
            <span class="n">equal</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">equal</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">random_policy</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">greedy_policy</span> <span class="o">=</span> <span class="n">policy_improvement</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">policies_equal</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">greedy_policy</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
            <span class="k">break</span>

        <span class="n">policy</span> <span class="o">=</span> <span class="n">greedy_policy</span>

    <span class="k">return</span> <span class="n">policy</span>
</pre></div>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="value_iteration.html" title="next page">Value Iteration</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>