
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Policy Iteration &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Value Iteration" href="value_iteration.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-evaluation">
   Policy Evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theory">
     Theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm">
     Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-improvement">
   Policy Improvement
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solving-an-mdp">
   Solving An MDP
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Implementation
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="policy-iteration">
<h1>Policy Iteration<a class="headerlink" href="#policy-iteration" title="Permalink to this headline">¶</a></h1>
<p>The policy iteration algorithm is an iterative method. Iterative methods start with initial (usually random or 0) values as approximations and improve the subsequent approximations with each iteration using the previous approximations as input.</p>
<div class="figure align-center" id="id6">
<img alt="../../_images/iterative_algorithm.svg" src="../../_images/iterative_algorithm.svg" /><p class="caption"><span class="caption-text">Iterative Algorithm.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>The above image shows the general idea of an iterative algorithm. The blue box in the top left corner is the initial value. To arrive at the value for the next box, the value of the previous box is used as input. Each of the boxes gets closer and closer to the desired value, as indicated by the yellow box at the bottom right corner.</p>
<p>The policy iteration algorithm consists of two steps.</p>
<ul class="simple">
<li><p>The policy evaluation step calculates the value function for a given policy.</p></li>
<li><p>The policy improvement step improves the given policy based on the calculated value function.</p></li>
</ul>
<p>Both steps run after each other to form the policy iteration algorithm.</p>
<div class="figure align-center" id="id7">
<img alt="../../_images/evaluation_improvement.svg" src="../../_images/evaluation_improvement.svg" /><p class="caption"><span class="caption-text">Policy Iteration.</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>The algorithm starts with the definition of an arbitrary policy (blue box in the top left corner). The policy can be random or it can be defined by the user. In both cases the algorithm will find an optimal policy, yet the closer the starting policy is to the optimal policy, the faster policy iteration will converge. For the given policy a policy evaluation step is applied, meaning that we find the value function (first red box) for that policy. Policy evaluation requires many steps to find the correct value function for a policy, as indicated by a wiggly line. The policy improvement step takes the calculated value function to improve the previous policy. Unlike policy evaluation, policy improvement takes only one step, as indicated by a direct line. Each policy evaluation and policy improvement step gets closer to the optimal policy/value function as indicated by the growing yellow box.</p>
<div class="section" id="policy-evaluation">
<h2>Policy Evaluation<a class="headerlink" href="#policy-evaluation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="theory">
<h3>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h3>
<p>The goal of policy evaluation is to find the true value function <img class="math" src="../../_images/math/fca07cf65356d5ea39189ade564697a31977f3be.svg" alt="v_{\pi}"/> of the policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/>.</p>
<div class="math">
<p><img src="../../_images/math/12697051e8f3cf7c447d156cf7afb5d51d6c9411.svg" alt="\begin{align*}
v_{\pi}(s) &amp; \doteq \mathbb{E}_{\pi}[G_t \mid S_t = s] \\
&amp; = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
&amp; = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s] \\
&amp; = \sum_{a}\pi(a \mid s)\sum_{s', r}p(s', r \mid s, a)[r + \gamma v_{\pi}(s')]
\end{align*}"/></p>
</div><p>When we have a deterministic policy the expression is slightly easier.</p>
<div class="math">
<p><img src="../../_images/math/97a2485bf7c6c4668c0d22c8d2fe9870c745e732.svg" alt="v_{\pi}(s) \doteq \sum_{s', r}p(s', r \mid s, \mu(s))[r + \gamma v_{\pi}(s')]"/></p>
</div><p>When we start the policy evaluation algorithm, the first step is to create a lookup table for the value of each state. The initial values are set either randomly or to zero (terminal states should be set to 0). When we start to use the above equation we will not surprisingly discover that the random/zero lookup table (the left side of the above equation) and the expected value of the reward plus value for the next state (the right side of the above equation) will diverge quite a lot. The goal of the policy evaluation algorithm is to make the left side of the equation and the right side of the equation be exactly equal. That is done in an iterative process where at each step the difference between both sides is reduced. In practice we do not expect the difference between the two to go all the way down to zero. Instead we define a threshold value. For example a threshold value of 0.0001 indicates that we can interrupt the iterative process as soon as for all of the states the difference between the left and the right side of the equation is below the value.</p>
<p>How exactly does the iterative process work? The policy evaluation algorithm uses the mathematical definition of the value function and turns it into an iterative algorithm.</p>
<div class="math">
<p><img src="../../_images/math/4fc81de30f52390b61469c94ad75583f7ea9e15c.svg" alt="v_{k+1}(s) \doteq \sum_{a}\pi(a \mid s)\sum_{s', r}p(s', r \mid s, a)[r + \gamma v_{k}(s')]"/></p>
</div><p>At each iteration step <img class="math" src="../../_images/math/133a254ed54e4805d831f1886d0f4e4041e27ca4.svg" alt="k+1"/> the left side of the equation is updated by using the state values from the previous iteration and the model of the MDP. At this point it should become apparent why the Bellman equation is useful. Only the reward from the next time step is required to improve the approximation, because all subsequent rewards are already condensed into the value function from the next time step. That allows the algorithm to use the model to look only one step into the future for the reward and use the approximated value function for the next time step. By repeating the update step over and over again the rewards are getting embedded into the value function and the approximation gets better and better. It can be shown mathematically that if the update step is repeated an unlimited number of times, the approximate value function approaches the true value function of the policy. In practice the improvement is done as long as the value function between two iterations is large enough.</p>
<div class="figure align-center" id="id8">
<img alt="../../_images/policy_evaluation.svg" src="../../_images/policy_evaluation.svg" /><p class="caption"><span class="caption-text">Policy Evaluation.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<p>The above image shows the interdependencies between the values at different states. The large arrows indicate the policy for which we want to calculate the value function. The smaller blue arrows show from what states the rewards and values are needed to calculate the update of the value function. For example in the top left corner (the initial state) the agent deterministically chooses the action to move right. There is a chance to stay in the same state, to move right or to move bottom with 33.3% probability. Consequently the rewards and values of those states are required for the update. If you look carefully you can detect different paths that lead all the way from the bottom right corner (the goal state) to the top left corner. These are the paths that propagate the reward of 1 to the starting state.</p>
</div>
<div class="section" id="algorithm">
<h3>Algorithm<a class="headerlink" href="#algorithm" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><img src="../../_images/math/98934f0695669a4a36b301583a5ace72fa61e690.svg" alt="\begin{algorithm}[H]
    \caption{Iterative Policy Evaluation (Deterministic Case)}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: policy $\mu$, model $p$, state set $\mathcal{S}$, stop criterion $\theta &gt; 0$, discount factor $\gamma$
    \STATE Initialize: $V(s)$ and $V_{old}(s)$, for all $s \in \mathcal{S}$ with zeros
    \REPEAT
        \STATE $\Delta \leftarrow 0$
        \STATE $V_{old}(s) = V(s)$ for all $s \in \mathcal{S}$
        \FORALL{$s \in \mathcal{S}$}
            \STATE $V(s) \leftarrow \sum_{s', r}p(s', r \mid s, \mu(s))[r + \gamma V_{old}(s')]$
            \STATE $\Delta \leftarrow \max(\Delta,|V_{old}(s) - V(s)|)$
        \ENDFOR
    \UNTIL{$\Delta &lt; \theta$}
    \STATE Output: value function $V(s)$
\end{algorithmic}
\end{algorithm}"/></p>
</div><p>In order to calculate the value function the algorithm needs 5 inputs.</p>
<ul class="simple">
<li><p>The deterministic policy <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/> is a function that gets a state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> as an input and generates an action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> as an output. We are not going to deal with stochastic agents yet, therefore <img class="math" src="../../_images/math/337206c5434d221d9bada59c52ddcb07e3d471fe.svg" alt="\pi(a \mid s) = 1"/> in the update step. <img class="math" src="../../_images/math/a9447568f89c9b36e179357bd8fe22dde9e188f0.svg" alt="a = \mu (s)"/></p></li>
<li><p>The model p will take the current state and action as input and return the next state, the reward, the terminal flag and the corresponding probability. <img class="math" src="../../_images/math/f8ee97f06e8e100314f491a3198fa8bd209209a8.svg" alt="probability, next\_state, reward, is\_terminal = p(s, a)"/></p></li>
<li><p><img class="math" src="../../_images/math/3f98591dff3521c50335f61964d0bab6fb82df24.svg" alt="\mathcal{S}"/> is the state set of the MDP</p></li>
<li><p>Theta <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/>  is the criterion to stop the algorithm once the difference between the old value function <img class="math" src="../../_images/math/40aef5248c1a6b97e9ba7eca2071542ba5068d25.svg" alt="V_{old}"/> and new value function <img class="math" src="../../_images/math/caf39627574b86c3d37285b2a9154f85d78dc6d9.svg" alt="V"/> is less than <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/></p></li>
<li><p>Gamma <img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/> is the discount factor for the update step in the Bellman equation</p></li>
</ul>
<p>We are going to keep two versions of the value function, <img class="math" src="../../_images/math/40aef5248c1a6b97e9ba7eca2071542ba5068d25.svg" alt="V_{old}"/> and <img class="math" src="../../_images/math/caf39627574b86c3d37285b2a9154f85d78dc6d9.svg" alt="V"/>. During an update iteration we move through the states one at a time. In my opinion it is more intuitive to update all the states with old values before updating the value function as a whole. If we used only one value function we would update some of the states with the already updated values and some with old values. Both versions of updates are valid, but the “inplace” version is not going to be used in this chapter.</p>
<p>At each time step we loop through all states and adjust <img class="math" src="../../_images/math/41fc31585b3b13e811d8deb0b700ce4e1d0f2c1e.svg" alt="V(s)"/> for that particular state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> using the Bellman equation. Once the difference between <img class="math" src="../../_images/math/bb93e9fc8859c5aa4c323bc982628aada78ba88d.svg" alt="V_{old}(s)"/> and <img class="math" src="../../_images/math/41fc31585b3b13e811d8deb0b700ce4e1d0f2c1e.svg" alt="V(s)"/> for all <img class="math" src="../../_images/math/be8524ed26a8bccab0eb3e8984c5317d38df658b.svg" alt="s \in S"/> is smaller than <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/> we leave the loop and return the value function.</p>
</div>
<div class="section" id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h3>
<p>The imports consist only of OpenAI Gym and NumPy. Gym for the MDP and NumPy to make calculations more efficient.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
<p>We are going to calculate the value function for the Frozen Lake environment, but the algorithm is general and can be applied to many different MDPs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The policy function contains internally a mapping table that maps states to actions deterministically. This policy corresponds to the policy discussed above and in previous lectures.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="c1">#     LEFT = 0</span>
<span class="c1">#     DOWN = 1</span>
<span class="c1">#     RIGHT = 2</span>
<span class="c1">#     UP = 3</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="mi">4</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">5</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">6</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">7</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">8</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">9</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">10</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">11</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">12</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">13</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">14</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">15</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">mu</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
</pre></div>
</div>
<p>The model returns the list of possible next states, rewards and corresponding probabilities given the current state and action.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
</pre></div>
</div>
<p>The state set <img class="math" src="../../_images/math/3f98591dff3521c50335f61964d0bab6fb82df24.svg" alt="\mathcal{S}"/> and action set is <img class="math" src="../../_images/math/8d026053f0eefe613ccf100261807688868d9c3a.svg" alt="\mathcal{A}"/> are implemented as lists.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
</pre></div>
</div>
<p>The below code covers the actual policy evaluation algorithm.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="c1"># initialize value functions with zeros</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
    <span class="n">V_old</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">V_old</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
            <span class="c1"># we avoid the loop over the actions as the policy is deterministic</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

            <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
                <span class="n">value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V_old</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done</span><span class="p">))</span>
            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># check for stop criterion and break if necessary</span>
        <span class="n">max_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_old</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">max_diff</span> <span class="o">&lt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">V</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="policy-improvement">
<h2>Policy Improvement<a class="headerlink" href="#policy-improvement" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Theory<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Remember what it means to solve an MDP?</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To solve an MDP is to find an optimal policy.</p>
</div>
<p>What do we need to find an optimal policy for a finite MDP?</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To find an optimal policy for a finite MDP the optimal value-function is required.</p>
</div>
<p>The policy evaluation step is a prerequisite to compare two policies and to determine which is better, but an additional step, policy improvement, is required to take an existing policy and to make it better.</p>
<p>There is a line of arguments that has to be made in order to show that the policy improvement step is a valid approach.</p>
<p>Let us assume that we have a policy <img class="math" src="../../_images/math/cd89edb8d677e76ff2422b13ab39ad709e93db2e.svg" alt="\mu(s)"/> and contemplate if instead of following the policy strictly, in the current state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> just once we would like to take a different action <img class="math" src="../../_images/math/70b0f96cf249bacc26d34b1f5438fefccf1bde1e.svg" alt="a \neq \mu(s)"/>. After that action we will stick to the old policy <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/> and follow it until the terminal state <img class="math" src="../../_images/math/573c03b1afb9413354760f358c959354ca9577f0.svg" alt="T"/>. The value of using the action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> at state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and then following the policy <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/> is essentially the definition of the action-value function, which plays a key role in the policy improvement step.</p>
<div class="math">
<p><img src="../../_images/math/e4f911298244abbb55f2c4347e5940fec44f5e45.svg" alt="q_{\mu}(s, a) \doteq \mathbb{E}[R_{t+1} + \gamma v_{\mu}(S_{t+1}) \mid S_t = s, A_t = a]"/></p>
</div><p>What if we compare <img class="math" src="../../_images/math/1fc2df706e9625fa32bbbfea1500a7e783745c0f.svg" alt="v(s)"/> and <img class="math" src="../../_images/math/bbfcb7390eb48572ff6fa3e9ee3aa2955fefef2b.svg" alt="q(s, a)"/> and we find out that taking some action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> and then following <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/> generates an advantage?</p>
<div class="math">
<p><img src="../../_images/math/35bca49d28752145bc050be725dfa690dbc9b175.svg" alt="q_{\mu}(s, a) &gt; v_{\mu}(s)"/></p>
</div><p>Does that suggest that we should always take the action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> when we face the state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and thus adjust the policy to create a new policy <img class="math" src="../../_images/math/28c5045d9588b9bfe005b156f73242676055a41b.svg" alt="\mu'"/> or should we stick to the old policy <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/>? The policy improvement theorem suggests the former.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Policy Improvement theorem</p>
<div class="math">
<p><img src="../../_images/math/34fb16271e67e2ee4509e094b0617aaf25783669.svg" alt="q_{\mu}(s, \mu'(s)) \geq v_{\mu}(s) \Rightarrow v_{\mu'}(s) \geq v_{\mu}(s), \forall s \in \mathcal{S}"/></p>
</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Policy Improvement Theorem Proof</p>
<div class="math">
<p><img src="../../_images/math/2fae920e2be953d907fbc3743e8e070a85c5d072.svg" alt="\begin{align*}
v_{\mu}(s) &amp; \leq q_{\mu}(s, \mu'(s)) \\
&amp; = \mathbb{E}[R_{t+1} + \gamma v_{\mu}(S_{t+1}) \mid S_t = s, A_t = \mu'(s)] \\
&amp; \leq \mathbb{E}[R_{t+1} + \gamma q_{\mu}(S_{t+1}, \mu'(S_{t+1})) \mid S_t = s, A_t = \mu'(s)] \\
&amp; = \mathbb{E}[R_{t+1} + \gamma \mathbb{E}[R_{t+2} + \gamma v_{\mu}(S_{t+2}) \mid S_{t+1}, A_{t+1} = \mu' (S_{t+1})] \mid S_t = s, A_t = \mu'(s)] \\
&amp; \vdots \\
&amp; \leq \mathbb{E}_{\mu'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \mid S_t = s] \\
&amp; = v_{\mu'}(s)
\end{align*}"/></p>
</div></div>
<p>How can we implement the proof of the policy improvement theorem into an algorithm? At each iteration step for at least one state we have to find an action that would create a higher value. If we find such an action we create a new policy <img class="math" src="../../_images/math/ac7faf0a0b7000fb64cbed3b6ce2c2a8f7930a0d.svg" alt="\mu’"/> that always takes the new action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> at state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/>. The question still remains: how do you find such an action? The simplest strategy would be to look at all the actions at state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and choose the one that generates the highest value. The approach is undertaken for all states <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/>.</p>
<div class="math">
<p><img src="../../_images/math/4e41e23d4a658722c2297c9810f1eb9e58393f62.svg" alt="\mu'(s) = \arg\max_a q_{\mu}(s, a)"/></p>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Greedy means acting short-sighted by maximizing the short term gain.</p>
</div>
<p>By creating <img class="math" src="../../_images/math/28c5045d9588b9bfe005b156f73242676055a41b.svg" alt="\mu'"/> we create a so-called greedy policy, but acting greedily means acting according to the policy improvement theorem, which guarantees an overall better policy.</p>
<div class="figure align-center" id="id9">
<img alt="../../_images/policy_improvement.svg" src="../../_images/policy_improvement.svg" /><p class="caption"><span class="caption-text">Policy Improvement With Greedy Action-Selection.</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>The above image shows how the action-value looks like for the policy used throughout the chapter. The green lines show the current deterministic actions based on the state. The red arrows show the new policy based on greedy action-selection.</p>
</div>
<div class="section" id="id2">
<h3>Implementation<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>The policy improvement algorithm with greedy action selection can be constructed with one single function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_improvement</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>

    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">A</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
                <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="ow">not</span> <span class="n">done</span><span class="p">))</span>


    <span class="n">policy_mapping</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">policy_mapping</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">policy</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="solving-an-mdp">
<h2>Solving An MDP<a class="headerlink" href="#solving-an-mdp" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>Theory<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>The idea of policy iteration is to alternate between policy evaluation and policy improvement until the optimal policy has been reached. Once the new policy and the old policy are exactly the same then we have reached the optimal policy.</p>
</div>
<div class="section" id="id4">
<h3>Algorithm<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><img src="../../_images/math/824eb6d4b56a2a4be08ed693a4c0049bbbd32157.svg" alt="\begin{algorithm}[H]
    \caption{Policy Iteration (Deterministic Policy)}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: model $p$, state set $\mathcal{S}$, action set $\mathcal{A}$, stop criterion $\theta &gt; 0$, discount factor $\gamma$
    \STATE Initialize:
    \STATE $V(s)$ and $V_{old}(s)$, for all $s \in \mathcal{S}$ with zeros
    \STATE $\mu(s) \in \mathcal{A}(s)$ randomly
    \STATE Policy Iteration
    \REPEAT
        \STATE Policy Evaluation
        \REPEAT
            \STATE $\Delta \leftarrow 0$
            \STATE $V_{old}(s) = V(s)$ for all $s \in \mathcal{S}$
            \FORALL{$s \in \mathcal{S}$}
                \STATE $V(s) \leftarrow \sum_{s', r}p(s', r \mid s, \mu(a))[r + \gamma V_{old}(s')]$
                \STATE $\Delta \leftarrow \max(\Delta,|V_{old}(s) - V(s)|)$
            \ENDFOR
        \UNTIL{$\Delta &lt; \theta$}
        \STATE Policy Improvement
        \STATE policy-stable $\leftarrow$ true
        \FORALL{$s \in \mathcal{S}$}
            \STATE old-action $\leftarrow \mu(s)$
            \STATE $\mu(s) \leftarrow \arg\max_a \sum_{s', r}p(s', r \mid s, a)[r + \gamma V(s')]$
            \IF{old-action $\neq \mu(s)$}
                \STATE policy-stable $\leftarrow$ false
            \ENDIF
        \ENDFOR
    \UNTIL policy-stable
    \STATE Output: policy function $\mu(s)$, value function $V(s)$
\end{algorithmic}
\end{algorithm}"/></p>
</div><p>The policy iteration algorithm alternates between policy evaluation and policy improvement. The algorithm continuous until a stable policy is reached.</p>
</div>
<div class="section" id="id5">
<h3>Implementation<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>We will start with a random policy and still arrive at an optimal policy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_policy</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
    <span class="n">policy_mapping</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">policy_mapping</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</pre></div>
</div>
<p>The below function compares if two policies are equal.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policies_equal</span><span class="p">(</span><span class="n">policy_1</span><span class="p">,</span> <span class="n">policy_2</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
    <span class="n">equal</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">policy_1</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">!=</span> <span class="n">policy_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
            <span class="n">equal</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">equal</span>
</pre></div>
</div>
<p>The policy iteration algorithm calls policy_evaluation and policy_improvement functions and determines if the old and the new policy are equal. Once they are the iterative process is stopped and the loop is broken.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">random_policy</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">greedy_policy</span> <span class="o">=</span> <span class="n">policy_improvement</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">policies_equal</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">greedy_policy</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
            <span class="k">break</span>

        <span class="n">policy</span> <span class="o">=</span> <span class="n">greedy_policy</span>

    <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">V</span>
</pre></div>
</div>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="value_iteration.html" title="next page">Value Iteration</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>