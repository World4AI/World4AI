
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bandits &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Epsilon()-Greedy" href="epsilon_greedy.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mathematics of Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../math/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/on_policy_approximation.html">
   On Policy Approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/off_policy_approximation.html">
   Off Policy Approximation
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approximation-of-the-values">
   Approximation Of The Values
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computational-efficiency">
   Computational Efficiency
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-of-bandits-in-openai-gym">
   Implementation of Bandits in OpenAI Gym
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="bandits">
<h1>Bandits<a class="headerlink" href="#bandits" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Usually the mathematical introduction into the topic of exploration vs exploitation is done by the means of so-called k-armed bandits. Unlike the full reinforcement learning problem in a bandits setting we face the same single state over and over again and have to choose one of the k actions. Once the action is chosen the agent receives the reward and the state is reset again.</p>
<p>The name k-armed bandits comes from the casino game “One Armed Bandit”. In such a game the player has to push a single lever and after each push he either receives a reward or not with a certain probability. The outcome of the next pull does not depend on past actions. In k-armed bandits there are k levers. For each of the levers there is a corresponding probability for a certain reward. The idea is to find the lever that maximizes the expected reward.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike in an MDP in a bandit problem the action value function does not depend on the state.</p>
<div class="math">
<p><img src="../../_images/math/635c36cd8705747fc87f9b4e9cb2650506175e4e.svg" alt="q_*(a) \doteq \mathbb{E}[R_t \mid A_t = a]"/></p>
</div></div>
<p>At the very first glance the solutions to k-armed bandits problems seem to be of limited use when faced with full reinforcement learning MDP tasks, but there is a particular reason to still cover the topic. We remember that reinforcement learning is learning through trial and error and delayed reward. Bandits basically remove the “delayed” part of the definition and deal only with trial and error and reward, as the reward is received for each of the actions directly and the environment is reset to the single not terminal state. That reduces complexity by not caring about the sequential problems but still allows us to discover different types of exploration-exploitation techniques, which eventually can be applied to full reinforcement learning tasks. We can say that bandits are a special case of an MDP.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To select the optimal action is to select the action with the highest value.</p>
<div class="math">
<p><img src="../../_images/math/eccab1dcf856cbb342da1da7b55fe05c83ce651e.svg" alt="A_t \doteq \arg\max_a q_*(a)"/></p>
</div></div>
<p>If you possess the true value function, then the solution is as simple as selecting the action with the highest value. There is of course always the option to get the model directly to calculate the expected reward for each of the actions, but this chapter is the precursor to full reinforcement learning solutions, therefore we will use interaction to <em>approximate</em> the value function.</p>
</div>
<div class="section" id="approximation-of-the-values">
<h2>Approximation Of The Values<a class="headerlink" href="#approximation-of-the-values" title="Permalink to this headline">¶</a></h2>
<p>The intuitive way to estimate the action-value function is to select certain actions and build averages of the rewards. If you continue doing that, the approximation is going to get better and better, meaning it is going to get closer to the true action-value function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The approximate action-values are calculated by dividing the sum of rewards which  occurred when the action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> was taken by the number of times the action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> was taken.</p>
<div class="math">
<p><img src="../../_images/math/f415f9625e8de301418d2ec70360a4cc2a625cdc.svg" alt="Q_t(a) \doteq \frac{\sum_{t-1}^{i=1} R_i \cdot 1_{A_i=a}}{\sum_{t-1}^{i=1} 1_{A_i=a}}"/></p>
</div></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Exploitation is the selection of the greedy action, the action with the highest value, based on the current estimation of the action-value function.</p>
<p>Exploration is the selection of the non greedy action based on the current estimation of the action-value function.</p>
</div>
<p>When estimating the value function using the above approach it is of course entirely possible that the value of a certain action appears only larger due to an approximation error while the actual true greedy action was not selected enough in the past to approach the true value. The problem gives rise to the necessity to balance exploration and exploitation. In the next sections we are going to discuss several techniques that are used to deal with the exploration-exploitation tradeoff. For the rest of this chapter we will cover techniques that make the computation of the approximation of the action-value function more efficient.</p>
</div>
<div class="section" id="computational-efficiency">
<h2>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Permalink to this headline">¶</a></h2>
<p>If we follow the above definition of the calculation of the estimate for the action-value function then the natural approach would be to keep a list of all the actions taken and all the rewards received. The higher the number of actions taken the larger the list gets and the higher the memory requirements to keep the elements in memory and the higher the computational cost to loop through all the elements in the list to calculate the average.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The average at time step n can be expressed in terms of the average at time step n-1 and the newest element.</p>
<div class="math">
<p><img src="../../_images/math/c50f4ff7aa52576ca0cc8d194df0443387e348ee.svg" alt="\begin{align*}
\mu_n &amp; = \frac{1}{n} \sum_{i=1}^{n}x_i \\
&amp; = \frac{1}{n}(x_n + \sum_{i=1}^{n-1}x_i) \\
&amp; = \frac{1}{n}(x_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1}x_i) \\
&amp; = \frac{1}{n}(x_n + (n-1) \mu_{n-1}) \\
&amp; = \frac{1}{n}\mu_{n-1} + \frac{1}{n} n \mu_{n-1} - \frac{1}{n}x_n \\
&amp; = \mu_{n-1} + \frac{1}{n}(x_n - \mu_{n-1})
\end{align*}"/></p>
</div></div>
<p>Using the expression above we can rewrite the calculation of the action-value for a particular action as follows.</p>
<div class="math">
<p><img src="../../_images/math/3bb495032c8ee2f481ded2843e3cb61345c8bc40.svg" alt="Q_{n+1} = Q_n  + \frac{1}{n}[R_n - Q_n]"/></p>
</div><p>The update rule requires only the last estimate of the action-value for a state a and the last reward received, no lists are necessary to track the average. More generally speaking the update can be described as follows.</p>
<div class="math">
<p><img src="../../_images/math/f93c617f8ad4815396f93dd760ff4bb5b08304b1.svg" alt="NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]"/></p>
</div><p>The target indicates the <em>true</em> value that was experienced and the idea is to push the new estimate towards the <em>true</em> value. The expression <em>Target - Estimate</em> is often described as <em>error</em>, as it is the difference between the true reward received after taking an action and the estimated reward. The <em>step size</em>, also called the learning rate, is the magnitude with which you change the estimate of the action-value function. From the expression above it follows that the <em>step size</em> is 1/n, which decreases with experience.  In practice it is customary to either set the learning rate to a constant value or start with a relatively high value and reduce it until it reaches a minimum constant value.</p>
<div class="math">
<p><img src="../../_images/math/5b22f4784675f4736076e71dc5749e0decc1d07f.svg" alt="Q_{n+1} = Q_n  + \alpha[R_n - Q_n]"/></p>
</div><p>The greek letter <img class="math" src="../../_images/math/c79ec9e86f4f2b22c56742ffd242fc41b8af040e.svg" alt="\alpha"/> is often used for the learning rate.</p>
</div>
<div class="section" id="implementation-of-bandits-in-openai-gym">
<h2>Implementation of Bandits in OpenAI Gym<a class="headerlink" href="#implementation-of-bandits-in-openai-gym" title="Permalink to this headline">¶</a></h2>
<p>The Bandit class takes two lists to initialize the environment. The first list contains the probabilities between 0 and 1 that indicate with what probability the agent receives a reward when the action is taken that corresponds with the position in the list. The second list contains the value of the reward that is given in case of success. The reward of 0 is given in the case of no success.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">spaces</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># for a full implementation of bandits in gym look for the github repo by JKCooper2</span>
<span class="c1"># https://github.com/JKCooper2/gym-bandits</span>
<span class="c1"># this implementation is a simplified version</span>

<span class="k">class</span> <span class="nc">Bandit</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="p">[],</span> <span class="n">rewards</span><span class="o">=</span><span class="p">[]):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Probability list and reward list must be of equal length&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span>
        <span class="c1"># k as in k-armed bandits, number of arms</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">probs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">probs</span><span class="p">[</span><span class="n">action</span><span class="p">]:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div>
</div>
<p>The below example shows an environment that allows 2 actions. The first action gives with a probability of 50% a reward of 10 and the second action gives a reward of 1 with a probability of 100%.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">Bandit</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">rewards</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="epsilon_greedy.html" title="next page">Epsilon(<img class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg" alt="\epsilon"/>)-Greedy</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>