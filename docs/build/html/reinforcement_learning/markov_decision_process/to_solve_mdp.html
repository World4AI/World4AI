
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>To Solve an MDP &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Introduction" href="../dynamic_programming/introduction.html" />
    <link rel="prev" title="Definition of a Markov Decision Process" href="definition_markov_decision_process.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#return">
   Return
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy">
   Policy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#value-functions">
   Value Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bellman-equations">
   Bellman Equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimality">
   Optimality
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bellman-optimality-equations">
   Bellman Optimality Equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="to-solve-an-mdp">
<h1>To Solve an MDP<a class="headerlink" href="#to-solve-an-mdp" title="Permalink to this headline">¶</a></h1>
<p>Once a particular MDP, the components of the tuple <img class="math" src="../../_images/math/2bad0a556ccdc9fbb4a2d612b078fbc76b9de743.svg" alt="(\mathcal{S, A}, P, R, \gamma)"/>,  has been defined the next logical step would be to solve the MDP. In this chapter I am going to explain what it actually means to solve an MDP. In future chapters I will try to explore possible approaches to a solution.</p>
<div class="section" id="return">
<h2>Return<a class="headerlink" href="#return" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Episodic Tasks</strong>:</p>
<p>In episodic tasks the return is the sum of rewards in a single episode starting from time step <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> and going up to the terminal time step <img class="math" src="../../_images/math/573c03b1afb9413354760f358c959354ca9577f0.svg" alt="T"/>.</p>
<p><img class="math" src="../../_images/math/3dca8a943374d3fd1a31ca868ca9244009662952.svg" alt="G_t = R_{t+1} + R_{t+2} + … + R_T"/></p>
<p><strong>Continuing Tasks</strong>:</p>
<p>In continuing tasks the return is the sum of rewards starting at time step t and going to possibly infinity, <img class="math" src="../../_images/math/3cc526b72c8187aa8469062697490fd8aff88799.svg" alt="T = \infty"/>.</p>
<p><img class="math" src="../../_images/math/092b11737003a7c9f4383d0c3fd518a082f7eadf.svg" alt="G_t = R_{t+1} + R_{t+2} + R_{t+3} + …  = \sum_{k=0}^\infty{R_{k+t+1}}"/></p>
</div>
<p>In order to simplify notation I will introduce the notion of a return <img class="math" src="../../_images/math/7674515a21e3ea004ba30a13c5d826d6a407810d.svg" alt="G"/>. A return is simply the sum of rewards starting from some time <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> and going either to some terminal state <img class="math" src="../../_images/math/573c03b1afb9413354760f358c959354ca9577f0.svg" alt="T"/> or to infinity. The letter <img class="math" src="../../_images/math/7674515a21e3ea004ba30a13c5d826d6a407810d.svg" alt="G"/> stands for “Goal”, because the goal of the environment is encoded in the rewards.</p>
<div class="figure align-center" id="id1">
<img alt="../../_images/frozen_lake_return.svg" src="../../_images/frozen_lake_return.svg" /><p class="caption"><span class="caption-text">Frozen Lake Return.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>In order to calculate the full return of an episode we have to play through the sequence of states, actions and rewards all the way through from the initial state to the terminal state. Let us assume that the sequence of states and the corresponding rewards played out as indicated in the image above. The agent received the reward of 0 for getting to the non terminal states and a reward of 1 for getting to the terminal state at the bottom right corner. Altogether there were 9 time steps starting from the initial time step t = 0 and ending with the timestep T = 8.</p>
<div class="math">
<p><img src="../../_images/math/9f6cca9c30f8241107b878b3cb8ae412bb36a66b.svg" alt="\begin{align*}
G_0 &amp; = R_1 + R_2 + R_3 + R_4 + R_5 + R_6 + R_7 + R_8 \\
&amp; = 0 + 0 + 0 + 0 + 0 + 0 + 0 + 1 = 1
\end{align*}"/></p>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Discounted Return</p>
<div class="math">
<p><img src="../../_images/math/435bef738adcdc6bc8326e0d9ef8a22e2d74c6e7.svg" alt="G_t = R_{t+1} + \gamma{R_{t+2}} + \gamma^2{R_{t+3}} + …  = \sum_{k=0}^\infty{\gamma^k{R_{k+t+1}}}"/></p>
</div></div>
<p>To avoid an infinite return (in continuing tasks), future rewards  are discounted by <img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/>. Episodic tasks use discounting to emphasize the time value of rewards. Looking at the same example from above the return <img class="math" src="../../_images/math/fc1028873348e8cbf12e886652684ff245017ebe.svg" alt="G_0"/> looks as follows when we assume a gamma <img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/> of <em>0.9</em>.</p>
<div class="math">
<p><img src="../../_images/math/27ae44cfb4a638058b883477186d567155082128.svg" alt="\begin{align*}
G_0 &amp; = R_1 + 0.9R_2 + 0.9^2 R_3 + 0.9^3 R_4 + 0.9^4 R_5 + 0.9^5 R_6 + 0.9^6 R_7 + 0.9^7 R_8 \\
&amp; = 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0.48 = 0.48
\end{align*}"/></p>
</div><p>In the example without discounting the return would always be 1 as long as the agent reaches the goal state. That means that it is generally not important how many steps the agent takes. This is not the case with discounting. Taking the shorter route corresponds to a higher expected return, as the time value of a reward is reduced the farther away it is from the current time step <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/>. That essentially means that in the above example discounting generally encourages the agent to take as few steps as possible.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The goal of the agent is to maximize the expected (discounted) sum of rewards. In other words the goal of the agent is to maximize the expected return <img class="math" src="../../_images/math/92f197498d79ecf351250c0b5d929e1d82cdf600.svg" alt="\mathbb{E}(G_t)"/>.</p>
</div>
<p>To solve an MDP means for the agent to maximize the expected return.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math">
<p><img src="../../_images/math/a086d9dc62f7236e105f63cff3de4112405a03eb.svg" alt="\begin{align*}
G_t &amp; = R_{t+1} + \gamma{R_{t+2}} + \gamma^2{R_{t+3}} + … \\
&amp; = R_{t+1} + \gamma{(R_{t+2} + \gamma{R_{t+3}} + ...)} \\
&amp; = R_{t+1} + \gamma{G_{t+1}}
\end{align*}"/></p>
</div></div>
<p>An important property of returns is that they can be expressed in terms of future returns.</p>
<p>Using this property we get the same result as above.</p>
<div class="math">
<p><img src="../../_images/math/fcd3a611403d8499633691a93b0a2eeb520f4dca.svg" alt="\begin{align*}
&amp; G_0 = R_1 + \gamma G_1 \\
&amp; R_1 = 0 \\
&amp; G_1 = R_2 + 0.9 R_3 + 0.9^2 R_4 + 0.9^3 R_5 + 0.9^4 R_6 + 0.9^5 R_7 + 0.9^6 R_8 = 0.53 \\
&amp; G_0 = 0 + 0.9 * 0.53 = 0.48
\end{align*}"/></p>
</div></div>
<div class="section" id="policy">
<h2>Policy<a class="headerlink" href="#policy" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A policy can be defined in several ways.</p>
<p>If a policy is <em>deterministic</em> then it is usually the case that we define policy as a mapping from state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> to action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/>. In that case the notation that we use for policy is <img class="math" src="../../_images/math/cd89edb8d677e76ff2422b13ab39ad709e93db2e.svg" alt="\mu(s)"/>. To generate an action <img class="math" src="../../_images/math/7f3546078d71a3138f4636f7f7359e290f4ca71c.svg" alt="A_t"/> at timestep <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> we input the state <img class="math" src="../../_images/math/edea1c48e2215a6477d60bb7cb19ff85b7474f4c.svg" alt="S_t"/> into the policy function:  <img class="math" src="../../_images/math/c9444899513c3a616d89eeeca7df9c4db09daae3.svg" alt="A_t = \mu(S_t)"/>.</p>
<p>If a policy is <em>stochastic</em> then we define policy as a mapping from a state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> to a probability of an action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> and the mathematical notation is <img class="math" src="../../_images/math/65e0b286ef9b523786c3bde6dd3ca6b8d22697e2.svg" alt="\pi{(a \mid s)} = Pr[A_t = a \mid S_t = s]"/>. The “probability” notation can also be used in a deterministic case. For a deterministic policy <img class="math" src="../../_images/math/49a4c2bd9a832f216b21737ebfc9345a7ce1ad81.svg" alt="\pi{(a \mid s) = 1}"/> for for the selected action and <img class="math" src="../../_images/math/7fbc94d05334c9ed42513cd1dd5442955736c26d.svg" alt="\pi{(a \mid s) = 0}"/> for the rest of the actions.</p>
<p>To generate an action we consider <img class="math" src="../../_images/math/5f5a3fca1b0942506c9e168bd0c70768e57dd731.svg" alt="\pi{(. \mid S_t)}"/> to be the distribution of actions given states. Actions are draws from a policy distribution <img class="math" src="../../_images/math/64cac9897dc1ed3542ee295b1865f198074b2b08.svg" alt="A_t \sim \pi{(. \mid S_t)}"/>, where in a deterministic case the same action is always drawn given the same state.</p>
</div>
<p>The policy of an agent determines the behaviour of the agent expressed in terms of actions based on the current state of the environment.</p>
<div class="figure align-center" id="id2">
<img alt="../../_images/frozen_lake_policy.svg" src="../../_images/frozen_lake_policy.svg" /><p class="caption"><span class="caption-text">Frozen Lake Policy.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>The above image shows the actions that would be generated by a deterministic policy based on the 15 distinct states.</p>
<table class="table" id="id3">
<caption><span class="caption-text">Policy for the frozen lake</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>State <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/67a6a5377794a4e8a68eb432c89e4dfad58d6309.svg" alt="\pi(left \mid s)"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/b21cc62056cecf73ae4aa781baddac060feb0ecd.svg" alt="\pi(top \mid s)"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/4da6d2fab3b50816df11a647d8c3a5903b1312f1.svg" alt="\pi(right \mid s)"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/661cb6aba385ffbbc6be9b474c2455600fcefc41.svg" alt="\pi(bottom \mid s)"/></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>The table is the policy that corresponds to the image above.</p>
</div>
<div class="section" id="value-functions">
<h2>Value Functions<a class="headerlink" href="#value-functions" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>State-Value Function:</em> <img class="math" src="../../_images/math/d7ac52f3990f07809397f192f6cc45a41731f99d.svg" alt="v_{\pi}(s) = \mathbb{E_{\pi}}[G_t \mid S_t = s]"/></p>
<p><em>Action-Value Function:</em> <img class="math" src="../../_images/math/88cfe0c044daee0af6423189b00a4757fd1d5730.svg" alt="q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_t \mid S_t = s, A_t = a]"/></p>
</div>
<p>Value functions map states or state-action pairs to “goodness” values, where goodness is expressed as the expected sum of rewards. Higher values mean more favorable states or state-action pairs.</p>
<p>The state-value function expresses the expected return when following a particular policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> given the state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/>. The action-value function expresses the expected return given the state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> while taking the action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> in the current step and following the policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> afterwards.</p>
<p>To make the definitions clear we are going to use the example from below and calculate the value-functions for the policy as it is indicated by the arrows.</p>
<div class="figure align-center" id="id4">
<img alt="../../_images/frozen_lake_policy.svg" src="../../_images/frozen_lake_policy.svg" /><p class="caption"><span class="caption-text">Frozen Lake Policy.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>The numbers in the cells in the image below show the state-values for the corresponding state when the agent follows the given policy. The left top corner contains the number 0.04. This means that if the agent follows this policy the expected sum of discounted rewards is 0.04. The discount factor that was used in the calculation is 0.99. Some of the numbers are noteworthy. For example the values in terminal states are 0. This is due to the fact that after the terminal state the agent can not expect any additional rewards. Furthermore it seems that generally speaking in the Frozen Lake environment the expected return is larger the closer the agent is to the terminal state in the bottom right corner.</p>
<div class="figure align-center" id="id5">
<img alt="../../_images/frozen_lake_state_value.svg" src="../../_images/frozen_lake_state_value.svg" /><p class="caption"><span class="caption-text">Frozen Lake State-Value Function.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>The numbers in the image below show the action-values for the policy given above. For example in the top left corner in state 0 if the agent goes north and then follows the policy from the above image then the expected sum of rewards is 0.03.</p>
<div class="figure align-center" id="id6">
<img alt="../../_images/frozen_lake_action_value.svg" src="../../_images/frozen_lake_action_value.svg" /><p class="caption"><span class="caption-text">Frozen Lake Action-Value Function.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>The green arrows indicate the current policy. The red arrows indicate actions that have higher action-values than the current policy. Generally speaking there are a lot of states where it would be more advantageous for the agent to choose a different direction. We will see in future chapters that the action-value function will play a major role in finding a policy that is better than the current policy.</p>
</div>
<div class="section" id="bellman-equations">
<h2>Bellman Equations<a class="headerlink" href="#bellman-equations" title="Permalink to this headline">¶</a></h2>
<p>By using the properties of returns <img class="math" src="../../_images/math/fa309ec25c85e0173ceed1bd8bf5166ee63b704c.svg" alt="G_t"/> where each return can be expressed in terms of future returns <img class="math" src="../../_images/math/646765d5a05e05a0ccbc7b1f82c3772cc27a3fcf.svg" alt="G_t = r_{t+1} + \gamma G_{t+1}"/> we can arrive at recursive equations, where a value of a state can be defined in terms of values of the next state.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Bellman equation for the state-value function</p>
<div class="math">
<p><img src="../../_images/math/7f6a6feb49fe40719f565018c6691630e5d0cb70.svg" alt="\begin{align*}
v_{\pi}(s) &amp; = \mathbb{E_{\pi}}[G_t \mid S_t = s] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s]
\end{align*}"/></p>
</div><p>Bellman equation for the action-value function</p>
<div class="math">
<p><img src="../../_images/math/57c7916fc9b36e42ddc201b48ad47dbc5c774006.svg" alt="\begin{align*}
q_{\pi}(s, a) &amp; = \mathbb{E_{\pi}}[G_t \mid S_t = s, A_t = a] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = a]
\end{align*}"/></p>
</div></div>
<p>Equations of the above form are called Bellman equations, named after the mathematician Richard E. Bellman. At the very first glance it might not seem like the equations add additional benefit to the definition of value functions, but the recursive relationships is what makes many of the reinforcement learning algorithms work.</p>
<p>For example if we want to calculate the value of the state 0 in the top left corner then we need the values from the 0, 1 and 4 states. It might seem slightly strange that you need to have the value of state 0 to calculate the value of state 0, but this is absolutely legal with the Bellman equation.  The reward for landing at state 0, 1 and 4 is 0. If the agent wants to move to state 1 (as indicated by the policy) there is 33.3% chance to move in that direction, 33.3% chance to stay in state 0 and 33.3% chance to move to state 4.</p>
<div class="figure align-center" id="id7">
<img alt="../../_images/frozen_lake_bellman.svg" src="../../_images/frozen_lake_bellman.svg" /><p class="caption"><span class="caption-text">Frozen Lake Bellman Equation.</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>If we insert the values in the Bellman equation for the state-value function, we get the result of 0.0395. The result is not exactly 0.04, but this is due to rounding errors and the fact that the algorithms we are going to use will allow for some (very small) errors.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="mi">0</span> <span class="o">+</span> <span class="mf">0.99</span> <span class="o">*</span> <span class="mf">0.04</span><span class="p">)</span>  <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="mi">0</span> <span class="o">+</span> <span class="mf">0.99</span> <span class="o">*</span> <span class="mf">0.02</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="mi">0</span> <span class="o">+</span> <span class="mf">0.99</span> <span class="o">*</span> <span class="mf">0.06</span><span class="p">)</span>
<span class="go">0.039599999999999996</span>
</pre></div>
</div>
</div>
<div class="section" id="optimality">
<h2>Optimality<a class="headerlink" href="#optimality" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To solve an MDP is to find the optimal policy!</p>
</div>
<p>At the beginning of the chapter we asked ourselves what it means to solve a Markov decision process. The solution of an MDP means that the agent has learned an optimal policy function. Optimality implies that there is a way to compare different policies and to determine which of the policies is better.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Policies are evaluated in terms of their value functions</p>
<p><img class="math" src="../../_images/math/2203bb8ccc1c7023178692ea6c0ac5e4636bdf43.svg" alt="\pi \geq \pi’"/> if and only if <img class="math" src="../../_images/math/63dbc78b44693f7ca2514b7f4e7c5c0c6d5d1267.svg" alt="v_{\pi}(s) \geq v_{\pi'}(s)"/> for all <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/></p>
</div>
<p>In finite MDPs value functions are used as a metric of the goodness of a policy. The policy  pi is said to be better than the policy pi’ if and only if the value function of pi is larger or equal to the value function of policy pi’ for all states in the state set S.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The optimal policy <img class="math" src="../../_images/math/65f0bb840045c41b9051bc4ceac137de493ddc3a.svg" alt="\pi_*"/> is defined as</p>
<p><img class="math" src="../../_images/math/e3a642404e544540a05b9ca6a40ee097cc7e49db.svg" alt="\pi_* \geq \pi"/> for all <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/></p>
</div>
<p>The optimal policy is the policy that is better (or at least not worse) than any other policy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The optimal state-value funtion:</p>
<p><img class="math" src="../../_images/math/cf5a0e11c9430d7fc0f4a13818ffc3280ce88c1e.svg" alt="v_*(s) = \max_{\pi} v_{\pi}(s)"/> for all states <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/></p>
<p>The optimal action-value function:</p>
<p><img class="math" src="../../_images/math/14caff5f2371c95085a0f2d81233eaac6ea2c78d.svg" alt="q_*(s, a) = \max_{\pi} q_{\pi}(s, a)"/> for all states <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/> and all actions <img class="math" src="../../_images/math/bb1b6a28eceffc4b685eb189e30c62e16ef0b311.svg" alt="a \in \mathcal{A}"/></p>
</div>
<p>The state-value function and the action-value function that are based on the optimal policy are called optimal state-value and optimal action-value function respectively.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>There might be several optimal policies, but there is always only one optimal value function.</p>
</div>
</div>
<div class="section" id="bellman-optimality-equations">
<h2>Bellman Optimality Equations<a class="headerlink" href="#bellman-optimality-equations" title="Permalink to this headline">¶</a></h2>
<p>For finite MDPs the common approach is to calculate the optimal value functions <img class="math" src="../../_images/math/cd470423c63a627c055755b318a6273c585bb9be.svg" alt="v_*"/> and/or <img class="math" src="../../_images/math/afeeb7361af5b1430e75641c240ed1df2423959b.svg" alt="q_*"/> and to deduce the optimal policy <img class="math" src="../../_images/math/65f0bb840045c41b9051bc4ceac137de493ddc3a.svg" alt="\pi_*"/> from those.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Bellman Optimality Equation for the state-value function:</p>
<div class="math">
<p><img src="../../_images/math/b66d8b3741fce7085fa5c188caf356dff00c9276.svg" alt="\begin{align*}
v_*(s) &amp; = \max_{a} q_{{\pi}_*}(s, a) \\
&amp; = \max_{a} \mathbb{E_{\pi_{*}}}[G_t \mid S_t = s, A_t = a] \\
&amp; = \max_{a} \mathbb{E_{\pi_{*}}}[R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
&amp; = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a]
\end{align*}"/></p>
</div><p>Bellman Optimality Equation for the state-value function:</p>
<div class="math">
<p><img src="../../_images/math/9b31bb8f420fcf82e777091934381ed288223c05.svg" alt="\begin{align*}
q_*(s, a) &amp; = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a] \\
&amp; = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \mid S_t = s, A_t = a]
\end{align*}"/></p>
</div></div>
<p>For that purpose the Bellman equations of optimal value functions are required. These are called Bellman optimality equations.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In order to solve an MDP we need to learn the optimal policy. The optimal policy in turn is calculated using optimal state-value and action-value functions.</p>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="definition_markov_decision_process.html" title="previous page">Definition of a Markov Decision Process</a>
    <a class='right-next' id="next-link" href="../dynamic_programming/introduction.html" title="next page">Introduction</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>