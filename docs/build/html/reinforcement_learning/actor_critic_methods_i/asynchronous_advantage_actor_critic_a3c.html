
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Asynchronous Advantage Actor-Critic (A3C) &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Generalized Advantage Estimation (GAE)" href="generalized_advantage_estimation_gae.html" />
    <link rel="prev" title="Baseline Methods vs Actor-Critic-Methods" href="baseline_vs_actor_critic.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Intuition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/components.html">
   Agent and Environment Components
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/terminology.html">
   Reinforcement Learning Terminology
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/bias_variance_tradeoff.html">
   Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/double_q_learning.html">
   Double Q-Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/convergence_optimality.html">
   Convergence And Optimality
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/prioritized_experience_replay.html">
   Prioritized Experience Replay
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/additional_dqn_improvements.html">
   Additional DQN Improvements
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Policy Gradient Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/policy_gradient_derivation.html">
   Policy Gradient Derivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce.html">
   REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce_with_baseline_vpg.html">
   REINFORCE With Baseline | Vanilla Policy Gradient  (VPG)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods I
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="baseline_vs_actor_critic.html">
   Baseline Methods vs Actor-Critic-Methods
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Asynchronous Advantage Actor-Critic (A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generalized_advantage_estimation_gae.html">
   Generalized Advantage Estimation (GAE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="advantage_actor_critic_a2c.html">
   Advantage Actor-Critic (A2C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="trust_region_policy_optimization_trpo.html">
   Trust Region Policy Optimization (TRPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="proximal_policy_optimization_ppo.html">
   Proximal Policy Optimization (PPO)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods II
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/introduction.html">
   Intoduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/deep_deterministic_policy_gradient_ddpg.html">
   Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/twin_delayed_ddpg_td3.html">
   Twin Delayed DDPG (TD3)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/soft_actor_critic_sac.html">
   Soft Actor-Critc (SAC)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory">
   Theory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-structure">
     General Structure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#n-steps-as-psi">
     n-steps as
     <img alt="\Psi" class="math" src="../../_images/math/4150d5adeb3e51afa5b7c2820aafdb11061f9da6.svg"/>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#entropy">
     Entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architecture">
     Architecture
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sources">
   Sources
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="asynchronous-advantage-actor-critic-a3c">
<h1>Asynchronous Advantage Actor-Critic (A3C)<a class="headerlink" href="#asynchronous-advantage-actor-critic-a3c" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>“The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU <a class="footnote-reference brackets" href="#id2" id="id1">1</a>.”</p>
</div>
</div></blockquote>
<p>Policy gradient algorithms are on-policy algorithms. To see why this is the case let us look at the policy gradient.</p>
<div class="math">
<p><img src="../../_images/math/60cf7634ecc6a6c0a920bbf87ceba3a9df616464.svg" alt="\begin{align*}
\nabla_{\theta} J(\theta) &amp; = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t^H \nabla_{\theta} \log \pi_{\theta}(A_t \mid S_t)  \Psi_t]
\end{align*}"/></p>
</div><p>In order to caluculate the gradient, the expectation has to be estimated based on trajectories that are sampled with the help of the policy <img class="math" src="../../_images/math/54a5510ce494cc840bca3bdea542c10c4bb44c2c.svg" alt="\pi_{\theta}"/>. But each time we perform a gradient descent step the weights <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/> are adjusted, which changes the policy and therefore changes the expectation. Therefore after each optimization step old experience tuples have to be thrown away and can not be reused in an off-policy fashion.</p>
<p>Because of its off-policy nature the experience replay that was used with value based reinforcement learning algorithms can not be used in the same way with policy gradient algorithms. But the memory buffer tried to solve the problem of highly correlated data that is generated in reinforcement learning. The asynchronous advantage actor-critic (A3C) tries to solve the problem by running the same algorithm in parallel on different processor cores. Each core has a distinct agent that interacts with its own instance of the environment and updates the policy periodically. It is reasonable to assume that each agent faces different states and rewards when interacting with the environment, thus reducing the correlation problem.</p>
</div>
<div class="section" id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h2>
<div class="section" id="general-structure">
<h3>General Structure<a class="headerlink" href="#general-structure" title="Permalink to this headline">¶</a></h3>
<div class="figure align-center" id="id3">
<img alt="../../_images/structure1.svg" src="../../_images/structure1.svg" /><p class="caption"><span class="caption-text">A3C Structure.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>At the core of the A3C algorithm as an asynchronous learning mechanism. The value and the policy function (implemented through a neural network) is initialized with shared weights that can be accessed by other processes on presumably different cores. Each agent that is spawned on a different process copies the shared weights into the local functions before each interaction sequence. After a certain number of steps or when the agent encounters the terminal state, the agent runs the gradient descent step on the shared weights. This process continues with the copying of the global shared weights. The copying and updating is done in an asynchronous, non blocking, way. That means that at the same time several agents can copy the same shared weights and update them based on the same policy. Sometimes the gradients that are copied back into the shared value and policy network might be overridden by a different agent. This algorithm, called Hogwild, still manages to work well, even when the agents that live on different processes do not communicate with each other. Many optimization algorithms like RMSprop and Adam have internal variables that are calculated based on previous optimization steps. These variables are also shared among all different agents.</p>
<p>The A3C algorithm uses only different cores of the CPU and avoids the use of the GPU altogether. Still, according to the authors the algorithm outperforms the classical DQN algorithms in all Atari games in terms of computational speed and score performance.</p>
</div>
<div class="section" id="n-steps-as-psi">
<h3>n-steps as <img class="math" src="../../_images/math/4150d5adeb3e51afa5b7c2820aafdb11061f9da6.svg" alt="\Psi"/><a class="headerlink" href="#n-steps-as-psi" title="Permalink to this headline">¶</a></h3>
<p>The A3C algorithm calculates the advantage by using up to n interaction steps. If the environment terminates before n steps could be taken, then the optimization is done with the data that is available. In the original paper the authors use 5 interaction steps before optimization. The rest of the trajectory return is calculated through bootstrapping, which makes the algorithm an actor-critic algorithm.</p>
<div class="math">
<p><img src="../../_images/math/e35b1bfe4403956b9cd152de79118b29da0bd67b.svg" alt="\begin{align*}
&amp; \Psi_{t+0} = R_{t+1} + R_{t+2} + R_{t+3} + R_{t+4} + R_{t+5} + V(S_{t+5}) - V(S_{t}) \\
&amp; \Psi_{t+1} = R_{t+2} + R_{t+3} + R_{t+4} + R_{t+5} + V(S_{t+5}) - V(S_{t+1}) \\
&amp; \Psi_{t+2} = R_{t+3} + R_{t+4} + R_{t+5} + V(S_{t+5}) - V(S_{t+2}) \\
&amp; \Psi_{t+3} = R_{t+4} + R_{t+5} + V(S_{t+5}) - V(S_{t+3}) \\
&amp; \Psi_{t+4} = R_{t+5} + V(S_{t+5}) - V(S_{t+4}) \\
\end{align*}"/></p>
</div><p>After 5 steps the agent has 5 consecutive rewards at its disposal and can therefore use up to 5 rewards for the calculation of the advantage. At the end the agent can only use a single reward to estimate the advantage.</p>
</div>
<div class="section" id="entropy">
<h3>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h3>
<p>Entropy measures how much uncertainty is inherent in a probability distribution. The more certain the actions become the lower the entropy gets. If we assume for example a policy with two actions, then a policy which tends to select the same action for a given state with almost 100% probability will have a very low entropy. If on the other hand both actions will be selected with 50% probability for a given state, then the entropy will be high.</p>
<p>In the A3C the authors add an entropy term to the gradient ascent step of the policy gradient algorithm.</p>
<div class="math">
<p><img src="../../_images/math/45e946c291448edb53531b57420c2ddf68f6a094.svg" alt="\beta \nabla_{\theta} H(\pi_{\theta}(S_t))"/></p>
</div><p><img class="math" src="../../_images/math/107e19c45335923850db35344db2dd2a8b423b1b.svg" alt="H"/> calculates entropy and beta <img class="math" src="../../_images/math/360fe98852917a82dfb233e5a37dbb1ba0d15fd3.svg" alt="\beta"/> is used to measure the importance of the entropy. The reason for using entropy is to encourage exploration. The general idea of the policy gradient algorithm is to maximize actions with the highest advantage, but if the convergence to certain actions happen too soon, then it is possible that the agent misses on more favorable actions. Higher entropy forces the policy function to contain more uncertainty and therefore explore more.</p>
</div>
<div class="section" id="architecture">
<h3>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h3>
<div class="figure align-center" id="id4">
<img alt="../../_images/architecture.svg" src="../../_images/architecture.svg" /><p class="caption"><span class="caption-text">A3C Architecture.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>So far we have used separate networks for the agent and the critic. A3C uses the same neural network for the initial layers and only the last layer is separated into the policy and the value outputs. This approach is especially useful when several convolutional layers have to be trained in order to evaluate images and the weight sharing might facilitate training.</p>
<p>In our implementation below we use shared weights, but also add additional separate layers at the end of the network. This approach  produced better results in tests with simple environments like Lunar Lander.</p>
</div>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.core.shape_base</span> <span class="kn">import</span> <span class="n">block</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules.linear</span> <span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">Value</span><span class="p">,</span> <span class="n">Queue</span>
<span class="kn">from</span> <span class="nn">torch.distributions.categorical</span> <span class="kn">import</span> <span class="n">Categorical</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SharedRMSprop</span><span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">):</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">params</span><span class="p">,</span>
               <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
               <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
               <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">centered</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">SharedRMSprop</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
            <span class="n">centered</span><span class="o">=</span><span class="n">centered</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
               <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
               <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                  <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                  <span class="n">state</span><span class="p">[</span><span class="s1">&#39;shared_step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>
                  <span class="n">state</span><span class="p">[</span><span class="s1">&#39;square_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>
                  <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s1">&#39;momentum_buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>
                  <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;centered&#39;</span><span class="p">]:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s1">&#39;grad_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>

   <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
               <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                  <span class="k">continue</span>
               <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
               <span class="n">state</span><span class="p">[</span><span class="s1">&#39;steps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;shared_step&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
               <span class="n">state</span><span class="p">[</span><span class="s1">&#39;shared_step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

      <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the actor-critic shares the first layers</span>
<span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">ActorCritic</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">shared_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
      <span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">n_actions</span><span class="p">)</span>
            <span class="p">)</span>

   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

      <span class="n">distribution</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
      <span class="n">log_prob</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">entropy</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>

      <span class="k">return</span> <span class="n">v</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">():</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">agent_id</span><span class="p">,</span>
               <span class="n">env_name</span><span class="p">,</span>
               <span class="n">n_features</span><span class="p">,</span>
               <span class="n">n_actions</span><span class="p">,</span>
               <span class="n">actor_critic_function</span><span class="p">,</span>
               <span class="n">max_episodes</span><span class="p">,</span>
               <span class="n">solved_average_reward</span><span class="p">,</span>
               <span class="n">shared_counter</span><span class="p">,</span>
               <span class="n">shared_return_tracker</span><span class="p">,</span>
               <span class="n">shared_actor_critic</span><span class="p">,</span>
               <span class="n">shared_optimizer</span><span class="p">,</span>
               <span class="n">n_step</span><span class="p">,</span>
               <span class="n">beta</span><span class="p">,</span>
               <span class="n">gamma</span><span class="p">):</span>

      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;NEW AGENT WITH ID </span><span class="si">{</span><span class="n">agent_id</span><span class="si">}</span><span class="s1"> CREATED&#39;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">agent_id</span> <span class="o">=</span> <span class="n">agent_id</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">local_actor_critic</span> <span class="o">=</span> <span class="n">actor_critic_function</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">max_episodes</span> <span class="o">=</span> <span class="n">max_episodes</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">solved_average_reward</span> <span class="o">=</span> <span class="n">solved_average_reward</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">shared_counter</span> <span class="o">=</span> <span class="n">shared_counter</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">shared_return_tracker</span> <span class="o">=</span> <span class="n">shared_return_tracker</span>
      <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shared_return_tracker</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">shared_actor_critic</span> <span class="o">=</span> <span class="n">shared_actor_critic</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">shared_optimizer</span> <span class="o">=</span> <span class="n">shared_optimizer</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">n_step</span> <span class="o">=</span> <span class="n">n_step</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

   <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">entropies</span> <span class="o">=</span> <span class="p">[]</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">local_actor_critic</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shared_actor_critic</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

   <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="n">len_trajectory</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>
      <span class="n">gammas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">**</span><span class="n">exp</span> <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_trajectory</span><span class="p">)])</span>

      <span class="n">returns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">:])</span> <span class="o">*</span> <span class="n">gammas</span><span class="p">[:</span><span class="n">len_trajectory</span><span class="o">-</span><span class="n">t</span><span class="p">])</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_trajectory</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
      <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">returns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="p">)</span>
      <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
      <span class="n">entropies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entropies</span><span class="p">)</span>

      <span class="c1"># OPTIMIZE</span>

      <span class="c1">#clear past gradients</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">shared_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

      <span class="c1">#OPTIMIZATION</span>
      <span class="c1">#-----------------------------------------</span>
      <span class="c1">#calculate the advantages</span>
      <span class="n">advantages</span> <span class="o">=</span> <span class="n">returns</span> <span class="o">-</span> <span class="n">values</span>
      <span class="c1">#minus to make descent from ascent</span>
      <span class="n">pi_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">advantages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
      <span class="n">entropy_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">entropies</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
      <span class="n">v_loss</span> <span class="o">=</span> <span class="n">advantages</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

      <span class="n">loss</span> <span class="o">=</span> <span class="n">pi_loss</span> <span class="o">+</span> <span class="n">entropy_loss</span> <span class="o">+</span> <span class="n">v_loss</span>

      <span class="c1">#calcualte gradients</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

      <span class="c1"># push gradients into the shared network from the local network</span>
      <span class="k">for</span> <span class="n">local_param</span><span class="p">,</span> <span class="n">shared_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_actor_critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_actor_critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
            <span class="k">if</span> <span class="n">shared_param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
               <span class="n">shared_param</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">local_param</span><span class="o">.</span><span class="n">grad</span>


      <span class="c1">#gradient descent</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">shared_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


   <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
      <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">reward_sum</span> <span class="o">=</span> <span class="mi">0</span>

      <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>

            <span class="n">value</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_actor_critic</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
            <span class="n">next_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">reward_sum</span><span class="o">+=</span><span class="n">reward</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="p">)</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">next_obs</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_step</span><span class="p">:</span>
               <span class="c1"># Bootstrapped value for the target</span>
               <span class="c1"># We pack the value in the reward list to make the calculations easier</span>
               <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                  <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_actor_critic</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
                  <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
               <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                  <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
               <span class="c1"># gradient descent</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
               <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>

               <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">shared_counter</span><span class="o">.</span><span class="n">value</span> <span class="o">+=</span> <span class="mi">1</span>
                  <span class="n">returns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_return_tracker</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                  <span class="n">returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">)</span>
                  <span class="n">reward_mean</span> <span class="o">=</span> <span class="mi">0</span>
                  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">:</span>
                        <span class="n">reward_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">returns</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">shared_return_tracker</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>

                  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_counter</span><span class="o">.</span><span class="n">value</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_episodes</span><span class="p">:</span>
                        <span class="k">break</span>

                  <span class="k">if</span> <span class="n">reward_mean</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">solved_average_reward</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SOLVED&quot;</span><span class="p">)</span>
                        <span class="k">break</span>

                  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Game Nr: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shared_counter</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s1"> Agent Nr: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">agent_id</span><span class="si">}</span><span class="s1"> achieved reward of </span><span class="si">{</span><span class="n">reward_sum</span><span class="si">}</span><span class="s1">. Average Reward: </span><span class="si">{</span><span class="n">reward_mean</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

                  <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
                  <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                  <span class="n">reward_sum</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PARAMETERS FOR LUNAR LANDER</span>
<span class="n">ENV_NAME</span> <span class="o">=</span> <span class="s1">&#39;LunarLander-v2&#39;</span>
<span class="n">ENV</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV_NAME</span><span class="p">)</span>
<span class="n">N_FEATURES</span> <span class="o">=</span> <span class="n">ENV</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">N_ACTIONS</span> <span class="o">=</span> <span class="n">ENV</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">SOLVED_AVERAGE_REWARD</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">MAX_EPISODES</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">N_STEP</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">0.0005</span>
<span class="n">BETA</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">ACTOR_CRITIC_FUNCTION</span> <span class="o">=</span> <span class="n">ActorCritic</span>
<span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># shared variables</span>
<span class="n">COUNTER</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Value</span><span class="p">(</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">RETURN_TRACKER</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">()</span>
<span class="n">RETURN_TRACKER</span><span class="o">.</span><span class="n">put</span><span class="p">([])</span>
<span class="n">ACTOR_CRITIC</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="n">N_FEATURES</span><span class="p">,</span> <span class="n">N_ACTIONS</span><span class="p">)</span><span class="o">.</span><span class="n">share_memory</span><span class="p">()</span>
<span class="n">ACTOR_CRITIC_OPTIMIZER</span> <span class="o">=</span> <span class="n">SharedRMSprop</span><span class="p">(</span><span class="n">ACTOR_CRITIC</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">ALPHA</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">spawn_agent</span><span class="p">(</span><span class="n">agent_id</span><span class="p">,</span> <span class="n">shared_counter</span><span class="p">,</span> <span class="n">shared_return_tracker</span><span class="p">,</span> <span class="n">shared_actor_critic</span><span class="p">,</span> <span class="n">shared_optimizer</span><span class="p">):</span>

   <span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span>
               <span class="n">agent_id</span><span class="o">=</span><span class="n">agent_id</span><span class="p">,</span>
               <span class="n">env_name</span><span class="o">=</span><span class="n">ENV_NAME</span><span class="p">,</span>
               <span class="n">n_features</span><span class="o">=</span><span class="n">N_FEATURES</span><span class="p">,</span>
               <span class="n">n_actions</span><span class="o">=</span><span class="n">N_ACTIONS</span><span class="p">,</span>
               <span class="n">actor_critic_function</span><span class="o">=</span><span class="n">ACTOR_CRITIC_FUNCTION</span><span class="p">,</span>
               <span class="n">max_episodes</span><span class="o">=</span><span class="n">MAX_EPISODES</span><span class="p">,</span>
               <span class="n">solved_average_reward</span><span class="o">=</span><span class="n">SOLVED_AVERAGE_REWARD</span><span class="p">,</span>
               <span class="n">shared_counter</span><span class="o">=</span><span class="n">shared_counter</span><span class="p">,</span>
               <span class="n">shared_return_tracker</span><span class="o">=</span><span class="n">shared_return_tracker</span><span class="p">,</span>
               <span class="n">shared_actor_critic</span><span class="o">=</span><span class="n">shared_actor_critic</span><span class="p">,</span>
               <span class="n">shared_optimizer</span><span class="o">=</span><span class="n">shared_optimizer</span><span class="p">,</span>
               <span class="n">n_step</span><span class="o">=</span><span class="n">N_STEP</span><span class="p">,</span>
               <span class="n">beta</span><span class="o">=</span><span class="n">BETA</span><span class="p">,</span>
               <span class="n">gamma</span><span class="o">=</span><span class="n">GAMMA</span><span class="p">)</span>

   <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
   <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-----------------------------------------------------------------------&#39;</span><span class="p">)</span>

   <span class="n">workers</span> <span class="o">=</span> <span class="p">[</span><span class="n">mp</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">spawn_agent</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">COUNTER</span><span class="p">,</span> <span class="n">RETURN_TRACKER</span><span class="p">,</span> <span class="n">ACTOR_CRITIC</span><span class="p">,</span> <span class="n">ACTOR_CRITIC_OPTIMIZER</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_WORKERS</span><span class="p">)]</span>
   <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">]</span>
   <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">join</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">]</span>

   <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;FINISHED&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="sources">
<h2>Sources<a class="headerlink" href="#sources" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Mnih V. et al. Asynchronous Methods for Deep Reinforcement Learning. 2016. <a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a></p>
</dd>
</dl>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="baseline_vs_actor_critic.html" title="previous page">Baseline Methods vs Actor-Critic-Methods</a>
    <a class='right-next' id="next-link" href="generalized_advantage_estimation_gae.html" title="next page">Generalized Advantage Estimation (GAE)</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>