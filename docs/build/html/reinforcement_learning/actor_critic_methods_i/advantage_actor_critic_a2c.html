
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Advantage Actor-Critic (A2C) &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Trust Region Policy Optimization (TRPO)" href="trust_region_policy_optimization_trpo.html" />
    <link rel="prev" title="Generalized Advantage Estimation (GAE)" href="generalized_advantage_estimation_gae.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Intuition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/components.html">
   Agent and Environment Components
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/terminology.html">
   Reinforcement Learning Terminology
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/bias_variance_tradeoff.html">
   Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/double_q_learning.html">
   Double Q-Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/convergence_optimality.html">
   Convergence And Optimality
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/prioritized_experience_replay.html">
   Prioritized Experience Replay
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/additional_dqn_improvements.html">
   Additional DQN Improvements
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Policy Gradient Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/policy_gradient_derivation.html">
   Policy Gradient Derivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce.html">
   REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce_with_baseline_vpg.html">
   REINFORCE With Baseline | Vanilla Policy Gradient  (VPG)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods I
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="baseline_vs_actor_critic.html">
   Baseline Methods vs Actor-Critic-Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="asynchronous_advantage_actor_critic_a3c.html">
   Asynchronous Advantage Actor-Critic (A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generalized_advantage_estimation_gae.html">
   Generalized Advantage Estimation (GAE)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Advantage Actor-Critic (A2C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="trust_region_policy_optimization_trpo.html">
   Trust Region Policy Optimization (TRPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="proximal_policy_optimization_ppo.html">
   Proximal Policy Optimization (PPO)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods II
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/introduction.html">
   Intoduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/deep_deterministic_policy_gradient_ddpg.html">
   Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/twin_delayed_ddpg_td3.html">
   Twin Delayed DDPG (TD3)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/soft_actor_critic_sac.html">
   Soft Actor-Critc (SAC)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sources">
   Sources
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="advantage-actor-critic-a2c">
<h1>Advantage Actor-Critic (A2C)<a class="headerlink" href="#advantage-actor-critic-a2c" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>The improvements to performance that were made with the asynchronous advantage actor critic (A3C) were not necessarily due to the asynchronous aspect of the algorithm. Soon after the release of the algorithm researchers found out that the performance boost comes from the experiences that are generated by different environments and not the asynchronous updates that were made with Hogwild. In A2C (advantage actor critic) several environments are generated and a batch of experiences is gathered. But the update is done centrally by a single agent. The advantage of this approach is the easier implementation and debugging, while keeping the decorelated experience tuples that result from the many distinct environments. Additionally as the update is done with experiences that can be packed into a batch, GPUs are heavily used with A2C. Common A2C implementations also make use of GAEs, which were introduced after the A3C algorithm.</p>
<div class="figure align-center" id="id3">
<img alt="../../_images/structure.svg" src="../../_images/structure.svg" /><p class="caption"><span class="caption-text">A2C Structure.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>There is no single paper that can be pointed to for the A2C algorithm. But the name has become popular after OpenAI released its baseline implementations for most common state of the art reinforcement learning algorithms <a class="footnote-reference brackets" href="#id2" id="id1">1</a>.</p>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>The following implementation is still unstable.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.distributions.categorical</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the actor-critic shares the first layers</span>
<span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ActorCritic</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">n_actions</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">action</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">env_name</span><span class="p">,</span>
                <span class="n">n_envs</span><span class="p">,</span>
                <span class="n">n_steps</span><span class="p">,</span>
                <span class="n">max_steps</span><span class="p">,</span>
                <span class="n">actor_critic_function</span><span class="p">,</span>
                <span class="n">solved_average_reward</span><span class="p">,</span>
                <span class="n">alpha</span><span class="p">,</span>
                <span class="n">beta</span><span class="p">,</span>
                <span class="n">gamma</span><span class="p">,</span>
                <span class="n">tau</span><span class="p">,</span>
                <span class="n">device</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>

        <span class="n">n_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor_critic</span> <span class="o">=</span> <span class="n">actor_critic_function</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">alpha</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">make_env</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
            <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
                <span class="n">env</span> <span class="o">=</span>  <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>
                <span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">env</span>
            <span class="k">return</span> <span class="n">closure</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">envs</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">AsyncVectorEnv</span><span class="p">([</span><span class="n">make_env</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_envs</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">solved_average_reward</span> <span class="o">=</span> <span class="n">solved_average_reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">=</span> <span class="n">max_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">eval_rewards</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg_eval_reward_sum</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_eval_reward_sum</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_avg_eval_reward_sum</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dones</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entropies</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="p">)</span>
        <span class="n">entropies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entropies</span><span class="p">)</span>

        <span class="c1">#calculate gaes</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>

        <span class="n">prev_gae</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span><span class="p">)):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">values</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dones</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">-</span> <span class="n">values</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">prev_gae</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dones</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">prev_gae</span>
            <span class="n">advantages</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_gae</span>

        <span class="n">returns</span> <span class="o">=</span> <span class="n">advantages</span> <span class="o">+</span> <span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># OPTIMIZE</span>

        <span class="c1">#clear past gradients</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">advantages</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">returns</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1">#OPTIMIZATION</span>
        <span class="c1">#-----------------------------------------</span>
        <span class="c1">#calculate the advantages</span>

        <span class="c1">#minus to make descent from ascent</span>
        <span class="n">pi_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">advantages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">entropy_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">entropies</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">v_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">-</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">pi_loss</span> <span class="o">+</span> <span class="n">entropy_loss</span> <span class="o">+</span> <span class="n">v_loss</span>

        <span class="c1">#calcualte gradients</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1">#gradient descent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">n_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">n_update</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_updates</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span><span class="p">):</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">value</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_critic</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
                <span class="n">next_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">envs</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="p">)</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">next_obs</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="c1">#additional value for bootstrapping</span>
                <span class="n">bootstrapped_obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">value</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_critic</span><span class="p">(</span><span class="n">bootstrapped_obs</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Steps: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">eval_reward_sum</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_critic</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
                <span class="n">next_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="n">obs</span> <span class="o">=</span> <span class="n">next_obs</span>
                <span class="n">eval_reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>

            <span class="c1"># EVALUATION AND LOGGING</span>
            <span class="c1">#-----------------------------------------------------------</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eval_reward_sum</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">eval_reward_sum</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_eval_reward_sum</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_eval_reward_sum</span> <span class="o">=</span> <span class="n">eval_reward_sum</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">avg_eval_reward_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_rewards</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_eval_reward_sum</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_avg_eval_reward_sum</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_avg_eval_reward_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_eval_reward_sum</span>

            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--------------------------------&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Reward Sum: </span><span class="si">{</span><span class="n">eval_reward_sum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Max Reward Sum: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_eval_reward_sum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Avg. Reward Sum: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">avg_eval_reward_sum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Max Avg. Reward Sum: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_avg_eval_reward_sum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># PARAMETERS FOR LUNAR LANDER</span>
    <span class="n">ENV_NAME</span> <span class="o">=</span> <span class="s1">&#39;LunarLander-v2&#39;</span>
    <span class="n">SOLVED_AVERAGE_REWARD</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">MAX_STEPS</span> <span class="o">=</span> <span class="mi">500000</span>
    <span class="n">N_STEPS</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">0.0005</span>
    <span class="n">BETA</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
    <span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.95</span>
    <span class="n">ACTOR_CRITIC_FUNCTION</span> <span class="o">=</span> <span class="n">ActorCritic</span>
    <span class="n">N_ENVS</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Working on device: </span><span class="si">{</span><span class="n">DEVICE</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span>
                <span class="n">env_name</span><span class="o">=</span><span class="n">ENV_NAME</span><span class="p">,</span>
                <span class="n">n_envs</span><span class="o">=</span><span class="n">N_ENVS</span><span class="p">,</span>
                <span class="n">n_steps</span><span class="o">=</span><span class="n">N_STEPS</span><span class="p">,</span>
                <span class="n">max_steps</span><span class="o">=</span><span class="n">MAX_STEPS</span><span class="p">,</span>
                <span class="n">actor_critic_function</span><span class="o">=</span><span class="n">ACTOR_CRITIC_FUNCTION</span><span class="p">,</span>
                <span class="n">solved_average_reward</span><span class="o">=</span><span class="n">SOLVED_AVERAGE_REWARD</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">,</span>
                <span class="n">beta</span><span class="o">=</span><span class="n">BETA</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="n">GAMMA</span><span class="p">,</span>
                <span class="n">tau</span><span class="o">=</span><span class="n">TAU</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>

    <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="sources">
<h2>Sources<a class="headerlink" href="#sources" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>OpenAI Baselines A2C blog post. <a class="reference external" href="https://openai.com/blog/baselines-acktr-a2c/">https://openai.com/blog/baselines-acktr-a2c/</a></p>
</dd>
</dl>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="generalized_advantage_estimation_gae.html" title="previous page">Generalized Advantage Estimation (GAE)</a>
    <a class='right-next' id="next-link" href="trust_region_policy_optimization_trpo.html" title="next page">Trust Region Policy Optimization (TRPO)</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>