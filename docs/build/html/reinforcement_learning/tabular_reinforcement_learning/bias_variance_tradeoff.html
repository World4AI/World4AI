
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bias-Variance Tradeoff &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Double Q-Learning" href="double_q_learning.html" />
    <link rel="prev" title="Temporal Difference Learning" href="td_learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Intuition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/components.html">
   Agent and Environment Components
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/terminology.html">
   Reinforcement Learning Terminology
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="double_q_learning.html">
   Double Q-Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/convergence_optimality.html">
   Convergence And Optimality
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/prioritized_experience_replay.html">
   Prioritized Experience Replay
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/additional_dqn_improvements.html">
   Additional DQN Improvements
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Policy Gradient Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/policy_gradient_derivation.html">
   Policy Gradient Derivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce.html">
   REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../policy_gradient_methods/reinforce_with_baseline_vpg.html">
   REINFORCE With Baseline | Vanilla Policy Gradient  (VPG)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods I
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/baseline_vs_actor_critic.html">
   Baseline Methods vs Actor-Critic-Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/asynchronous_advantage_actor_critic_a3c.html">
   Asynchronous Advantage Actor-Critic (A3C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/generalized_advantage_estimation_gae.html">
   Generalized Advantage Estimation (GAE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/advantage_actor_critic_a2c.html">
   Advantage Actor-Critic (A2C)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/trust_region_policy_optimization_trpo.html">
   Trust Region Policy Optimization (TRPO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_i/proximal_policy_optimization_ppo.html">
   Proximal Policy Optimization (PPO)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Actor-Critic Methods II
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/introduction.html">
   Intoduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/deep_deterministic_policy_gradient_ddpg.html">
   Deep Deterministic Policy Gradient (DDPG)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/twin_delayed_ddpg_td3.html">
   Twin Delayed DDPG (TD3)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../actor_critic_methods_ii/soft_actor_critic_sac.html">
   Soft Actor-Critc (SAC)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias">
   Bias
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance">
   Variance
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="bias-variance-tradeoff">
<h1>Bias-Variance Tradeoff<a class="headerlink" href="#bias-variance-tradeoff" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>The bias-variance tradeoff plays a tremendous role in reinforcement learning and machine learning in general. Ideally we want to reduce both as much as possible, but reducing one means increasing the other at the same time. There is a tradeoff to be made.</p>
<p>In this section we will look at the definitions of bias and variance and discuss at what side of the spectrum monte carlo and temporal difference methods are.</p>
</div>
<div class="section" id="bias">
<h2>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math">
<p><img src="../../_images/math/f59aa0ae876ee62b6e662e2db4500459ac15be39.svg" alt="bias(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta"/></p>
</div></div>
<p>In statiscics we define the bias of the estimator as the difference between the expected value of the estimator <img class="math" src="../../_images/math/103a6be1b9e7f61842a7501a4dac471aadbb2959.svg" alt="\mathbb{E}[\hat{\theta}]"/> and the true parameter <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/>. The true parameter <img class="math" src="../../_images/math/a90c917b59868527c9644dedcb304f87a6241b9f.svg" alt="\theta"/> that we are most interested in is the expected value <img class="math" src="../../_images/math/9f0d624a8590ba1d79af682e4c1ea5b2d4a219f0.svg" alt="\mathbb{E}[X]"/> of some random variable <img class="math" src="../../_images/math/80c2bd2011eacc54782b1b5d7f86c078a8316143.svg" alt="X"/>.</p>
<div class="figure align-center" id="id1">
<img alt="../../_images/random_variable.svg" src="../../_images/random_variable.svg" /><p class="caption"><span class="caption-text">Distribution of a random variable X.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>In the image above we see the distribution of the random variable <img class="math" src="../../_images/math/80c2bd2011eacc54782b1b5d7f86c078a8316143.svg" alt="X"/>. The calculation of the expectation is straightforward when we know the actual probability distribution.</p>
<div class="math">
<p><img src="../../_images/math/307138f7cf80fe09188eab3d5d98528c8b748172.svg" alt="\mathbb{E}[X] = 0.15 * 1 + 0.15 * 2 + 0.3 * 3 + 0.3 * 4 = 2.55"/></p>
</div><p>We do not know the true distribution of <img class="math" src="../../_images/math/80c2bd2011eacc54782b1b5d7f86c078a8316143.svg" alt="X"/> and can not directly calculate the expected value, therefore we have to use some estimate <img class="math" src="../../_images/math/dd7c22d1d24f8b59b984dba49af93fcfd072d1b9.svg" alt="\hat{\theta}"/> as a proxy for the true <img class="math" src="../../_images/math/9f0d624a8590ba1d79af682e4c1ea5b2d4a219f0.svg" alt="\mathbb{E}[X]"/>. The most straightforward way to estimate the expected value of a random variable is to draw samples from the distribution and to use the individual samples <img class="math" src="../../_images/math/80c2bd2011eacc54782b1b5d7f86c078a8316143.svg" alt="X"/> as the estimate <img class="math" src="../../_images/math/dd7c22d1d24f8b59b984dba49af93fcfd072d1b9.svg" alt="\hat{\theta}"/>. Is there any bias by using the random samples as an estimate. No, using <img class="math" src="../../_images/math/80c2bd2011eacc54782b1b5d7f86c078a8316143.svg" alt="X"/> as an estimate for <img class="math" src="../../_images/math/9f0d624a8590ba1d79af682e4c1ea5b2d4a219f0.svg" alt="\mathbb{E}[X]"/> intorduces no bias.</p>
<div class="math">
<p><img src="../../_images/math/76cb077bbbb1a75dd69e2157e10edc4fd4879fa9.svg" alt="\begin{align*}
&amp; \theta = \mathbb{E}[X] \\
&amp; \hat{\theta} = X \\
&amp; bias(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta = \mathbb{E}[X] - \mathbb{E}[X] = 0
\end{align*}"/></p>
</div><p>If on the other hand we used the number 3 constantly as the estimate of the expected value, then that would definitely introduce a bias.</p>
<div class="math">
<p><img src="../../_images/math/44b90a49ec7fb8533e33837f4af8859b6301c2d8.svg" alt="\begin{align*}
&amp; \theta = \mathbb{E}[X] \\
&amp; \hat{\theta} = 3 \\
&amp; bias(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta = \mathbb{E}[3] - \mathbb{E}[X] = 3 - 2.55 = 0.45
\end{align*}"/></p>
</div><div class="figure align-center" id="id2">
<img alt="../../_images/simple_mdp.svg" src="../../_images/simple_mdp.svg" /><p class="caption"><span class="caption-text">Distribution of a random variable X.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>But how do the discussions regarding bias extend to the choice between monte carlo methods and temporal difference methods? The example above might provide some answers. Imagine an episodic environment where the agent starts at the left yellow box and has to arrive at the right red box. Each step that the agent takes generates a negative reward of -1. Only the terminal red state provides a positive reward. To make the calculations simple we make the environment and the policy of the agent fully deterministic. In our example the agent follows the strategy of always going right and the MDP always transitions in the desired way. We also do not use any discounting. Following these assumptions the agent receives a return <img class="math" src="../../_images/math/fa309ec25c85e0173ceed1bd8bf5166ee63b704c.svg" alt="G_t"/> of -2 at each single episode when he starts at the yellow state, which we designate as state 0.</p>
<p>When we use policy evaluation our goal is to find the correct value function <img class="math" src="../../_images/math/fc4a32e1a50cc2d8e5b50909ae74cdb02b135704.svg" alt="v_{\pi}(s)"/>, which is the expected value of the rewards <img class="math" src="../../_images/math/fa309ec25c85e0173ceed1bd8bf5166ee63b704c.svg" alt="G_t"/> when following the policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/>. It turns out that monte carlo methods are not biased, because we use the full returns <img class="math" src="../../_images/math/fa309ec25c85e0173ceed1bd8bf5166ee63b704c.svg" alt="G_t"/> as an estimator of the value function.</p>
<div class="math">
<p><img src="../../_images/math/c4dc0215468b24b67c8dd3001b5d3838fa1a07f2.svg" alt="\begin{align*}
&amp; \theta = \mathbb{E}[G_t] \\
&amp; \hat{\theta} = G_t \\
&amp; bias(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta = \mathbb{E}[G_t] - \mathbb{E}[G_t] = -2 - (-2) = 0
\end{align*}"/></p>
</div><p>The return is an unbiased estimator of the value function and that would not change even if the environment and the policy of the agent were stochastic. Therefore it is reasonable to collect samples of <img class="math" src="../../_images/math/fa309ec25c85e0173ceed1bd8bf5166ee63b704c.svg" alt="G_t"/> for the state 0 and to calculate the average <img class="math" src="../../_images/math/421638de6085500bcf7a9c26253e5a6dd3e60dfa.svg" alt="\frac{1}{N}\sum_t^N G_t"/>.</p>
<p>This is not the case for temporal difference methods. Let us assume that we set the initial values of the state value function to 0, <img class="math" src="../../_images/math/297e2772e90cacb8f9934a3f86fa29eb17b824ac.svg" alt="v_{\pi}(s) = 0"/>. TD methods use bootstrapping, therefore the estimate of the value for the 0th state is <img class="math" src="../../_images/math/307ce10b68f35e6a1e7fa2cd5fa9fd6401b955bf.svg" alt="R_t + v_{\pi}(1) = -1 + 0 = -1"/>, which makes bootstrapping biased, <img class="math" src="../../_images/math/177d6268c656aa8efd8475cae20ec78754780b96.svg" alt="\mathbb{E}[R_t + v_{\pi}(S_{t+1})] \neq \mathbb{E}[G_t]"/>.</p>
<p>The reason we can get away with this type of bias in bootstrapping is by improving the estimate of the next state <img class="math" src="../../_images/math/ee3c7475da634e80bf1798c477262213f983d45e.svg" alt="v_{\pi}(S_{t+1})"/> with each iteration and thereby reducing the bias.</p>
</div>
<div class="section" id="variance">
<h2>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math">
<p><img src="../../_images/math/6f02fe678709c6c4340b81767b0fbd4e45a59a79.svg" alt="var(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2]"/></p>
</div></div>
<p>Intuitively the variance of an estimator tells us how strongly the estimator varies around the expected value of an estimator.</p>
<div class="figure align-center" id="id3">
<img alt="../../_images/low_variance.svg" src="../../_images/low_variance.svg" /><p class="caption"><span class="caption-text">Low Variance.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>Let us assume we face the above distribution and decide that we would like to use the random variable <img class="math" src="../../_images/math/80c2bd2011eacc54782b1b5d7f86c078a8316143.svg" alt="X"/> as the estimator for the expected value <img class="math" src="../../_images/math/9f0d624a8590ba1d79af682e4c1ea5b2d4a219f0.svg" alt="\mathbb{E}[X]"/>. The true expected value of the estimate is 0, but the individual draws from the distribution can vary around the expected value, the variance is not 0.</p>
<div class="math">
<p><img src="../../_images/math/ac95ff2bdde28521425b90d2f98ec752fb4076ec.svg" alt="\begin{align*}
var(\hat{\theta}) &amp; = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2] \\
&amp; = \mathbb{E}[(\hat{\theta} - 0)^2] \\
&amp; = 0.5 [(1 - 0)^2] +  0.5 [(-1 - 0)^2] = 1
\end{align*}"/></p>
</div><div class="figure align-center" id="id4">
<img alt="../../_images/high_variance.svg" src="../../_images/high_variance.svg" /><p class="caption"><span class="caption-text">High Variance.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>The calculation looks different if we use the random variable <img class="math" src="../../_images/math/80c2bd2011eacc54782b1b5d7f86c078a8316143.svg" alt="X"/> as the estimate of the expected value of the above distribution. The expected value is the same, but the variance as you can imagine is higher.</p>
<div class="math">
<p><img src="../../_images/math/aebf426e1a7b3a682bd59ba24730577ec6a0c4ba.svg" alt="\begin{align*}
var(\hat{\theta}) &amp; = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2] \\
&amp; = \mathbb{E}[(\hat{\theta} - 0)^2] \\
&amp; = 0.5 [(5 - 0)^2] +  0.5 [(-5 - 0)^2] \\
&amp; = 0.5 * 25 +  0.5 * 25 = 25
\end{align*}"/></p>
</div><div class="figure align-center" id="id5">
<img alt="../../_images/random_mdp.svg" src="../../_images/random_mdp.svg" /><p class="caption"><span class="caption-text">Simple MDP with random policy.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>Let us build some intuition regarding the variance in monte carlo vs temporal difference methods. We are going to use the same deterministic environment, but the policy is going to be absolutely random.</p>
<div class="figure align-center" id="id6">
<img alt="../../_images/monte_carlo.svg" src="../../_images/monte_carlo.svg" /><p class="caption"><span class="caption-text">Trajectories with Monte Carlo methods.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>Taking random actions even in a deterministic environment will generate different trajectories. Considering that the environment is also usually stochastic and the rules of the MPD are much more complex, you generally require a lot of samples to approximate the expected value. Unlike in the above example where we only draw a single random variable, in monte carlo methods in reinforcement learning the randomness accumulates through many actions and transitions.</p>
<div class="figure align-center" id="id7">
<img alt="../../_images/temporal_difference.svg" src="../../_images/temporal_difference.svg" /><p class="caption"><span class="caption-text">Trajectories with Temporal Difference methods.</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>The randomness in temporal difference methods is on the other hand manageable. In the simplest temporal difference approach we take a single action and add the result to the state or action value of the next state. In the above example the randomness is only present in the single stochastic action. The rest of randomness is already incorporated in the state value function.</p>
<p>Monte Carlo methods have more variance and temporal difference methods have more bias. In practice we usually look for some compromise between the two extremes.</p>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="td_learning.html" title="previous page">Temporal Difference Learning</a>
    <a class='right-next' id="next-link" href="double_q_learning.html" title="next page">Double Q-Learning</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>