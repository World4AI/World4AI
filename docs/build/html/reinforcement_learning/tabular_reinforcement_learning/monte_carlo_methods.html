
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Monte Carlo Methods &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Temporal Difference Learning" href="td_learning.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Intuition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/components.html">
   Agent and Environment Components
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../intuition/terminology.html">
   Reinforcement Learning Terminology
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Process
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markov_decision_process/to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/generalized_policy_iteration.html">
   Generalized Policy Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bias_variance_tradeoff.html">
   Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="double_q_learning.html">
   Double Q-Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Approximative Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/cart_pole.html">
   Cart Pole Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_with_oracle.html">
   Value Approximation With An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/approximation_without_oracle.html">
   Value Approximation Without An Oracle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../approximative_reinforcement_learning/convergence_optimality.html">
   Convergence And Optimality
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Modern Value-Based Approximation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/nfq.html">
   Neural Fitted Q Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/dqn.html">
   Deep Q-Network (DQN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/double_dqn.html">
   Double DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/prioritized_experience_replay.html">
   Prioritized Experience Replay
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/duelling_dqn.html">
   Duelling DQN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../modern_value_based_approximation/additional_dqn_improvements.html">
   Additional DQN Improvements
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-policy-iteration">
   Generalized Policy Iteration
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-estimation">
     Policy Estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#theory">
       Theory
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithm">
       Algorithm
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementation">
       Implementation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-improvement-and-control">
     Policy Improvement and Control
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Theory
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       Algorithm
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       Implementation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sources">
   Sources
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="monte-carlo-methods">
<h1>Monte Carlo Methods<a class="headerlink" href="#monte-carlo-methods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Monte Carlo methods are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results <a class="footnote-reference brackets" href="#id5" id="id1">1</a>.</p>
</div>
<p>If we look at any definition of Monte Carlo methods, there is a high chance that the definition contains random sampling.</p>
<div class="figure align-center" id="id6">
<img alt="../../_images/paths.svg" src="../../_images/paths.svg" /><p class="caption"><span class="caption-text">Paths Generated Through Monte Carlo.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>When we apply Monte Carlo methods to reinforcement learning we sample episode paths, also called trajectories. The agent interacts with the environment and collects experience tuples that consist of states, actions and rewards.</p>
<p>Monte Carlo methods are similar in spirit to bandit methods. The state-value and action-value functions can be estimated by taking the sampled trajectories and building averages. Unlike in bandits though, Monte Carlo methods are able to deal with environments where several non terminal states exist.</p>
<p>Estimations can only be made once the trajectory is complete when the episode finishes, which means that Monte Carlo methods only work for episodic tasks.</p>
</div>
<div class="section" id="generalized-policy-iteration">
<h2>Generalized Policy Iteration<a class="headerlink" href="#generalized-policy-iteration" title="Permalink to this headline">¶</a></h2>
<p>The Monte Carlo algorithm will follow general policy iteration. We alternate between policy evaluation and policy improvement to find the optimal policy.</p>
<div class="section" id="policy-estimation">
<h3>Policy Estimation<a class="headerlink" href="#policy-estimation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="theory">
<h4>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h4>
<p>Policy estimation deals with finding the true value function of a given policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/>. Mathematically speaking we are looking for the expected sum of discounted rewards (also called returns) when the agent follow the policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/>.</p>
<p><img class="math" src="../../_images/math/c1c29b4501816c57985d217aa8b2ec20e341c66a.svg" alt="v_\pi(s) = \mathbb{E}[G_t \mid S_t = s]"/></p>
<p>A natural way to estimate the expected value of a random variable is to get samples from a distribution and to use the average as an estimate. In reinforcement learning the agent can estimate the expected value of returns for a policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> by interacting with the environment, generating trajectories over and over again and building averages over the returns of the trajectories.</p>
<div class="figure align-center" id="id7">
<img alt="../../_images/mc_backup.svg" src="../../_images/mc_backup.svg" /><p class="caption"><span class="caption-text">Monte Carlo Trajectories.</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>The above image shows different trajectories that were created by following a policy pi. The large circles represent a certain state, the smaller black circles represent the actions and the boxes are the terminal states. To estimate the state value for the grey state located at the left we calculate the (discounted) return that is generated after the grey state and build a simple average. The same process is repeated for the yellow, green and all the other states.</p>
<p>Generally there are two methods to calculate the averages. Each time the agent faces a state during an episode is called a visit. In the “First Visit” Monte Carlo method only the return from the first visit to that state until the end of the episode is calculated. If the state is visited several times during an episode, the additional visits are not considered in the calculation. While in the “Every Visit” method each visit is counted. The “First Visit” method is more popular and generally more straightforward and is going to be covered in this section, but the algorithms can be easily adjusted to account for the “Every Visit” method.</p>
<p>To make the calculations of the averages computationally efficient we are going to use the incremental implementation that we already used for n-armed bandits.</p>
<div class="math">
<p><img src="../../_images/math/f93c617f8ad4815396f93dd760ff4bb5b08304b1.svg" alt="NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]"/></p>
</div></div>
<div class="section" id="algorithm">
<h4>Algorithm<a class="headerlink" href="#algorithm" title="Permalink to this headline">¶</a></h4>
<p>The algorithm is divided into two steps.</p>
<ul class="simple">
<li><p>In the first step using the policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> (or <img class="math" src="../../_images/math/8d48296c0461799ee293045b7060e90bfbc9968f.svg" alt="\mu"/> if the policy is deterministic) the agent generates a trajectory.</p></li>
<li><p>In the second step the agent improves the estimation for the state value function <img class="math" src="../../_images/math/41fc31585b3b13e811d8deb0b700ce4e1d0f2c1e.svg" alt="V(s)"/>. For that purpose the agent loops over the previously generated trajectory and for each experience tuple he determines if he deals with a first visit to that state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/>. If he does he calculates the discounted sum of rewards from that point on to the terminal state <img class="math" src="../../_images/math/590bafd0c30d1295e8abda9e493aa98d67766ee0.svg" alt="G_{t:T} = \sum_{k=t}^T \gamma^{k-t}R_t"/>. Finally the agent performs an update step by using the incremental average calculation <img class="math" src="../../_images/math/0542aec32ce57c083780457d52be30086bfc559c.svg" alt="V(s) = V(s) + \alpha [G_{t:T} - V(s)]"/>.</p></li>
</ul>
<div class="math">
<p><img src="../../_images/math/37ae5ed34da780220c46dc868b2a87666f02408f.svg" alt="\begin{algorithm}[H]
    \caption{Monte Carlo Prediction: First Visit}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: environment $env$, policy $\mu$, state set $\mathcal{S}$, number of episodes, learning rate $\alpha$, discount factor $\gamma$
    \STATE Initialize:
    \STATE $V(s)$ for all $s \in \mathcal{S}$ with zeros
    \FOR{$i=0$ to number of episodes}
        \STATE ...................................................................................................................................
        \STATE (1) INTERACTION WITH THE ENVIRONMENT
        \STATE create $trajectory$ as empty list [...]
        \REPEAT
            \STATE Generate experience $tuple$ $(State, Action, Reward)$ using policy $\mu$ and MDP $env$
            \STATE Push $tuple$ into $trajectory$
        \UNTIL{state is terminal}
        \STATE ...................................................................................................................................
        \STATE (2) ESTIMATION OF VALUE FUNCTION
        \STATE Create Visited(s) = $False$ for all $s \in \mathcal{S}$
        \FOR{$t=0$ to number of tuples in $trajectory$ $T$}
            \STATE $s \leftarrow$ state from $trajectory[t]$
            \IF {Visited($s$) is True}
                \STATE go to next tuple
            \ELSE
                \STATE Visited($s$) = True
            \ENDIF
            \STATE $G_{t:T} = \sum_{k=t}^T \gamma^{k-t}R_t$
            \STATE $V(s) = V(s) + \alpha [G_{t:T} - V(s)]$
        \ENDFOR
    \ENDFOR
    \STATE
    \STATE RETURN V(s)
\end{algorithmic}
\end{algorithm}"/></p>
</div></div>
<div class="section" id="implementation">
<h4>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h4>
<p>Once again we are going to utilize the already discussed frozen lake environment to demonstrate the implementation of the algorithms.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)]</span>
</pre></div>
</div>
<p>The below policy is going to be evaluated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="c1">#     LEFT = 0</span>
<span class="c1">#     DOWN = 1</span>
<span class="c1">#     RIGHT = 2</span>
<span class="c1">#     UP = 3</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="mi">4</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">5</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">6</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">7</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">8</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">9</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">10</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">11</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="mi">12</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">13</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">14</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="mi">15</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">mu</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
</pre></div>
</div>
<p>The following code shows a relatively straightforward implementation of Monte Carlo evaluation. To get better results we could still increase the number of episodes and reduce the learning rate <img class="math" src="../../_images/math/c79ec9e86f4f2b22c56742ffd242fc41b8af040e.svg" alt="\alpha"/> over time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mc_prediction</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>

    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>

        <span class="c1"># generate episode</span>
        <span class="n">episode</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">experience</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
            <span class="n">episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># makes an array with nr. timesteps as rows and 3 columns</span>
        <span class="n">episode</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="n">time_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>

        <span class="c1"># discounts</span>
        <span class="n">discounts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gamma</span><span class="o">**</span><span class="n">time_step</span> <span class="k">for</span> <span class="n">time_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">time_steps</span><span class="p">)])</span>

        <span class="c1"># update state-value function</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">time_step</span><span class="p">,</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">episode</span><span class="p">):</span>

            <span class="k">if</span> <span class="n">visited</span><span class="p">[</span><span class="n">state</span><span class="p">]:</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">visited</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">returns</span> <span class="o">=</span> <span class="n">episode</span><span class="p">[</span><span class="n">time_step</span><span class="p">:,</span> <span class="mi">2</span><span class="p">]</span>
            <span class="n">remain_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">returns</span> <span class="o">*</span> <span class="n">discounts</span><span class="p">[:</span><span class="n">remain_steps</span><span class="p">])</span>

            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">V</span>
</pre></div>
</div>
<p>The following state-value function is calculated for the policy <img class="math" src="../../_images/math/cd89edb8d677e76ff2422b13ab39ad709e93db2e.svg" alt="\mu(s)"/>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.03475</span><span class="o">|</span><span class="mf">0.02148</span><span class="o">|</span><span class="mf">0.04602</span><span class="o">|</span><span class="mf">0.02590</span><span class="o">|</span>
<span class="mf">0.05337</span><span class="o">|</span><span class="mf">0.00000</span><span class="o">|</span><span class="mf">0.09429</span><span class="o">|</span><span class="mf">0.00000</span><span class="o">|</span>
<span class="mf">0.11221</span><span class="o">|</span><span class="mf">0.28622</span><span class="o">|</span><span class="mf">0.30213</span><span class="o">|</span><span class="mf">0.00000</span><span class="o">|</span>
<span class="mf">0.00000</span><span class="o">|</span><span class="mf">0.46326</span><span class="o">|</span><span class="mf">0.64936</span><span class="o">|</span><span class="mf">0.00000</span><span class="o">|</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="policy-improvement-and-control">
<h3>Policy Improvement and Control<a class="headerlink" href="#policy-improvement-and-control" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id2">
<h4>Theory<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>The value iteration algorithm that we applied in the dynamic programming section used the following update step.</p>
<div class="math">
<p><img src="../../_images/math/5debc259bdb13224f7a5d5fcf4d0bbf97d9ef4c9.svg" alt="\begin{align*}
v_{k+1}(s) &amp; \doteq \max_a \mathbb{E}[R_{t+1} + \gamma v_k (S_{t+1}) \mid S_t = s, A_t = a] \\
&amp; = \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_k (s')]
\end{align*}"/></p>
</div><p>This exact update step is not going to work with Monte Carlo methods, because that would require the full knowledge of the model. We would have to know the transition probabilities from state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> to state <img class="math" src="../../_images/math/c06a68d1f33b6af4fc737ed4bbc5da38ad2176dc.svg" alt="s'"/> and the corresponding reward.</p>
<p>If we look closely at the above expression, we should notice that we can rewrite the update rule in terms of an action-value function?</p>
<div class="math">
<p><img src="../../_images/math/da8f6bec67b0ab131bdca5895dbb60dead6554ee.svg" alt="\begin{align*}
v_{k+1}(s) &amp; \doteq \max_a \mathbb{E}[R_{t+1} + \gamma v_k (S_{t+1}) \mid S_t = s, A_t = a] \\
&amp; = \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_k (s')] \\
&amp; = \max_a q_k(s, a) \\
\end{align*}"/></p>
</div><p>With those rewrites we do not require the knowledge of the model, but it becomes obvious that the key is to estimate the action-value function and not the state-value function. Having an estimate of an action-value function allows the agent to select better actions by acting greedily and to gradually improve the policy towards the optimal policy. To estimate the action-value function we will still generate episodes and compute averages, but the averages are not going to be for a state, but for a state-action pair.</p>
<p>There is still one problem that we face without the knowledge of the model of the MDP though. If our policy is fully deterministic and thus avoids some state-action pairs by design, then we can not compute a good estimate for certain state-action pairs and thus might not arrive at the optimal policy. The solution is to use an <img class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg" alt="\epsilon"/>-greedy policy, meaning that with a probability of <img class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg" alt="\epsilon"/> we take a random action and with probability of <img class="math" src="../../_images/math/8ca63b57ca74b73571f69b8c297a6939525544ad.svg" alt="1-\epsilon"/> we take the greedy action. That way we are guaranteed that all state-action pairs are going to be visited.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>On-policy</strong> methods improve the same policy that is used to generate the trajectory.</p>
<p><strong>Off-policy</strong> methods improve a policy that is different from the one that is used to generate the trajectory.</p>
</div>
<p>Before we move on to the implementation of the Monte Carlo control algorithm it is important to discuss the difference between on-policy and off-policy methods. Once the need arises to explore the environment we could ask ourselves, “Do we need to improve the same policy that is used to generate actions or can we learn the optimal policy while using the data that was produced by a different policy?”. To frame the question differently “Is it possible to learn the optimal policy while only selecting random actions?”. That depends on the design of the algorithm. On-policy methods improve the same policy that is also used to generate the actions, while off-policy methods improve a policy that is not the one that is used to generate the trajectories. The algorithm that is covered below is an on-policy algorithm.</p>
</div>
<div class="section" id="id3">
<h4>Algorithm<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>Once again the algorithm follows a two step approach.</p>
<ul class="simple">
<li><p>In the first step the agent generates the trajectory using the <img class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg" alt="\epsilon"/>-greedy policy.</p></li>
<li><p>In the second step the agent iterates over the trajectory and uses Monte Carlo and incremental improvement to estimate the action-values.</p></li>
</ul>
<div class="math">
<p><img src="../../_images/math/85dad09417e3e843ac343b5971180a182ea07732.svg" alt="\begin{algorithm}[H]
    \caption{Monte Carlo Control: First Visit}
    \label{alg1}
\begin{algorithmic}
    \STATE Input: environment $env$, state set $\mathcal{S}$, action set $\mathcal{A}$, number of episodes, learning rate $\alpha$, discount factor $\gamma$, epsilon $\epsilon$
    \STATE Initialize:
    \STATE $Q(s, a)$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$ with zeros
    \STATE
    \STATE policy $\pi(a \mid s)$ for all $a \in \mathcal{A}$, where $A \sim \pi(. \mid s)$
    \STATE $r \leftarrow$ random number
    \IF {$r &lt; \epsilon$}
        \STATE $A \leftarrow$ random action
    \ELSE
    \STATE $A \leftarrow \arg\max_aQ(a)$
    \ENDIF
    \STATE
    \FOR{$i=0$ to number of episodes}
        \STATE ...................................................................................................................................
        \STATE (1) INTERACTION WITH THE ENVIRONMENT
        \STATE create $trajectory$ as empty list [...]
        \REPEAT
            \STATE Generate experience $tuple$ $(State, Action, Reward)$ using policy $\pi$ and MDP $env$
            \STATE Push $tuple$ into $trajectory$
        \UNTIL{state is terminal}
        \STATE ...................................................................................................................................
        \STATE (2) ESTIMATION OF VALUE FUNCTION
        \STATE Create Visited(s) = $False$ for all $s \in \mathcal{S}$
        \FOR{$t=0$ to number of tuples in $trajectory$ $T$}
            \STATE $s \leftarrow$ state from $trajectory[t]$
            \STATE $a \leftarrow$ action from $trajectory[t]$
            \IF {Visited($s$) is True}
                \STATE go to next tuple
            \ELSE
                \STATE Visited($s$) = True
            \ENDIF
            \STATE $G_{t:T} = \sum_{k=t}^T \gamma^{k-t}R_t$
            \STATE $Q(s, a) = Q(s, a) + \alpha [G_{t:T} - Q(s, a)]$
        \ENDFOR
    \ENDFOR
    \STATE
    \STATE RETURN policy, Q(s, a)
\end{algorithmic}
\end{algorithm}"/></p>
</div></div>
<div class="section" id="id4">
<h4>Implementation<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>The python implementation follows directly from the above described algorithm.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mc_control</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>

    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>

    <span class="c1"># generate policy</span>
    <span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">action</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>

        <span class="c1"># generate episode</span>
        <span class="n">episode</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">experience</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
            <span class="n">episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># makes an array with nr. timesteps as rows and 3 columns</span>
        <span class="n">episode</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="n">time_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>

        <span class="c1"># discounts</span>
        <span class="n">discounts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gamma</span><span class="o">**</span><span class="n">time_step</span> <span class="k">for</span> <span class="n">time_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">time_steps</span><span class="p">)])</span>

        <span class="c1"># update action-value function</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">time_step</span><span class="p">,</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">episode</span><span class="p">):</span>

            <span class="k">if</span> <span class="n">visited</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]:</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">visited</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">returns</span> <span class="o">=</span> <span class="n">episode</span><span class="p">[</span><span class="n">time_step</span><span class="p">:,</span> <span class="mi">2</span><span class="p">]</span>
            <span class="n">remain_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">returns</span> <span class="o">*</span> <span class="n">discounts</span><span class="p">[:</span><span class="n">remain_steps</span><span class="p">])</span>

            <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">])</span>

    <span class="n">policy_mapping</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">policy_mapping</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">Q</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sources">
<h2>Sources<a class="headerlink" href="#sources" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Monte_Carlo_method">https://en.wikipedia.org/wiki/Monte_Carlo_method</a></p>
</dd>
</dl>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="td_learning.html" title="next page">Temporal Difference Learning</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>