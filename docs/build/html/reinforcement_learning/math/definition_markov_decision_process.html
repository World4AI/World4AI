
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Definition of a Markov Decision Process &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="To Solve an MDP" href="to_solve_mdp.html" />
    <link rel="prev" title="Agent and Environment Components" href="../introduction/components.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mathematics of Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="to_solve_mdp.html">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-a-stochastic-process-to-an-mdp">
   From a Stochastic Process to an MDP
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-process">
     Stochastic Process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chain">
     Markov Chain
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mdp">
     MDP
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mdp-as-a-tuple">
   MDP as a Tuple
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#frozen-lake">
     Frozen Lake
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathcal-s-states">
     <img alt="\mathcal{S}" class="math" src="../../_images/math/3f98591dff3521c50335f61964d0bab6fb82df24.svg"/>
     : States
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathcal-a-actions">
     <img alt="\mathcal{A}" class="math" src="../../_images/math/8d026053f0eefe613ccf100261807688868d9c3a.svg"/>
     : Actions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#p-transitions">
     <img alt="P" class="math" src="../../_images/math/252a21b8f2dc53f52d666f0f84cfdd74de3a1684.svg"/>
     : Transitions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-rewards">
     <img alt="R" class="math" src="../../_images/math/91b1ab42e2af8861f5db27a19ac6b74c73e7004c.svg"/>
     : Rewards
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gamma-discounts">
     <img alt="\gamma" class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg"/>
     : Discounts
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="definition-of-a-markov-decision-process">
<h1>Definition of a Markov Decision Process<a class="headerlink" href="#definition-of-a-markov-decision-process" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>In order to find a solution to a reinforcement learning problem the first step should be to formalize the problem in a mathematical framework. This allows researchers and practitioners to study the properties of the problem and to prove that certain types of solutions are guaranteed to have certain properties. The tool that is commonly used for those purposes is the Markov decision process (MDP).</p>
<p>Many of the components of a Markov Decision Process (MDP) were already introduced in one of the previous chapters. The main focus of those chapters was the building of an intuitive foundation. In the following chapters I am going to reiterate the already introduced material, but the focus is going to be on the more formal explanations of MDPs. The mathematics that is going to be introduced in the next chapters will form the basis of much of reinforcement learning.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A Markov Decision Process (MPD) is a formal description of a sequential decision problem with uncertainty.</p>
</div>
<p>In essence an MDP allows us to formalize the interaction loop between the agent and the environment, where the actions of the agent influence future states/rewards and the agent might have to decide to forego the current reward to get higher rewards in the future. The common assumption in reinforcement learning is the existence of an MDP at the core of each environment.</p>
<div class="figure align-center" id="id1">
<img alt="../../_images/mdp.svg" src="../../_images/mdp.svg" /><p class="caption"><span class="caption-text">Interaction Loop.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>The interaction is done sequentially, where the agent and the environment take turns to react to each other. Each iteration of actions, rewards and states happens in a period of time, called a time step, <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/>. The time step is a discrete variable starting at 0 and increasing by 1 after each iteration. During the first time step the agent receives the initial state of the environment <img class="math" src="../../_images/math/e51aa575dbe30b7827fdbf72d9456f1ef701d3ac.svg" alt="S_0"/> and reacts accordingly with the action <img class="math" src="../../_images/math/faab6b00d1f6be3243ddf4965f76de7679a0d083.svg" alt="A_0"/>. The environment transitions into a new state <img class="math" src="../../_images/math/9333d3e99a561226a06c410e3d0e0728ae031c31.svg" alt="S_1"/> and generates the reward <img class="math" src="../../_images/math/044e55b6408cf4698a7a2d6dde05cafc9dff7840.svg" alt="R_1"/>. The agent in turn reacts with the action <img class="math" src="../../_images/math/f1612b7c64291ca2cb4da785bbcb703ca3f2ef74.svg" alt="A_1"/> and the interaction continues. The general notation of writing States, Actions and Rewards is <img class="math" src="../../_images/math/32521b1075952d9b867fc6404714b24fcf4bb62a.svg" alt="S_t, A_t, R_t"/> where the subscript <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> represents a particular time step.</p>
</div>
<div class="section" id="from-a-stochastic-process-to-an-mdp">
<h2>From a Stochastic Process to an MDP<a class="headerlink" href="#from-a-stochastic-process-to-an-mdp" title="Permalink to this headline">¶</a></h2>
<p>A Markov decision process consists of three parts. It involves a stochastic <strong>process</strong>, it abides by the <strong>markov</strong> property and there is the possibility to influence the states through <strong>decisions</strong>.</p>
<div class="section" id="stochastic-process">
<h3>Stochastic Process<a class="headerlink" href="#stochastic-process" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A stochastic or random process can be defined as a sequence of random variables.</p>
</div>
<div class="figure align-center" id="id2">
<img alt="../../_images/bernoulli.svg" src="../../_images/bernoulli.svg" /><p class="caption"><span class="caption-text">Bernoulli Process.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>In the above image there are no actions or rewards yet and the state evolves randomly over time following a stochastic process. There are two distinct states that the process can be in, the 1 and the 0. Each of the states can be reached  with 50% probability and the new state does not depend on any previous states. In essence this random process corresponds to a sequence of coin tosses where for example heads would correspond to 0 and tails to 1. This is a Bernoulli process.</p>
<p>For the above process the following can be said: <img class="math" src="../../_images/math/a2ed04293bf13c9a4ca14f4a74ab5df4a70ca679.svg" alt="Pr(S_{t+1} \mid S_t) = Pr(S_{t+1})"/></p>
<p><img class="math" src="../../_images/math/519aaabd716fadd2f9834fa0a6b931eb6f9aafd0.svg" alt="Pr(S_{t+1})"/> is the probability that a certain state will be tossed, in the above case <img class="math" src="../../_images/math/d8b8d98a0ab1eb793493f3d6e5ccbc96f776262b.svg" alt="Pr(S_{t+1}=HEADS) = 0.5"/> and <img class="math" src="../../_images/math/7ae040e0a912276227b520d237770d3a1efb6298.svg" alt="Pr(S_{t+1}=TAILS)=0.5"/>. <img class="math" src="../../_images/math/3d35ccd93e96eebe5c6957403425b63e5d7c8bab.svg" alt="Pr(S_{t+1} \mid S_t)"/>, reads as x given y, depicts a conditional probability, where the probability of being in the new state <img class="math" src="../../_images/math/b39ba9f9a36260a7dfcdeb1a29ac6278f97267a2.svg" alt="S_{t+1}"/> depends on the current state <img class="math" src="../../_images/math/edea1c48e2215a6477d60bb7cb19ff85b7474f4c.svg" alt="S_t"/>. For example <img class="math" src="../../_images/math/73b27bed7295ffe2497ec3cc9ae9e3d46e0a92fa.svg" alt="Pr(S_{t+1}=HEADS|S_t=TAILS)"/> shows the probability of a coin toss having a value of HEADS when the previous toss had a value of TAILS. When you consider a coin toss, then the new occurrence of either heads or tails does not depend on the previous specific value of the toss. The events are independent. <img class="math" src="../../_images/math/0454645daa922256f3b4e1fe0de4d30e7204fb8a.svg" alt="Pr(S_{t+1} \mid S_t) = Pr(S_t)"/> means that knowing the last value of a coin toss does not give us any more knowledge regarding the future toss. <img class="math" src="../../_images/math/26f29ead250952cf55712d7ba6774695542f1a81.svg" alt="Pr(S_{t+1} \mid S_t) = Pr(S_t) = 0.5"/>.</p>
<table class="table" id="id3">
<caption><span class="caption-text">Bernoulli Process</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Process</p></td>
<td><p>yes</p></td>
</tr>
<tr class="row-even"><td><p>Markov</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-odd"><td><p>Decisions</p></td>
<td><p>no</p></td>
</tr>
</tbody>
</table>
<p>The Bernoulli process is a stochastic process as the states evolve randomly but to be considered an MDP it needs to be Markovian. Furthermore there is no way to influence the environment through actions.</p>
</div>
<div class="section" id="markov-chain">
<h3>Markov Chain<a class="headerlink" href="#markov-chain" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A Markov chain is a stochastic process that has the Markov property.</p>
<p>Markov Property: <img class="math" src="../../_images/math/f55ba384a30888c8309addad7d8b1bef5195c7c8.svg" alt="Pr[S_{t+1} \mid S_t] = Pr[S_{t+1} \mid S_1, .... , S_t]"/></p>
<p>The Markov property, or memorylessness, means that the next state only depends on the current state and not the states before that.</p>
</div>
<div class="figure align-center" id="id4">
<img alt="../../_images/markov_property.svg" src="../../_images/markov_property.svg" /><p class="caption"><span class="caption-text">Markov Property.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>Unlike in the coin toss example, in a Markov chain the probability to be in the state <img class="math" src="../../_images/math/b39ba9f9a36260a7dfcdeb1a29ac6278f97267a2.svg" alt="S_{t+1}"/> depends on previous states, but only the most recent state, <img class="math" src="../../_images/math/edea1c48e2215a6477d60bb7cb19ff85b7474f4c.svg" alt="S_t"/>, is relevant. The above image shows that the new state depends on the previous state, while the previous information, the states that came before that, is irrelevant. The markov property is extremely convenient, as only the most recent events need to be tracked, which allows for more tractable computations.</p>
<div class="figure align-center" id="id5">
<img alt="../../_images/markov_chain.svg" src="../../_images/markov_chain.svg" /><p class="caption"><span class="caption-text">Markov Chain.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>Each of the color coded circles above represents a state, while the numeric values near the arrows represent the transition probabilities from one state to another state. For example when the state is yellow there is a 20% chance to land in the red state and an 80% chance to land in the grey state. What state came before the yellow state is absolutely irrelevant.</p>
<div class="figure align-center" id="id6">
<img alt="../../_images/markov_chain_unrolled.svg" src="../../_images/markov_chain_unrolled.svg" /><p class="caption"><span class="caption-text">An unfolded Markov Chain.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>If we unfold the process and follow it for a while, the above sequence might for example appear. The mathematical notation would look as follows.</p>
<div class="math">
<p><img src="../../_images/math/dfd42f497bcf4e5a5a4f6987babf462547f1dd50.svg" alt="S_0, S_1, S_2, S_3, S_4, ..., S_t"/></p>
</div><table class="table" id="id7">
<caption><span class="caption-text">Markov Chain</span><a class="headerlink" href="#id7" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Process</p></td>
<td><p>yes</p></td>
</tr>
<tr class="row-even"><td><p>Markov</p></td>
<td><p>yes</p></td>
</tr>
<tr class="row-odd"><td><p>Decisions</p></td>
<td><p>no</p></td>
</tr>
</tbody>
</table>
<p>The Markov chain is a stochastic process. The sequence of states develops randomly, but unlike the Bernoulli process which does not depend on any previous state the Markov process depends on the last and only on the last state. Still the agent can not take any actions that impact the environment.</p>
</div>
<div class="section" id="mdp">
<h3>MDP<a class="headerlink" href="#mdp" title="Permalink to this headline">¶</a></h3>
<p>A Markov chain can be extended to a Markov Decision Process with the introduction of rewards and actions. While in the case of a Markov chain the states evolve without any possibility of an influence on the environment, in the case of an MDP the agent has “agency” over his actions and gets rewards for his behaviour.</p>
<div class="figure align-center" id="id8">
<img alt="../../_images/markov_decision_process.svg" src="../../_images/markov_decision_process.svg" /><p class="caption"><span class="caption-text">A Markov Decision Process.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<p>The above image depicts an MDP with 4 states (yellow, red, grey and blue). The blue is the final state that the agent has to reach. The agent can influence the environment by taking one of the 2 available actions (the reddish and the bluish action). Landing in the first 3 states generates a negative reward (inner red circle), while landing in the blue state generates a positive reward (inner green circle).</p>
<p>The unrolled MDP forms a sequence of States, Actions and Rewards, called a trajectory.</p>
<div class="math">
<p><img src="../../_images/math/2eeefa3721753034a0a7f589849870a9afa3742f.svg" alt="S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, ..."/></p>
</div><table class="table" id="id9">
<caption><span class="caption-text">MDP</span><a class="headerlink" href="#id9" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Process</p></td>
<td><p>yes</p></td>
</tr>
<tr class="row-even"><td><p>Markov</p></td>
<td><p>yes</p></td>
</tr>
<tr class="row-odd"><td><p>Decisions</p></td>
<td><p>yes</p></td>
</tr>
</tbody>
</table>
<p>An MDP is a stochastic process as the generation of new states develops in a random fashion. It is Markov, as only the last state <img class="math" src="../../_images/math/edea1c48e2215a6477d60bb7cb19ff85b7474f4c.svg" alt="S_t"/> and the last action <img class="math" src="../../_images/math/7f3546078d71a3138f4636f7f7359e290f4ca71c.svg" alt="A_t"/> are necessary to calculate the new State <img class="math" src="../../_images/math/b39ba9f9a36260a7dfcdeb1a29ac6278f97267a2.svg" alt="S_{t+1}"/>. The agent can interact with the MDP to influence future states and rewards.</p>
</div>
</div>
<div class="section" id="mdp-as-a-tuple">
<h2>MDP as a Tuple<a class="headerlink" href="#mdp-as-a-tuple" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A tuple is a finite ordered list of elements</p>
</div>
<p>In more mathematical terms a Markov decision process is a 5-tuple, <img class="math" src="../../_images/math/2bad0a556ccdc9fbb4a2d612b078fbc76b9de743.svg" alt="(\mathcal{S, A}, P, R, \gamma)"/>. In the following sections we will take a look at each of the contents of the tuple individually.</p>
<div class="section" id="frozen-lake">
<h3>Frozen Lake<a class="headerlink" href="#frozen-lake" title="Permalink to this headline">¶</a></h3>
<div class="figure align-center" id="id10">
<img alt="../../_images/frozen_lake.svg" src="../../_images/frozen_lake.svg" /><p class="caption"><span class="caption-text">The Frozen Lake Environment.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>To explain the contents of the tuple I will introduce a new environment. “Frozen Lake” is a beginner level environment, suited well to explain the underlying components of an MDP. As the name of the environment suggests, the surface of the lake is frozen. This causes the surface to be either safe, but slippery or not safe at all. The player starts at the top left corner (indicated by the letter S as in Start). The goal of the environment is to reach the bottom right corner (indicated by the letter G as in Goal). The safe surface comprises the F (F as in Frozen) and the G cells. The unsafe surface is indicated by the H (H as in Hole) cells. The reward is in most cases 0, unless the agent reaches the goal where he achieves a reward of 1. The agent can move in 4 directions. When the agent tries to move into the direction of the wall the resulting state is the same as the previous state. The environment itself is stochastic. When the agent chooses an action in ⅓ of the cases the environment moves the player in that direction, while in ⅔ of the cases the player is moved left or right of the desired direction (divided equally).</p>
<div class="figure align-center" id="id11">
<img alt="../../_images/orthogonal.svg" src="../../_images/orthogonal.svg" /><p class="caption"><span class="caption-text">Moving in the Frozen Lake Environment.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="mathcal-s-states">
<h3><img class="math" src="../../_images/math/3f98591dff3521c50335f61964d0bab6fb82df24.svg" alt="\mathcal{S}"/>: States<a class="headerlink" href="#mathcal-s-states" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><img class="math" src="../../_images/math/3f98591dff3521c50335f61964d0bab6fb82df24.svg" alt="\mathcal{S}"/> is the set of all legal states</p>
</div>
<div class="figure align-center" id="id12">
<img alt="../../_images/frozen_lake_states.svg" src="../../_images/frozen_lake_states.svg" /><p class="caption"><span class="caption-text">States in the Frozen Lake Environment.</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>In the Frozen Lake environment the set of all allowed states looks as follows <img class="math" src="../../_images/math/fdd7d9942a759071db41663a7d4e8f6f9cd2f5e3.svg" alt="\mathcal{S} = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\}"/>. A number from the set corresponds to a location in the grid world and is not allowed to deviate from the set, e.g. the environment can not present the agent with a state that corresponds to the number 32.</p>
</div>
<div class="section" id="mathcal-a-actions">
<h3><img class="math" src="../../_images/math/8d026053f0eefe613ccf100261807688868d9c3a.svg" alt="\mathcal{A}"/>: Actions<a class="headerlink" href="#mathcal-a-actions" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><img class="math" src="../../_images/math/8d026053f0eefe613ccf100261807688868d9c3a.svg" alt="\mathcal{A}"/> is the set of all legal actions</p>
</div>
<div class="figure align-center" id="id13">
<img alt="../../_images/frozen_lake_actions.svg" src="../../_images/frozen_lake_actions.svg" /><p class="caption"><span class="caption-text">Actions in the Frozen Lake Environment.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p>In the Frozen Lake environment the set of all legal actions is <img class="math" src="../../_images/math/b1fe97a1eae5ac11f210936c99c49e57bf2e4110.svg" alt="\mathcal{A} = \{0, 1, 2, 3\}"/>, where</p>
<ul class="simple">
<li><p>0 = Left</p></li>
<li><p>1 = Down</p></li>
<li><p>2 = Right</p></li>
<li><p>3 = Up</p></li>
</ul>
<p>From the set of actions you can recognize that movement in diagonal directions or jumping over several cells is not allowed.</p>
</div>
<div class="section" id="p-transitions">
<h3><img class="math" src="../../_images/math/252a21b8f2dc53f52d666f0f84cfdd74de3a1684.svg" alt="P"/>: Transitions<a class="headerlink" href="#p-transitions" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><img class="math" src="../../_images/math/252a21b8f2dc53f52d666f0f84cfdd74de3a1684.svg" alt="P"/> is the transition model.</p>
<p><img class="math" src="../../_images/math/4ed8e555d76a2ef17e8acdca0d798a709965675c.svg" alt="P(s' \mid s, a) \doteq Pr[S_{t+1}=s' \mid S_t=s, A_t=a]"/></p>
<p>The transition model is the function  that calculates the probability of landing in some state <img class="math" src="../../_images/math/c06a68d1f33b6af4fc737ed4bbc5da38ad2176dc.svg" alt="s'"/> at timestep <img class="math" src="../../_images/math/4268b7f9a6a5266438bd62c5426c082aefba1657.svg" alt="t+1"/> when at timestep <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> the state corresponds to <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and the action taken by the agent is <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/>.</p>
</div>
<div class="figure align-center" id="id14">
<img alt="../../_images/frozen_lake_transition.svg" src="../../_images/frozen_lake_transition.svg" /><p class="caption"><span class="caption-text">Transision Model in the Frozen Lake Environment.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p>In the above example the state is marked by the yellow circle, which corresponds to the state with the value of 6. If the agent takes the action 1 (going down), the transition function will calculate the following results.</p>
<ul class="simple">
<li><p><img class="math" src="../../_images/math/0bcb84b2df5ac5075ca6d8cf78979a1f635ac715.svg" alt="P(5 \mid 6, 1) = 1/3 = 33.3\%"/>, chance of moving left</p></li>
<li><p><img class="math" src="../../_images/math/5c2b2ade80087e1b5af8cbb62fd865887c865ee9.svg" alt="P(7 \mid 6, 1) = 1/3 = 33.3\%"/>, chance of moving right</p></li>
<li><p><img class="math" src="../../_images/math/5998561b1e0cf0918b999a79cede823358c6663c.svg" alt="P(10 \mid 6, 1) = 1/3 = 33.3\%"/>, chance of moving down</p></li>
</ul>
</div>
<div class="section" id="r-rewards">
<h3><img class="math" src="../../_images/math/91b1ab42e2af8861f5db27a19ac6b74c73e7004c.svg" alt="R"/>: Rewards<a class="headerlink" href="#r-rewards" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><img class="math" src="../../_images/math/91b1ab42e2af8861f5db27a19ac6b74c73e7004c.svg" alt="R"/> is the reward model.</p>
<p><img class="math" src="../../_images/math/39b2b1cd3f2f37cdc1190b05d141e67cf06ede67.svg" alt="R(s,a) \doteq \mathbb{E}[R_{t+1} \mid S_{t}=s, A_{t}=a]"/></p>
<p>The reward model is the function that calculates the expected value of the reward given state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> and action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> at time step <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/>.</p>
</div>
<div class="figure align-center" id="id15">
<img alt="../../_images/frozen_lake_reward.svg" src="../../_images/frozen_lake_reward.svg" /><p class="caption"><span class="caption-text">Reward Model in the Frozen Lake Environment.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>In the above image the state of the environment equals 14. The agent selects to move right (action 2).</p>
<div class="math">
<p><img src="../../_images/math/5de06a9543bd06a1fea630417a16b6fe47e4b1c1.svg" alt="R(14, 2) = \mathbb{E}[R_{t+1} \mid S_t = 14, A_t = 2] = 0 * 1/3 + 0 * 1/3 + 1 * 1/3 = 1/3."/></p>
</div><p>The expected reward equals 1/3 when the agent takes the action 2 in the state 14.</p>
</div>
<div class="section" id="gamma-discounts">
<h3><img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/>: Discounts<a class="headerlink" href="#gamma-discounts" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/> (gamma) is the discount factor, where <img class="math" src="../../_images/math/4ae96783164c48860f2b53f5fa38f75c60e6f47b.svg" alt="0 \leq \gamma \leq 1"/>.</p>
<p>Gamma is used to calculate the current value of future rewards.</p>
</div>
<p>Consider the following example. You can get 1000$ now or 1000$ in 10 years. What would you choose? The answer is hopefully 1000$ now. The reason every rational agent would choose 1000$ is the time value of money or generally speaking the time value of rewards. In the case of dollars you could invest the money for 10 years and get an amount that is larger. Therefore there should be a compensation if the agent decides to delay his reward. The gamma or discount factor is used to adjust the value of rewards. Future rewards are considered of less value.</p>
<p>The value of rewards from the perspective of the agent at time step <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> is as following:</p>
<ul class="simple">
<li><p>The value of a reward received at timestep <img class="math" src="../../_images/math/4268b7f9a6a5266438bd62c5426c082aefba1657.svg" alt="t+1"/> is <img class="math" src="../../_images/math/35186e8179eac89e5387e6ebed6fffbd7db8e43d.svg" alt="\gamma^0 * R_{t+1}"/></p></li>
<li><p>The value of a reward received at timestep <img class="math" src="../../_images/math/a919e3930d3757b15d2d37c81767d6195338a296.svg" alt="t+2"/> is <img class="math" src="../../_images/math/6c959c675c82739beb626b18c4ea4ddfe32d2ccf.svg" alt="\gamma^1 * R_{t+2}"/></p></li>
<li><p>The value of a reward received at timestep <img class="math" src="../../_images/math/c3af1726285a483b0b29c92cfa75285ad8556064.svg" alt="t+3"/> is <img class="math" src="../../_images/math/89742b6245dc25dc485824c83ad4f61880a5f748.svg" alt="\gamma^2 * R_{t+3}"/></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Episodic tasks are tasks that have a natural ending.</p>
<p>Continuing tasks are tasks that do not have a natural ending and may theoretically go on forever.</p>
</div>
<p>Mathematically speaking if you are dealing with episodic tasks, like the Frozen Lake environment,  then the discount factors are not strictly required. For continuing tasks a discount factor is required. The reason for that is the need for the agent to maximize the expected sum of future rewards. If the task is continuing then the sum of rewards might become infinite and the agent can not deal with that. If the value of gamma is between 0 and 1 then the sum becomes finite.</p>
<p>Usually the value of gamma is between 0.9 and 0.99.</p>
<p>Let’s assume a gamma with a value of 0.9.</p>
<ul class="simple">
<li><p>At <img class="math" src="../../_images/math/c3af1726285a483b0b29c92cfa75285ad8556064.svg" alt="t+3"/>: the discount factor is <img class="math" src="../../_images/math/27b88799e4e8777ebfc2ed686585bf45ec460657.svg" alt="\gamma^2 = 0.81"/></p></li>
<li><p>At <img class="math" src="../../_images/math/d7fde224d271b9d6c35694c6633ffe99d174d280.svg" alt="t+5"/>: the discount factor is <img class="math" src="../../_images/math/5de5b96649463578abc33812e2adeea72774ce52.svg" alt="\gamma^4 = 0.66"/></p></li>
<li><p>At <img class="math" src="../../_images/math/d38368fd24b9416b2737d63e85ef0bc21f065672.svg" alt="t+11"/>: the discount factor is <img class="math" src="../../_images/math/bf3f1e0f819624a5465d2dee180a83992fd6b1fe.svg" alt="\gamma^{10} = 0.35"/></p></li>
<li><p>At <img class="math" src="../../_images/math/e2ba5a34f153b6787278e562a8348bb530e29beb.svg" alt="t+21"/>: the discount factor is <img class="math" src="../../_images/math/a5a48badb280260f360280abe5c9ba58a669df75.svg" alt="\gamma^{20} = 0.12"/></p></li>
<li><p>At <img class="math" src="../../_images/math/97c7d044e3dfc6c0ebadc1f7fb544084e0de3a35.svg" alt="t+51"/>: the discount factor is <img class="math" src="../../_images/math/460b9493a5ca6582be4771cd80b9950287c4a631.svg" alt="\gamma^{50} = 0.005"/></p></li>
</ul>
<p>The discount factor keeps approaching 0, which makes the value of rewards in the far future almost 0. That prevents an infinite sum of rewards.</p>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="../introduction/components.html" title="previous page">Agent and Environment Components</a>
    <a class='right-next' id="next-link" href="to_solve_mdp.html" title="next page">To Solve an MDP</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>