
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>To Solve an MDP &#8212; World4AI 0.1 documentation</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Introduction" href="../dynamic_programming/introduction.html" />
    <link rel="prev" title="Definition of a Markov Decision Process" href="definition_markov_decision_process.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="../../index.html">
<p class="title">World4AI</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  Reinforcement Learning
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/World4AI/World4AI" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/applications.html">
   Reinforcement Learning Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/agent_env.html">
   Agent and Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/properties.html">
   Properties of Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/interaction.html">
   States, Actions, Rewards
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/exploration_vs_exploitation.html">
   Exploration vs Exploitation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/components.html">
   Agent and Environment Components
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mathematics of Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="definition_markov_decision_process.html">
   Definition of a Markov Decision Process
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   To Solve an MDP
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/policy_iteration.html">
   Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/value_iteration.html">
   Value Iteration
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Exploration Exploitation Tradeoff
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/bandits.html">
   Bandits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exploration_exploitation_tradeoff/epsilon_greedy.html">
   Epsilon(
   <img alt="\epsilon" class="math" src="../../_images/math/13b33819efce3da0df3eb849f77d88d02df50c45.svg"/>
   )-Greedy
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tabular Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/monte_carlo_methods.html">
   Monte Carlo Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tabular_reinforcement_learning/td_learning.html">
   Temporal Difference Learning
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning Libraries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/motivation.html">
   Motivation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl_libraries/openai_gym.html">
   OpenAI Gym
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#return">
   Return
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy">
   Policy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#value-functions">
   Value Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bellman-equations">
   Bellman Equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimality">
   Optimality
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bellman-optimality-equations">
   Bellman Optimality Equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="to-solve-an-mdp">
<h1>To Solve an MDP<a class="headerlink" href="#to-solve-an-mdp" title="Permalink to this headline">¶</a></h1>
<p>Once a particular MDP, the components of the tuple <img class="math" src="../../_images/math/2bad0a556ccdc9fbb4a2d612b078fbc76b9de743.svg" alt="(\mathcal{S, A}, P, R, \gamma)"/>,  has been defined the next logical step would be to solve the MDP. In this chapter I am going to explain what it actually means to solve an MDP. In future chapters I will try to explore possible approaches to a solution.</p>
<div class="section" id="return">
<h2>Return<a class="headerlink" href="#return" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Episodic Tasks</strong>:</p>
<p>In episodic tasks the return is the sum of rewards in a single episode starting from time step <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> and going up to the terminal time step <img class="math" src="../../_images/math/573c03b1afb9413354760f358c959354ca9577f0.svg" alt="T"/>.</p>
<div class="math">
<p><img src="../../_images/math/9ac5c92385dd4e87cb218b3d82123cecabc16811.svg" alt="G_t = R_{t+1} + R_{t+2} + … + R_T,"/></p>
</div><p><strong>Continuing Tasks</strong>:</p>
<p>In continuing tasks the return is the sum of rewards starting at time step t and going to possibly infinity, <img class="math" src="../../_images/math/3cc526b72c8187aa8469062697490fd8aff88799.svg" alt="T = \infty"/>.</p>
<div class="math">
<p><img src="../../_images/math/9ef7f228741369f942ec8bb0612a5417e0e2b7a9.svg" alt="G_t = R_{t+1} + R_{t+2} + R_{t+3} + …  = \sum_{k=0}^\infty{R_{k+t+1}}"/></p>
</div></div>
<p>In order to simplify notation I will introduce the notion of a return <img class="math" src="../../_images/math/7674515a21e3ea004ba30a13c5d826d6a407810d.svg" alt="G"/>. A return is simply the sum of rewards starting from some time <img class="math" src="../../_images/math/d6ee135adec5f7da4630191266c918e5c8225a75.svg" alt="t"/> and going either to some terminal state <img class="math" src="../../_images/math/573c03b1afb9413354760f358c959354ca9577f0.svg" alt="T"/> or to infinity. The letter <img class="math" src="../../_images/math/7674515a21e3ea004ba30a13c5d826d6a407810d.svg" alt="G"/> stands for “Goal”, because the goal of the environment is encoded in the rewards.</p>
<div class="figure align-center" id="id1">
<img alt="../../_images/frozen_lake_return.svg" src="../../_images/frozen_lake_return.svg" /><p class="caption"><span class="caption-text">Frozen Lake Return.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>In order to calculate the full return of an episode we have to play through the sequence of states, actions and rewards all the way through from the initial state to the terminal state. Let us assume that the sequence of states and the corresponding rewards played out as indicated in the image above. The agent received the reward of 0 for getting to the non terminal states and a reward of 1 for getting to the terminal state at the bottom right corner. Altogether there were 9 time steps starting from the initial time step t = 0 and ending with the timestep T = 8.</p>
<div class="math">
<p><img src="../../_images/math/9f6cca9c30f8241107b878b3cb8ae412bb36a66b.svg" alt="\begin{align*}
G_0 &amp; = R_1 + R_2 + R_3 + R_4 + R_5 + R_6 + R_7 + R_8 \\
&amp; = 0 + 0 + 0 + 0 + 0 + 0 + 0 + 1 = 1
\end{align*}"/></p>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Discounted Return</p>
<div class="math">
<p><img src="../../_images/math/435bef738adcdc6bc8326e0d9ef8a22e2d74c6e7.svg" alt="G_t = R_{t+1} + \gamma{R_{t+2}} + \gamma^2{R_{t+3}} + …  = \sum_{k=0}^\infty{\gamma^k{R_{k+t+1}}}"/></p>
</div></div>
<p>To avoid an infinite return, future rewards (in continuing tasks) are discounted by <img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/>. Episodic tasks use discounting to emphasize the time value of rewards. Looking at the same example from above the return <img class="math" src="../../_images/math/fc1028873348e8cbf12e886652684ff245017ebe.svg" alt="G_0"/> looks as follows when we assume a gamma <img class="math" src="../../_images/math/355a306aef281c7020403b8261b86eb28463dcbc.svg" alt="\gamma"/> of <em>0.9</em>.</p>
<div class="math">
<p><img src="../../_images/math/27ae44cfb4a638058b883477186d567155082128.svg" alt="\begin{align*}
G_0 &amp; = R_1 + 0.9R_2 + 0.9^2 R_3 + 0.9^3 R_4 + 0.9^4 R_5 + 0.9^5 R_6 + 0.9^6 R_7 + 0.9^7 R_8 \\
&amp; = 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0.48 = 0.48
\end{align*}"/></p>
</div><p>In the example without discounting the return would always be 1 as long as the agent reaches the goal state. That means that it is generally not important how many steps the agent takes. This is not the case with discounting. Taking the shorter route means getting a higher return, as the time value of a reward reduces the farther away it is in the future. That essentially means that in the above example discounting generally encourages the agent to take as few steps as possible.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The goal of the agent is to maximize the expected (discounted) sum of rewards. In other words the goal of the agent to maximize the expected return <img class="math" src="../../_images/math/92f197498d79ecf351250c0b5d929e1d82cdf600.svg" alt="\mathbb{E}(G_t)"/>.</p>
</div>
<p>To solve an MDP means for the agent to maximize the expected return.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math">
<p><img src="../../_images/math/a086d9dc62f7236e105f63cff3de4112405a03eb.svg" alt="\begin{align*}
G_t &amp; = R_{t+1} + \gamma{R_{t+2}} + \gamma^2{R_{t+3}} + … \\
&amp; = R_{t+1} + \gamma{(R_{t+2} + \gamma{R_{t+3}} + ...)} \\
&amp; = R_{t+1} + \gamma{G_{t+1}}
\end{align*}"/></p>
</div></div>
<p>An important property of returns is that they can be expressed in terms of future returns.</p>
<p>Using this property we get the same result as above.</p>
<div class="math">
<p><img src="../../_images/math/fcd3a611403d8499633691a93b0a2eeb520f4dca.svg" alt="\begin{align*}
&amp; G_0 = R_1 + \gamma G_1 \\
&amp; R_1 = 0 \\
&amp; G_1 = R_2 + 0.9 R_3 + 0.9^2 R_4 + 0.9^3 R_5 + 0.9^4 R_6 + 0.9^5 R_7 + 0.9^6 R_8 = 0.53 \\
&amp; G_0 = 0 + 0.9 * 0.53 = 0.48
\end{align*}"/></p>
</div></div>
<div class="section" id="policy">
<h2>Policy<a class="headerlink" href="#policy" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math">
<p><img src="../../_images/math/6ac1c93b7c31ba613c3b52f1c14d83e9b8ecd6a8.svg" alt="\pi{(a \mid s)} = Pr[A_t = a \mid S_t = s]"/></p>
</div><p>A policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> is a mapping from a state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> to a probability of an action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/>.</p>
<p>For a deterministic policy <img class="math" src="../../_images/math/49a4c2bd9a832f216b21737ebfc9345a7ce1ad81.svg" alt="\pi{(a \mid s) = 1}"/> for for the selected action and <img class="math" src="../../_images/math/7fbc94d05334c9ed42513cd1dd5442955736c26d.svg" alt="\pi{(a \mid s) = 0}"/> for the rest of the actions. Therefore in deterministic environments a policy is often interpreted as a direct mapping from states to actions.</p>
<p><img class="math" src="../../_images/math/5f5a3fca1b0942506c9e168bd0c70768e57dd731.svg" alt="\pi{(. \mid S_t)}"/> is the distribution of actions given states.</p>
<p><img class="math" src="../../_images/math/64cac9897dc1ed3542ee295b1865f198074b2b08.svg" alt="A_t \sim \pi{(. \mid S_t)}"/></p>
<p>Actions are draws from a policy distribution, where in a deterministic case the same action is always drawn given the same state.</p>
</div>
<p>The policy of an agent determines the behaviour of the agent expressed in terms of actions based on the current state of the environment.</p>
<div class="figure align-center" id="id2">
<img alt="../../_images/frozen_lake_policy.svg" src="../../_images/frozen_lake_policy.svg" /><p class="caption"><span class="caption-text">Frozen Lake Policy.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>The above image shows the actions that would be generated by a deterministic policy based on the 15 distinct states.</p>
<table class="table" id="id3">
<caption><span class="caption-text">Policy for the frozen lake</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>State <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/67a6a5377794a4e8a68eb432c89e4dfad58d6309.svg" alt="\pi(left \mid s)"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/b21cc62056cecf73ae4aa781baddac060feb0ecd.svg" alt="\pi(top \mid s)"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/4da6d2fab3b50816df11a647d8c3a5903b1312f1.svg" alt="\pi(right \mid s)"/></p></th>
<th class="head"><p><img class="math" src="../../_images/math/661cb6aba385ffbbc6be9b474c2455600fcefc41.svg" alt="\pi(bottom \mid s)"/></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>The table is the policy that corresponds to the image above.</p>
</div>
<div class="section" id="value-functions">
<h2>Value Functions<a class="headerlink" href="#value-functions" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>State-Value Function:</p>
<div class="math">
<p><img src="../../_images/math/c82c7940fbc849f77b56bc41bfbbee17016d14a5.svg" alt="v_{\pi}(s) = \mathbb{E_{\pi}}[G_t \mid S_t = s]"/></p>
</div><p>Action-Value Function:</p>
<div class="math">
<p><img src="../../_images/math/6c9da184a6810d05a18a273efcd0611f3d7ce21d.svg" alt="q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_t \mid S_t = s, A_t = a]"/></p>
</div></div>
<p>Value functions map states or state-action pairs to “goodness” values, where goodness is expressed as the expected sum of rewards. Higher values mean more favorable states or state-action pairs.</p>
<p>The state-value function expresses the expected return when following a particular policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> given the state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/>. The action-value function expresses the expected return given the state <img class="math" src="../../_images/math/bd8c7c19971fab23be30458e8d4533d9ddb280d7.svg" alt="s"/> while taking the action <img class="math" src="../../_images/math/039a355730982dfbc0ab9905c990b653c7d9128c.svg" alt="a"/> in the current step and following the policy <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/> afterwards.</p>
</div>
<div class="section" id="bellman-equations">
<h2>Bellman Equations<a class="headerlink" href="#bellman-equations" title="Permalink to this headline">¶</a></h2>
<p>By using the properties of returns <img class="math" src="../../_images/math/fa309ec25c85e0173ceed1bd8bf5166ee63b704c.svg" alt="G_t"/> where each return can be expressed in terms of future returns <img class="math" src="../../_images/math/646765d5a05e05a0ccbc7b1f82c3772cc27a3fcf.svg" alt="G_t = r_{t+1} + \gamma G_{t+1}"/> we can arrive at recursive equations, where a value of a state can be defined in terms of values of the next state.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Bellman equation for the state-value function</p>
<div class="math">
<p><img src="../../_images/math/7f6a6feb49fe40719f565018c6691630e5d0cb70.svg" alt="\begin{align*}
v_{\pi}(s) &amp; = \mathbb{E_{\pi}}[G_t \mid S_t = s] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s]
\end{align*}"/></p>
</div><p>Bellman equation for the action-value function</p>
<div class="math">
<p><img src="../../_images/math/57c7916fc9b36e42ddc201b48ad47dbc5c774006.svg" alt="\begin{align*}
q_{\pi}(s, a) &amp; = \mathbb{E_{\pi}}[G_t \mid S_t = s, A_t = a] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
&amp; = \mathbb{E_{\pi}}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = a]
\end{align*}"/></p>
</div></div>
<p>Equations of the above form are called Bellman equations, named after the mathematician Richard E. Bellman. At the very first glance it might not seem like the equations add additional benefit to the definition of value functions, but the recursive relationships is what makes many of the reinforcement learning algorithms work.</p>
</div>
<div class="section" id="optimality">
<h2>Optimality<a class="headerlink" href="#optimality" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To solve an MDP is to find the optimal policy!</p>
</div>
<p>At the beginning of the chapter we asked ourselves what it means to solve a Markov decision process. The solution of an MDP means that the agent has learned an optimal policy function. Optimality implies that there is a way to compare different policies and to determine which of the policies is better.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Policies are evaluated in terms of their value functions</p>
<p><img class="math" src="../../_images/math/2203bb8ccc1c7023178692ea6c0ac5e4636bdf43.svg" alt="\pi \geq \pi’"/> if and only if <img class="math" src="../../_images/math/63dbc78b44693f7ca2514b7f4e7c5c0c6d5d1267.svg" alt="v_{\pi}(s) \geq v_{\pi'}(s)"/> for all <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/></p>
</div>
<p>In finite MDPs value functions are used as a metric of the goodness of a policy. The policy  pi is said to be better than the policy pi’ if and only if the value function of pi is larger or equal to the value function of policy pi’ for all states in the state set S.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The optimal policy <img class="math" src="../../_images/math/65f0bb840045c41b9051bc4ceac137de493ddc3a.svg" alt="\pi_*"/> is defined as</p>
<p><img class="math" src="../../_images/math/e3a642404e544540a05b9ca6a40ee097cc7e49db.svg" alt="\pi_* \geq \pi"/> for all <img class="math" src="../../_images/math/219b0d64dd3864293c420734a8924f4905a2a8d1.svg" alt="\pi"/></p>
</div>
<p>The optimal policy is the policy that is better (or at least not worse) than any other policy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The optimal state-value funtion:</p>
<p><img class="math" src="../../_images/math/cf5a0e11c9430d7fc0f4a13818ffc3280ce88c1e.svg" alt="v_*(s) = \max_{\pi} v_{\pi}(s)"/> for all states <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/></p>
<p>The optimal action-value function:</p>
<p><img class="math" src="../../_images/math/14caff5f2371c95085a0f2d81233eaac6ea2c78d.svg" alt="q_*(s, a) = \max_{\pi} q_{\pi}(s, a)"/> for all states <img class="math" src="../../_images/math/93d7fd1da4319dc76dd4d696c2027716b025727f.svg" alt="s \in \mathcal{S}"/> and all actions <img class="math" src="../../_images/math/bb1b6a28eceffc4b685eb189e30c62e16ef0b311.svg" alt="a \in \mathcal{A}"/></p>
</div>
<p>The state-value function and the action-value function that are based on the optimal policy are called optimal state-value and optimal action-value function respectively.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>There might be several optimal policies, but there is always only one optimal value function.</p>
</div>
</div>
<div class="section" id="bellman-optimality-equations">
<h2>Bellman Optimality Equations<a class="headerlink" href="#bellman-optimality-equations" title="Permalink to this headline">¶</a></h2>
<p>For finite MDPs the common approach is to calculate the optimal value functions <img class="math" src="../../_images/math/cd470423c63a627c055755b318a6273c585bb9be.svg" alt="v_*"/> and/or <img class="math" src="../../_images/math/afeeb7361af5b1430e75641c240ed1df2423959b.svg" alt="q_*"/> and to deduce the optimal policy <img class="math" src="../../_images/math/65f0bb840045c41b9051bc4ceac137de493ddc3a.svg" alt="\pi_*"/> from those.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Bellman Optimality Equation for the state-value function:</p>
<div class="math">
<p><img src="../../_images/math/b66d8b3741fce7085fa5c188caf356dff00c9276.svg" alt="\begin{align*}
v_*(s) &amp; = \max_{a} q_{{\pi}_*}(s, a) \\
&amp; = \max_{a} \mathbb{E_{\pi_{*}}}[G_t \mid S_t = s, A_t = a] \\
&amp; = \max_{a} \mathbb{E_{\pi_{*}}}[R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
&amp; = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a]
\end{align*}"/></p>
</div><p>Bellman Optimality Equation for the state-value function:</p>
<div class="math">
<p><img src="../../_images/math/9b31bb8f420fcf82e777091934381ed288223c05.svg" alt="\begin{align*}
q_*(s, a) &amp; = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a] \\
&amp; = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \mid S_t = s, A_t = a]
\end{align*}"/></p>
</div></div>
<p>For that purpose the Bellman equations of optimal value functions are required. These are called Bellman optimality equations.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In order to solve an MDP we need to learn the optimal policy. The optimal policy in turn is calculated using optimal state-value and action-value functions.</p>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="definition_markov_decision_process.html" title="previous page">Definition of a Markov Decision Process</a>
    <a class='right-next' id="next-link" href="../dynamic_programming/introduction.html" title="next page">Introduction</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, World4AI Team.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>