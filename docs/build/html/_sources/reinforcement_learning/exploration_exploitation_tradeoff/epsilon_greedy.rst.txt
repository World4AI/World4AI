=================================
Epsilon(:math:`\epsilon`)-Greedy
=================================

Epsilon-Greedy is on the one hand the easiest strategy to balance exploration and exploitation and on the other hand probably the most common one.

With the probability of :math:`\epsilon` the agent selects the greedy action and with the probability of :math:`1 - \epsilon` the agent selects a random action. Often :math:`\epsilon` is not constant and starts with a relatively high value to encourage exploration when the agent knows nothing about the environment. The value is decreased over time and reaches a final minimal value after a designated number of steps. The minimal value often lies in the range between 0.01 and 0.1.


.. math::
    :nowrap:

    \begin{algorithm}[H]
        \caption{Bandit $e$-greedy}
        \label{alg1}
    \begin{algorithmic}
        \STATE Input: bandit, action set $\mathcal{A}$, number of episodes,  learning rate $\alpha$, epsilon $\epsilon$
        \STATE Initialize: 
        \STATE $Q(a)$, for all $a \in \mathcal{A}$ with zeros
        \FOR{$i=0$ to number of episodes}
            \STATE $r \leftarrow$ random number
            \IF {$r < \epsilon$}
                \STATE $A \leftarrow$ random action
            \ELSE
            \STATE $A \leftarrow \arg\max_aQ(a)$
            \ENDIF
            \STATE $R \leftarrow bandit(A)$
            \STATE $Q(A) \leftarrow Q(A) + \alpha[R - Q(A)]$
        \ENDFOR
    \end{algorithmic}
    \end{algorithm}


.. code:: python

    env = Bandit(probs=[0.01, 0.5,  1], rewards=[1000, 10, 1])
    A = [x for x in range(env.action_space.n)]


.. code:: python

    def bandit_algorithm(bandit, A, num_episodes=1000000, alpha=0.00005, epsilon=0.5):
    num_actions = len(A)
    Q = np.zeros(num_actions)
    
    for episode in range(num_episodes):
        
        if np.random.rand() < epsilon:
            action = np.random.choice(num_actions)
        else:
            action = Q.argmax()
        
        _, reward, _, _ = bandit.step(action)
        
        Q[action] = Q[action] + alpha * (reward - Q[action])
        
        if episode % 100000 == 0:
            print(Q)
        
    return Q

.. code::
    
    [0. 0. 0.]
    [10.19164014  2.8005554   0.56541809]
    [9.74250886 4.06751509 0.80981186]
    [10.46367616  4.56865385  0.91688351]
    [9.92591477 4.81001628 0.9637959 ]
    [10.15193048  4.84188377  0.98415424]
    [9.68863263 4.94749768 0.99321726]
    [9.7815899  5.04649584 0.9970355 ]
    [10.0928007   5.02973625  0.99869044]
    [10.60228385  5.01708181  0.99941974]
    