===========================
Exploration vs Exploitation
===========================

.. note::
    *Exploitation:*
    
    The agent exploits his knowledge about the environment to get a high sum of rewards.
    
    *Exploration:*
    
    The agent explores the environment to gather new experience and knowledge about the environment. 
    

The main idea of exploration vs exploitation is that the agent has to decide at each time step if he wants to utilize the knowledge he already possesses to get a relatively high sum of rewards or does he want to explore the environment to maybe find a better path with a higher sum of rewards.

.. figure:: ../_static/images/reinforcement_learning/exploration_exploitation/exploit_explore.svg
   :align: center

   Exploration vs Exploitation

The above image depicts an agent who has discovered the path as indicated by the arrows. Generally the agent might keep taking the same path to reach the triangle, but if he kept exploring the environment he could discover that there is actually a big reward in the bottom right corner. Exploration would enable the agent to learn a strategy with a higher sum of rewards.

.. note::
    *Deterministic Environment*:
    
    Given the same state by the environment and the same action by the agent the next state and the corresponding reward are always the same.
    
    *Stochastic Environment*: 
    
    Given the same state by the environment and the same action by the agent the next state and the corresponding reward are calculated using a probability distribution of the environment.
    
The grid environment I covered so far was deterministic. I assumed that there is no uncertainty and given the same circumstances the outcome would be the same. 

.. figure:: ../_static/images/reinforcement_learning/exploration_exploitation/deterministic.svg
   :align: center

   An example of a deterministic environment

For example whenever the agent chose the action to go right in the first state for example the environment transitioned in such a way that the circle moved right. Each and every single time. No matter how often the action was repeated. 

Most of the environments or the real world for that matter are not that simple, they are not deterministic but stochastic. Which means that there is a probability distribution of how the environment will react given a certain state and the action of the agent. 

.. figure:: ../_static/images/reinforcement_learning/exploration_exploitation/stochastic.svg
   :align: center

   An example of a stochastic environment

The image above shows the simplest imaginable stochastic gridworld, where the game ends after just one time step. The agent can choose to go either left or right. If he chooses the left action there is a 100% chance to get a reward of 1. If he chooses the right action there is a 99% chance to get a reward of 0 and a 1% chance to get a reward of 1000000. After trying out each action once the agent receives a reward of 1 for the left action and (most likely) a reward of 0 for the right action. In order to exploit his knowledge the agent would have to take the left action. On average the right action is the one that produces a better reward, but in order to discover that the agent has to keep exploring.

.. note::
    The goal of the agent is to maximize the expected sum of rewards.

In stochastic environments the agent has to maximize the expected sum of rewards. Intuitively speaking that means that the agent has to choose the strategy that would give him the largest sum of rewards if he played an infinite number of games. The above definition for the goal of the agent is going to be used for the rest of the course, as in deterministic environments the expected sum of rewards would equal the sum of rewards. 

The problem that the agent faces is that he does not know exactly how the distribution of the environment looks like. Therefore the agent has to use trial and error to determine the path that leads to the highest sum of rewards in expectation. The common problem that  the agent faces in that situation is the so-called “exploration exploitation dilemma”. On the one hand the agent wants to get the highest reward based on his current knowledge. So he wants to exploit the knowledge he possesses. On the other hand in order to find a better path he needs to explore the environment. And the dilemma that he faces is the fact that he can not do both at the same time. At each single step the agent either explores or exploits. 




    