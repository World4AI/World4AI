=======
Bandits
=======

Introduction
============

Usually the mathematical introduction into the topic of exploration vs exploitation is done by the means of so-called k-armed bandits. Unlike the full reinforcement learning problem in a bandits setting we face the same single state over and over again and have to choose one of the k actions. Once the action is chosen the agent receives the reward and the state is reset again. 

The name k-armed bandits comes from the casino game “One Armed Bandit”. In such a game the player has to push a single lever and after each push he either receives a reward or not with a certain probability. The outcome of the next pull does not depend on past actions. In k-armed bandits there are k levers. For each of the levers there is a corresponding probability for a certain reward. The idea is to find the lever that maximizes the expected reward.


.. note::
   
    Unlike in an MDP in a bandit problem the action value function does not depend on the state.

    .. math::

        q_*(a) \doteq \mathbb{E}[R_t \mid A_t = a]
    

At the very first glance the solutions to k-armed bandits problems seem to be of limited use when faced with full reinforcement learning MDP tasks, but there is a particular reason to still cover the topic. We remember that reinforcement learning is learning through trial and error and delayed reward. Bandits basically remove the “delayed” part of the definition and deal only with trial and error and reward, as the reward is received for each of the actions directly and the environment is reset to the single not terminal state. That reduces complexity by not caring about the sequential problems but still allows us to discover different types of exploration-exploitation techniques, which eventually can be applied to full reinforcement learning tasks. We can say that bandits are a special case of an MDP.

.. note::
    To select the optimal action is to select the action with the highest value.

    .. math::
    
        A_t \doteq \arg\max_a q_*(a)


If you possess the true value function, then the solution is as simple as selecting the action with the highest value. There is of course always the option to get the model directly to calculate the expected reward for each of the actions, but this chapter is the precursor to full reinforcement learning solutions, therefore we will use interaction to *approximate* the value function.

Approximation Of The Values
===========================

The intuitive way to estimate the action-value function is to select certain actions and build averages of the rewards. If you continue doing that, the approximation is going to get better and better, meaning it is going to get closer to the true action-value function.

.. note::

    The approximate action-values are calculated by dividing the sum of rewards which  occurred when the action :math:`a` was taken by the number of times the action :math:`a` was taken.

    .. math::

        Q_t(a) \doteq \frac{\sum_{t-1}^{i=1} R_i \cdot 1_{A_i=a}}{\sum_{t-1}^{i=1} 1_{A_i=a}}


.. note::

    Exploitation is the selection of the greedy action, the action with the highest value, based on the current estimation of the action-value function.

    Exploration is the selection of the non greedy action based on the current estimation of the action-value function.


When estimating the value function using the above approach it is of course entirely possible that the value of a certain action appears only larger due to an approximation error while the actual true greedy action was not selected enough in the past to approach the true value. The problem gives rise to the necessity to balance exploration and exploitation. In the next sections we are going to discuss several techniques that are used to deal with the exploration-exploitation tradeoff. For the rest of this chapter we will cover techniques that make the computation of the approximation of the action-value function more efficient.

Computational Efficiency
========================

If we follow the above definition of the calculation of the estimate for the action-value function then the natural approach would be to keep a list of all the actions taken and all the rewards received. The higher the number of actions taken the larger the list gets and the higher the memory requirements to keep the elements in memory and the higher the computational cost to loop through all the elements in the list to calculate the average. 


.. note::
    The average at time step n can be expressed in terms of the average at time step n-1 and the newest element.

    .. math::
        :nowrap:

        \begin{align*}
        \mu_n & = \frac{1}{n} \sum_{i=1}^{n}x_i \\
        & = \frac{1}{n}(x_n + \sum_{i=1}^{n-1}x_i) \\
        & = \frac{1}{n}(x_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1}x_i) \\
        & = \frac{1}{n}(x_n + (n-1) \mu_{n-1}) \\
        & = \frac{1}{n}\mu_{n-1} + \frac{1}{n} n \mu_{n-1} - \frac{1}{n}x_n \\    
        & = \mu_{n-1} + \frac{1}{n}(x_n - \mu_{n-1})
        \end{align*}

Using the expression above we can rewrite the calculation of the action-value for a particular action as follows. 

.. math::

    Q_{n+1} = Q_n  + \frac{1}{n}[R_n - Q_n]

The update rule requires only the last estimate of the action-value for a state a and the last reward received, no lists are necessary to track the average. More generally speaking the update can be described as follows.

.. math::

    NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]


The target indicates the *true* value that was experienced and the idea is to push the new estimate towards the *true* value. The expression *Target - Estimate* is often described as *error*, as it is the difference between the true reward received after taking an action and the estimated reward. The *step size*, also called the learning rate, is the magnitude with which you change the estimate of the action-value function. From the expression above it follows that the *step size* is 1/n, which decreases with experience.  In practice it is customary to either set the learning rate to a constant value or start with a relatively high value and reduce it until it reaches a minimum constant value. 

.. math::

    Q_{n+1} = Q_n  + \alpha[R_n - Q_n]

The greek letter :math:`\alpha` is often used for the learning rate.

Implementation of Bandits in OpenAI Gym
=======================================

The Bandit class takes two lists to initialize the environment. The first list contains the probabilities between 0 and 1 that indicate with what probability the agent receives a reward when the action is taken that corresponds with the position in the list. The second list contains the value of the reward that is given in case of success. The reward of 0 is given in the case of no success.

.. code:: python

    import gym
    from gym import spaces
    import numpy as np

    # for a full implementation of bandits in gym look for the github repo by JKCooper2
    # https://github.com/JKCooper2/gym-bandits
    # this implementation is a simplified version

    class Bandit(gym.Env):
        def __init__(self, probs=[], rewards=[]):
            if len(probs) != len(rewards):
                raise ValueError("Probability list and reward list must be of equal length")
            
            self.probs = probs
            self.rewards = rewards
            # k as in k-armed bandits, number of arms
            k = len(self.probs)
            
            self.action_space = spaces.Discrete(k)
            self.observation_space = spaces.Discrete(1)
        
        def step(self, action):
            assert self.action_space.contains(action)
            
            if np.random.rand() < self.probs[action]:
                reward = self.rewards[action]
            else:
                reward = 0
                
            return 0, reward, True, {}
        
        def reset(self):
            return 0

The below example shows an environment that allows 2 actions. The first action gives with a probability of 50% a reward of 10 and the second action gives a reward of 1 with a probability of 100%.
  
.. code:: python

    env = Bandit(probs=[0.5, 1], rewards=[10, 1])