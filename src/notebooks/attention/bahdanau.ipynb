{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9b7d37-5da0-4a36-8f86-703cd48db5cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Translation using Bahdanau Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f826c-ae4b-412d-887e-02467d1a2c98",
   "metadata": {},
   "source": [
    "Major parts of this notebooks are copied from our encoder-decoder notebook: [Translation with Encoder-Decoder RNN](http://localhost:5173/blocks/deep_learning/sequence_modelling/pytorch_implementations/encoder_decoder_translation). Use that for reference, if some parts seem unfamiliar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa82834c-970e-4e04-8c8a-3d77f4d7ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a72721b-fe59-491f-84c2-1efcf59782a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25bf14-e5e4-433e-a0f3-65db7bf1e36a",
   "metadata": {},
   "source": [
    "We once again use the English-German Anki dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2a006b-c673-4bd9-ac7b-9844cacf5fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9376k  100 9376k    0     0   913k      0  0:00:10  0:00:10 --:--:-- 1381k\n",
      "/home/petruschka/repos/World4AI/website/src/notebooks/attention\n"
     ]
    }
   ],
   "source": [
    "!cd ../datasets/ && { curl -O https://www.manythings.org/anki/deu-eng.zip ; cd -; }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b736cd28-baee-47d2-8290-79373465ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../datasets/deu-eng.zip\n",
      "  inflating: ../datasets/deu_eng/deu.txt  \n",
      "  inflating: ../datasets/deu_eng/_about.txt  \n"
     ]
    }
   ],
   "source": [
    "!rm -rf ../datasets/deu_eng/\n",
    "!unzip ../datasets/deu-eng.zip -d ../datasets/deu_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87cb4a-ca90-48ad-9dd3-c9104e727a88",
   "metadata": {},
   "source": [
    "And we use a simple tokenizer that lowercases the text, strips unnecessary whitespace and adds some padding between words and tokens like .!?. We also use special tokens `<sos>` and `<eos>` to  indicate the start and end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0332a0a8-06ed-455a-93cf-b3eaa7299f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    return s\n",
    "\n",
    "def tokenizer(s):\n",
    "    s = normalize(s)\n",
    "    s = s.split(' ')\n",
    "    s.insert(0, '<sos>')\n",
    "    s.append('<eos>')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "413bc681-ae4b-42c3-a507-20fbd07c0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(max_len=20):\n",
    "    print(\"Reading lines...\")\n",
    "    en_seq = []\n",
    "    de_seq = []\n",
    "    with open('../datasets/deu_eng/deu.txt', 'r', encoding='utf-8') as file:\n",
    "        print(f\"Tokenizing and removing sentences larger than {max_len}\")\n",
    "        for line in file:\n",
    "            pairs = line.split('\\t')\n",
    "            \n",
    "            en_sentence, de_sentence = tokenizer(pairs[0]), tokenizer(pairs[1])\n",
    "            \n",
    "            if len(en_sentence) <= max_len and len(de_sentence) <= max_len:\n",
    "                en_seq.append(en_sentence)\n",
    "                de_seq.append(de_sentence)\n",
    "        print(f\"The dataset has {len(en_seq)} pairs\")\n",
    "        return en_seq, de_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff624e79-6813-49ba-b9b5-b377638a0ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Tokenizing and removing sentences larger than 20\n",
      "The dataset has 255279 pairs\n"
     ]
    }
   ],
   "source": [
    "en_seq, de_seq = read_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e389b-bda7-4966-bce0-57ef965a13fa",
   "metadata": {},
   "source": [
    "We divide our dataset into the train, validation and test sets using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e2eb335-6871-4b64-b4e0-460589cc0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a09cbf-53d4-4d11-8d94-dea170dd3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into train test split\n",
    "# train_frac = 0.8\n",
    "# val_frac = 0.1\n",
    "# test_frac = 0.1\n",
    "train_en, test_val_en, train_de, test_val_de = train_test_split(en_seq, de_seq, test_size=0.2)\n",
    "val_en, test_en, val_de, test_de = train_test_split(test_val_en, test_val_de, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8507c0a9-ef46-415e-ac9e-67f0f32fb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, en, de):\n",
    "        assert len(en) == len(de)\n",
    "        self.en = en\n",
    "        self.de = de\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.en[idx], self.de[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d4ff779-6838-415f-bbbf-279ebbe11b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PairDataset(train_en, train_de)\n",
    "val_dataset = PairDataset(val_en, val_de)\n",
    "test_dataset = PairDataset(test_en, test_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8abc1d-59cb-413c-8ec6-25b4e6466cd8",
   "metadata": {},
   "source": [
    "And we create an English and a German vocabulary using torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99763d07-e766-47c5-8188-d1f6340f8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f5a425-969d-403c-acb2-272b5c0e83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_counter = Counter()\n",
    "de_counter = Counter()\n",
    "\n",
    "for line in train_en:\n",
    "    en_counter.update(line)\n",
    "\n",
    "for line in train_de:\n",
    "    de_counter.update(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcda074a-8a8f-49a2-a5b0-604fa7d61550",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sorted_by_freq_tuples = sorted(en_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "en_ordered_dict = OrderedDict(en_sorted_by_freq_tuples)\n",
    "\n",
    "de_sorted_by_freq_tuples = sorted(de_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "de_ordered_dict = OrderedDict(de_sorted_by_freq_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af822cd1-11a2-4cca-a437-c8f23f7dc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "en_vocab = torchtext.vocab.vocab(en_ordered_dict, min_freq = 5, specials=['<pad>', '<unk>', '<sos>', '<eos>'], special_first = True)\n",
    "de_vocab = torchtext.vocab.vocab(de_ordered_dict, min_freq = 5, specials=['<pad>', '<unk>', '<sos>', '<eos>'], special_first = True)\n",
    "\n",
    "en_vocab.set_default_index(1)\n",
    "de_vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67623b03-6684-47e6-9f63-ef5531cd670f",
   "metadata": {},
   "source": [
    "The collate function is required to zero pad shorter sentences to the sentence lenght of the larger sentece in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d6ac325-500d-426c-aea0-124dad650fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    en, de = [], []\n",
    "    for en_token, de_token in batch:\n",
    "        en.append(torch.tensor(en_vocab(en_token), dtype=torch.int64))\n",
    "        de.append(torch.tensor(de_vocab(de_token), dtype=torch.int64))\n",
    "    en_padded = nn.utils.rnn.pad_sequence(en, batch_first=True)\n",
    "    de_padded = nn.utils.rnn.pad_sequence(de, batch_first=True)\n",
    "    return en_padded, de_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d359ccf0-0c97-4ed5-bbe8-27d65a39cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=2,\n",
    "                              drop_last=True,\n",
    "                              collate_fn=collate)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              num_workers=2,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=collate)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              num_workers=2,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407af9ab-2874-449c-8b23-ef98fe922bd7",
   "metadata": {},
   "source": [
    "We use just one layer for encoder and decoder to make the calculations simpler, but we use a biderectional LSTM, as in the paper. We could achieve a better performance with several layers, but the purpose of this notebook is not to achieve state of the art results, but to provide a simple and intuitive explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a510fffd-d4ed-4f8f-bbc6-b4a6d8787b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim=128, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        return outputs, h_n, c_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c4fab-08d3-4ca0-918f-257de11c1bda",
   "metadata": {},
   "source": [
    "The decoder looks a little more complicated, so let's try and explain its components.\n",
    "\n",
    "1. The `energy` linear layer is the input to the softmax function, that outputs attention weights. To create energy we concatenate encoder_outputs and hidden state h and use that as input to the linear layer. This linear layer maps a 128*2 (hidden + output) sized vectors and outputs just a single value per encoder output.\n",
    "\n",
    "2. The context is produced by multiplying encoder outputs with attention weights. We combine those with the decoder embeddings using the `combine` linear layer. The output is used as input into the LSTMCell.\n",
    "\n",
    "3. The last fully connected layer `fc` is responsible for producing logits. These will be used in greedy samplin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9491323b-ce70-4782-8e10-c909e87a1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim=128, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "        self.energy = nn.Linear(hidden_size*2, 1)\n",
    "        self.combine = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_embeddings)\n",
    "    \n",
    "    def forward(self, x, h, c, encoder_outputs):\n",
    "        embedding = self.embedding(x) \n",
    "        h, c = h.squeeze(0), c.squeeze(0)\n",
    "        \n",
    "        energy_input = h.unsqueeze(1).repeat(1, encoder_outputs.shape[1], 1)\n",
    "        energy_input = torch.cat((encoder_outputs, energy_input), dim=2)\n",
    "        energy = self.energy(energy_input).squeeze(2)\n",
    "        attention = torch.softmax(energy, dim=1)\n",
    "        \n",
    "        context = attention.unsqueeze(1) @ encoder_outputs\n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        x = self.combine(torch.cat((context, embedding), dim=1))\n",
    "        (h_n, c_n) = self.lstm_cell(x, (h, c))\n",
    "        logits = self.fc(h_n)\n",
    "        return logits, h_n, c_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aab302-cf2e-4d42-8fb6-731e01e03c9c",
   "metadata": {},
   "source": [
    "The rest of the implementation is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1813cbf7-3950-498e-a6c9-ea31ed38de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    \n",
    "    def forward(self, en_sequence, de_sequence):\n",
    "        batch_size, sequence_len, num_de_embeddings = de_sequence.size()[0], de_sequence.size()[1], self.decoder.embedding.num_embeddings\n",
    "        \n",
    "        # minus 1 due to fewer predictions as inputs, we don't predict <sos>\n",
    "        outputs = torch.zeros(batch_size, sequence_len-1, num_de_embeddings, device=DEVICE)\n",
    "\n",
    "        encoder_outputs, h_n, c_n = self.encoder(en_sequence)\n",
    "        inp = de_sequence[:, 0]\n",
    "        for i in range(1, sequence_len):\n",
    "            logits, h_n, c_n = decoder(inp, h_n, c_n, encoder_outputs)\n",
    "            outputs[:, i-1] = logits\n",
    "            \n",
    "            force = random.random() < self.teacher_forcing_ratio\n",
    "            if force:\n",
    "                inp = de_sequence[:, i]\n",
    "            else:\n",
    "                inp = logits.argmax(dim=1)\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1104bc7-39e7-4043-8025-3ca27e08feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_performance(dataloader, model, criterion):\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    num_iterations = 0\n",
    "\n",
    "    # no need to calculate gradients\n",
    "    with torch.inference_mode():\n",
    "        for en_sequence, de_sequence in dataloader:\n",
    "            en_sequence = en_sequence.to(DEVICE)\n",
    "            de_sequence = de_sequence.to(DEVICE)\n",
    "\n",
    "            logits = model(en_sequence, de_sequence)\n",
    "            \n",
    "            # we don't actually predict the <sos> token\n",
    "            labels = de_sequence[:, 1:]\n",
    "            # we need to reshape in order to be able to use these tensors with CrossEntropyLoss\n",
    "            logits = logits.reshape(-1, logits.size()[2])\n",
    "            labels = labels.reshape(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_sum += loss.cpu().item()\n",
    "            num_iterations+=1\n",
    "\n",
    "    # we return the average loss\n",
    "    return loss_sum/num_iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8eedecc2-1f89-4aa6-90b9-12f78bf885e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_dataloader, val_dataloader, model, optimizer, criterion, scheduler=None):\n",
    "    min_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "        for en_sequence, de_sequence in train_dataloader:\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            en_sequence = en_sequence.to(DEVICE)\n",
    "            de_sequence = de_sequence.to(DEVICE)\n",
    "\n",
    "            logits = model(en_sequence, de_sequence)\n",
    "            # we don't actually predict the <sos> token\n",
    "            labels = de_sequence[:, 1:]\n",
    "\n",
    "            # we need to reshape in order to be able to use these tensors with CrossEntropyLoss\n",
    "            logits = logits.reshape(-1, logits.size()[2])\n",
    "            labels = labels.reshape(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += loss.cpu().item()\n",
    "            num_iterations += 1\n",
    "        train_loss=loss_sum/num_iterations\n",
    "        val_loss = track_performance(val_dataloader, model, criterion)\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        print(f'Epoch: {epoch+1:>2}/{num_epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f}')\n",
    "        \n",
    "        if val_loss < min_loss:\n",
    "            print(\"Saving Weights!\")\n",
    "            min_loss = val_loss\n",
    "            torch.save({'encoder_weights': encoder.state_dict(), 'decoder_weights': decoder.state_dict()}, f='../temp/encoder_decoder.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f711e59-f852-4ed4-87f7-4149509c04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_embeddings=len(en_vocab), embedding_dim=128)\n",
    "decoder = Decoder(num_embeddings=len(de_vocab), embedding_dim=128)\n",
    "seq2seq = EncoderDecoder(encoder, decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "701cb41f-3f2f-4fce-a898-ad0944684f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='min',\n",
    "                                                       patience=2,\n",
    "                                                       verbose=True)\n",
    "\n",
    "num_epochs=25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2810f6e-7591-49de-bfa3-9883cbc6748e",
   "metadata": {},
   "source": [
    "The validation loss looks very similar to the one in the simple encoder-decoder implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "579c5e78-8445-4a7e-be90-ec3354e7a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/25 | Train Loss: 4.66697 | Val Loss: 3.96499\n",
      "Saving Weights!\n",
      "Epoch:  2/25 | Train Loss: 3.67404 | Val Loss: 3.43004\n",
      "Saving Weights!\n",
      "Epoch:  3/25 | Train Loss: 3.22975 | Val Loss: 3.10357\n",
      "Saving Weights!\n",
      "Epoch:  4/25 | Train Loss: 2.92408 | Val Loss: 2.87413\n",
      "Saving Weights!\n",
      "Epoch:  5/25 | Train Loss: 2.69841 | Val Loss: 2.70660\n",
      "Saving Weights!\n",
      "Epoch:  6/25 | Train Loss: 2.51097 | Val Loss: 2.57303\n",
      "Saving Weights!\n",
      "Epoch:  7/25 | Train Loss: 2.37196 | Val Loss: 2.49242\n",
      "Saving Weights!\n",
      "Epoch:  8/25 | Train Loss: 2.25726 | Val Loss: 2.39603\n",
      "Saving Weights!\n",
      "Epoch:  9/25 | Train Loss: 2.15084 | Val Loss: 2.35311\n",
      "Saving Weights!\n",
      "Epoch: 10/25 | Train Loss: 2.07572 | Val Loss: 2.30002\n",
      "Saving Weights!\n",
      "Epoch: 11/25 | Train Loss: 2.00712 | Val Loss: 2.24609\n",
      "Saving Weights!\n",
      "Epoch: 12/25 | Train Loss: 1.94464 | Val Loss: 2.23760\n",
      "Saving Weights!\n",
      "Epoch: 13/25 | Train Loss: 1.88540 | Val Loss: 2.18921\n",
      "Saving Weights!\n",
      "Epoch: 14/25 | Train Loss: 1.85016 | Val Loss: 2.16090\n",
      "Saving Weights!\n",
      "Epoch: 15/25 | Train Loss: 1.80471 | Val Loss: 2.15824\n",
      "Saving Weights!\n",
      "Epoch: 16/25 | Train Loss: 1.76320 | Val Loss: 2.12537\n",
      "Saving Weights!\n",
      "Epoch: 17/25 | Train Loss: 1.72699 | Val Loss: 2.12143\n",
      "Saving Weights!\n",
      "Epoch: 18/25 | Train Loss: 1.68742 | Val Loss: 2.11100\n",
      "Saving Weights!\n",
      "Epoch: 19/25 | Train Loss: 1.66355 | Val Loss: 2.09473\n",
      "Saving Weights!\n",
      "Epoch: 20/25 | Train Loss: 1.62794 | Val Loss: 2.09629\n",
      "Epoch: 21/25 | Train Loss: 1.60741 | Val Loss: 2.08574\n",
      "Saving Weights!\n",
      "Epoch: 22/25 | Train Loss: 1.57845 | Val Loss: 2.09997\n",
      "Epoch: 23/25 | Train Loss: 1.55798 | Val Loss: 2.07132\n",
      "Saving Weights!\n",
      "Epoch: 24/25 | Train Loss: 1.53316 | Val Loss: 2.04923\n",
      "Saving Weights!\n",
      "Epoch: 25/25 | Train Loss: 1.51612 | Val Loss: 2.08269\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs, train_dataloader, val_dataloader, seq2seq, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b07358a-1e25-4d56-80f7-c99fc2ba35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('../temp/encoder_decoder.pt')\n",
    "encoder_weights = weights['encoder_weights']\n",
    "decoder_weights = weights['decoder_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21e58c7b-f938-47a2-beb7-46b2946f546d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(encoder_weights)\n",
    "decoder.load_state_dict(decoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd4dee66-61d6-4d20-9e4e-df4707ab9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, vocab, encoder, decoder):\n",
    "    with torch.inference_mode():\n",
    "        outputs = []\n",
    "        \n",
    "        start_token = [\"<sos>\"]\n",
    "        end_token = [\"<eos>\"]\n",
    "        start_idx = vocab(start_token)[0]\n",
    "        end_idx = vocab(end_token)[0]\n",
    "                \n",
    "        encoder_outputs, h_n, c_n = encoder(sentence)\n",
    "        inp = torch.tensor([start_idx], device=DEVICE)\n",
    "        while True:\n",
    "            logits, h_n, c_n = decoder(inp, h_n, c_n, encoder_outputs)\n",
    "            h_n = h_n.unsqueeze(0)\n",
    "            c_n = c_n.unsqueeze(0)\n",
    "            inp = logits.argmax(dim=1)\n",
    "            outputs.append(inp.cpu().item())\n",
    "            if inp.item() == end_idx:\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19010865-7d4f-402e-b4c6-7eb7de5d1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sequence, de_sequence = next(iter(test_dataloader))\n",
    "en_sequence = en_sequence.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1f77f-bfe0-4b50-9bda-cb7cfe10020b",
   "metadata": {},
   "source": [
    "Similar to the encoder-decoder implementation, the quality of the translation is not optimal. But given our small model and the limited amount of data, this result is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4938fd1-44e6-4c3d-805a-f750e02f44ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'he', 'tried', 'to', 'approach', 'her', 'using', 'every', 'possible', 'means', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'er', 'versuchte', 'auf', 'jede', '<unk>', '<unk>', 'an', 'sie', '<unk>', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['er', 'versuchte', 'jeden', '<unk>', 'von', '<unk>', 'zu', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'please', \"don't\", 'touch', 'the', '<unk>', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'bitte', 'fasst', 'die', '<unk>', 'nicht', 'an', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['bitte', 'anfassen', 'die', 'die', 'nicht', 'berühren', '!', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', \"don't\", 'ride', 'that', 'horse', '.', \"he'll\", 'throw', 'you', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'auf', 'dem', 'pferd', '<unk>', '<unk>', 'das', 'wirft', 'dich', 'ab', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['geh', 'nicht', 'auf', 'dem', 'pferd', '!', 'er', 'ist', 'nicht', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'those', 'twins', 'look', 'like', 'two', '<unk>', 'in', 'a', '<unk>', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'die', 'zwillinge', 'gleichen', 'sich', 'wie', 'ein', 'ei', 'dem', 'anderen', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['die', '<unk>', '<unk>', '<unk>', '<unk>', 'in', 'einer', 'kleinen', '<unk>', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'i', 'just', 'had', 'to', 'see', 'this', 'for', 'myself', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'ich', 'musste', 'das', 'einfach', 'mit', 'eigenen', 'augen', 'sehen', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['ich', 'musste', 'ich', 'das', 'nicht', 'selbst', 'sehen', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'it', 'is', 'he', 'who', 'is', 'to', 'blame', 'for', 'the', 'accident', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'er', 'ist', 'am', 'unfall', 'schuld', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['er', 'ist', 'für', 'den', 'schuld', 'schuld', 'schuld', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'tom', 'told', 'mary', 'that', 'he', \"didn't\", 'like', 'her', 'anymore', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'tom', 'teilte', 'maria', 'mit,', 'dass', 'er', 'sie', 'nicht', 'mehr', 'gern', 'habe', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['tom', 'sagte', 'maria,', 'dass', 'sie', 'nicht', 'mehr', 'mehr', 'liebt', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', \"what's\", 'your', \"cat's\", 'name', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'wie', 'heißt', 'eure', 'katze', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['wie', 'heißt', 'deine', 'katze', '?', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'tom', 'is', 'training', 'his', 'dog', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'tom', '<unk>', 'seinen', 'hund', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['tom', 'füttert', 'seinen', 'hund', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'you', 'know', 'that', \"i'm\", 'different', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'du', 'weißt,', 'dass', 'ich', 'anders', 'bin', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['du', 'weißt,', 'dass', 'ich', 'anders', 'bin', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    en_sentence = en_sequence[i].unsqueeze(0)\n",
    "    de_sentence = de_sequence[i].unsqueeze(0)\n",
    "    translation = translate_sentence(en_sentence, en_vocab, encoder, decoder)\n",
    "    print('-'*130)\n",
    "    print(f'English Sentence: {en_vocab.lookup_tokens(en_sentence[0].cpu().tolist())}')\n",
    "    print(f'German Translation: {de_vocab.lookup_tokens(de_sentence[0].cpu().tolist())}')\n",
    "    print(f'Model Translation: {de_vocab.lookup_tokens(translation)}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
