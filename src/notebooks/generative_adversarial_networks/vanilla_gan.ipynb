{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4468ad7c-4a25-4da1-a74d-a6ad4ca9c475",
   "metadata": {},
   "source": [
    "# Vanilla Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6355a2-147e-4ed4-a894-12294730c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5b2a7a-f65c-4eab-97aa-07bc263077a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd6ca80-773d-4305-9a15-c82a8d8c1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "LR = 2e-4\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LATENT_SIZE = 128\n",
    "IMG_SIZE = 28*28\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed79f81-3382-44fb-9d83-7960de4a8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(LATENT_SIZE, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, IMG_SIZE),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bfeeb8-be90-45aa-a94e-98eb034b1028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check generator\n",
    "gen = Generator()\n",
    "dummy_input = torch.randn(BATCH_SIZE, LATENT_SIZE)\n",
    "gen(dummy_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef004a4-0bdb-4f3e-9390-15722ba100a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(IMG_SIZE, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f995d90-346a-4434-a462-c41c8bb835cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check discriminator\n",
    "dis = Discriminator()\n",
    "dummy_input = torch.randn(BATCH_SIZE, IMG_SIZE)\n",
    "dis(dummy_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4dc2df9-83fa-4839-8ab3-03aafa275474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "transform = T.Compose([T.ToTensor(), T.Normalize(mean=(0.5 , ), std=(0.5,)) ])\n",
    "dataset = datasets.MNIST(root='../datasets/', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89594fc6-62d7-469e-a59c-6158b3ea4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(DEVICE)\n",
    "discriminator = Discriminator().to(DEVICE)\n",
    "gen_optim = optim.Adam(generator.parameters(), lr=LR)\n",
    "dis_optim = optim.Adam(discriminator.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "writer = SummaryWriter('runs/gan_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e277381-43a8-47ae-b403-8f6fc835faec",
   "metadata": {},
   "source": [
    "Let us look at the value function again.\n",
    "\n",
    "$ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(\\mathbf{x})] + \\mathbb{E}_{ z \\sim p_{z}(z)}[\\log(1 - D(G(\\mathbf{z})))]$\n",
    "  \n",
    "We will transform the above expression into a format, that is easily digestible for deep learning frameworks, that means gradient descent and binary cross-entropy.\n",
    "\n",
    "Binary cross-entropy is defined as follows.\n",
    "\n",
    "$ L_n = - [ y_n \\cdot \\log D(x_n) + (1 - y_n) \\cdot \\log (1 - D(G(z_n))]$\n",
    "\n",
    "If the discriminator faces a true image, the loss will collapse to $[-y_n \\cdot \\log D(x_n)]$. Using this expression in gradient descent is the same as using gradient ascent for $[y_n \\cdot \\log D(x_n)]$.\n",
    "\n",
    "If the discriminator faces a fake image, the loss will collapse to $[-(1 - y_n) \\cdot \\log (1 - D(G(z_n))]$. Using this expression in gradient descent is the same as using gradient ascent for $[(1 - y_n) \\cdot \\log (1 - D(G(z_n))]$.\n",
    "\n",
    "The gradient descent and cross-entropy calculation for the generator is slightly more tricky. When the discriminator generates a high probability for a fake image the following expression $\\log (1 - D(G(z_n))$ will approach minus infinity so generally we want to use gradient descent on the following expression $[(1 - y_n) \\cdot \\log (1 - D(G(z_n))]$ in order to trick the discriminator more often. In practice we flip the labels for the discriminator (turn 0 label into 1 label) and minimize $-y_n\\log D(G(z_n)$. This trick makes sure, that the gradient is large at the beginning of training, when the generator does not produce convincing results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b57e78a-5959-4f0e-98c4-5bbb75579eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tensor is useful to track images that are created from a fixed latent variable\n",
    "# that way we can observe if the generator starts to create better images\n",
    "fixed_noise = torch.randn((BATCH_SIZE, LATENT_SIZE), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf6258b-2686-4801-986c-8fbaec788442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 || Discriminator Loss: 0.9665 || Generator Loss: 0.9535\n",
      "Epoch 2/50 || Discriminator Loss: 0.9857 || Generator Loss: 1.0272\n",
      "Epoch 3/50 || Discriminator Loss: 0.9247 || Generator Loss: 1.1063\n",
      "Epoch 4/50 || Discriminator Loss: 1.0139 || Generator Loss: 1.0841\n",
      "Epoch 5/50 || Discriminator Loss: 0.8948 || Generator Loss: 1.2844\n",
      "Epoch 6/50 || Discriminator Loss: 0.9805 || Generator Loss: 1.2402\n",
      "Epoch 7/50 || Discriminator Loss: 0.9157 || Generator Loss: 1.3692\n",
      "Epoch 8/50 || Discriminator Loss: 0.8919 || Generator Loss: 1.4542\n",
      "Epoch 9/50 || Discriminator Loss: 0.8678 || Generator Loss: 1.5171\n",
      "Epoch 10/50 || Discriminator Loss: 0.9165 || Generator Loss: 1.4911\n",
      "Epoch 11/50 || Discriminator Loss: 0.9299 || Generator Loss: 1.4868\n",
      "Epoch 12/50 || Discriminator Loss: 0.9716 || Generator Loss: 1.4354\n",
      "Epoch 13/50 || Discriminator Loss: 1.0068 || Generator Loss: 1.3822\n",
      "Epoch 14/50 || Discriminator Loss: 1.0173 || Generator Loss: 1.3718\n",
      "Epoch 15/50 || Discriminator Loss: 1.0230 || Generator Loss: 1.3437\n",
      "Epoch 16/50 || Discriminator Loss: 1.0331 || Generator Loss: 1.3259\n",
      "Epoch 17/50 || Discriminator Loss: 1.0624 || Generator Loss: 1.2890\n",
      "Epoch 18/50 || Discriminator Loss: 1.0586 || Generator Loss: 1.2976\n",
      "Epoch 19/50 || Discriminator Loss: 1.0626 || Generator Loss: 1.2831\n",
      "Epoch 20/50 || Discriminator Loss: 1.0694 || Generator Loss: 1.2815\n",
      "Epoch 21/50 || Discriminator Loss: 1.1076 || Generator Loss: 1.2231\n",
      "Epoch 22/50 || Discriminator Loss: 1.1132 || Generator Loss: 1.2013\n",
      "Epoch 23/50 || Discriminator Loss: 1.1231 || Generator Loss: 1.1831\n",
      "Epoch 24/50 || Discriminator Loss: 1.1264 || Generator Loss: 1.1630\n",
      "Epoch 25/50 || Discriminator Loss: 1.1210 || Generator Loss: 1.1698\n",
      "Epoch 26/50 || Discriminator Loss: 1.1248 || Generator Loss: 1.1651\n",
      "Epoch 27/50 || Discriminator Loss: 1.1258 || Generator Loss: 1.1546\n",
      "Epoch 28/50 || Discriminator Loss: 1.1254 || Generator Loss: 1.1528\n",
      "Epoch 29/50 || Discriminator Loss: 1.1399 || Generator Loss: 1.1353\n",
      "Epoch 30/50 || Discriminator Loss: 1.1348 || Generator Loss: 1.1326\n",
      "Epoch 31/50 || Discriminator Loss: 1.1411 || Generator Loss: 1.1287\n",
      "Epoch 32/50 || Discriminator Loss: 1.1526 || Generator Loss: 1.1109\n",
      "Epoch 33/50 || Discriminator Loss: 1.1574 || Generator Loss: 1.0981\n",
      "Epoch 34/50 || Discriminator Loss: 1.1600 || Generator Loss: 1.1014\n",
      "Epoch 35/50 || Discriminator Loss: 1.1548 || Generator Loss: 1.0970\n",
      "Epoch 36/50 || Discriminator Loss: 1.1617 || Generator Loss: 1.0829\n",
      "Epoch 37/50 || Discriminator Loss: 1.1627 || Generator Loss: 1.0678\n",
      "Epoch 38/50 || Discriminator Loss: 1.1661 || Generator Loss: 1.0747\n",
      "Epoch 39/50 || Discriminator Loss: 1.1675 || Generator Loss: 1.0707\n",
      "Epoch 40/50 || Discriminator Loss: 1.1706 || Generator Loss: 1.0662\n",
      "Epoch 41/50 || Discriminator Loss: 1.1735 || Generator Loss: 1.0649\n",
      "Epoch 42/50 || Discriminator Loss: 1.1702 || Generator Loss: 1.0682\n",
      "Epoch 43/50 || Discriminator Loss: 1.1729 || Generator Loss: 1.0554\n",
      "Epoch 44/50 || Discriminator Loss: 1.1669 || Generator Loss: 1.0605\n",
      "Epoch 45/50 || Discriminator Loss: 1.1791 || Generator Loss: 1.0508\n",
      "Epoch 46/50 || Discriminator Loss: 1.1801 || Generator Loss: 1.0504\n",
      "Epoch 47/50 || Discriminator Loss: 1.1783 || Generator Loss: 1.0487\n",
      "Epoch 48/50 || Discriminator Loss: 1.1811 || Generator Loss: 1.0494\n",
      "Epoch 49/50 || Discriminator Loss: 1.1792 || Generator Loss: 1.0488\n",
      "Epoch 50/50 || Discriminator Loss: 1.1748 || Generator Loss: 1.0493\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    dis_loss_col = []\n",
    "    gen_loss_col = []\n",
    "    for batch_idx, (features, _) in enumerate(dataloader):\n",
    "        real_images = features.view(-1, IMG_SIZE).to(DEVICE)\n",
    "        \n",
    "        # generate fake images from standar normal distributed latent vector\n",
    "        latent_vector = torch.randn(BATCH_SIZE, LATENT_SIZE, device=DEVICE)\n",
    "        fake_imgs = generator(latent_vector)\n",
    "        \n",
    "        # calculate logits for true and fake images\n",
    "        fake_logits = discriminator(fake_imgs.detach())\n",
    "        real_logits = discriminator(real_images)\n",
    "        \n",
    "        # calculate discriminator loss\n",
    "        dis_real_loss = criterion(real_logits, torch.ones(BATCH_SIZE, 1, device=DEVICE))\n",
    "        dis_fake_loss = criterion(fake_logits, torch.zeros(BATCH_SIZE, 1, device=DEVICE))\n",
    "        dis_loss = dis_real_loss + dis_fake_loss\n",
    "        \n",
    "        # optimize the discriminator \n",
    "        dis_optim.zero_grad()\n",
    "        dis_loss.backward()\n",
    "        dis_optim.step()\n",
    "\n",
    "        # calculate generator loss\n",
    "        gen_loss = criterion(discriminator(fake_imgs), torch.ones(BATCH_SIZE, 1, device=DEVICE))\n",
    "        \n",
    "        # optimize the generator\n",
    "        gen_optim.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        gen_optim.step()\n",
    "        \n",
    "        dis_loss_col.append(dis_loss.cpu().item())\n",
    "        gen_loss_col.append(gen_loss.cpu().item())\n",
    "\n",
    "    dis_loss = sum(dis_loss_col) / len(dis_loss_col)\n",
    "    gen_loss = sum(gen_loss_col) / len(gen_loss_col)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS} || Discriminator Loss: {dis_loss:.4f} || Generator Loss: {gen_loss:.4f}')\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        fake_imgs = generator(fixed_noise).view(-1, 1, 28, 28)\n",
    "        grid = torchvision.utils.make_grid(fake_imgs, normalize=True)\n",
    "\n",
    "        writer.add_image(\n",
    "            \"MNIST GAN Generated Images\", grid, global_step=epoch+1\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
