{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15eeeb44-69cd-4faa-9f1e-15240297e261",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ae02a-fcf9-4a01-9d81-ade09ceb8715",
   "metadata": {},
   "source": [
    "The IMDB dataset contains movie reviews with a corresponding sentiment toward the movie (can be either positive or negative). Our task is create and train a model that can derive the sentiment from the review text. \n",
    "\n",
    "This dataset is often regarded as the `MNIST of sequence modelling`, as this is considered to be a relatively simple task, that is well suited for beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa043213-ece6-48c8-8ad6-e31aac0041c7",
   "metadata": {},
   "source": [
    "Below we import functions from the `torchtext` library. Similar to `torchvision`, `torchtext` is a specialized library and as you can probably guess, it has a lot of uitility functions and classes for working with texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e798ab-5ec7-4985-895e-4d5d09b04118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kaggle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# utilities to work with text\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d35b4-d937-450d-aaaa-29f21b1c6a74",
   "metadata": {},
   "source": [
    "Similar to `torchvision`, `torchtext` has a lot of builtin datasets, including the IMDB dataset. The problem is, that `torchtext` uses [TorchData](https://github.com/pytorch/data), a new PyTorch library for dealing with data loading. This library is still in beta and will undergo many changes. We will therefore download the data from Kaggle and create the classical `Dataset` and `DataLoader` objects. Once `TorchData` is stable, we will update our notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1692f2-8835-4780-93e9-81223a1be05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# donwload the data\n",
    "!kaggle datasets download -p ../datasets -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7331f635-b2d8-4a03-a8b5-deed5e348bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../datasets/imdb-dataset-of-50k-movie-reviews.zip\n",
      "  inflating: ../datasets/imdb/IMDB Dataset.csv  \n"
     ]
    }
   ],
   "source": [
    "# unzip the data\n",
    "!rm -rf ../datasets/imdb\n",
    "!unzip -d ../datasets/imdb ../datasets/imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975a99a-8ba4-42a7-b935-521b2db9f630",
   "metadata": {},
   "source": [
    "We use pandas to read in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0bc3a2d-23bb-4dde-b495-49d3f61aefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/imdb/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666cb18-6341-4e72-8c1d-cf653eb18cd4",
   "metadata": {},
   "source": [
    "There are just two columns. The `review` column contains the review of the movie, this is our features, while the sentiment contains the binary class: positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9209c4fa-89db-4666-a4be-b80f49896309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb40ad-daee-424e-8a3e-d31498b4c5a5",
   "metadata": {},
   "source": [
    "When we look at the reviews, we realize that the text contains html tags, as the text was probably parsed from the internet. While the data is not completely clean, this is still good enought for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d13885-4d8e-467f-9c1b-a29525677980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example for positive review\n",
    "df.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b4caaf-3a7b-452f-bfd2-5620e7c45156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example for negative review\n",
    "df.review[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a541d35-4257-4886-aace-84e9c34fcd80",
   "metadata": {},
   "source": [
    "We separate the data into the training set with 30,000 samples and 10,000 samples for validation and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa85fc11-a2e6-4f2d-a25d-82cd56db935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:30000]\n",
    "val_df = df[30000:40000]\n",
    "test_df = df[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62986e5e-7710-4dd1-beb5-b381f92d3262",
   "metadata": {},
   "source": [
    "The sentiment field is distributed roughly equal in the three splits, we don't need to apply any additional stratified splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58899491-5ec4-4dbb-8536-2905f2c3f9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    15015\n",
       "negative    14985\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42456427-f7db-4849-919c-af5fcce5691a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    5022\n",
       "positive    4978\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fa01ad-1789-4618-8da9-50e8c4f44a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    5007\n",
       "negative    4993\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ab342-df80-4594-a0a3-1fa3ab67db05",
   "metadata": {},
   "source": [
    "The `Dataset` object is relatively simple. We extract the numpy array from the reviews and the sentiments and return the value with the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaaf1770-970a-4563-84d2-57c931048f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.length = len(df)\n",
    "        self.reviews = df[\"review\"].to_numpy()\n",
    "        self.sentiments = df[\"sentiment\"].to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.sentiments[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc007a7-3b4b-47b9-b00b-ae0267a4439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_df)\n",
    "val_dataset = IMDBDataset(val_df)\n",
    "test_dataset = IMDBDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6e1fe-1c0a-4fbb-b3f9-233f0e6274bc",
   "metadata": {},
   "source": [
    "Now it is time to utilize the `torchtext` functions. The `get_tokenizer(tokenizer)` function returns a tokenizer object. When we provide `basic_english` as a parameter, the generated tokenizer basically splits the sentences by white space and lowercases the words. Theoretically we can provide tokenizers from specialized libraries like [spaCy](https://spacy.io/), but we will keep it simple and stick to the simple `basic_english` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217ff9aa-9b35-4ff0-a8a4-76b04492f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83154aef-8c81-456a-8f3f-b1abdae72935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'are', 'you', 'doing', 'today', '?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokenizer(\"How are you doing today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628ab64-dcf2-494f-b0af-03526ca41790",
   "metadata": {},
   "source": [
    "The `torchtext` library provides `Vocab` the vocab class, that turns a token into a corresponding index. The `vocab` function can construct such a vocabulary for us if we provide an OrderedDict with tokens as keys and the number of occurences of those tokens in our dataset as values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672918c-8e08-475e-9502-42992f571af4",
   "metadata": {},
   "source": [
    "We will use the `Counter` class to count the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523974b1-2a73-4621-9f78-bc53c688f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary\n",
    "# vocab expects with word\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "086e786c-b669-4492-910a-f4a1abac2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for review, _ in train_dataset:\n",
    "    counter.update(tokenizer(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a57037-35ed-43e7-8fec-3ea1ecf82115",
   "metadata": {},
   "source": [
    "Alltogether we are faced with 112,119 distinct tokens. We will reduce that number in a separate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582038f4-85c9-4d7c-94fd-a0566092d31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112119\n"
     ]
    }
   ],
   "source": [
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae852b57-73f0-4970-84e2-2b9a00543db1",
   "metadata": {},
   "source": [
    "Finally we sort the counts and transform those into an `OrderedDict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47715bc5-480a-4c1f-b3c7-450d50f12ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899858c-9c0c-492d-9d7a-1a2508060cc4",
   "metadata": {},
   "source": [
    "For example the word `the` has the correspoinding number of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d4ee30-bb25-44af-b7ec-30d76c30ef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399138"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_dict.get(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82728992-6e2a-41b9-991a-098095fcf7e2",
   "metadata": {},
   "source": [
    "We provide a couple of arguments to the `vocab` function. We add two special tokens for padding and unknown tokens and insert those at the beginnig of the vocabulary, so that the have index 0 and 1. We set the default index to 1, which transforms all unknown words into \"<unk>\". \n",
    "    \n",
    "We also want to reduce our vocabulary and only keep those words that apper at least 5 times in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d69b4be-f99a-4799-86b4-350b131ef8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_vocab = vocab(ordered_dict, min_freq=5, specials=[\"<pad>\", \"<unk>\"], special_first=True)\n",
    "imdb_vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cc5c3-a9f7-47ff-bb5a-8a92a0edb745",
   "metadata": {},
   "source": [
    "This reduces the number of tokens by more than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2bdb9b7-424f-4a29-a3e8-1af3e7738516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33031"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduced number of tokens\n",
    "len(imdb_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec5047-200f-4516-9cad-62ecc256533d",
   "metadata": {},
   "source": [
    "The `Vocab` object takes a list of tokens and returns the corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f05d9e97-3363-400e-9499-5e5b2a2baa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54, 30, 25, 401]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_vocab([\"what\", \"are\", \"you\", \"doing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541a158-6990-4fd1-82f2-774cfae031cd",
   "metadata": {},
   "source": [
    "Now it is time to talk about the collate function, that can be provided to a `DataLoader`. This function is responsible for taking the list of samples from the dataset as input and generating a tensor batch. Usually this is done automatically, but that is only possible because all the samples are of equal length. The sentences that we deal with are of different length, so we have to provide a custom collate function. We transform sentences into tokens and turn them into indices. In order to make those sequences of equal length, we pad sequences that are of smaller length, which means we add values of 0 at the end of the sequences. We also return the length of each sequence in a tensor, because those will become important during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c92b96c-ffa4-45d6-8412-58ed30d6cfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# similar to the implementation in Sebastian Raschkas book: Machine Learning with PyTorch and Scikit-Learn.\n",
    "def collate_fn(batch):\n",
    "    token_ls, sentiment_ls, len_ls = [], [], []\n",
    "    for review, sentiment in batch:\n",
    "        tokens = [imdb_vocab[token] for token in tokenizer(review)]\n",
    "        sentiment_idx = 1 if sentiment == 'positive' else 0\n",
    "        token_ls.append(torch.tensor(tokens, dtype=torch.int64))\n",
    "        len_ls.append(len(tokens))\n",
    "        sentiment_ls.append(sentiment_idx)\n",
    "    return nn.utils.rnn.pad_sequence(token_ls, batch_first=True), torch.tensor(sentiment_ls), torch.tensor(len_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f42b4-b808-4e94-a83d-5e214a75ac53",
   "metadata": {},
   "source": [
    "Let's see what we end up with, using a small batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e3572fb-d331-41e7-8576-d27963f8bbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  35,    7,    2,  ..., 4164,  516,    3],\n",
       "         [   6,  386,  126,  ...,    0,    0,    0],\n",
       "         [  13,  204,   14,  ...,    0,    0,    0],\n",
       "         [ 673,   46,    9,  ...,    0,    0,    0]]),\n",
       " tensor([1, 1, 1, 0]),\n",
       " tensor([374, 177, 187, 158]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3957097-8d57-43a3-b3b7-0b35f74dd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "NUM_EMBEDDINGS=len(imdb_vocab)\n",
    "EMBEDDING_DIM=10\n",
    "LSTM_HIDDEN_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14c3edd8-4948-4847-bf14-177095dc0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77110e1a-35be-4b76-9102-22eae70806bf",
   "metadata": {},
   "source": [
    "Below we implement our model. For the most part there are not many surprises. We turn the word indices into embeddings, use those embeddings as inputs into the 2-layer LSTM neural network and use a fully connected layer for the classification.\n",
    "\n",
    "The `pack_padded_sequence` method is probably something new to you. As we padded our sequences, many sequences get as large as the largest sequence in a batch. That can lead to the vanishing gradients problem. The method makes sure that the LSTM layers only traverse the sequence until the padded values. For that we provide the sizes that we calculated in our collate function. In fact if you remove the `pack_padded_sequence` from the code below, you will notice that your model won't be able to improve. An alternative strategy would be to cut the sequence at a fixed length. Let's say 300 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc023302-d494-4f05-8c9c-4269b16bde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=NUM_EMBEDDINGS, embedding_dim=EMBEDDING_DIM, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=EMBEDDING_DIM, hidden_size=LSTM_HIDDEN_SIZE, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=LSTM_HIDDEN_SIZE, out_features=1)\n",
    "    \n",
    "    def forward(self, x, sizes):\n",
    "        x = self.embedding(x)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, sizes, enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # we take the hidden values from the last (top) layer\n",
    "        x = h_n[-1, ...]\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a352a-1ee1-4db5-87b4-8675ba584a68",
   "metadata": {},
   "source": [
    "The rest of the notebook uses those functions, that we used many times previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69549b11-2257-4bb9-b805-59e61829fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b76db1f9-8897-4ac7-b671-5348a5d4535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_performance(dataloader, model, criterion):\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    # no need to calculate gradients\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (features, labels, sizes) in enumerate(dataloader):\n",
    "            features = features.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).view(-1, 1).float()\n",
    "            logits = model(features, sizes)\n",
    "            probs = torch.sigmoid(logits)\n",
    "                        \n",
    "            predictions = (probs > 0.5).float()\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            loss_sum += loss.cpu().item()\n",
    "            num_samples += len(features)\n",
    "    \n",
    "    # we return the average loss and the accuracy\n",
    "    return loss_sum/num_samples, num_correct/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce140e6-24ee-4371-9500-7766b3e9ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_dataloader, val_dataloader, model, criterion, optimizer, scheduler=None):\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (features, labels, sizes) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            features = features.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).view(-1, 1).float()\n",
    "            \n",
    "            # Empty the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            logits = model(features, sizes)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Descent\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss, train_acc = track_performance(train_dataloader, model, criterion)\n",
    "        val_loss, val_acc = track_performance(val_dataloader, model, criterion)\n",
    "\n",
    "        if scheduler:\n",
    "          scheduler.step(val_acc)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:>2}/{num_epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}')\n",
    "    return history            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faadfee0-9203-4fe7-ba87-6e639d89b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       patience=2,\n",
    "                                                       verbose=True)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a7eedd2-4fad-45c6-aef4-465dafa277ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/10 | Train Loss: 0.02334 | Val Loss: 0.02347 | Train Acc: 0.535 | Val Acc: 0.532\n",
      "Epoch:  2/10 | Train Loss: 0.01936 | Val Loss: 0.01952 | Train Acc: 0.656 | Val Acc: 0.652\n",
      "Epoch:  3/10 | Train Loss: 0.01368 | Val Loss: 0.01430 | Train Acc: 0.807 | Val Acc: 0.797\n",
      "Epoch:  4/10 | Train Loss: 0.01046 | Val Loss: 0.01169 | Train Acc: 0.857 | Val Acc: 0.837\n",
      "Epoch:  5/10 | Train Loss: 0.00931 | Val Loss: 0.01103 | Train Acc: 0.885 | Val Acc: 0.858\n",
      "Epoch:  6/10 | Train Loss: 0.00786 | Val Loss: 0.00996 | Train Acc: 0.901 | Val Acc: 0.861\n",
      "Epoch:  7/10 | Train Loss: 0.00613 | Val Loss: 0.00931 | Train Acc: 0.925 | Val Acc: 0.880\n",
      "Epoch:  8/10 | Train Loss: 0.00511 | Val Loss: 0.00894 | Train Acc: 0.940 | Val Acc: 0.886\n",
      "Epoch:  9/10 | Train Loss: 0.00426 | Val Loss: 0.00901 | Train Acc: 0.953 | Val Acc: 0.889\n",
      "Epoch: 10/10 | Train Loss: 0.00363 | Val Loss: 0.00930 | Train Acc: 0.961 | Val Acc: 0.886\n"
     ]
    }
   ],
   "source": [
    "history = train(10, train_dataloader, val_dataloader, model, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83fb7884-2074-43e9-97fe-fd126ee6a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = track_performance(test_dataloader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6e8ee-aad1-4703-b221-70efccfcaf18",
   "metadata": {},
   "source": [
    "Our test accuracy is close to 89%. While there are better implementations, this is not a bad result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b164bd18-c2d4-4485-a0d3-b2cb185aabb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.00945 | Test Acc: 0.885\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
