{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c424c707-69a9-4e1c-8885-cb6ad0eba89d",
   "metadata": {},
   "source": [
    "# Modules, Optimizers and Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cba10-081a-4c75-a5ca-62769ccae41c",
   "metadata": {},
   "source": [
    "The training loop that we implemented in the last section works, but there are better approaches to solve the same problem. Once our neural network architectures get more and more complex, we will be glad that we are able to utilize a more efficient training approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb2c01-285f-4a2a-b699-4daabdac0875",
   "metadata": {},
   "source": [
    "While there is a lot to improve from the last chapter, many of the components will also feel familiar.\n",
    "\n",
    "First we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f30cee4-3cca-4f7c-b722-81ed9c90f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611847b-a4cb-4524-8a14-5567540b38e8",
   "metadata": {},
   "source": [
    "We get a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44b8af1-a4bd-4cc6-a3d7-42755a9ea7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"../datasets\", train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06647d44-0b00-4fef-b79a-50eeeb53c0cd",
   "metadata": {},
   "source": [
    "We set the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a904ee49-55e2-4d24-83f2-ec4c37730e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "DEVICE = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS=10\n",
    "BATCH_SIZE=32\n",
    "\n",
    "#number of hidden units in the first and second hidden layer\n",
    "HIDDEN_SIZE_1 = 100\n",
    "HIDDEN_SIZE_2 = 50\n",
    "NUM_LABELS = 10\n",
    "NUM_FEATURES = 28*28\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab4b92-cfde-4357-9618-e3d15b2eaa88",
   "metadata": {},
   "source": [
    "And we initialize the DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b202ffb7-2b20-49f2-bcd2-f988700fbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True,\n",
    "                              num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c32fc5-e5c8-481b-8e20-cb2036eed76b",
   "metadata": {},
   "source": [
    "This time around we will start by looking at our final product, the training loop in order to understand what we need to make our code clean, modular and scalable. Instad of calculating one layer after another we will calculate our forward pass using a single call to `model`. The model is essentially the function that generates the predictions we are interested in. In our case the model will contain all the matrix multiplications and activation functions needed to predict the probability that the features belong to a certain number. The criterion is essentially a loss function, in our case it is the cross-entropy. The optimizer loops through the parameters and applies gradient descent when we call `optimizer.step` and clears all the gradients when we call `optimizer.zero_grad()`. The rest of the implementation is actually the same, but we hope that you notice how much more understandable the code gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeae2688-622e-4332-a390-6da346838e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer):\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_sum = 0\n",
    "        batch_nums = 0\n",
    "        for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "            # move features and labels to GPU\n",
    "            features = features.view(-1, NUM_FEATURES).to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # ------ FORWARD PASS --------\n",
    "            probs = model(features)\n",
    "\n",
    "            # ------CALCULATE LOSS --------\n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            # ------BACKPROPAGATION --------\n",
    "            loss.backward()\n",
    "\n",
    "            # ------GRADIENT DESCENT --------\n",
    "            optimizer.step()\n",
    "\n",
    "            # ------CLEAR GRADIENTS --------\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ------TRACK LOSS --------\n",
    "            batch_nums += 1\n",
    "            loss_sum += loss.detach().cpu()\n",
    "\n",
    "        print(f'Epoch: {epoch+1} Loss: {loss_sum / batch_nums}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fdc24-4253-48fd-bdb4-cf3d6c1b9a10",
   "metadata": {},
   "source": [
    "Now the time has finally come to talk about modules, optimizers and losses.\n",
    "\n",
    "In order to make our calculations more modular, we will create a `Module` class. You can think about a module as a piece of a neural network. Usually modules are those pieces of a network, that we apply over and over again. In essence you create a neural network by defining and stacking modules. As we need to apply linear transformations several times, we put the logic of a linear layer into a separate class and we call that class `Module`. This module initializes a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$. For easier access at a later point we create an attribute `parameters`, which is just a list holding the weights and biases. We also implement the `__call__` method, which contains the logic for the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e2f4bb-65f8-4ba4-ac08-91019add827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.W = torch.normal(mean=0, \n",
    "                              std=0.1, \n",
    "                              size=(out_features, in_features), \n",
    "                              requires_grad=True, \n",
    "                              device=DEVICE, \n",
    "                              dtype=torch.float32)\n",
    "        self.b = torch.zeros(1, \n",
    "                             out_features, \n",
    "                             requires_grad=True, \n",
    "                             device=DEVICE, \n",
    "                             dtype=torch.float32)\n",
    "        self.parameters = [self.W, self.b]\n",
    "                \n",
    "    def __call__(self, features):\n",
    "        return features @ self.W.T + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278ce0c-ca30-485b-99b9-6cd22fa4653b",
   "metadata": {},
   "source": [
    "This procedure could for example look as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4062ab7f-c7d1-470d-9b06-c65d0ad7228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0735, -0.0272,  0.0012,  0.1392, -0.0333],\n",
      "        [ 0.0268, -0.0219,  0.0403, -0.1075, -0.0814],\n",
      "        [ 0.1632, -0.0679,  0.0277,  0.2101, -0.1172],\n",
      "        [ 0.0622, -0.0454,  0.0755, -0.1778, -0.1576],\n",
      "        [ 0.0847, -0.0413,  0.0345,  0.0291, -0.0958],\n",
      "        [ 0.1066, -0.0480,  0.0302,  0.0892, -0.0976],\n",
      "        [ 0.0538, -0.0309,  0.0373, -0.0423, -0.0875],\n",
      "        [-0.0066, -0.0063,  0.0289, -0.1275, -0.0474],\n",
      "        [-0.0408,  0.0046,  0.0342, -0.2158, -0.0421],\n",
      "        [ 0.1061, -0.0353, -0.0116,  0.2543, -0.0248]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "features = torch.randn(10, 2).to(DEVICE)\n",
    "linear_module = Module(2, 5)\n",
    "print(linear_module(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249bad8-1085-4118-8560-8d787e6fa7aa",
   "metadata": {},
   "source": [
    "Our model also needs a sigmoid and a softmax activation functions. These are the same implementations as in the previoius section. Here we define activations as separate functions in order to be able to reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd6adaa-bf2c-46b4-b1f2-85f3f6d59f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd875ec1-7bb7-45c7-9a79-32eeca40b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    numerator = torch.exp(z)\n",
    "    denominator = numerator.sum(dim=1, keepdim=True)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86accd32-0362-4d84-b24b-ca4af172f788",
   "metadata": {},
   "source": [
    "The model initializes three linear modules. In the `__call__` method we implement the full forward pass. So when we call `model(features)`, the features are processed by the neural network and the outputs are returned to the origin of the call. Additionally we implement the `parameters` method, which just all parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb09053a-17aa-4278-9da6-0717c9e7153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.linear_1 = Module(NUM_FEATURES, HIDDEN_SIZE_1)\n",
    "        self.linear_2 = Module(HIDDEN_SIZE_1, HIDDEN_SIZE_2)\n",
    "        self.linear_3 = Module(HIDDEN_SIZE_2, NUM_LABELS)\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        x = self.linear_1(features)\n",
    "        x = sigmoid(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = sigmoid(x)\n",
    "        x = self.linear_3(x)\n",
    "        x = softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        parameters = [*self.linear_1.parameters, \n",
    "                      *self.linear_2.parameters,\n",
    "                       *self.linear_3.parameters]\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75776351-6d2c-4dc3-b6c1-3afd920a697e",
   "metadata": {},
   "source": [
    "Applying the forward pass of a predefined model feels more intuitive than our previous implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68618cb-c829-471c-84bc-c5bf38fbf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.randn(BATCH_SIZE, NUM_FEATURES).to(DEVICE)\n",
    "model = Model()\n",
    "output = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e8fdf-dc2d-4e6b-a625-4f22e7f11b27",
   "metadata": {},
   "source": [
    "The optimizer class is responsible for applying gradient descent and for clearing the gradients. Our class needs the learning rate (alpha) and the parameters of the model. When we call `step` we loop over all parameters and apply gradient descent and when we call `zero_grad()` we clear all the gradients. Notice that the optimizer logic works independent of the exact architecture of the model, making the code more managable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "245b2531-8e28-4587-86de-4b3f6b88c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    \n",
    "    def __init__(self, alpha, parameters):\n",
    "        self.alpha = alpha\n",
    "        self.parameters = parameters\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.inference_mode():\n",
    "            for parameter in self.parameters:\n",
    "                parameter.sub_(self.alpha * parameter.grad)\n",
    "                \n",
    "    def zero_grad(self):\n",
    "        with torch.inference_mode():\n",
    "            for parameter in self.parameters:\n",
    "                parameter.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea940f16-a663-4a1e-8424-76aba9fc9895",
   "metadata": {},
   "source": [
    "Finally we implement the loss function. Once again the calculation of the loss is independent of the model or the optimizer. When we change one of the components, we do not introduce any breaking changes. If we replace the negative log likelihood by mean squared error our training loop will still keep working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d2912c-7529-4e9b-aef2-34da04233ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nll_loss(outputs, labels):\n",
    "    one_hot_labels = torch.zeros(BATCH_SIZE, NUM_LABELS).to(DEVICE)\n",
    "    for sample_idx, label in enumerate(labels):\n",
    "        one_hot_labels[sample_idx][label] = 1\n",
    "        \n",
    "    return -(one_hot_labels * torch.log(outputs)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfccc90-f870-44a7-ab53-4fceeacdc7ac",
   "metadata": {},
   "source": [
    "Now we have all components, that are required by our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e9f6d0b-bfdb-4d0c-996a-6607e09e603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = SGDOptimizer(ALPHA, model.parameters())\n",
    "criterion = nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "355c14ff-923f-49a2-94b8-cad067f668cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.22587831318378448\n",
      "Epoch: 2 Loss: 0.2053816169500351\n",
      "Epoch: 3 Loss: 0.15154482424259186\n",
      "Epoch: 4 Loss: 0.10453948378562927\n",
      "Epoch: 5 Loss: 0.0807894766330719\n",
      "Epoch: 6 Loss: 0.0671185776591301\n",
      "Epoch: 7 Loss: 0.05817437916994095\n",
      "Epoch: 8 Loss: 0.05199269950389862\n",
      "Epoch: 9 Loss: 0.04752832278609276\n",
      "Epoch: 10 Loss: 0.044177427887916565\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb83407-9066-42fe-a7c8-856b58791015",
   "metadata": {},
   "source": [
    "You can probaly guess, that PyTorch provides classes and functions out of the box. `torch.nn` contains the base `Module` class and `torch.nn.functional` contains activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d05d7e6-ca2a-49ca-b184-54c094318016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fad06-3a9a-4dce-a4d3-06b3cc23d5cb",
   "metadata": {},
   "source": [
    "When we write custom modules we need to subclass `nn.Module`. All trainable parameters are put into the `nn.parameter.Parameter()` class. This tells PyTorch to put those tensors into the parameters list (which is used by the optimizer) and the tensors are automatically tracked for gradient computation. Instad of defining `__call__` as we did before, we define the `forward` method. PyTorch calls this method automatically, when we call the module object. You should never call this method directly, as PyTorch does additional calculations during the forward pass (instead of `module.forward(features)` use `module(features)`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cedf2998-8165-4fec-a0f8-652abe6f50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.W = nn.parameter.Parameter(torch.normal(mean=0, std=0.1, \n",
    "                              size=(out_features, in_features)))\n",
    "        self.b = nn.parameter.Parameter(torch.zeros(1, out_features))\n",
    "\n",
    "    def forward(self, features):\n",
    "        return features @ self.W.T + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ae6ed-1e81-42cc-b7dc-f61810db5dd6",
   "metadata": {},
   "source": [
    "The great thing about PyTorch modules is their composability. Earlier created modules can be used in subsequent modules. Below for example we use the above defined `Module` in the `Model` module. In later chapter we will see how we can create blocks of arbitrary complexity using this simple approach. \n",
    "\n",
    "You should notice, that we use the `log_softmax` activation function, instead of a simple softmax. According to the PyTorch documentation `log_softmax` is faster and has better numerical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "481a4597-42bb-4cab-965e-aad9ea82d42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = Module(NUM_FEATURES, HIDDEN_SIZE_1)\n",
    "        self.linear_2 = Module(HIDDEN_SIZE_1, HIDDEN_SIZE_2)\n",
    "        self.linear_3 = Module(HIDDEN_SIZE_2, NUM_LABELS)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        x = self.linear_1(features)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear_3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947819e-8022-44a1-9ef3-c37985346464",
   "metadata": {},
   "source": [
    "PyTorch obviously provides loss functions and optimizers. The `NLLLoss` calculates the negative log likelihood loss based on log probabilities from our model and true labels. You can find all optimizers in `torch.optim`. For know we will use stochastic gradient descent, but there are many more optimizers that we will encounter soon. The interface of the loss functions and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7b013a3-10d0-4028-9acb-a3a59c863881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d62d0d0-77f9-4d17-bee2-5b03c7010635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.0678449869155884\n",
      "Epoch: 2 Loss: 0.36083656549453735\n",
      "Epoch: 3 Loss: 0.2820141315460205\n",
      "Epoch: 4 Loss: 0.23701903223991394\n",
      "Epoch: 5 Loss: 0.20419947803020477\n",
      "Epoch: 6 Loss: 0.17840386927127838\n",
      "Epoch: 7 Loss: 0.15804895758628845\n",
      "Epoch: 8 Loss: 0.14154206216335297\n",
      "Epoch: 9 Loss: 0.12750738859176636\n",
      "Epoch: 10 Loss: 0.1162610873579979\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b723f4-42a7-4aaa-b776-c69c580f6306",
   "metadata": {},
   "source": [
    "PyTorch provides a lot of modules out of the box. A linear transformation layer is a common procedure, therefore you should use `nn.Linear` instead of implementing your solutions from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d287f58e-dc3a-43d1-90cd-586d32f1dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(NUM_FEATURES, HIDDEN_SIZE_1)\n",
    "        self.linear_2 = nn.Linear(HIDDEN_SIZE_1, HIDDEN_SIZE_2)\n",
    "        self.linear_3 = nn.Linear(HIDDEN_SIZE_2, NUM_LABELS)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.linear_1(features)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear_3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "458e1125-afcd-4684-a63d-73fa07365a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1590d2d-f198-4cc4-b5fe-13525997904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.466644525527954\n",
      "Epoch: 2 Loss: 0.4674857556819916\n",
      "Epoch: 3 Loss: 0.3282727301120758\n",
      "Epoch: 4 Loss: 0.2696901261806488\n",
      "Epoch: 5 Loss: 0.22721980512142181\n",
      "Epoch: 6 Loss: 0.19520707428455353\n",
      "Epoch: 7 Loss: 0.17021693289279938\n",
      "Epoch: 8 Loss: 0.15134933590888977\n",
      "Epoch: 9 Loss: 0.13562604784965515\n",
      "Epoch: 10 Loss: 0.12347173690795898\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d1b78-f745-4e95-b217-b732639f0cb5",
   "metadata": {},
   "source": [
    "To finish this chapter let us discuss a couple more of PyTorch conveniences, that you will find useful. You might have noticed, that all modules and activation functions are called one after another, where the output of one module (or activation) is used as the input into the next. In that case we can pack all modules and activations into a `nn.Sequential` object. When we call that object, the components will be executed in a sequential order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fe7c19e-33a8-416b-8e57-4e0da0fb537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Linear(NUM_FEATURES, HIDDEN_SIZE_1),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(HIDDEN_SIZE_1, HIDDEN_SIZE_2),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(HIDDEN_SIZE_2, NUM_LABELS),\n",
    "            )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        return self.layers(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2258f15-fb9d-4938-b772-29f9fddc9b44",
   "metadata": {},
   "source": [
    "Above we have omitted the `nn.LogSoftmax()` activation function and we replace the `nn.NLLLoss()` loss function by the `nn.CrossEntropyLoss`. We do that, because `nn.CrossEntropyLoss()` combines a log softmax layer with the negative log likelihood loss. This combination is expected to be numerically more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f27c7bf7-3c48-4209-a14c-8d7ffe49bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55a309ba-4518-424d-9009-87c6f803d09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.4635180234909058\n",
      "Epoch: 2 Loss: 0.4384552836418152\n",
      "Epoch: 3 Loss: 0.32038575410842896\n",
      "Epoch: 4 Loss: 0.265774130821228\n",
      "Epoch: 5 Loss: 0.22523467242717743\n",
      "Epoch: 6 Loss: 0.1935357302427292\n",
      "Epoch: 7 Loss: 0.170162171125412\n",
      "Epoch: 8 Loss: 0.15130649507045746\n",
      "Epoch: 9 Loss: 0.13578613102436066\n",
      "Epoch: 10 Loss: 0.12304723262786865\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868f2f2-bb01-47f2-ba49-2a8e2cb7276f",
   "metadata": {},
   "source": [
    "We know that this chapter contained a lot of new information, but rest assured, that as you move along with your studies of PyTorch all those concepts will become second nature to you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
