<script>
  import Container from "$lib/Container.svelte";
  import Highlight from "$lib/Highlight.svelte";
  import Alert from "$lib/Alert.svelte";

  import { DeterministicAgent } from "$lib/reinforcement_learning/grid_world/DeterministicAgent";
  import { GridEnvironment } from "$lib/reinforcement_learning/grid_world/GridEnvironment";
  import {
    pickupGridMap,
    gridMap,
  } from "$lib/reinforcement_learning/grid_world/maps";
  import Grid from "$lib/reinforcement_learning/grid_world/Grid.svelte";
  import { Interaction } from "$lib/reinforcement_learning/grid_world/Interaction";

  let env_1 = new GridEnvironment(pickupGridMap);
  let agent_1 = new DeterministicAgent(
    env_1.getObservationSpace(),
    env_1.getActionSpace()
  );
  let deterministicInteraction = new Interaction(agent_1, env_1, 2);

  let deterministicCellsStore = env_1.cellsStore;
  let deterministicPlayerStore = deterministicInteraction.observationStore;

  $: deterministicCells = $deterministicCellsStore;
  $: deterministicPlayer = $deterministicPlayerStore;

  let env_2 = new GridEnvironment(gridMap, true);
  let agent_2 = new DeterministicAgent(
    env_2.getObservationSpace(),
    env_2.getActionSpace()
  );
  let stochasticInteraction = new Interaction(agent_2, env_2, 2);

  let stochasticCellsStore = env_2.cellsStore;
  let stochasticPlayerStore = stochasticInteraction.observationStore;

  $: stochasticCells = $stochasticCellsStore;
  $: stochasticPlayer = $stochasticPlayerStore;
</script>

<svelte:head>
  <title>Exploration-Exploitation Dilemma - World4AI</title>
  <meta
    name="description"
    content="In reinforcement learning the agent faces the so called exploration exploitation dilemma. At each step the agent has to decide to either explore or to exploit. Doing both at the same time is not possible."
  />
</svelte:head>

<h1>Exploration-Exploitation Dilemma</h1>
<div class="separator" />

<Container>
  <p>
    At each timestep the agent has to make the decision to either explore the
    environment or to exploit the current knowledge about the environment. The
    problem that the agent faces when deciding between the two options is the so
    called
    <Highlight>exploration-exploitation dilemma</Highlight>.
  </p>
  <Alert type="info"
    >The agent can either explore the environment or exploit the already
    accumulated knowledge. The exploration-exploitation dilemma describes the
    fact that the agent can not do both at the same time.
  </Alert>
  <p>
    On the one hand the agent aims to get the highest sum of rewards that is
    achievable based on the current knowledge - it wants to exploit. On the
    other hand in order to find a sequence of actions which lead to a higher sum
    of rewards the agent needs to explore the environment. The dilemma is the
    fact that the agent can not do both at the same time. At each single step
    the agent either explores or exploits.
  </p>
  <div class="separator" />

  <h2>Exploration in Deterministic Environments</h2>
  <Alert type="info">
    <Highlight>Deterministic Environment</Highlight>: Given the same state of
    the environment and the same action by the agent the next state and the
    corresponding reward are always the same.
  </Alert>
  <p>
    The grid environment we covered so far was deterministic. We assumed that
    there is no uncertainty and given the same circumstances the outcome would
    be the same. For example whenever the agent chose the action to go right in
    the first state the environment transitioned in such a way that the circle
    moved actually move right. Each and every single time. Yet even in a
    deterministic environment the agent has to explore in order to find the
    optimal sequence of actions.
  </p>
  <p>
    The grid world below shows an agent which has discovered the shortest route
    from the starting position to the goal position (triangle). At each timestep
    the agent earns a negative reward of -1. Once the agent reaches the goal,
    the environment gives a positive feedback of +1 reward and the game
    restarts. Generally the agent might keep taking the same path to reach the
    triangle, but if it kept exploring the environment it could discover that
    there is actually a big reward of +10 in the bottom right corner. The agent
    could pick up the reward first and then keep moving towards the goal. The
    high reward would make up for the additional few steps. In this
    deterministic example exploration would enable the agent to learn a strategy
    with a higher sum of rewards.
  </p>
  <Grid
    cells={deterministicCells}
    player={deterministicPlayer}
    showColoredReward={true}
    showReward={true}
  />
  <div class="separator" />

  <h2>Exploration in Stochastic Environments</h2>
  <Alert type="info">
    <Highlight>Stochastic Environment</Highlight>: Given the same state of the
    environment and the same action by the agent, the next state and the
    corresponding reward are calculated using a probability distribution.
  </Alert>
  <p>
    Most of the environments (or the real world for that matter) are not
    deterministic, they are stochastic. That means that the next state and
    reward are calculated based on a probability distribution. That means that
    given the same state and action, the next state and rewards are not going to
    be consistent.
  </p>
  <Grid cells={stochasticCells} player={stochasticPlayer} />
  <p>
    As in the previous examples the agent above is already trained and tries to
    follow the shortest route. The grid world on the other hand represents a
    stochastic environment. The environment transitions into the desired state
    of the agent with the probability of 50%. With the probability of 50% the
    direction is chosen randomly (this might also include the desired
    direction). For an untrained agent this makes the job of finding the
    shortest route a lot more complex. The agent does not know exactly how the
    distribution of the environment looks like. Therefore the agent has to
    explore and to determine the path that leads to the highest sum of rewards
    in <Highlight>expectation</Highlight>.
  </p>
  <Alert type="info">
    The goal of the agent is to maximize the <Highlight
      >expected sum of rewards</Highlight
    >.
  </Alert>
  <p>
    In stochastic environments the agent has to maximize the expected sum of
    rewards. Intuitively speaking that means that the agent has to choose the
    strategy that would give him the largest sum of rewards if the agent played
    an infinite number of games.
  </p>

  <div class="separator" />
</Container>
