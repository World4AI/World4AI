{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7",
   "metadata": {
    "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7"
   },
   "source": [
    "# Gated PixelCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc",
   "metadata": {
    "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc"
   },
   "source": [
    "In this notebook we implement the `gated PixelCNN` autoregressive generative model, including the version conditioned on the labels.\n",
    "\n",
    "This notebook is highly inspired by the following [notebook](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html) and we borrow many parts in our implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ab572b-4608-4903-bb1c-820343fe3e1c",
   "metadata": {
    "id": "77ab572b-4608-4903-bb1c-820343fe3e1c"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d710e-0705-4d55-ae0d-6597e4910274",
   "metadata": {},
   "source": [
    "We use different parameters than in the two previous notebooks. We increase the number of epochs from 50 to 100 and increase the size of the channels from 16 to 64. These adjustments should produce higher quality digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201",
   "metadata": {
    "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201"
   },
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "MIN_LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b2ead-95c2-456d-98cb-0122d873968a",
   "metadata": {},
   "source": [
    "The `MaskedConvolution` module expects a mask as an input parameter. Horizonal and vertical stack will have different types of masks. We generate a convolutional layer based on the shape of the mask and multiply the weights of the convolution each forward pass to zero some of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195",
   "metadata": {
    "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195"
   },
   "outputs": [],
   "source": [
    "class MaskedConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, mask, dilation=1):\n",
    "        super().__init__()\n",
    "        kernel_size = mask.shape\n",
    "        padding = tuple([dilation * (size-1)//2 for size in kernel_size])\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels, \n",
    "                              kernel_size=kernel_size,\n",
    "                              padding=padding,\n",
    "                              dilation=dilation)\n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight *= self.mask \n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc2be9-4cbf-47b8-b3b7-ca9680c93730",
   "metadata": {},
   "source": [
    "The `VertickalStackConvolution` module processes the pixels above the pixel we would like to produce. We are still dealing with type 'A' and type 'B' masks. 'A' masks are only required for the very first layer, afterwards we are allowed to use type 'B' masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765",
   "metadata": {
    "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765"
   },
   "outputs": [],
   "source": [
    "class VerticalStackConvolution(MaskedConvolution):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
    "        assert mask_type in ['A', 'B']\n",
    "        mask = torch.ones(kernel_size, kernel_size)\n",
    "        mask[kernel_size//2+1:,:] = 0\n",
    "        if mask_type=='A':\n",
    "            mask[kernel_size//2,:] = 0\n",
    "        \n",
    "        super().__init__(in_channels, out_channels, mask, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d8bc99-7d6d-46ee-a3b6-b3d4eca1579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# type A mask\n",
    "vertical = VerticalStackConvolution(in_channels=1, out_channels=1, mask_type='A')\n",
    "print(vertical.mask)\n",
    "# type B mask\n",
    "vertical = VerticalStackConvolution(in_channels=1, out_channels=1, mask_type='B')\n",
    "print(vertical.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764ca37-23ba-41fa-842b-159cd1296ec6",
   "metadata": {},
   "source": [
    "The `HorizontalStackConvolution` on the other hand attends to pixels to the left of the pixel we would like to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9207114-f6df-401b-8d87-d93764b4a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalStackConvolution(MaskedConvolution):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
    "        assert mask_type in ['A', 'B']\n",
    "        mask = torch.ones(1, kernel_size)\n",
    "        mask[0, kernel_size//2+1:] = 0\n",
    "        if mask_type=='A':\n",
    "            mask[0, kernel_size//2] = 0\n",
    "        super().__init__(in_channels, out_channels, mask, dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98c5e423-8baa-4188-9d4e-7a989d71d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.]])\n",
      "tensor([[1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# type A mask\n",
    "horizontal = HorizontalStackConvolution(in_channels=1, out_channels=1, mask_type='A')\n",
    "print(horizontal.mask)\n",
    "\n",
    "# type B mask\n",
    "horizontal = HorizontalStackConvolution(in_channels=1, out_channels=1, mask_type='B')\n",
    "print(horizontal.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215079e-efd2-4eee-867f-098047bfa192",
   "metadata": {},
   "source": [
    "The `GatedResidualBlock` follows the [diagram](https://www.world4ai.org/blocks/deep_learning/generative_models/autoregressive/gated_pixel_cnn) we discussed in the theoretical section. We suggest you compare the diagram to the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tx52b6uW44dY",
   "metadata": {
    "id": "Tx52b6uW44dY"
   },
   "outputs": [],
   "source": [
    "class GatedResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size=3, dilation=1):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels, out_channels=2*in_channels, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.h = HorizontalStackConvolution(in_channels, out_channels=2*in_channels, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.v_to_h = nn.Conv2d(2*in_channels, 2*in_channels, kernel_size=1)\n",
    "        self.v_to_res = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, v_prev, h_prev):\n",
    "        \n",
    "        # vertical stack\n",
    "        v = self.v(v_prev)\n",
    "        v_f, v_g = v.chunk(2, dim=1)\n",
    "        v_out = torch.tanh(v_f) * torch.sigmoid(v_g)\n",
    "\n",
    "        # vertical to horizontal\n",
    "        v_to_h = self.v_to_h(v)\n",
    "        \n",
    "        # horizontal stack\n",
    "        h = self.h(h_prev) + v_to_h\n",
    "        h_f, h_g = h.chunk(2, dim=1)\n",
    "        h_out = torch.tanh(h_f) * torch.sigmoid(h_g)\n",
    "        \n",
    "        # skip connection\n",
    "        h_out = self.v_to_res(h_out)\n",
    "        h_out += h_prev\n",
    "\n",
    "        return v_out, h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583193f4-e39a-41ff-bb35-d0ef7246eb99",
   "metadata": {},
   "source": [
    "Finally we implement the full model. We start we a vertical and horizontal stack with a 7x7 kernel and a mask of type 'A'. The results are then processed by 7 3x3 type 'B' residual blocks. \n",
    "\n",
    "You should notice that the model uses convolutions with dilations. Normally a kernel attends to adjacent pixels, a kernel with dilations has gaps between the neurons, thus increasing the visual field. If this makes no sense here is a [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) that provides a great visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6115b45-b690-463f-be12-a8125b36c863",
   "metadata": {
    "id": "f6115b45-b690-463f-be12-a8125b36c863"
   },
   "outputs": [],
   "source": [
    "class GatedPixelCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, dilations=[1, 2, 1, 4, 1, 2, 1]):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels=1,\n",
    "                                          out_channels=hidden_dim,\n",
    "                                          kernel_size=7,\n",
    "                                          mask_type='A')\n",
    "        self.h = HorizontalStackConvolution(in_channels=1,\n",
    "                                          kernel_size=7,\n",
    "                                          out_channels=hidden_dim,\n",
    "                                          mask_type='A')\n",
    "        \n",
    "        self.gated_residual_blocks = nn.ModuleList([GatedResidualBlock(hidden_dim, kernel_size=3, dilation=dilation) \n",
    "                                                 for dilation in dilations])\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=hidden_dim, \n",
    "                              out_channels=hidden_dim,\n",
    "                              kernel_size=1)\n",
    "        \n",
    "        # we apply a 256 way softmax\n",
    "        self.output = nn.Conv2d(in_channels=hidden_dim, \n",
    "                                out_channels=256,\n",
    "                                kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        v = self.v(x)\n",
    "        h = self.h(x)\n",
    "        \n",
    "        for gated_layer in self.gated_residual_blocks:\n",
    "            v, h = gated_layer(v, h)\n",
    "        \n",
    "        out = self.conv(F.relu(h))\n",
    "        out = self.output(F.relu(out))\n",
    "        # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
    "        out = out.unsqueeze(dim=2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b808566-9cea-4fc4-a57e-00e128fbb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "                                           train=True, \n",
    "                                           transform=T.PILToTensor(), \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "                                           train=False, \n",
    "                                           transform=T.PILToTensor(), \n",
    "                                           download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94052701-2e81-4519-bf91-49f6ab4bfd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c",
   "metadata": {
    "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c"
   },
   "outputs": [],
   "source": [
    "model = GatedPixelCNN(hidden_dim=HIDDEN_DIM).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, min_lr=MIN_LEARNING_RATE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X9U3ryrChI0r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9U3ryrChI0r",
    "outputId": "eba450b0-5be7-424e-bd83-a5dd43e13770"
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    drive_path = pathlib.Path('/content/gdrive')\n",
    "    weights_path = drive_path / 'MyDrive/weights'\n",
    "except:\n",
    "    weights_path = pathlib.Path('../weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
    "outputId": "df81f892-7df3-486c-cf80-c0d63761b98e"
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train():\n",
    "    best_loss = float(\"inf\")\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for features, _ in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features = features.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                logits = model(features.float() / 255)\n",
    "                loss = criterion(logits, features.long())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss.cpu().item())\n",
    "\n",
    "\n",
    "        # evaluate on the test dataset\n",
    "        with torch.inference_mode():\n",
    "            for features, _ in test_dataloader:\n",
    "                features = features.to(DEVICE)\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    logits = model(features.float() / 255)\n",
    "                    loss = criterion(logits, features.long())\n",
    "                test_losses.append(loss.cpu().item())\n",
    "\n",
    "        end_time = time.time()\n",
    "        ce_train = sum(train_losses)/len(train_losses)\n",
    "        ce_test = sum(test_losses)/len(test_losses)\n",
    "        print(f'Epoch: {epoch+1}/{NUM_EPOCHS}, Cross Entropy Train: {ce_train:.4f}, Cross Entropy Test: {ce_test:.4f}, Elapsed Time: {end_time-start_time:.2f}sec')\n",
    "\n",
    "        if ce_test < best_loss:\n",
    "            print(\"Saving Weights\")\n",
    "            best_loss = ce_test\n",
    "            torch.save(model.state_dict(), f=weights_path / 'gated_pixel_cnn.pt')\n",
    "        scheduler.step(ce_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec359806-7ee2-48c1-9d98-be7e8d2c2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75ea38-c403-4a97-a561-18301a8d2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f=weights_path / 'gated_pixel_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f0096-8f3d-4530-8470-0397e9d537f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample(model, num_images):\n",
    "    width = 28\n",
    "    height = 28\n",
    "    img = torch.zeros((num_images, 1, height, width)).to(DEVICE)\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "                pred = model(img / 255.0)[:, :, :, h, w].squeeze()\n",
    "                probs = F.softmax(pred, dim=1)\n",
    "                sampled = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "                img[:, 0, h, w] = sampled\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea0327-8a37-47de-a068-4d86454d1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images = sample(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd34c4-ebdd-46a2-a863-f20bf1091e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "columns = 5\n",
    "rows = 2\n",
    "for i, img in enumerate(sampled_images):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E3EUZlICXESo",
   "metadata": {
    "id": "E3EUZlICXESo"
   },
   "source": [
    "## Conditional PixelCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185547c-62b7-4a64-9c92-e0d792483c0f",
   "metadata": {},
   "source": [
    "A conditional `PixelCNN` conditions the model on the label. So that when we provide the trained model with a label of 1, the image should correspond to the label. That way we gain control over what images we produce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a20a86-e777-49de-8af0-5033d7a1e336",
   "metadata": {},
   "source": [
    "We do the conditioning in the residial block, by generating two embeddings, that we add to the previous vertical and horizontal stack respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fd184-3a85-467b-aa11-76fc148c6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGatedResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size=3, dilation=1):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels, out_channels=2*in_channels, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.h = HorizontalStackConvolution(in_channels, out_channels=2*in_channels, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.v_to_h = nn.Conv2d(2*in_channels, 2*in_channels, kernel_size=1)\n",
    "        self.v_to_res = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        \n",
    "        self.v_embedding = nn.Embedding(num_embeddings=10, embedding_dim=in_channels)\n",
    "        self.h_embedding = nn.Embedding(num_embeddings=10, embedding_dim=in_channels)\n",
    "\n",
    "    def forward(self, v_prev, h_prev, num_cls):\n",
    "        # calculate embeddings to condition the model\n",
    "        v_embedding = self.v_embedding(num_cls).unsqueeze(-1).unsqueeze(-1)\n",
    "        h_embedding = self.h_embedding(num_cls).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # vertical stack\n",
    "        v = self.v(v_prev + v_embedding)\n",
    "        v_f, v_g = v.chunk(2, dim=1)\n",
    "        v_out = torch.tanh(v_f) * torch.sigmoid(v_g)\n",
    "\n",
    "        # vertical to horizontal\n",
    "        v_to_h = self.v_to_h(v)\n",
    "        \n",
    "        # horizontal stack\n",
    "        h = self.h(h_prev + h_embedding) + v_to_h\n",
    "        h_f, h_g = h.chunk(2, dim=1)\n",
    "        h_out = torch.tanh(h_f) * torch.sigmoid(h_g)\n",
    "        \n",
    "        # skip connection\n",
    "        h_out = self.v_to_res(h_out)\n",
    "        h_out += h_prev\n",
    "\n",
    "        return v_out, h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f23fc2-868a-4059-815a-e581d2f3f91d",
   "metadata": {},
   "source": [
    "The model is almost identical. The only difference is the label that we provide to each gated layer in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bad86-bc5c-4891-ba95-1b746b9c9413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConditionalGatedPixelCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, dilations=[1, 2, 1, 4, 1, 2, 1]):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels=1,\n",
    "                                          out_channels=hidden_dim,\n",
    "                                          kernel_size=7,\n",
    "                                          mask_type='A')\n",
    "        self.h = HorizontalStackConvolution(in_channels=1,\n",
    "                                          kernel_size=7,\n",
    "                                          out_channels=hidden_dim,\n",
    "                                          mask_type='A')\n",
    "        \n",
    "        self.gated_residual_blocks = nn.ModuleList([ConditionalGatedResidualBlock(hidden_dim, kernel_size=3, dilation=dilation) \n",
    "                                                 for dilation in dilations])\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=hidden_dim, \n",
    "                              out_channels=hidden_dim,\n",
    "                              kernel_size=1)\n",
    "        \n",
    "        # we apply a 256 way softmax\n",
    "        self.output = nn.Conv2d(in_channels=hidden_dim, \n",
    "                                out_channels=256,\n",
    "                                kernel_size=1)\n",
    "    def forward(self, x, label):\n",
    "        v = self.v(x)\n",
    "        h = self.h(x)\n",
    "        \n",
    "        for gated_layer in self.gated_residual_blocks:\n",
    "            v, h = gated_layer(v, h, label)\n",
    "        \n",
    "        out = self.conv(F.relu(h))\n",
    "        out = self.output(F.relu(out))\n",
    "        # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
    "        out = out.unsqueeze(dim=2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df04dc-3385-4ce7-bac3-bdd769d6e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalGatedPixelCNN(hidden_dim=HIDDEN_DIM).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, min_lr=MIN_LEARNING_RATE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f68e31-7f95-4543-b181-30327f95ceb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train():\n",
    "    best_loss = float(\"inf\")\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for features, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features = features.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                logits = model(features.float() / 255, labels)\n",
    "                loss = criterion(logits, features.long())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss.cpu().item())\n",
    "\n",
    "\n",
    "        # evaluate on the test dataset\n",
    "        with torch.inference_mode():\n",
    "            for features, labels in test_dataloader:\n",
    "                features = features.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    logits = model(features.float() / 255, labels)\n",
    "                    loss = criterion(logits, features.long())\n",
    "                test_losses.append(loss.cpu().item())\n",
    "\n",
    "        end_time = time.time()\n",
    "        ce_train = sum(train_losses)/len(train_losses)\n",
    "        ce_test = sum(test_losses)/len(test_losses)\n",
    "        print(f'Epoch: {epoch+1}/{NUM_EPOCHS}, Cross Entropy Train: {ce_train:.4f}, Cross Entropy Test: {ce_test:.4f}, Elapsed Time: {end_time-start_time:.2f}sec')\n",
    "\n",
    "        if ce_test < best_loss:\n",
    "            print(\"Saving Weights\")\n",
    "            best_loss = ce_test\n",
    "            torch.save(model.state_dict(), f=weights_path / 'conditional_gated_pixel_cnn.pt')\n",
    "        scheduler.step(ce_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b6803-9f09-4745-b811-72966d9da831",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6afe7-6dc1-4ffb-8aa1-04c97724da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f=weights_path / 'conditional_gated_pixel_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f39cc-00eb-4708-8e39-57de26a14859",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample(model, num_images):\n",
    "    width = 28\n",
    "    height = 28\n",
    "    img = torch.zeros((num_images, 1, height, width), device=DEVICE)\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "                labels = torch.arange(10, device=DEVICE)\n",
    "                pred = model(img / 255.0, labels)[:, :, :, h, w].squeeze()\n",
    "                probs = F.softmax(pred, dim=1)\n",
    "                sampled = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "                img[:, 0, h, w] = sampled\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a8aec-1e94-41da-87fc-389cbd4b7aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images = sample(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5a7c0-2856-486b-8ce3-75893f4f75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "columns = 5\n",
    "rows = 2\n",
    "for i, img in enumerate(sampled_images):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
