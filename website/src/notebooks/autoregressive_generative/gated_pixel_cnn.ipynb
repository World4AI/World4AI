{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7",
      "metadata": {
        "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7"
      },
      "source": [
        "# Gated PixelCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc",
      "metadata": {
        "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc"
      },
      "source": [
        "This part is highly inspired by [uvadlc-notebooks](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "i_bdSoJhYSRu",
        "outputId": "7aa64fb4-8e41-4ece-b06f-2185207fe25e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i_bdSoJhYSRu",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 23 18:24:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "77ab572b-4608-4903-bb1c-820343fe3e1c",
      "metadata": {
        "id": "77ab572b-4608-4903-bb1c-820343fe3e1c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pathlib\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201",
      "metadata": {
        "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201"
      },
      "outputs": [],
      "source": [
        "## hyperparameters\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "NUM_CHANNELS = 64\n",
        "NUM_LAYERS = 7\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad",
      "metadata": {
        "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad"
      },
      "outputs": [],
      "source": [
        "def discretize(sample):\n",
        "    return (sample * 255).to(torch.long)\n",
        "\n",
        "transform = T.Compose([T.ToTensor(),\n",
        "                        discretize])\n",
        "\n",
        "\n",
        "train_dataset = MNIST(root='../datasets/', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='../datasets/', train=False, download=False, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615",
      "metadata": {
        "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615"
      },
      "outputs": [],
      "source": [
        "train_dataloader = data.DataLoader(dataset=train_dataset, \n",
        "                                   batch_size=BATCH_SIZE, \n",
        "                                   shuffle=True, \n",
        "                                   num_workers=2,\n",
        "                                   drop_last=True,\n",
        "                                   pin_memory=True)\n",
        "test_dataloader = data.DataLoader(dataset=test_dataset, \n",
        "                                   batch_size=BATCH_SIZE, \n",
        "                                   shuffle=True, \n",
        "                                   num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195",
      "metadata": {
        "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195"
      },
      "outputs": [],
      "source": [
        "class MaskedConvolution(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, mask, **kwargs):\n",
        "        super().__init__()\n",
        "        kernel_size = mask.shape\n",
        "        dilation = 1 if \"dilation\" not in kwargs else kwargs[\"dilation\"]\n",
        "        padding = tuple([dilation * (size-1)//2 for size in kernel_size])\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, \n",
        "                              out_channels=out_channels, \n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              **kwargs)\n",
        "        \n",
        "        self.register_buffer('mask', mask)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "          self.conv.weight *= self.mask \n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533",
      "metadata": {
        "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533"
      },
      "outputs": [],
      "source": [
        "mask = torch.rand(3, 3)\n",
        "img = torch.randn(1, 1, 28, 28)\n",
        "mask_conv = MaskedConvolution(1, 32, mask, dilation=2)\n",
        "test = mask_conv(img)\n",
        "# test = test.sum()\n",
        "# test.backward()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class MaskedConvolution(nn.Module):\n",
        "\n",
        "#     def __init__(self, c_in, c_out, mask, **kwargs):\n",
        "#         \"\"\"\n",
        "#         Implements a convolution with mask applied on its weights.\n",
        "#         Inputs:\n",
        "#             c_in - Number of input channels\n",
        "#             c_out - Number of output channels\n",
        "#             mask - Tensor of shape [kernel_size_H, kernel_size_W] with 0s where\n",
        "#                    the convolution should be masked, and 1s otherwise.\n",
        "#             kwargs - Additional arguments for the convolution\n",
        "#         \"\"\"\n",
        "#         super().__init__()\n",
        "#         # For simplicity: calculate padding automatically\n",
        "#         kernel_size = (mask.shape[0], mask.shape[1])\n",
        "#         dilation = 1 if \"dilation\" not in kwargs else kwargs[\"dilation\"]\n",
        "#         padding = tuple([dilation*(kernel_size[i]-1)//2 for i in range(2)])\n",
        "#         # Actual convolution\n",
        "#         self.conv = nn.Conv2d(c_in, c_out, kernel_size, padding=padding, **kwargs)\n",
        "\n",
        "#         # Mask as buffer => it is no parameter but still a tensor of the module\n",
        "#         # (must be moved with the devices)\n",
        "#         self.register_buffer('mask', mask[None,None])\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         self.conv.weight.data *= self.mask # Ensures zero's at masked positions\n",
        "#         return self.conv(x)\n"
      ],
      "metadata": {
        "id": "A7ogo1yn4kfg"
      },
      "id": "A7ogo1yn4kfg",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765",
      "metadata": {
        "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765"
      },
      "outputs": [],
      "source": [
        "# class VerticalStackConvolution(MaskedConvolution):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "#         assert mask_type in ['A', 'B']\n",
        "#         mask = torch.ones(kernel_size, kernel_size)\n",
        "#         mask[kernel_size//2+1:,:] = 0\n",
        "#         if mask_type=='A':\n",
        "#             mask[kernel_size//2,:] = 0\n",
        "        \n",
        "#         super().__init__(in_channels, out_channels, mask, dilation=dilation)\n",
        "        \n",
        "# class HorizontalStackConvolution(MaskedConvolution):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "#         assert mask_type in ['A', 'B']\n",
        "#         mask = torch.ones(1, kernel_size)\n",
        "#         mask[0, kernel_size//2+1:] = 0\n",
        "#         if mask_type=='A':\n",
        "#             mask[0, kernel_size//2] = 0\n",
        "#         super().__init__(in_channels, out_channels, mask, dilation=dilation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VerticalStackConvolution(MaskedConvolution):\n",
        "\n",
        "    def __init__(self, c_in, c_out, kernel_size=3, mask_center=False, **kwargs):\n",
        "        # Mask out all pixels below. For efficiency, we could also reduce the kernel\n",
        "        # size in height, but for simplicity, we stick with masking here.\n",
        "        mask = torch.ones(kernel_size, kernel_size)\n",
        "        mask[kernel_size//2+1:,:] = 0\n",
        "\n",
        "        # For the very first convolution, we will also mask the center row\n",
        "        if mask_center:\n",
        "            mask[kernel_size//2,:] = 0\n",
        "\n",
        "        super().__init__(c_in, c_out, mask, **kwargs)\n",
        "\n",
        "class HorizontalStackConvolution(MaskedConvolution):\n",
        "\n",
        "    def __init__(self, c_in, c_out, kernel_size=3, mask_center=False, **kwargs):\n",
        "        # Mask out all pixels on the left. Note that our kernel has a size of 1\n",
        "        # in height because we only look at the pixel in the same row.\n",
        "        mask = torch.ones(1,kernel_size)\n",
        "        mask[0,kernel_size//2+1:] = 0\n",
        "\n",
        "        # For the very first convolution, we will also mask the center pixel\n",
        "        if mask_center:\n",
        "            mask[0,kernel_size//2] = 0\n",
        "\n",
        "        super().__init__(c_in, c_out, mask, **kwargs)\n"
      ],
      "metadata": {
        "id": "2ZCrsZ7B4x0p"
      },
      "id": "2ZCrsZ7B4x0p",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
      "metadata": {
        "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050aca97-5ac3-4bfa-a051-45a2454f1ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 0.]])\n",
            "tensor([[1., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "test = VerticalStackConvolution(1, 32, 3)\n",
        "print(test.mask)\n",
        "test = VerticalStackConvolution(1, 32, 3, 'A')\n",
        "print(test.mask)\n",
        "test = HorizontalStackConvolution(1, 32, 3)\n",
        "print(test.mask)\n",
        "test = HorizontalStackConvolution(1, 32, 3, 'A')\n",
        "print(test.mask)\n",
        "del test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedMaskedConv(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, **kwargs):\n",
        "        \"\"\"\n",
        "        Gated Convolution block implemented the computation graph shown above.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv_vert = VerticalStackConvolution(c_in, c_out=2*c_in, **kwargs)\n",
        "        self.conv_horiz = HorizontalStackConvolution(c_in, c_out=2*c_in, **kwargs)\n",
        "        self.conv_vert_to_horiz = nn.Conv2d(2*c_in, 2*c_in, kernel_size=1, padding=0)\n",
        "        self.conv_horiz_1x1 = nn.Conv2d(c_in, c_in, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, v_stack, h_stack):\n",
        "        # Vertical stack (left)\n",
        "        v_stack_feat = self.conv_vert(v_stack)\n",
        "        v_val, v_gate = v_stack_feat.chunk(2, dim=1)\n",
        "        v_stack_out = torch.tanh(v_val) * torch.sigmoid(v_gate)\n",
        "\n",
        "        # Horizontal stack (right)\n",
        "        h_stack_feat = self.conv_horiz(h_stack)\n",
        "        h_stack_feat = h_stack_feat + self.conv_vert_to_horiz(v_stack_feat)\n",
        "        h_val, h_gate = h_stack_feat.chunk(2, dim=1)\n",
        "        h_stack_feat = torch.tanh(h_val) * torch.sigmoid(h_gate)\n",
        "        h_stack_out = self.conv_horiz_1x1(h_stack_feat)\n",
        "        h_stack_out = h_stack_out + h_stack\n",
        "\n",
        "        return v_stack_out, h_stack_out\n"
      ],
      "metadata": {
        "id": "Tx52b6uW44dY"
      },
      "id": "Tx52b6uW44dY",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "40460c19-a097-483a-a5d7-ef39118242df",
      "metadata": {
        "id": "40460c19-a097-483a-a5d7-ef39118242df"
      },
      "outputs": [],
      "source": [
        "# # test gated convolution\n",
        "# gated = GatedConvolution(NUM_CHANNELS, dilation=2)\n",
        "# v = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "# h = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "# v, h = gated(v, h)\n",
        "# print(v.shape, h.shape)\n",
        "# del gated, v, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f6115b45-b690-463f-be12-a8125b36c863",
      "metadata": {
        "id": "f6115b45-b690-463f-be12-a8125b36c863"
      },
      "outputs": [],
      "source": [
        "# class GatedPixelCNN(nn.Module):\n",
        "#     def __init__(self, hidden_channels, num_gated=7):\n",
        "#         super().__init__()\n",
        "#         self.v = VerticalStackConvolution(in_channels=1,\n",
        "#                                           out_channels=hidden_channels,\n",
        "#                                           kernel_size=7,\n",
        "#                                           mask_type='A')\n",
        "#         self.h = HorizontalStackConvolution(in_channels=1,\n",
        "#                                           kernel_size=7,\n",
        "#                                           out_channels=hidden_channels,\n",
        "#                                           mask_type='A')\n",
        "        \n",
        "#         self.gated_convolutions = nn.ModuleList([GatedConvolution(hidden_channels, kernel_size=7) \n",
        "#                                                  for _ in range(num_gated)])\n",
        "#         # we apply a 256 way softmax\n",
        "#         self.output = nn.Conv2d(in_channels=hidden_channels, \n",
        "#                                 out_channels=256,\n",
        "#                                 kernel_size=1)\n",
        "#     def forward(self, x):\n",
        "#         x = (x.float() / 255.0) * 2 - 1\n",
        "\n",
        "#         v = self.v(x)\n",
        "#         h = self.h(x)\n",
        "        \n",
        "#         for gated_layer in self.gated_convolutions:\n",
        "#             v, h = gated_layer(v, h)\n",
        "#         out = self.output(F.elu(h))\n",
        "#         # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
        "#         out = out.unsqueeze(dim=2)\n",
        "#         return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_hidden):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolutions skipping the center pixel\n",
        "        self.conv_vstack = VerticalStackConvolution(c_in, c_hidden, mask_center=True)\n",
        "        self.conv_hstack = HorizontalStackConvolution(c_in, c_hidden, mask_center=True)\n",
        "        # Convolution block of PixelCNN. We use dilation instead of downscaling\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            GatedMaskedConv(c_hidden),\n",
        "            GatedMaskedConv(c_hidden, dilation=2),\n",
        "            GatedMaskedConv(c_hidden),\n",
        "            GatedMaskedConv(c_hidden, dilation=4),\n",
        "            GatedMaskedConv(c_hidden),\n",
        "            GatedMaskedConv(c_hidden, dilation=2),\n",
        "            GatedMaskedConv(c_hidden)\n",
        "        ])\n",
        "        # Output classification convolution (1x1)\n",
        "        self.conv_out = nn.Conv2d(c_hidden, c_in * 256, kernel_size=1, padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward image through model and return logits for each pixel.\n",
        "        Inputs:\n",
        "            x - Image tensor with integer values between 0 and 255.\n",
        "        \"\"\"\n",
        "        # Scale input from 0 to 255 back to -1 to 1\n",
        "        x = (x.float() / 255.0) * 2 - 1\n",
        "\n",
        "        # Initial convolutions\n",
        "        v_stack = self.conv_vstack(x)\n",
        "        h_stack = self.conv_hstack(x)\n",
        "        # Gated Convolutions\n",
        "        for layer in self.conv_layers:\n",
        "            v_stack, h_stack = layer(v_stack, h_stack)\n",
        "        # 1x1 classification convolution\n",
        "        # Apply ELU before 1x1 convolution for non-linearity on residual connection\n",
        "        out = self.conv_out(F.elu(h_stack))\n",
        "\n",
        "        # Output dimensions: [Batch, Classes, Channels, Height, Width]\n",
        "        out = out.reshape(out.shape[0], 256, out.shape[1]//256, out.shape[2], out.shape[3])\n",
        "        return out\n",
        "\n",
        "    def calc_likelihood(self, x):\n",
        "        # Forward pass with bpd likelihood calculation\n",
        "        pred = self.forward(x)\n",
        "        nll = F.cross_entropy(pred, x, reduction='none')\n",
        "        bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))\n",
        "        return bpd.mean()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, img_shape, img=None):\n",
        "        \"\"\"\n",
        "        Sampling function for the autoregressive model.\n",
        "        Inputs:\n",
        "            img_shape - Shape of the image to generate (B,C,H,W)\n",
        "            img (optional) - If given, this tensor will be used as\n",
        "                             a starting image. The pixels to fill\n",
        "                             should be -1 in the input tensor.\n",
        "        \"\"\"\n",
        "        # Create empty image\n",
        "        if img is None:\n",
        "            img = torch.zeros(img_shape, dtype=torch.long).to(device) - 1\n",
        "        # Generation loop\n",
        "        for h in tqdm(range(img_shape[2]), leave=False):\n",
        "            for w in range(img_shape[3]):\n",
        "                for c in range(img_shape[1]):\n",
        "                    # Skip if not to be filled (-1)\n",
        "                    if (img[:,c,h,w] != -1).all().item():\n",
        "                        continue\n",
        "                    # For efficiency, we only have to input the upper part of the image\n",
        "                    # as all other parts will be skipped by the masked convolutions anyways\n",
        "                    pred = self.forward(img[:,:,:h+1,:])\n",
        "                    probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
        "                    img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
        "        return img"
      ],
      "metadata": {
        "id": "aP44YFxy5FJB"
      },
      "id": "aP44YFxy5FJB",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813",
      "metadata": {
        "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813"
      },
      "outputs": [],
      "source": [
        "# # test pixelcnn\n",
        "# gated_pixel_cnn = GatedPixelCNN(NUM_CHANNELS)\n",
        "# #print(gated_pixel_cnn)\n",
        "# img = torch.randn(1, 1, 28, 28)\n",
        "# labels = (torch.rand(1, 1, 28, 28) * 256).long()\n",
        "# output = gated_pixel_cnn(img)\n",
        "# print(output.shape, labels.shape)\n",
        "# test = F.cross_entropy(output, labels, reduction='mean')\n",
        "# # print(test)\n",
        "# del gated_pixel_cnn, img, labels, output, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c",
      "metadata": {
        "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c"
      },
      "outputs": [],
      "source": [
        "model = PixelCNN(c_in=1, c_hidden=64).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_path = pathlib.Path('/content/gdrive')\n",
        "weights_path = drive_path / 'MyDrive/weights'"
      ],
      "metadata": {
        "id": "X9U3ryrChI0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba450b0-5be7-424e-bd83-a5dd43e13770"
      },
      "id": "X9U3ryrChI0r",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
      "metadata": {
        "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "df81f892-7df3-486c-cf80-c0d63761b98e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50, BPD Train: 1.3411, BPD Test: 1.1581, Elapsed Time: 68.71sec\n",
            "Saving Weights\n",
            "Epoch: 2/50, BPD Train: 1.1454, BPD Test: 1.1353, Elapsed Time: 66.82sec\n",
            "Saving Weights\n",
            "Epoch: 3/50, BPD Train: 1.1207, BPD Test: 1.1120, Elapsed Time: 66.42sec\n",
            "Saving Weights\n",
            "Epoch: 4/50, BPD Train: 1.1041, BPD Test: 1.0898, Elapsed Time: 66.45sec\n",
            "Saving Weights\n",
            "Epoch: 5/50, BPD Train: 1.0919, BPD Test: 1.0829, Elapsed Time: 66.57sec\n",
            "Saving Weights\n",
            "Epoch: 6/50, BPD Train: 1.0730, BPD Test: 1.0610, Elapsed Time: 67.33sec\n",
            "Saving Weights\n",
            "Epoch: 7/50, BPD Train: 1.0664, BPD Test: 1.0527, Elapsed Time: 66.76sec\n",
            "Saving Weights\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8cca43c23561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "best_loss = float(\"inf\")\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    for features, _ in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        features = features.to(DEVICE)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          loss = model.calc_likelihood(features)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "      \n",
        "        train_losses.append(loss.cpu().item())\n",
        "\n",
        "        \n",
        "    # evaluate on the test dataset\n",
        "    with torch.inference_mode():\n",
        "        for features, _ in test_dataloader:\n",
        "            features = features.to(DEVICE)\n",
        "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "              loss = model.calc_likelihood(features)\n",
        "            test_losses.append(loss.cpu().item())\n",
        "\n",
        "    end_time = time.time()\n",
        "    bpd_train = sum(train_losses)/len(train_losses)\n",
        "    bpd_test = sum(test_losses)/len(test_losses)\n",
        "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS}, BPD Train: {bpd_train:.4f}, BPD Test: {bpd_test:.4f}, Elapsed Time: {end_time-start_time:.2f}sec')\n",
        "    \n",
        "    if bpd_test < best_loss:\n",
        "      print(\"Saving Weights\")\n",
        "      best_loss = bpd_test\n",
        "      torch.save(model.state_dict(), f=weights_path / 'pixel_cnn.pt')\n",
        "    scheduler.step(bpd_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6bfcf6-33f5-4b13-858f-8dd8a8c5a94b",
      "metadata": {
        "id": "bb6bfcf6-33f5-4b13-858f-8dd8a8c5a94b"
      },
      "outputs": [],
      "source": [
        "# sample an image\n",
        "@torch.no_grad()\n",
        "def sample(model, img_shape):\n",
        "    img = torch.zeros(img_shape, device=DEVICE) - 1\n",
        "    # Generation loop\n",
        "    _, channel, height, width = img_shape\n",
        "    for h in range(height):\n",
        "        for w in range(width):\n",
        "            for c in range(channel):\n",
        "                pred = model(img[:,:,:h+1,:])\n",
        "                probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
        "                img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f46f61-1b47-49ca-a47f-b8ebbb23c429",
      "metadata": {
        "id": "11f46f61-1b47-49ca-a47f-b8ebbb23c429"
      },
      "outputs": [],
      "source": [
        "img = sample(model, img_shape=(1, 1, 28, 28))\n",
        "print(img.shape)\n",
        "plt.imshow(img.cpu().long().squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional PixelCNN"
      ],
      "metadata": {
        "id": "E3EUZlICXESo"
      },
      "id": "E3EUZlICXESo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}