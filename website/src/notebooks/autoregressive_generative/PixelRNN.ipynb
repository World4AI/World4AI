{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7",
   "metadata": {},
   "source": [
    "# PixelRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f160d8-3791-4da2-a1bb-7f5a6d395d48",
   "metadata": {},
   "source": [
    "Partially inspired by https://github.com/heechan95/PixelRNN-pytorch/blob/master/PixelRNN%20pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b62d1b-ed3a-4d49-9724-ab1b2c906b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03975ce-4cf8-4d01-8109-012515fef359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from the paper\n",
    "batch_size = 16\n",
    "num_layers = 7\n",
    "hidden_dim = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc7415-6a9e-4aa4-b449-609ac07ead83",
   "metadata": {},
   "source": [
    "The architecture is designed for MNIST only and for static inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1236a856-b083-4515-ad3f-c06827ceeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "# 1. 7 x 7 with conv mask A\n",
    "# 2. Row LSTM with residual blocks\n",
    "#    a. i-s: 3x1 mask with conv mask B\n",
    "#    b. s-s: 3x1 no mask\n",
    "# 3. ReLU + 1x1 conv layer with mask B (2 layers)\n",
    "# 4. 256 Ways softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9648172b-1e26-4f75-9f82-3b980cf94485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type A masked Conv2d \n",
    "class MaskedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels=1, \n",
    "                 out_channels=hidden_dim,\n",
    "                 kernel_size=7,\n",
    "                 padding=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             padding=padding)\n",
    "        \n",
    "        mask_idxs = torch.arange(0, kernel_size**2).view(kernel_size, kernel_size)\n",
    "        mask = (mask_idxs < padding * kernel_size + padding).float()\n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # apply mask, see below link for more info\n",
    "        # https://discuss.pytorch.org/t/applying-custom-mask-on-kernel-for-cnn/87099\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2108fbbd-5d09-46ea-93bf-af54285f02a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2dLayer = MaskedConv2d()\n",
    "testdata = torch.randn(1, 1, 28, 28)\n",
    "test2dLayer(testdata).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbeb2c72-1af2-488d-ba57-7fa160d2d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type B masked Conv1d\n",
    "class MaskedConv1d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=hidden_dim,\n",
    "                 out_channels=hidden_dim,\n",
    "                 kernel_size=3,\n",
    "                 padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding=padding)\n",
    "        \n",
    "        mask = torch.ones(kernel_size)\n",
    "        mask[2] = 0\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb5445b7-aaa5-4714-bc5f-5dd7ff3f3752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1dLayer = MaskedConv1d()\n",
    "# 16 -> hidden size (number of feature maps), 28 -> width\n",
    "# the layer processes one row at a time\n",
    "testdata = torch.randn(1, 16, 28)\n",
    "test1dLayer(testdata).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e268b8-97cf-495d-91ef-d14a724c00a9",
   "metadata": {},
   "source": [
    "The LSTM cell receives one cell at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c34f46be-f3f6-4265-8cda-fe35e01e8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_i_s = nn.Conv1d(in_channels=hidden_dim, \n",
    "                                  out_channels=4 * hidden_dim,\n",
    "                                  kernel_size=3,\n",
    "                                  padding=1)\n",
    "        \n",
    "        self.conv_s_s = MaskedConv1d(in_channels=hidden_dim, \n",
    "                                        out_channels=4 * hidden_dim,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        i_s = self.conv_i_s(x)\n",
    "        s_s = self.conv_s_s(h_prev)\n",
    "        \n",
    "        o, f, i, g = torch.split(i_s + s_s, hidden_dim, 1)\n",
    "        o = torch.sigmoid(o)\n",
    "        f = torch.sigmoid(f)\n",
    "        i = torch.sigmoid(i)\n",
    "        g = torch.tanh(g)\n",
    "        \n",
    "        c = f * c_prev + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67edfe46-52c3-42c8-805e-a1b234022041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 28]) torch.Size([1, 16, 28])\n"
     ]
    }
   ],
   "source": [
    "# test Cell\n",
    "row = torch.randn(1, 16, 28)\n",
    "prev_h = torch.randn(1, 16, 28)\n",
    "prev_c = torch.randn(1, 16, 28)\n",
    "\n",
    "cell = RowLSTMCell()\n",
    "h, c = cell(row, prev_h, prev_c)\n",
    "print(h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a96c9b74-01f2-4909-88e7-6ad149dbf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cell = RowLSTMCell()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, _, num_rows, _ = x.shape\n",
    "        h_prev = torch.zeros(batch_size, hidden_dim, 28)\n",
    "        c_prev = torch.zeros(batch_size, hidden_dim, 28)\n",
    "\n",
    "        rows = []\n",
    "        for row_idx in range(num_rows):\n",
    "            image_row = x[:, :, row_idx, :]\n",
    "            h_prev, c_prev = self.cell(image_row, h_prev, c_prev)\n",
    "            rows.append(h_prev.unsqueeze(dim=2))\n",
    "        return torch.cat(rows, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "036463fb-eae5-406a-8793-d302a6027c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 28, 28])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test row lstm\n",
    "image = torch.randn(batch_size, 16, 28, 28)\n",
    "row_lstm = RowLSTM()\n",
    "row_lstm(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b38fe1-bbcc-47ea-8c8c-d6fdc2338a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0fb017-7685-488f-96a6-ac4dd361f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "                                           train=True, \n",
    "                                           transform=T.ToTensor(), \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "                                           train=False, \n",
    "                                           transform=T.ToTensor(), \n",
    "                                           download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2545fb-47fe-497f-979a-c2b99b4633f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
