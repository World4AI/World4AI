{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7",
   "metadata": {},
   "source": [
    "# PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b62d1b-ed3a-4d49-9724-ab1b2c906b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# from torch import nn, optim\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# import torchvision\n",
    "# import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03975ce-4cf8-4d01-8109-012515fef359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters from the paper\n",
    "# batch_size = 16\n",
    "# num_layers = 7\n",
    "# num_channels = 32\n",
    "\n",
    "# # other parameters\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# lr = 0.001\n",
    "# num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc7415-6a9e-4aa4-b449-609ac07ead83",
   "metadata": {},
   "source": [
    "The architecture is designed for MNIST only and for static inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1236a856-b083-4515-ad3f-c06827ceeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "# 1. 7 x 7 with conv mask A\n",
    "# 2. stack 3 x 3 with conv mask B\n",
    "# 3. ReLU + 1x1 conv layer with mask B (2 layers)\n",
    "# 4. 256 Ways softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9648172b-1e26-4f75-9f82-3b980cf94485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Type A or B masked Conv2d \n",
    "# class MaskedConv2d(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, mask='A'): \n",
    "#         super().__init__()\n",
    "#         # calculate the padding automatically \n",
    "#         # so that input width+height dims = output dims\n",
    "#         padding = int((kernel_size - 1) / 2)\n",
    "        \n",
    "#         self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "#                              out_channels=out_channels,\n",
    "#                              kernel_size=kernel_size,\n",
    "#                              padding=padding)\n",
    "        \n",
    "#         # calculate masks A or B as in the original paper\n",
    "#         mask_idxs = torch.arange(0, kernel_size**2).view(kernel_size, kernel_size)\n",
    "#         if mask == 'A':\n",
    "#             mask = (mask_idxs < padding * kernel_size + padding).float().to(device)\n",
    "#         elif mask == 'B':\n",
    "#             mask = (mask_idxs <= padding * kernel_size + padding).float().to(device)\n",
    "#         self.register_buffer('mask', mask)\n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         # apply mask, see below link for more info\n",
    "#         # https://discuss.pytorch.org/t/applying-custom-mask-on-kernel-for-cnn/87099\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
    "#         return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2108fbbd-5d09-46ea-93bf-af54285f02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2dLayer = MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7).to(device)\n",
    "# testdata = torch.randn(1, 1, 28, 28, device=device)\n",
    "# test2dLayer(testdata).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b38fe1-bbcc-47ea-8c8c-d6fdc2338a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PixelCNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#                 MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7, mask='A'), \n",
    "#                 nn.ReLU(),\n",
    "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
    "#                 nn.ReLU(),\n",
    "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Conv2d(num_channels, num_channels*2, kernel_size=1),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Conv2d(num_channels*2, 1, kernel_size=1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56925f7-b69a-48fa-9e12-714ab7fe3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "# model = PixelCNN().to(device)\n",
    "# model(test_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0fb017-7685-488f-96a6-ac4dd361f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "#                                            train=True, \n",
    "#                                            transform=T.ToTensor(), \n",
    "#                                            download=True)\n",
    "\n",
    "# test_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "#                                            train=False, \n",
    "#                                            transform=T.ToTensor(), \n",
    "#                                            download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "265ab1e6-f142-4eb4-a43b-d5c293cc272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
    "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28992ff-e41f-4478-aeec-d2072adac04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PixelCNN().to(device)\n",
    "# # mse would most likely also work\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b2545fb-47fe-497f-979a-c2b99b4633f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     for epoch in range(1, num_epochs+1):\n",
    "#         train_loss = []\n",
    "#         for features, _ in train_dataloader:\n",
    "#             features = features.to(device)\n",
    "#             logits = model(features)\n",
    "#             loss = criterion(logits, features)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss.append(loss.cpu().item())\n",
    "        \n",
    "#         print(f'Epoch: {epoch}/{num_epochs}, Loss: {sum(train_loss)/len(train_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907049b4-a617-44d8-b662-33c59924c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3ba7ec-c6f8-4151-aa82-f1ed60d76ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the reconstruction by comparing original images with the reconstructed images\n",
    "# with torch.inference_mode():\n",
    "#     for idx in range(5):\n",
    "#         image, _ = test_dataset[idx]\n",
    "#         image = image.unsqueeze(0)\n",
    "#         generated_image = model(image.to(device))\n",
    "#         generated_image = torch.sigmoid(generated_image)\n",
    "#         generated_image = generated_image.squeeze()\n",
    "\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "#         plt.title(\"Original\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(generated_image.cpu(), cmap=\"gray\")\n",
    "#         plt.title(\"Reconstructed\")\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2f4bbd-25c5-4687-85ad-df98940ec041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, _ = test_dataset[0]\n",
    "# image = image.squeeze()\n",
    "# for row_idx, row in enumerate(image):\n",
    "#     for col_idx, _ in enumerate(image):\n",
    "#         if row_idx >= 14:\n",
    "#             image[row_idx][col_idx] = 0\n",
    "# plt.imshow(image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fba9a90-be3c-4e63-aee0-9d829b3eef1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image, _ = test_dataset[0]\n",
    "# image = image.squeeze()\n",
    "# for row_idx, row in enumerate(image):\n",
    "#     for col_idx, _ in enumerate(image):\n",
    "#         if row_idx >= 14:\n",
    "#             image[row_idx][col_idx] = 0\n",
    "\n",
    "# image = image.unsqueeze(0).unsqueeze(0).to(device) \n",
    "# with torch.inference_mode():\n",
    "#     for row_idx in range(28):\n",
    "#         for col_idx in range(28):\n",
    "#             generated_image = model(image)\n",
    "#             generated_image = torch.sigmoid(generated_image)\n",
    "#             generated_image = generated_image.squeeze()\n",
    "#             if row_idx >=14:\n",
    "#                 image[0][0][row_idx][col_idx] = generated_image[row_idx][col_idx]\n",
    "            \n",
    "# plt.imshow(image.cpu().squeeze(), cmap=\"gray\")\n",
    "# plt.title(\"Reconstructed\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cda0e-7112-4a0c-a064-9801e6d1d524",
   "metadata": {},
   "source": [
    "## PixelCNN with Gated Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc",
   "metadata": {},
   "source": [
    "This part is highly inspired by [uvadlc-notebooks](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "77ab572b-4608-4903-bb1c-820343fe3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "NUM_CHANNELS = 64\n",
    "NUM_LAYERS = 7\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='../datasets/', train=True, download=True, transform=T.ToTensor())\n",
    "test_dataset = MNIST(root='../datasets/', train=False, download=False, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(dataset=train_dataset, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   num_workers=2,\n",
    "                                   drop_last=True)\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, mask):\n",
    "        super().__init__()\n",
    "        kernel_size = mask.shape\n",
    "        padding = tuple([(size-1)//2 for size in kernel_size])\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels, \n",
    "                              kernel_size=kernel_size,\n",
    "                              padding=padding)\n",
    "        \n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.rand(3, 3)\n",
    "img = torch.randn(1, 1, 28, 28)\n",
    "mask_conv = MaskedConvolution(1, 32, mask)\n",
    "mask_conv(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalStackConvolution(MaskedConvolution):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B'):\n",
    "        assert mask_type in ['A', 'B']\n",
    "        mask = torch.ones(kernel_size, kernel_size)\n",
    "        mask[kernel_size//2+1:,:] = 0\n",
    "        if mask_type=='A':\n",
    "            mask[kernel_size//2,:] = 0\n",
    "        super().__init__(in_channels, out_channels, mask)\n",
    "        \n",
    "class HorizontalStackConvolution(MaskedConvolution):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B'):\n",
    "        assert mask_type in ['A', 'B']\n",
    "        mask = torch.ones(1, kernel_size)\n",
    "        mask[0, kernel_size//2+1:] = 0\n",
    "        if mask_type=='A':\n",
    "            mask[0, kernel_size//2] = 0\n",
    "        super().__init__(in_channels, out_channels, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 0.]])\n",
      "tensor([[1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "test = VerticalStackConvolution(1, 32, 3)\n",
    "print(test.mask)\n",
    "test = VerticalStackConvolution(1, 32, 3, 'A')\n",
    "print(test.mask)\n",
    "test = HorizontalStackConvolution(1, 32, 3)\n",
    "print(test.mask)\n",
    "test = HorizontalStackConvolution(1, 32, 3, 'A')\n",
    "print(test.mask)\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d654477f-4001-4c80-835a-dc98a41f8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConvolution(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels=in_channels,\n",
    "                                                   out_channels=in_channels*2)\n",
    "        \n",
    "        self.h = HorizontalStackConvolution(in_channels=in_channels,\n",
    "                                                   out_channels=in_channels*2)\n",
    "        \n",
    "        self.v_to_h = nn.Conv2d(in_channels=in_channels*2,\n",
    "                                     out_channels=in_channels*2,\n",
    "                                     kernel_size=1)\n",
    "        \n",
    "        self.h_out = nn.Conv2d(in_channels=in_channels,\n",
    "                                      out_channels=in_channels,\n",
    "                                      kernel_size=1)\n",
    "        \n",
    "    def forward(self, v_prev, h_prev):\n",
    "        v = self.v(v_prev)\n",
    "        v_f, v_g = torch.chunk(v, chunks=2, dim=1)\n",
    "        v_out = torch.tanh(v_f) * torch.sigmoid(v_g)\n",
    "        \n",
    "        h = self.h(h_prev)\n",
    "        h += self.v_to_h(v)\n",
    "        h_f, h_g = torch.chunk(h, chunks=2, dim=1)\n",
    "        h_out = torch.tanh(h_f) * torch.sigmoid(h_g)\n",
    "        h_out = self.h_out(h_out)\n",
    "        h_out += h_prev\n",
    "        \n",
    "        return v_out, h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40460c19-a097-483a-a5d7-ef39118242df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 28, 28]) torch.Size([1, 64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# test gated convolution\n",
    "gated = GatedConvolution(NUM_CHANNELS)\n",
    "v = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
    "h = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
    "v, h = gated(v, h)\n",
    "print(v.shape, h.shape)\n",
    "del gated, v, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6115b45-b690-463f-be12-a8125b36c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedPixelCNN(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_gated):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels=1,\n",
    "                                          out_channels=hidden_channels,\n",
    "                                          mask_type='A')\n",
    "        self.h = HorizontalStackConvolution(in_channels=1,\n",
    "                                          out_channels=hidden_channels,\n",
    "                                          mask_type='A')\n",
    "        \n",
    "        self.gated_convolutions = nn.ModuleList([GatedConvolution(hidden_channels) \n",
    "                                                 for _ in range(num_gated)])\n",
    "        # later we apply a 256 way softmax\n",
    "        self.output = nn.Conv2d(in_channels=hidden_channels, \n",
    "                                out_channels=256,\n",
    "                                kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        v = self.v(x)\n",
    "        h = self.h(x)\n",
    "        \n",
    "        for gated_layer in self.gated_convolutions:\n",
    "            v, h = gated_layer(v, h)\n",
    "        out = self.output(F.relu(h))\n",
    "        # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
    "        out = out.unsqueeze(dim=2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1, 28, 28]) torch.Size([1, 1, 28, 28])\n",
      "tensor(5.5833, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test pixelcnn\n",
    "gated_pixel_cnn = GatedPixelCNN(NUM_CHANNELS, NUM_LAYERS)\n",
    "img = torch.randn(1, 1, 28, 28)\n",
    "labels = (torch.rand(1, 1, 28, 28) * 256).long()\n",
    "output = gated_pixel_cnn(img)\n",
    "print(output.shape, labels.shape)\n",
    "test = F.cross_entropy(output, labels, reduction='mean')\n",
    "print(test)\n",
    "del gated_pixel_cnn, img, labels, output, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GatedPixelCNN(hidden_channels=NUM_CHANNELS, num_gated=NUM_LAYERS).to(DEVICE)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.1, \n",
    "                                                 patience=5, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4bff071b-81f4-4068-b309-f4ec6bad0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bits per dimension \n",
    "def criterion(model, logits, labels):\n",
    "    nll = F.cross_entropy(logits, labels, reduction='none')\n",
    "    bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))\n",
    "    return bpd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BPD Train: 1.3207, BPD Test: 1.2802\n",
      "Epoch: 2, BPD Train: 1.2739, BPD Test: 1.2575\n",
      "Epoch: 3, BPD Train: 1.2572, BPD Test: 1.2459\n",
      "Epoch: 4, BPD Train: 1.2473, BPD Test: 1.2362\n",
      "Epoch: 5, BPD Train: 1.2396, BPD Test: 1.2318\n",
      "Epoch: 6, BPD Train: 1.2329, BPD Test: 1.2224\n",
      "Epoch: 7, BPD Train: 1.2271, BPD Test: 1.2182\n",
      "Epoch: 8, BPD Train: 1.2223, BPD Test: 1.2136\n",
      "Epoch: 9, BPD Train: 1.2186, BPD Test: 1.2101\n",
      "Epoch: 10, BPD Train: 1.2152, BPD Test: 1.2070\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for features, _ in train_dataloader:\n",
    "        features = features.to(DEVICE)\n",
    "        labels = (features * 255).long()\n",
    "        logits = model(features)\n",
    "        loss = criterion(model, logits, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.cpu().item())\n",
    "        \n",
    "    \n",
    "    # evaluate on the test dataset\n",
    "    with torch.inference_mode():\n",
    "        for features, _ in test_dataloader:\n",
    "            features = features.to(DEVICE)\n",
    "            labels = (features * 255).long()\n",
    "            logits = model(features)\n",
    "            loss = criterion(model, logits, labels)\n",
    "            test_losses.append(loss.cpu().item())\n",
    "\n",
    "    \n",
    "    bpd_train = sum(train_losses)/len(train_losses)\n",
    "    bpd_test = sum(test_losses)/len(test_losses)\n",
    "    print(f'Epoch: {epoch+1}, BPD Train: {bpd_train:.4f}, BPD Test: {bpd_test:.4f}')\n",
    "    scheduler.step(bpd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb6bfcf6-33f5-4b13-858f-8dd8a8c5a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an image\n",
    "@torch.inference_mode()\n",
    "def sample(model, img_shape):\n",
    "    img = torch.zeros(img_shape, device=DEVICE)\n",
    "    # Generation loop\n",
    "    _, channel, height, width = img_shape\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            for c in range(channel):\n",
    "                pred = model(img[:,:,:h+1,:])\n",
    "                probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
    "                img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11f46f61-1b47-49ca-a47f-b8ebbb23c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sample(model, img_shape=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "15092d43-38e7-4209-b71e-740fafee0973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8bf4531390>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb615c-e229-438f-aab2-e26a6816517e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
