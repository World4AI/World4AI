{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7",
      "metadata": {
        "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7"
      },
      "source": [
        "# PixelCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "43b62d1b-ed3a-4d49-9724-ab1b2c906b3e",
      "metadata": {
        "id": "43b62d1b-ed3a-4d49-9724-ab1b2c906b3e"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch\n",
        "# from torch import nn, optim\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# import torchvision\n",
        "# import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e03975ce-4cf8-4d01-8109-012515fef359",
      "metadata": {
        "id": "e03975ce-4cf8-4d01-8109-012515fef359"
      },
      "outputs": [],
      "source": [
        "# # parameters from the paper\n",
        "# batch_size = 16\n",
        "# num_layers = 7\n",
        "# num_channels = 32\n",
        "\n",
        "# # other parameters\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# lr = 0.001\n",
        "# num_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7abc7415-6a9e-4aa4-b449-609ac07ead83",
      "metadata": {
        "id": "7abc7415-6a9e-4aa4-b449-609ac07ead83"
      },
      "source": [
        "The architecture is designed for MNIST only and for static inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1236a856-b083-4515-ad3f-c06827ceeaa0",
      "metadata": {
        "id": "1236a856-b083-4515-ad3f-c06827ceeaa0"
      },
      "outputs": [],
      "source": [
        "# architecture\n",
        "# 1. 7 x 7 with conv mask A\n",
        "# 2. stack 3 x 3 with conv mask B\n",
        "# 3. ReLU + 1x1 conv layer with mask B (2 layers)\n",
        "# 4. 256 Ways softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9648172b-1e26-4f75-9f82-3b980cf94485",
      "metadata": {
        "id": "9648172b-1e26-4f75-9f82-3b980cf94485"
      },
      "outputs": [],
      "source": [
        "# # Type A or B masked Conv2d \n",
        "# class MaskedConv2d(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size, mask='A'): \n",
        "#         super().__init__()\n",
        "#         # calculate the padding automatically \n",
        "#         # so that input width+height dims = output dims\n",
        "#         padding = int((kernel_size - 1) / 2)\n",
        "        \n",
        "#         self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "#                              out_channels=out_channels,\n",
        "#                              kernel_size=kernel_size,\n",
        "#                              padding=padding)\n",
        "        \n",
        "#         # calculate masks A or B as in the original paper\n",
        "#         mask_idxs = torch.arange(0, kernel_size**2).view(kernel_size, kernel_size)\n",
        "#         if mask == 'A':\n",
        "#             mask = (mask_idxs < padding * kernel_size + padding).float().to(device)\n",
        "#         elif mask == 'B':\n",
        "#             mask = (mask_idxs <= padding * kernel_size + padding).float().to(device)\n",
        "#         self.register_buffer('mask', mask)\n",
        "            \n",
        "#     def forward(self, x):\n",
        "#         # apply mask, see below link for more info\n",
        "#         # https://discuss.pytorch.org/t/applying-custom-mask-on-kernel-for-cnn/87099\n",
        "        \n",
        "#         with torch.no_grad():\n",
        "#             self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
        "#         return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2108fbbd-5d09-46ea-93bf-af54285f02a8",
      "metadata": {
        "id": "2108fbbd-5d09-46ea-93bf-af54285f02a8"
      },
      "outputs": [],
      "source": [
        "# test2dLayer = MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7).to(device)\n",
        "# testdata = torch.randn(1, 1, 28, 28, device=device)\n",
        "# test2dLayer(testdata).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "19b38fe1-bbcc-47ea-8c8c-d6fdc2338a1e",
      "metadata": {
        "id": "19b38fe1-bbcc-47ea-8c8c-d6fdc2338a1e"
      },
      "outputs": [],
      "source": [
        "# class PixelCNN(nn.Module):\n",
        "    \n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.layers = nn.Sequential(\n",
        "#                 MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7, mask='A'), \n",
        "#                 nn.ReLU(),\n",
        "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
        "#                 nn.ReLU(),\n",
        "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Conv2d(num_channels, num_channels*2, kernel_size=1),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Conv2d(num_channels*2, 1, kernel_size=1)\n",
        "#         )\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e56925f7-b69a-48fa-9e12-714ab7fe3695",
      "metadata": {
        "id": "e56925f7-b69a-48fa-9e12-714ab7fe3695"
      },
      "outputs": [],
      "source": [
        "# test_images = torch.randn(batch_size, 1, 28, 28, device=device)\n",
        "# model = PixelCNN().to(device)\n",
        "# model(test_images).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7a0fb017-7685-488f-96a6-ac4dd361f4ee",
      "metadata": {
        "id": "7a0fb017-7685-488f-96a6-ac4dd361f4ee"
      },
      "outputs": [],
      "source": [
        "# train_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
        "#                                            train=True, \n",
        "#                                            transform=T.ToTensor(), \n",
        "#                                            download=True)\n",
        "\n",
        "# test_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
        "#                                            train=False, \n",
        "#                                            transform=T.ToTensor(), \n",
        "#                                            download=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "265ab1e6-f142-4eb4-a43b-d5c293cc272f",
      "metadata": {
        "id": "265ab1e6-f142-4eb4-a43b-d5c293cc272f"
      },
      "outputs": [],
      "source": [
        "# train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d28992ff-e41f-4478-aeec-d2072adac04c",
      "metadata": {
        "id": "d28992ff-e41f-4478-aeec-d2072adac04c"
      },
      "outputs": [],
      "source": [
        "# model = PixelCNN().to(device)\n",
        "# # mse would most likely also work\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b2545fb-47fe-497f-979a-c2b99b4633f5",
      "metadata": {
        "id": "9b2545fb-47fe-497f-979a-c2b99b4633f5"
      },
      "outputs": [],
      "source": [
        "# def train():\n",
        "#     for epoch in range(1, num_epochs+1):\n",
        "#         train_loss = []\n",
        "#         for features, _ in train_dataloader:\n",
        "#             features = features.to(device)\n",
        "#             logits = model(features)\n",
        "#             loss = criterion(logits, features)\n",
        "            \n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "            \n",
        "#             train_loss.append(loss.cpu().item())\n",
        "        \n",
        "#         print(f'Epoch: {epoch}/{num_epochs}, Loss: {sum(train_loss)/len(train_loss)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "907049b4-a617-44d8-b662-33c59924c8a7",
      "metadata": {
        "id": "907049b4-a617-44d8-b662-33c59924c8a7"
      },
      "outputs": [],
      "source": [
        "# train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf3ba7ec-c6f8-4151-aa82-f1ed60d76ec3",
      "metadata": {
        "id": "bf3ba7ec-c6f8-4151-aa82-f1ed60d76ec3"
      },
      "outputs": [],
      "source": [
        "# # test the reconstruction by comparing original images with the reconstructed images\n",
        "# with torch.inference_mode():\n",
        "#     for idx in range(5):\n",
        "#         image, _ = test_dataset[idx]\n",
        "#         image = image.unsqueeze(0)\n",
        "#         generated_image = model(image.to(device))\n",
        "#         generated_image = torch.sigmoid(generated_image)\n",
        "#         generated_image = generated_image.squeeze()\n",
        "\n",
        "#         plt.subplot(1, 2, 1)\n",
        "#         plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "#         plt.title(\"Original\")\n",
        "#         plt.axis(\"off\")\n",
        "\n",
        "#         plt.subplot(1, 2, 2)\n",
        "#         plt.imshow(generated_image.cpu(), cmap=\"gray\")\n",
        "#         plt.title(\"Reconstructed\")\n",
        "#         plt.axis(\"off\")\n",
        "#         plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9d2f4bbd-25c5-4687-85ad-df98940ec041",
      "metadata": {
        "id": "9d2f4bbd-25c5-4687-85ad-df98940ec041"
      },
      "outputs": [],
      "source": [
        "# image, _ = test_dataset[0]\n",
        "# image = image.squeeze()\n",
        "# for row_idx, row in enumerate(image):\n",
        "#     for col_idx, _ in enumerate(image):\n",
        "#         if row_idx >= 14:\n",
        "#             image[row_idx][col_idx] = 0\n",
        "# plt.imshow(image.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2fba9a90-be3c-4e63-aee0-9d829b3eef1b",
      "metadata": {
        "tags": [],
        "id": "2fba9a90-be3c-4e63-aee0-9d829b3eef1b"
      },
      "outputs": [],
      "source": [
        "# image, _ = test_dataset[0]\n",
        "# image = image.squeeze()\n",
        "# for row_idx, row in enumerate(image):\n",
        "#     for col_idx, _ in enumerate(image):\n",
        "#         if row_idx >= 14:\n",
        "#             image[row_idx][col_idx] = 0\n",
        "\n",
        "# image = image.unsqueeze(0).unsqueeze(0).to(device) \n",
        "# with torch.inference_mode():\n",
        "#     for row_idx in range(28):\n",
        "#         for col_idx in range(28):\n",
        "#             generated_image = model(image)\n",
        "#             generated_image = torch.sigmoid(generated_image)\n",
        "#             generated_image = generated_image.squeeze()\n",
        "#             if row_idx >=14:\n",
        "#                 image[0][0][row_idx][col_idx] = generated_image[row_idx][col_idx]\n",
        "            \n",
        "# plt.imshow(image.cpu().squeeze(), cmap=\"gray\")\n",
        "# plt.title(\"Reconstructed\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20cda0e-7112-4a0c-a064-9801e6d1d524",
      "metadata": {
        "id": "a20cda0e-7112-4a0c-a064-9801e6d1d524"
      },
      "source": [
        "## PixelCNN with Gated Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc",
      "metadata": {
        "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc"
      },
      "source": [
        "This part is highly inspired by [uvadlc-notebooks](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "i_bdSoJhYSRu",
        "outputId": "91fd577c-678b-4d3d-b4f6-0e7e206cdaac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i_bdSoJhYSRu",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 20 15:44:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "77ab572b-4608-4903-bb1c-820343fe3e1c",
      "metadata": {
        "id": "77ab572b-4608-4903-bb1c-820343fe3e1c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201",
      "metadata": {
        "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "NUM_EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "NUM_CHANNELS = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad",
      "metadata": {
        "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad"
      },
      "outputs": [],
      "source": [
        "train_dataset = MNIST(root='../datasets/', train=True, download=True, transform=T.ToTensor())\n",
        "test_dataset = MNIST(root='../datasets/', train=False, download=False, transform=T.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615",
      "metadata": {
        "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615"
      },
      "outputs": [],
      "source": [
        "train_dataloader = data.DataLoader(dataset=train_dataset, \n",
        "                                   batch_size=BATCH_SIZE, \n",
        "                                   shuffle=True, \n",
        "                                   num_workers=2,\n",
        "                                   drop_last=True)\n",
        "test_dataloader = data.DataLoader(dataset=test_dataset, \n",
        "                                   batch_size=BATCH_SIZE, \n",
        "                                   shuffle=True, \n",
        "                                   num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195",
      "metadata": {
        "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195"
      },
      "outputs": [],
      "source": [
        "class MaskedConvolution(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, mask, dilation=1):\n",
        "        super().__init__()\n",
        "        kernel_size = mask.shape\n",
        "        padding = tuple([dilation * (size-1)//2 for size in kernel_size])\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, \n",
        "                              out_channels=out_channels, \n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              dilation=dilation)\n",
        "        \n",
        "        self.register_buffer('mask', mask)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533",
      "metadata": {
        "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533",
        "outputId": "880e9c35-f299-4723-c20a-684273cf33e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "mask = torch.rand(3, 3)\n",
        "img = torch.randn(1, 1, 28, 28)\n",
        "mask_conv = MaskedConvolution(1, 32, mask, dilation=2)\n",
        "mask_conv(img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765",
      "metadata": {
        "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765"
      },
      "outputs": [],
      "source": [
        "class VerticalStackConvolution(MaskedConvolution):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "        assert mask_type in ['A', 'B']\n",
        "        mask = torch.ones(kernel_size, kernel_size)\n",
        "        mask[kernel_size//2+1:,:] = 0\n",
        "        if mask_type=='A':\n",
        "            mask[kernel_size//2,:] = 0\n",
        "        super().__init__(in_channels, out_channels, mask, dilation=dilation)\n",
        "        \n",
        "class HorizontalStackConvolution(MaskedConvolution):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "        assert mask_type in ['A', 'B']\n",
        "        mask = torch.ones(1, kernel_size)\n",
        "        mask[0, kernel_size//2+1:] = 0\n",
        "        if mask_type=='A':\n",
        "            mask[0, kernel_size//2] = 0\n",
        "        super().__init__(in_channels, out_channels, mask, dilation=dilation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
      "metadata": {
        "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
        "outputId": "c6e27ea8-e08a-4ccd-a00d-4f9fa3233665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 0.]])\n",
            "tensor([[1., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "test = VerticalStackConvolution(1, 32, 3)\n",
        "print(test.mask)\n",
        "test = VerticalStackConvolution(1, 32, 3, 'A')\n",
        "print(test.mask)\n",
        "test = HorizontalStackConvolution(1, 32, 3)\n",
        "print(test.mask)\n",
        "test = HorizontalStackConvolution(1, 32, 3, 'A')\n",
        "print(test.mask)\n",
        "del test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d654477f-4001-4c80-835a-dc98a41f8060",
      "metadata": {
        "id": "d654477f-4001-4c80-835a-dc98a41f8060"
      },
      "outputs": [],
      "source": [
        "class GatedConvolution(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size=3, dilation=1):\n",
        "        super().__init__()\n",
        "        self.v = VerticalStackConvolution(in_channels=in_channels,\n",
        "                                          out_channels=in_channels*2,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          dilation=dilation)\n",
        "        \n",
        "        self.h = HorizontalStackConvolution(in_channels=in_channels,\n",
        "                                            out_channels=in_channels*2,\n",
        "                                            kernel_size=kernel_size,\n",
        "                                            dilation=dilation)\n",
        "        \n",
        "        self.v_to_h = nn.Conv2d(in_channels=in_channels*2,\n",
        "                                out_channels=in_channels*2,\n",
        "                                kernel_size=1)\n",
        "        \n",
        "        self.h_out = nn.Conv2d(in_channels=in_channels,\n",
        "                               out_channels=in_channels,\n",
        "                               kernel_size=1)\n",
        "        \n",
        "    def forward(self, v_prev, h_prev):\n",
        "        v = self.v(v_prev)\n",
        "        v_f, v_g = torch.chunk(v, chunks=2, dim=1)\n",
        "        v_out = torch.tanh(v_f) * torch.sigmoid(v_g)\n",
        "        \n",
        "        h = self.h(h_prev)\n",
        "        h += self.v_to_h(v)\n",
        "        h_f, h_g = torch.chunk(h, chunks=2, dim=1)\n",
        "        h_out = torch.tanh(h_f) * torch.sigmoid(h_g)\n",
        "        h_out = self.h_out(h_out)\n",
        "        h_out += h_prev\n",
        "        \n",
        "        return v_out, h_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "40460c19-a097-483a-a5d7-ef39118242df",
      "metadata": {
        "id": "40460c19-a097-483a-a5d7-ef39118242df",
        "outputId": "640a030d-c0fa-4c6b-a46b-08347c873939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 28, 28]) torch.Size([1, 64, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# test gated convolution\n",
        "gated = GatedConvolution(NUM_CHANNELS, dilation=2)\n",
        "v = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "h = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "v, h = gated(v, h)\n",
        "print(v.shape, h.shape)\n",
        "del gated, v, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f6115b45-b690-463f-be12-a8125b36c863",
      "metadata": {
        "id": "f6115b45-b690-463f-be12-a8125b36c863"
      },
      "outputs": [],
      "source": [
        "class GatedPixelCNN(nn.Module):\n",
        "    def __init__(self, hidden_channels, gated_config=[1, 2, 1, 4, 1, 2, 1]):\n",
        "        super().__init__()\n",
        "        self.v = VerticalStackConvolution(in_channels=1,\n",
        "                                          out_channels=hidden_channels,\n",
        "                                          mask_type='A')\n",
        "        self.h = HorizontalStackConvolution(in_channels=1,\n",
        "                                          out_channels=hidden_channels,\n",
        "                                          mask_type='A')\n",
        "        \n",
        "        self.gated_convolutions = nn.ModuleList([GatedConvolution(hidden_channels, dilation=dilation) \n",
        "                                                 for dilation in gated_config])\n",
        "        # later we apply a 256 way softmax\n",
        "        self.output = nn.Conv2d(in_channels=hidden_channels, \n",
        "                                out_channels=256,\n",
        "                                kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        v = self.v(x)\n",
        "        h = self.h(x)\n",
        "        \n",
        "        for gated_layer in self.gated_convolutions:\n",
        "            v, h = gated_layer(v, h)\n",
        "        out = self.output(F.relu(h))\n",
        "        # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
        "        out = out.unsqueeze(dim=2)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813",
      "metadata": {
        "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813",
        "outputId": "1de7fa15-1570-4016-f30b-de67865edfab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256, 1, 28, 28]) torch.Size([1, 1, 28, 28])\n",
            "tensor(5.5803, grad_fn=<NllLoss2DBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# test pixelcnn\n",
        "gated_pixel_cnn = GatedPixelCNN(NUM_CHANNELS)\n",
        "img = torch.randn(1, 1, 28, 28)\n",
        "labels = (torch.rand(1, 1, 28, 28) * 256).long()\n",
        "output = gated_pixel_cnn(img)\n",
        "print(output.shape, labels.shape)\n",
        "test = F.cross_entropy(output, labels, reduction='mean')\n",
        "print(test)\n",
        "del gated_pixel_cnn, img, labels, output, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c",
      "metadata": {
        "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fa1841-bd8f-4bd2-a6ca-584c2ec1426a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-03.\n"
          ]
        }
      ],
      "source": [
        "model = GatedPixelCNN(hidden_channels=NUM_CHANNELS).to(DEVICE)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99, verbose=True)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "#                                                  mode='min', \n",
        "#                                                  factor=0.5, \n",
        "#                                                  patience=5, \n",
        "#                                                  verbose=True,\n",
        "#                                                  min_lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4bff071b-81f4-4068-b309-f4ec6bad0558",
      "metadata": {
        "id": "4bff071b-81f4-4068-b309-f4ec6bad0558"
      },
      "outputs": [],
      "source": [
        "# bits per dimension/pixel\n",
        "def criterion_fn(logits, labels):\n",
        "    nll = F.cross_entropy(logits, labels, reduction='none')\n",
        "    bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))\n",
        "    return bpd.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9U3ryrChI0r",
        "outputId": "77354b62-f928-493b-d3da-1ae5e56e2b21"
      },
      "id": "X9U3ryrChI0r",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = pathlib.Path('/content/gdrive')\n",
        "weights_path = drive_path / 'MyDrive/weights'"
      ],
      "metadata": {
        "id": "-Fb2SW-sg53b"
      },
      "id": "-Fb2SW-sg53b",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = float(\"inf\")"
      ],
      "metadata": {
        "id": "CVMB5nv2jeFe"
      },
      "id": "CVMB5nv2jeFe",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "NUM_EPOCHS=10"
      ],
      "metadata": {
        "id": "Xrhe_r7KrPIG"
      },
      "id": "Xrhe_r7KrPIG",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
      "metadata": {
        "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
        "outputId": "e4e66f38-5394-4734-c977-5b837ad0cb0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time: 68.58sec, Epoch: 1, BPD Train: 1.0177, BPD Test: 1.0096\n",
            "Elapsed Time: 67.21sec, Epoch: 2, BPD Train: 1.0157, BPD Test: 1.0085\n",
            "Elapsed Time: 67.70sec, Epoch: 3, BPD Train: 1.0146, BPD Test: 1.0097\n",
            "Elapsed Time: 67.72sec, Epoch: 4, BPD Train: 1.0138, BPD Test: 1.0073\n",
            "Elapsed Time: 67.36sec, Epoch: 5, BPD Train: 1.0131, BPD Test: 1.0065\n",
            "Elapsed Time: 67.89sec, Epoch: 6, BPD Train: 1.0125, BPD Test: 1.0070\n",
            "Elapsed Time: 67.11sec, Epoch: 7, BPD Train: 1.0121, BPD Test: 1.0056\n",
            "Elapsed Time: 66.98sec, Epoch: 8, BPD Train: 1.0117, BPD Test: 1.0050\n",
            "Elapsed Time: 67.61sec, Epoch: 9, BPD Train: 1.0114, BPD Test: 1.0051\n",
            "Elapsed Time: 67.04sec, Epoch: 10, BPD Train: 1.0111, BPD Test: 1.0038\n",
            "Saving Weights\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    for features, _ in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        features = features.to(DEVICE)\n",
        "        labels = (features * 255).long()\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          logits = model(features)\n",
        "          loss = criterion_fn(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_losses.append(loss.cpu().item())\n",
        "        \n",
        "    # evaluate on the test dataset\n",
        "    with torch.inference_mode():\n",
        "        for features, _ in test_dataloader:\n",
        "            features = features.to(DEVICE)\n",
        "            labels = (features * 255).long()\n",
        "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "              logits = model(features)\n",
        "              loss = criterion_fn(logits, labels)\n",
        "            test_losses.append(loss.cpu().item())\n",
        "\n",
        "    end_time = time.time()\n",
        "    bpd_train = sum(train_losses)/len(train_losses)\n",
        "    bpd_test = sum(test_losses)/len(test_losses)\n",
        "    print(f'Elapsed Time: {end_time-start_time:.2f}sec, Epoch: {epoch+1}, BPD Train: {bpd_train:.4f}, BPD Test: {bpd_test:.4f}')\n",
        "    #scheduler.step()\n",
        "    if bpd_test < best_loss:\n",
        "      print(\"Saving Weights\")\n",
        "      best_loss = bpd_test\n",
        "      torch.save(model.state_dict(), f=weights_path / 'pixel_cnn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "bb6bfcf6-33f5-4b13-858f-8dd8a8c5a94b",
      "metadata": {
        "id": "bb6bfcf6-33f5-4b13-858f-8dd8a8c5a94b"
      },
      "outputs": [],
      "source": [
        "# sample an image\n",
        "@torch.inference_mode()\n",
        "def sample(model, img_shape):\n",
        "    img = torch.zeros(img_shape, device=DEVICE) - 1\n",
        "    # Generation loop\n",
        "    _, channel, height, width = img_shape\n",
        "    for h in range(height):\n",
        "        for w in range(width):\n",
        "            for c in range(channel):\n",
        "                pred = model(img[:,:,:h+1,:]/256)\n",
        "                probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
        "                img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "11f46f61-1b47-49ca-a47f-b8ebbb23c429",
      "metadata": {
        "id": "11f46f61-1b47-49ca-a47f-b8ebbb23c429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ff974dbc-b274-4ee1-d898-51c8c1aa1b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img = sample(model, img_shape=(1, 1, 28, 28))\n",
        "print(img.shape)\n",
        "plt.imshow(img.cpu().long().squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrSl2BuZyUru"
      },
      "id": "GrSl2BuZyUru",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}