{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7",
      "metadata": {
        "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7"
      },
      "source": [
        "# PixelCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "43b62d1b-ed3a-4d49-9724-ab1b2c906b3e",
      "metadata": {
        "id": "43b62d1b-ed3a-4d49-9724-ab1b2c906b3e"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch\n",
        "# from torch import nn, optim\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# import torchvision\n",
        "# import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e03975ce-4cf8-4d01-8109-012515fef359",
      "metadata": {
        "id": "e03975ce-4cf8-4d01-8109-012515fef359"
      },
      "outputs": [],
      "source": [
        "# # parameters from the paper\n",
        "# batch_size = 16\n",
        "# num_layers = 7\n",
        "# num_channels = 32\n",
        "\n",
        "# # other parameters\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# lr = 0.001\n",
        "# num_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7abc7415-6a9e-4aa4-b449-609ac07ead83",
      "metadata": {
        "id": "7abc7415-6a9e-4aa4-b449-609ac07ead83"
      },
      "source": [
        "The architecture is designed for MNIST only and for static inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1236a856-b083-4515-ad3f-c06827ceeaa0",
      "metadata": {
        "id": "1236a856-b083-4515-ad3f-c06827ceeaa0"
      },
      "outputs": [],
      "source": [
        "# architecture\n",
        "# 1. 7 x 7 with conv mask A\n",
        "# 2. stack 3 x 3 with conv mask B\n",
        "# 3. ReLU + 1x1 conv layer with mask B (2 layers)\n",
        "# 4. 256 Ways softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9648172b-1e26-4f75-9f82-3b980cf94485",
      "metadata": {
        "id": "9648172b-1e26-4f75-9f82-3b980cf94485"
      },
      "outputs": [],
      "source": [
        "# # Type A or B masked Conv2d \n",
        "# class MaskedConv2d(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size, mask='A'): \n",
        "#         super().__init__()\n",
        "#         # calculate the padding automatically \n",
        "#         # so that input width+height dims = output dims\n",
        "#         padding = int((kernel_size - 1) / 2)\n",
        "        \n",
        "#         self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "#                              out_channels=out_channels,\n",
        "#                              kernel_size=kernel_size,\n",
        "#                              padding=padding)\n",
        "        \n",
        "#         # calculate masks A or B as in the original paper\n",
        "#         mask_idxs = torch.arange(0, kernel_size**2).view(kernel_size, kernel_size)\n",
        "#         if mask == 'A':\n",
        "#             mask = (mask_idxs < padding * kernel_size + padding).float().to(device)\n",
        "#         elif mask == 'B':\n",
        "#             mask = (mask_idxs <= padding * kernel_size + padding).float().to(device)\n",
        "#         self.register_buffer('mask', mask)\n",
        "            \n",
        "#     def forward(self, x):\n",
        "#         # apply mask, see below link for more info\n",
        "#         # https://discuss.pytorch.org/t/applying-custom-mask-on-kernel-for-cnn/87099\n",
        "        \n",
        "#         with torch.no_grad():\n",
        "#             self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
        "#         return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2108fbbd-5d09-46ea-93bf-af54285f02a8",
      "metadata": {
        "id": "2108fbbd-5d09-46ea-93bf-af54285f02a8"
      },
      "outputs": [],
      "source": [
        "# test2dLayer = MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7).to(device)\n",
        "# testdata = torch.randn(1, 1, 28, 28, device=device)\n",
        "# test2dLayer(testdata).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "19b38fe1-bbcc-47ea-8c8c-d6fdc2338a1e",
      "metadata": {
        "id": "19b38fe1-bbcc-47ea-8c8c-d6fdc2338a1e"
      },
      "outputs": [],
      "source": [
        "# class PixelCNN(nn.Module):\n",
        "    \n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.layers = nn.Sequential(\n",
        "#                 MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7, mask='A'), \n",
        "#                 nn.ReLU(),\n",
        "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
        "#                 nn.ReLU(),\n",
        "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Conv2d(num_channels, num_channels*2, kernel_size=1),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Conv2d(num_channels*2, 1, kernel_size=1)\n",
        "#         )\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e56925f7-b69a-48fa-9e12-714ab7fe3695",
      "metadata": {
        "id": "e56925f7-b69a-48fa-9e12-714ab7fe3695"
      },
      "outputs": [],
      "source": [
        "# test_images = torch.randn(batch_size, 1, 28, 28, device=device)\n",
        "# model = PixelCNN().to(device)\n",
        "# model(test_images).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7a0fb017-7685-488f-96a6-ac4dd361f4ee",
      "metadata": {
        "id": "7a0fb017-7685-488f-96a6-ac4dd361f4ee"
      },
      "outputs": [],
      "source": [
        "# train_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
        "#                                            train=True, \n",
        "#                                            transform=T.ToTensor(), \n",
        "#                                            download=True)\n",
        "\n",
        "# test_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
        "#                                            train=False, \n",
        "#                                            transform=T.ToTensor(), \n",
        "#                                            download=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "265ab1e6-f142-4eb4-a43b-d5c293cc272f",
      "metadata": {
        "id": "265ab1e6-f142-4eb4-a43b-d5c293cc272f"
      },
      "outputs": [],
      "source": [
        "# train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d28992ff-e41f-4478-aeec-d2072adac04c",
      "metadata": {
        "id": "d28992ff-e41f-4478-aeec-d2072adac04c"
      },
      "outputs": [],
      "source": [
        "# model = PixelCNN().to(device)\n",
        "# # mse would most likely also work\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "9b2545fb-47fe-497f-979a-c2b99b4633f5",
      "metadata": {
        "id": "9b2545fb-47fe-497f-979a-c2b99b4633f5"
      },
      "outputs": [],
      "source": [
        "# def train():\n",
        "#     for epoch in range(1, num_epochs+1):\n",
        "#         train_loss = []\n",
        "#         for features, _ in train_dataloader:\n",
        "#             features = features.to(device)\n",
        "#             logits = model(features)\n",
        "#             loss = criterion(logits, features)\n",
        "            \n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "            \n",
        "#             train_loss.append(loss.cpu().item())\n",
        "        \n",
        "#         print(f'Epoch: {epoch}/{num_epochs}, Loss: {sum(train_loss)/len(train_loss)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "907049b4-a617-44d8-b662-33c59924c8a7",
      "metadata": {
        "id": "907049b4-a617-44d8-b662-33c59924c8a7"
      },
      "outputs": [],
      "source": [
        "# train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bf3ba7ec-c6f8-4151-aa82-f1ed60d76ec3",
      "metadata": {
        "id": "bf3ba7ec-c6f8-4151-aa82-f1ed60d76ec3"
      },
      "outputs": [],
      "source": [
        "# # test the reconstruction by comparing original images with the reconstructed images\n",
        "# with torch.inference_mode():\n",
        "#     for idx in range(5):\n",
        "#         image, _ = test_dataset[idx]\n",
        "#         image = image.unsqueeze(0)\n",
        "#         generated_image = model(image.to(device))\n",
        "#         generated_image = torch.sigmoid(generated_image)\n",
        "#         generated_image = generated_image.squeeze()\n",
        "\n",
        "#         plt.subplot(1, 2, 1)\n",
        "#         plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "#         plt.title(\"Original\")\n",
        "#         plt.axis(\"off\")\n",
        "\n",
        "#         plt.subplot(1, 2, 2)\n",
        "#         plt.imshow(generated_image.cpu(), cmap=\"gray\")\n",
        "#         plt.title(\"Reconstructed\")\n",
        "#         plt.axis(\"off\")\n",
        "#         plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9d2f4bbd-25c5-4687-85ad-df98940ec041",
      "metadata": {
        "id": "9d2f4bbd-25c5-4687-85ad-df98940ec041"
      },
      "outputs": [],
      "source": [
        "# image, _ = test_dataset[0]\n",
        "# image = image.squeeze()\n",
        "# for row_idx, row in enumerate(image):\n",
        "#     for col_idx, _ in enumerate(image):\n",
        "#         if row_idx >= 14:\n",
        "#             image[row_idx][col_idx] = 0\n",
        "# plt.imshow(image.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "2fba9a90-be3c-4e63-aee0-9d829b3eef1b",
      "metadata": {
        "tags": [],
        "id": "2fba9a90-be3c-4e63-aee0-9d829b3eef1b"
      },
      "outputs": [],
      "source": [
        "# image, _ = test_dataset[0]\n",
        "# image = image.squeeze()\n",
        "# for row_idx, row in enumerate(image):\n",
        "#     for col_idx, _ in enumerate(image):\n",
        "#         if row_idx >= 14:\n",
        "#             image[row_idx][col_idx] = 0\n",
        "\n",
        "# image = image.unsqueeze(0).unsqueeze(0).to(device) \n",
        "# with torch.inference_mode():\n",
        "#     for row_idx in range(28):\n",
        "#         for col_idx in range(28):\n",
        "#             generated_image = model(image)\n",
        "#             generated_image = torch.sigmoid(generated_image)\n",
        "#             generated_image = generated_image.squeeze()\n",
        "#             if row_idx >=14:\n",
        "#                 image[0][0][row_idx][col_idx] = generated_image[row_idx][col_idx]\n",
        "            \n",
        "# plt.imshow(image.cpu().squeeze(), cmap=\"gray\")\n",
        "# plt.title(\"Reconstructed\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20cda0e-7112-4a0c-a064-9801e6d1d524",
      "metadata": {
        "id": "a20cda0e-7112-4a0c-a064-9801e6d1d524"
      },
      "source": [
        "## PixelCNN with Gated Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc",
      "metadata": {
        "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc"
      },
      "source": [
        "This part is highly inspired by [uvadlc-notebooks](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "i_bdSoJhYSRu",
        "outputId": "d1c9c221-d0b7-4e22-a5a2-93e2064a0ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i_bdSoJhYSRu",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 21 11:31:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    26W /  70W |    614MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "77ab572b-4608-4903-bb1c-820343fe3e1c",
      "metadata": {
        "id": "77ab572b-4608-4903-bb1c-820343fe3e1c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201",
      "metadata": {
        "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201"
      },
      "outputs": [],
      "source": [
        "## hyperparameters\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "NUM_CHANNELS = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad",
      "metadata": {
        "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad"
      },
      "outputs": [],
      "source": [
        "def discretize(sample):\n",
        "    return (sample * 255).to(torch.long)\n",
        "\n",
        "transform = T.Compose([T.ToTensor(),\n",
        "                        discretize])\n",
        "\n",
        "\n",
        "train_dataset = MNIST(root='../datasets/', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='../datasets/', train=False, download=False, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615",
      "metadata": {
        "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615"
      },
      "outputs": [],
      "source": [
        "train_dataloader = data.DataLoader(dataset=train_dataset, \n",
        "                                   batch_size=BATCH_SIZE, \n",
        "                                   shuffle=True, \n",
        "                                   num_workers=2,\n",
        "                                   drop_last=True,\n",
        "                                   pin_memory=True)\n",
        "test_dataloader = data.DataLoader(dataset=test_dataset, \n",
        "                                   batch_size=BATCH_SIZE, \n",
        "                                   shuffle=True, \n",
        "                                   num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195",
      "metadata": {
        "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195"
      },
      "outputs": [],
      "source": [
        "class MaskedConvolution(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, mask, dilation=1):\n",
        "        super().__init__()\n",
        "        kernel_size = mask.shape\n",
        "        padding = tuple([dilation * (size-1)//2 for size in kernel_size])\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, \n",
        "                              out_channels=out_channels, \n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              dilation=dilation)\n",
        "        \n",
        "        self.register_buffer('mask', mask)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533",
      "metadata": {
        "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f630f8b7-ad5b-4c2a-bbc4-a5c4032269df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "mask = torch.rand(3, 3)\n",
        "img = torch.randn(1, 1, 28, 28)\n",
        "mask_conv = MaskedConvolution(1, 32, mask, dilation=2)\n",
        "mask_conv(img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765",
      "metadata": {
        "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765"
      },
      "outputs": [],
      "source": [
        "class VerticalStackConvolution(MaskedConvolution):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "        assert mask_type in ['A', 'B']\n",
        "        mask = torch.ones(kernel_size, kernel_size)\n",
        "        mask[kernel_size//2+1:,:] = 0\n",
        "        if mask_type=='A':\n",
        "            mask[kernel_size//2,:] = 0\n",
        "        \n",
        "        super().__init__(in_channels, out_channels, mask, dilation=dilation)\n",
        "        \n",
        "class HorizontalStackConvolution(MaskedConvolution):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "        assert mask_type in ['A', 'B']\n",
        "        mask = torch.ones(1, kernel_size)\n",
        "        mask[0, kernel_size//2+1:] = 0\n",
        "        if mask_type=='A':\n",
        "            mask[0, kernel_size//2] = 0\n",
        "        super().__init__(in_channels, out_channels, mask, dilation=dilation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
      "metadata": {
        "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5043fd-81ba-4e6d-ce1e-f1c7598fcafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 0.]])\n",
            "tensor([[1., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "test = VerticalStackConvolution(1, 32, 3)\n",
        "print(test.mask)\n",
        "test = VerticalStackConvolution(1, 32, 3, 'A')\n",
        "print(test.mask)\n",
        "test = HorizontalStackConvolution(1, 32, 3)\n",
        "print(test.mask)\n",
        "test = HorizontalStackConvolution(1, 32, 3, 'A')\n",
        "print(test.mask)\n",
        "del test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d654477f-4001-4c80-835a-dc98a41f8060",
      "metadata": {
        "id": "d654477f-4001-4c80-835a-dc98a41f8060"
      },
      "outputs": [],
      "source": [
        "class GatedConvolution(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size=3, dilation=1):\n",
        "        super().__init__()\n",
        "        self.v = VerticalStackConvolution(in_channels=in_channels,\n",
        "                                          out_channels=in_channels*2,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          dilation=dilation)\n",
        "        \n",
        "        self.h = HorizontalStackConvolution(in_channels=in_channels,\n",
        "                                            out_channels=in_channels*2,\n",
        "                                            kernel_size=kernel_size,\n",
        "                                            dilation=dilation)\n",
        "        \n",
        "        self.v_to_h = nn.Conv2d(in_channels=in_channels*2,\n",
        "                                out_channels=in_channels*2,\n",
        "                                kernel_size=1)\n",
        "        \n",
        "        self.h_out = nn.Conv2d(in_channels=in_channels,\n",
        "                               out_channels=in_channels,\n",
        "                              kernel_size=1)\n",
        "        \n",
        "    def forward(self, v_prev, h_prev):\n",
        "        v = self.v(v_prev)\n",
        "        v_f, v_g = torch.chunk(v, chunks=2, dim=1)\n",
        "        v_out = torch.tanh(v_f) * torch.sigmoid(v_g)\n",
        "        \n",
        "        h = self.h(h_prev)\n",
        "        h += self.v_to_h(v)\n",
        "        h_f, h_g = torch.chunk(h, chunks=2, dim=1)\n",
        "        h_out = torch.tanh(h_f) * torch.sigmoid(h_g)\n",
        "        h_out = self.h_out(h_out)\n",
        "        h_out += h_prev\n",
        "        \n",
        "        return v_out, h_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "40460c19-a097-483a-a5d7-ef39118242df",
      "metadata": {
        "id": "40460c19-a097-483a-a5d7-ef39118242df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04efcdbd-9aab-45f1-cc72-d1d7e5a44131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 28, 28]) torch.Size([1, 64, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# test gated convolution\n",
        "gated = GatedConvolution(NUM_CHANNELS, dilation=2)\n",
        "v = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "h = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "v, h = gated(v, h)\n",
        "print(v.shape, h.shape)\n",
        "del gated, v, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "f6115b45-b690-463f-be12-a8125b36c863",
      "metadata": {
        "id": "f6115b45-b690-463f-be12-a8125b36c863"
      },
      "outputs": [],
      "source": [
        "class GatedPixelCNN(nn.Module):\n",
        "    def __init__(self, hidden_channels, gated_config=[1, 1, 1, 1, 1, 1]):\n",
        "        super().__init__()\n",
        "        self.v = VerticalStackConvolution(in_channels=1,\n",
        "                                          out_channels=hidden_channels,\n",
        "                                          mask_type='A')\n",
        "        self.h = HorizontalStackConvolution(in_channels=1,\n",
        "                                          out_channels=hidden_channels,\n",
        "                                          mask_type='A')\n",
        "        \n",
        "        self.gated_convolutions = nn.ModuleList([GatedConvolution(hidden_channels, kernel_size=7) \n",
        "                                                 for dilation in gated_config])\n",
        "        # we apply a 256 way softmax\n",
        "        self.output = nn.Conv2d(in_channels=hidden_channels, \n",
        "                                out_channels=256,\n",
        "                                kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        x = (x.float() / 255.0) * 2 - 1\n",
        "\n",
        "        v = self.v(x)\n",
        "        h = self.h(x)\n",
        "        \n",
        "        for gated_layer in self.gated_convolutions:\n",
        "            v, h = gated_layer(v, h)\n",
        "        out = self.output(F.elu(h))\n",
        "        # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
        "        out = out.unsqueeze(dim=2)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813",
      "metadata": {
        "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67c7476-f194-4478-a4d8-edea7fdc3a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GatedPixelCNN(\n",
            "  (v): VerticalStackConvolution(\n",
            "    (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (h): HorizontalStackConvolution(\n",
            "    (conv): Conv2d(1, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
            "  )\n",
            "  (gated_convolutions): ModuleList(\n",
            "    (0): GatedConvolution(\n",
            "      (v): VerticalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "      )\n",
            "      (h): HorizontalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))\n",
            "      )\n",
            "      (v_to_h): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (h_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): GatedConvolution(\n",
            "      (v): VerticalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "      )\n",
            "      (h): HorizontalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))\n",
            "      )\n",
            "      (v_to_h): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (h_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): GatedConvolution(\n",
            "      (v): VerticalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "      )\n",
            "      (h): HorizontalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))\n",
            "      )\n",
            "      (v_to_h): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (h_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (3): GatedConvolution(\n",
            "      (v): VerticalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "      )\n",
            "      (h): HorizontalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))\n",
            "      )\n",
            "      (v_to_h): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (h_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (4): GatedConvolution(\n",
            "      (v): VerticalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "      )\n",
            "      (h): HorizontalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))\n",
            "      )\n",
            "      (v_to_h): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (h_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (5): GatedConvolution(\n",
            "      (v): VerticalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "      )\n",
            "      (h): HorizontalStackConvolution(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))\n",
            "      )\n",
            "      (v_to_h): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (h_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (output): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n",
            "torch.Size([1, 256, 1, 28, 28]) torch.Size([1, 1, 28, 28])\n",
            "tensor(5.5934, grad_fn=<NllLoss2DBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# test pixelcnn\n",
        "gated_pixel_cnn = GatedPixelCNN(NUM_CHANNELS)\n",
        "print(gated_pixel_cnn)\n",
        "img = torch.randn(1, 1, 28, 28)\n",
        "labels = (torch.rand(1, 1, 28, 28) * 256).long()\n",
        "output = gated_pixel_cnn(img)\n",
        "print(output.shape, labels.shape)\n",
        "test = F.cross_entropy(output, labels, reduction='mean')\n",
        "print(test)\n",
        "del gated_pixel_cnn, img, labels, output, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c",
      "metadata": {
        "id": "cc3dc8f5-0676-49fe-9a75-75170a5d6a6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80aadde2-75e2-46a3-d49f-f281cdf60593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-03.\n"
          ]
        }
      ],
      "source": [
        "model = GatedPixelCNN(hidden_channels=NUM_CHANNELS).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "4bff071b-81f4-4068-b309-f4ec6bad0558",
      "metadata": {
        "id": "4bff071b-81f4-4068-b309-f4ec6bad0558"
      },
      "outputs": [],
      "source": [
        "# bits per dimension/pixel\n",
        "def criterion_fn(logits, labels):\n",
        "    nll = F.cross_entropy(logits, labels, reduction='none')\n",
        "    bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))\n",
        "    return bpd.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_path = pathlib.Path('/content/gdrive')\n",
        "weights_path = drive_path / 'MyDrive/weights'"
      ],
      "metadata": {
        "id": "X9U3ryrChI0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8725f4-3fcd-4d78-ffec-7cf3f1bd2ca7"
      },
      "id": "X9U3ryrChI0r",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
      "metadata": {
        "id": "2d035343-c44f-451d-bc6c-e2b12f783c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68533559-c35b-42c5-c952-2606b931d8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed Time: 69.77sec, Epoch: 1, BPD Train: 1.1048, BPD Test: 1.0992\n",
            "Adjusting learning rate of group 0 to 5.9896e-04.\n",
            "Saving Weights\n",
            "Elapsed Time: 69.28sec, Epoch: 2, BPD Train: 1.1038, BPD Test: 1.0996\n",
            "Adjusting learning rate of group 0 to 5.9297e-04.\n",
            "Elapsed Time: 69.37sec, Epoch: 3, BPD Train: 1.1028, BPD Test: 1.0969\n",
            "Adjusting learning rate of group 0 to 5.8704e-04.\n",
            "Saving Weights\n",
            "Elapsed Time: 69.27sec, Epoch: 4, BPD Train: 1.1034, BPD Test: 1.0966\n",
            "Adjusting learning rate of group 0 to 5.8117e-04.\n",
            "Saving Weights\n",
            "Elapsed Time: 69.28sec, Epoch: 5, BPD Train: 1.1028, BPD Test: 1.0966\n",
            "Adjusting learning rate of group 0 to 5.7535e-04.\n",
            "Saving Weights\n",
            "Elapsed Time: 69.35sec, Epoch: 6, BPD Train: 1.1024, BPD Test: 1.0950\n",
            "Adjusting learning rate of group 0 to 5.6960e-04.\n",
            "Saving Weights\n",
            "Elapsed Time: 69.24sec, Epoch: 7, BPD Train: 1.1017, BPD Test: 1.0959\n",
            "Adjusting learning rate of group 0 to 5.6391e-04.\n",
            "Elapsed Time: 69.35sec, Epoch: 8, BPD Train: 1.1009, BPD Test: 1.0955\n",
            "Adjusting learning rate of group 0 to 5.5827e-04.\n",
            "Elapsed Time: 69.48sec, Epoch: 9, BPD Train: 1.1010, BPD Test: 1.0963\n",
            "Adjusting learning rate of group 0 to 5.5268e-04.\n",
            "Elapsed Time: 69.36sec, Epoch: 10, BPD Train: 1.1002, BPD Test: 1.0988\n",
            "Adjusting learning rate of group 0 to 5.4716e-04.\n",
            "Elapsed Time: 69.40sec, Epoch: 11, BPD Train: 1.0994, BPD Test: 1.0946\n",
            "Adjusting learning rate of group 0 to 5.4169e-04.\n",
            "Saving Weights\n",
            "Elapsed Time: 69.46sec, Epoch: 12, BPD Train: 1.0993, BPD Test: 1.0956\n",
            "Adjusting learning rate of group 0 to 5.3627e-04.\n",
            "Elapsed Time: 69.33sec, Epoch: 13, BPD Train: 1.0992, BPD Test: 1.0975\n",
            "Adjusting learning rate of group 0 to 5.3091e-04.\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS=100\n",
        "# training loop\n",
        "best_loss = float(\"inf\")\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    for features, _ in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        features = features.to(DEVICE)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          logits = model(features)\n",
        "          loss = criterion_fn(logits, features)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_losses.append(loss.cpu().item())\n",
        "        \n",
        "    # evaluate on the test dataset\n",
        "    with torch.inference_mode():\n",
        "        for features, _ in test_dataloader:\n",
        "            features = features.to(DEVICE)\n",
        "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "              logits = model(features)\n",
        "              loss = criterion_fn(logits, features)\n",
        "            test_losses.append(loss.cpu().item())\n",
        "\n",
        "    end_time = time.time()\n",
        "    bpd_train = sum(train_losses)/len(train_losses)\n",
        "    bpd_test = sum(test_losses)/len(test_losses)\n",
        "    print(f'Elapsed Time: {end_time-start_time:.2f}sec, Epoch: {epoch+1}, BPD Train: {bpd_train:.4f}, BPD Test: {bpd_test:.4f}')\n",
        "    scheduler.step()\n",
        "    if bpd_test < best_loss:\n",
        "      print(\"Saving Weights\")\n",
        "      best_loss = bpd_test\n",
        "      torch.save(model.state_dict(), f=weights_path / 'pixel_cnn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "bb6bfcf6-33f5-4b13-858f-8dd8a8c5a94b",
      "metadata": {
        "id": "bb6bfcf6-33f5-4b13-858f-8dd8a8c5a94b"
      },
      "outputs": [],
      "source": [
        "# sample an image\n",
        "@torch.inference_mode()\n",
        "def sample(model, img_shape):\n",
        "    img = torch.zeros(img_shape, device=DEVICE) - 1\n",
        "    # Generation loop\n",
        "    _, channel, height, width = img_shape\n",
        "    for h in range(height):\n",
        "        for w in range(width):\n",
        "            for c in range(channel):\n",
        "                pred = model(img[:,:,:h+1,:])\n",
        "                probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
        "                img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "11f46f61-1b47-49ca-a47f-b8ebbb23c429",
      "metadata": {
        "id": "11f46f61-1b47-49ca-a47f-b8ebbb23c429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "84c395e5-b93c-4737-f3b7-ed6df7c17b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img = sample(model, img_shape=(1, 1, 28, 28))\n",
        "print(img.shape)\n",
        "plt.imshow(img.cpu().long().squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional PixelCNN"
      ],
      "metadata": {
        "id": "E3EUZlICXESo"
      },
      "id": "E3EUZlICXESo"
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# import pathlib\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# import torch.utils.data as data\n",
        "\n",
        "# import torchvision\n",
        "# import torchvision.transforms as T\n",
        "# from torchvision.datasets import MNIST"
      ],
      "metadata": {
        "id": "GrSl2BuZyUru"
      },
      "id": "GrSl2BuZyUru",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # hyperparameters\n",
        "# NUM_EPOCHS = 50\n",
        "# BATCH_SIZE = 32\n",
        "# NUM_CHANNELS = 64\n",
        "# LEARNING_RATE = 1e-3\n",
        "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "p4uFd0tKXTWh"
      },
      "id": "p4uFd0tKXTWh",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "0bbsaC76XsJK"
      },
      "outputs": [],
      "source": [
        "# train_dataset = MNIST(root='../datasets/', train=True, download=True, transform=T.ToTensor())\n",
        "# test_dataset = MNIST(root='../datasets/', train=False, download=False, transform=T.ToTensor())"
      ],
      "id": "0bbsaC76XsJK"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TdVnrjg3XsJL"
      },
      "outputs": [],
      "source": [
        "# train_dataloader = data.DataLoader(dataset=train_dataset, \n",
        "#                                    batch_size=BATCH_SIZE, \n",
        "#                                    shuffle=True, \n",
        "#                                    num_workers=2,\n",
        "#                                    drop_last=True)\n",
        "# test_dataloader = data.DataLoader(dataset=test_dataset, \n",
        "#                                    batch_size=BATCH_SIZE, \n",
        "#                                    shuffle=True, \n",
        "#                                    num_workers=2)"
      ],
      "id": "TdVnrjg3XsJL"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVJ5F0HgXtNx"
      },
      "id": "uVJ5F0HgXtNx",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xbFdQ9bDX1Qp"
      },
      "outputs": [],
      "source": [
        "# class MaskedConvolution(nn.Module):\n",
        "    \n",
        "#     def __init__(self, in_channels, out_channels, mask, dilation=1):\n",
        "#         super().__init__()\n",
        "#         kernel_size = mask.shape\n",
        "#         padding = tuple([dilation * (size-1)//2 for size in kernel_size])\n",
        "#         self.conv = nn.Conv2d(in_channels=in_channels, \n",
        "#                               out_channels=out_channels, \n",
        "#                               kernel_size=kernel_size,\n",
        "#                               padding=padding,\n",
        "#                               dilation=dilation)\n",
        "        \n",
        "#         self.register_buffer('mask', mask)\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         with torch.no_grad():\n",
        "#             self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
        "#         return self.conv(x)"
      ],
      "id": "xbFdQ9bDX1Qp"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "aJi5x07FX1Qp"
      },
      "outputs": [],
      "source": [
        "# mask = torch.rand(3, 3)\n",
        "# img = torch.randn(1, 1, 28, 28)\n",
        "# mask_conv = MaskedConvolution(1, 32, mask, dilation=2)\n",
        "# mask_conv(img).shape"
      ],
      "id": "aJi5x07FX1Qp"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6bwyc0AHX1Qq"
      },
      "outputs": [],
      "source": [
        "# class VerticalStackConvolution(MaskedConvolution):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "#         assert mask_type in ['A', 'B']\n",
        "#         mask = torch.ones(kernel_size, kernel_size)\n",
        "#         mask[kernel_size//2+1:,:] = 0\n",
        "#         if mask_type=='A':\n",
        "#             mask[kernel_size//2,:] = 0\n",
        "#         super().__init__(in_channels, out_channels, mask, dilation=dilation)\n",
        "        \n",
        "# class HorizontalStackConvolution(MaskedConvolution):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B', dilation=1):\n",
        "#         assert mask_type in ['A', 'B']\n",
        "#         mask = torch.ones(1, kernel_size)\n",
        "#         mask[0, kernel_size//2+1:] = 0\n",
        "#         if mask_type=='A':\n",
        "#             mask[0, kernel_size//2] = 0\n",
        "#         super().__init__(in_channels, out_channels, mask, dilation=dilation)"
      ],
      "id": "6bwyc0AHX1Qq"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "DJh2wibrX1Qq"
      },
      "outputs": [],
      "source": [
        "# test = VerticalStackConvolution(1, 32, 3)\n",
        "# print(test.mask)\n",
        "# test = VerticalStackConvolution(1, 32, 3, 'A')\n",
        "# print(test.mask)\n",
        "# test = HorizontalStackConvolution(1, 32, 3)\n",
        "# print(test.mask)\n",
        "# test = HorizontalStackConvolution(1, 32, 3, 'A')\n",
        "# print(test.mask)\n",
        "# del test"
      ],
      "id": "DJh2wibrX1Qq"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "UzTlS4h-X1Qq"
      },
      "outputs": [],
      "source": [
        "# class GatedConvolution(nn.Module):\n",
        "#     def __init__(self, in_channels, kernel_size=3, dilation=1):\n",
        "#         super().__init__()\n",
        "#         self.encoded_label = nn.Embedding(num_embeddings=10, embedding_dim=in_channels*2)\n",
        "        \n",
        "#         self.v = VerticalStackConvolution(in_channels=in_channels,\n",
        "#                                           out_channels=in_channels*2,\n",
        "#                                           kernel_size=kernel_size,\n",
        "#                                           dilation=dilation)\n",
        "        \n",
        "#         self.h = HorizontalStackConvolution(in_channels=in_channels,\n",
        "#                                             out_channels=in_channels*2,\n",
        "#                                             kernel_size=kernel_size,\n",
        "#                                             dilation=dilation)\n",
        "        \n",
        "#         self.v_to_h = nn.Conv2d(in_channels=in_channels*2,\n",
        "#                                 out_channels=in_channels*2,\n",
        "#                                 kernel_size=1)\n",
        "        \n",
        "#         self.h_out = nn.Conv2d(in_channels=in_channels,\n",
        "#                                out_channels=in_channels,\n",
        "#                               kernel_size=1)\n",
        "        \n",
        "#     def forward(self, v_prev, h_prev, labels):\n",
        "#         # condition image generation on the label\n",
        "#         encoded_labels = self.encoded_label(labels).unsqueeze(2).unsqueeze(3)\n",
        "        \n",
        "#         v = self.v(v_prev)\n",
        "#         v += encoded_labels\n",
        "#         v_f, v_g = torch.chunk(v, chunks=2, dim=1)\n",
        "#         v_out = torch.tanh(v_f) * torch.sigmoid(v_g)\n",
        "        \n",
        "#         h = self.h(h_prev)\n",
        "#         h += encoded_labels\n",
        "#         h += self.v_to_h(v)\n",
        "#         h_f, h_g = torch.chunk(h, chunks=2, dim=1)\n",
        "#         h_out = torch.tanh(h_f) * torch.sigmoid(h_g)\n",
        "#         h_out = self.h_out(h_out)\n",
        "#         h_out += h_prev\n",
        "        \n",
        "#         return v_out, h_out"
      ],
      "id": "UzTlS4h-X1Qq"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "08AhjbchX1Qq"
      },
      "outputs": [],
      "source": [
        "# # test gated convolution\n",
        "# gated = GatedConvolution(NUM_CHANNELS, dilation=2)\n",
        "# v = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "# h = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
        "# labels = torch.randint(high=10, size=(1,))\n",
        "# print(labels)\n",
        "# v, h = gated(v, h, labels)\n",
        "# print(v.shape, h.shape)\n",
        "# del gated, v, h"
      ],
      "id": "08AhjbchX1Qq"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ghte2M2iX1Qq"
      },
      "outputs": [],
      "source": [
        "# class GatedPixelCNN(nn.Module):\n",
        "#     def __init__(self, hidden_channels, gated_config=[1, 2, 1, 4, 1, 2, 1]):\n",
        "#         super().__init__()\n",
        "#         self.v = VerticalStackConvolution(in_channels=1,\n",
        "#                                           out_channels=hidden_channels,\n",
        "#                                           kernel_size=7,\n",
        "#                                           mask_type='A')\n",
        "#         self.h = HorizontalStackConvolution(in_channels=1,\n",
        "#                                           out_channels=hidden_channels,\n",
        "#                                           kernel_size=7,\n",
        "#                                           mask_type='A')\n",
        "        \n",
        "#         self.gated_convolutions = nn.ModuleList([GatedConvolution(hidden_channels, dilation=dilation) \n",
        "#                                                  for dilation in gated_config])\n",
        "#         # later we apply a 256 way softmax\n",
        "#         self.output = nn.Conv2d(in_channels=hidden_channels, \n",
        "#                                 out_channels=256,\n",
        "#                                 kernel_size=1)\n",
        "#     def forward(self, x, labels):\n",
        "#         v = self.v(x)\n",
        "#         h = self.h(x)\n",
        "        \n",
        "#         for gated_layer in self.gated_convolutions:\n",
        "#             v, h = gated_layer(v, h, labels)\n",
        "#         out = self.output(F.elu(h))\n",
        "#         # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
        "#         out = out.unsqueeze(dim=2)\n",
        "#         return out"
      ],
      "id": "ghte2M2iX1Qq"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "RcHxMsbXX1Qr"
      },
      "outputs": [],
      "source": [
        "# test pixelcnn\n",
        "# gated_pixel_cnn = GatedPixelCNN(NUM_CHANNELS)\n",
        "# img = torch.randn(1, 1, 28, 28)\n",
        "# label = torch.randint(high=10, size=(1,))\n",
        "# output = gated_pixel_cnn(img, label)\n",
        "# print(output.shape, labels.shape)\n",
        "\n",
        "# labels = (torch.rand(1, 1, 28, 28) * 256).long()\n",
        "# loss = F.cross_entropy(output, labels, reduction='mean')\n",
        "# del gated_pixel_cnn, img, labels, output, loss"
      ],
      "id": "RcHxMsbXX1Qr"
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# drive_path = pathlib.Path('/content/gdrive')\n",
        "# weights_path = drive_path / 'MyDrive/weights'"
      ],
      "metadata": {
        "id": "v9S1oEXpYPPr"
      },
      "id": "v9S1oEXpYPPr",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = GatedPixelCNN(hidden_channels=NUM_CHANNELS).to(DEVICE)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99, verbose=True)\n",
        "\n",
        "# # bits per dimension/pixel\n",
        "# def criterion_fn(logits, labels):\n",
        "#     nll = F.cross_entropy(logits, labels, reduction='none')\n",
        "#     bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))\n",
        "#     return bpd.mean()"
      ],
      "metadata": {
        "id": "UCTzcK7qYDfL"
      },
      "id": "UCTzcK7qYDfL",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # training loop\n",
        "# best_loss = float(\"inf\")\n",
        "# scaler = torch.cuda.amp.GradScaler()\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     start_time = time.time()\n",
        "#     train_losses = []\n",
        "#     test_losses = []\n",
        "#     for features, labels in train_dataloader:\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         features = features.to(DEVICE)\n",
        "#         labels = labels.to(DEVICE)\n",
        "#         images = (features * 255).long()\n",
        "\n",
        "#         with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "#           logits = model((features * 2) - 1, labels)\n",
        "#           loss = criterion_fn(logits, images)\n",
        "\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "        \n",
        "#         train_losses.append(loss.cpu().item())\n",
        "        \n",
        "#     # evaluate on the test dataset\n",
        "#     with torch.inference_mode():\n",
        "#         for features, labels in test_dataloader:\n",
        "#             features = features.to(DEVICE)\n",
        "#             labels = labels.to(DEVICE)\n",
        "#             images = (features * 255).long()\n",
        "#             with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "#               logits = model((features * 2) - 1, labels)\n",
        "#               loss = criterion_fn(logits, images)\n",
        "#             test_losses.append(loss.cpu().item())\n",
        "\n",
        "#     end_time = time.time()\n",
        "#     bpd_train = sum(train_losses)/len(train_losses)\n",
        "#     bpd_test = sum(test_losses)/len(test_losses)\n",
        "#     print(f'Elapsed Time: {end_time-start_time:.2f}sec, Epoch: {epoch+1}, BPD Train: {bpd_train:.4f}, BPD Test: {bpd_test:.4f}')\n",
        "#     scheduler.step()\n",
        "#     if bpd_test < best_loss:\n",
        "#       print(\"Saving Weights\")\n",
        "#       best_loss = bpd_test\n",
        "#       torch.save(model.state_dict(), f=weights_path / 'pixel_cnn_conditional.pt')"
      ],
      "metadata": {
        "id": "Jk1a5slFYxDj"
      },
      "id": "Jk1a5slFYxDj",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # sample an image\n",
        "# @torch.inference_mode()\n",
        "# def sample(model, img_shape):\n",
        "#     img = torch.zeros(img_shape, device=DEVICE) - 1\n",
        "#     label = torch.tensor([1], device=DEVICE)\n",
        "#     # Generation loop\n",
        "#     _, channel, height, width = img_shape\n",
        "#     for h in range(height):\n",
        "#         for w in range(width):\n",
        "#             for c in range(channel):\n",
        "#                 pred = model((img[:,:,:h+1,:]/256 * 2) - 1, label)\n",
        "#                 probs = F.softmax(pred[:,:,c,h,w], dim=-1)\n",
        "#                 img[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
        "#     return img"
      ],
      "metadata": {
        "id": "iN6tllk9Y-ns"
      },
      "id": "iN6tllk9Y-ns",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img = sample(model, img_shape=(1, 1, 28, 28))\n",
        "# print(img.shape)\n",
        "# plt.imshow(img.cpu().long().squeeze(), cmap=\"gray\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "S42PBHARZDLk"
      },
      "id": "S42PBHARZDLk",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCKM_NwItDck"
      },
      "id": "iCKM_NwItDck",
      "execution_count": 72,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}