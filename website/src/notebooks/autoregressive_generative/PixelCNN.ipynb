{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a879bdd-54a7-4d23-9668-a461b20a61a7",
   "metadata": {},
   "source": [
    "# PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b62d1b-ed3a-4d49-9724-ab1b2c906b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# from torch import nn, optim\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# import torchvision\n",
    "# import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03975ce-4cf8-4d01-8109-012515fef359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters from the paper\n",
    "# batch_size = 16\n",
    "# num_layers = 7\n",
    "# num_channels = 32\n",
    "\n",
    "# # other parameters\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# lr = 0.001\n",
    "# num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc7415-6a9e-4aa4-b449-609ac07ead83",
   "metadata": {},
   "source": [
    "The architecture is designed for MNIST only and for static inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1236a856-b083-4515-ad3f-c06827ceeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "# 1. 7 x 7 with conv mask A\n",
    "# 2. stack 3 x 3 with conv mask B\n",
    "# 3. ReLU + 1x1 conv layer with mask B (2 layers)\n",
    "# 4. 256 Ways softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9648172b-1e26-4f75-9f82-3b980cf94485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Type A or B masked Conv2d \n",
    "# class MaskedConv2d(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size, mask='A'): \n",
    "#         super().__init__()\n",
    "#         # calculate the padding automatically \n",
    "#         # so that input width+height dims = output dims\n",
    "#         padding = int((kernel_size - 1) / 2)\n",
    "        \n",
    "#         self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "#                              out_channels=out_channels,\n",
    "#                              kernel_size=kernel_size,\n",
    "#                              padding=padding)\n",
    "        \n",
    "#         # calculate masks A or B as in the original paper\n",
    "#         mask_idxs = torch.arange(0, kernel_size**2).view(kernel_size, kernel_size)\n",
    "#         if mask == 'A':\n",
    "#             mask = (mask_idxs < padding * kernel_size + padding).float().to(device)\n",
    "#         elif mask == 'B':\n",
    "#             mask = (mask_idxs <= padding * kernel_size + padding).float().to(device)\n",
    "#         self.register_buffer('mask', mask)\n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         # apply mask, see below link for more info\n",
    "#         # https://discuss.pytorch.org/t/applying-custom-mask-on-kernel-for-cnn/87099\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
    "#         return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2108fbbd-5d09-46ea-93bf-af54285f02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2dLayer = MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7).to(device)\n",
    "# testdata = torch.randn(1, 1, 28, 28, device=device)\n",
    "# test2dLayer(testdata).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b38fe1-bbcc-47ea-8c8c-d6fdc2338a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PixelCNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#                 MaskedConv2d(in_channels=1, out_channels=num_channels, kernel_size=7, mask='A'), \n",
    "#                 nn.ReLU(),\n",
    "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
    "#                 nn.ReLU(),\n",
    "#                 MaskedConv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, mask='B'), \n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Conv2d(num_channels, num_channels*2, kernel_size=1),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Conv2d(num_channels*2, 1, kernel_size=1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56925f7-b69a-48fa-9e12-714ab7fe3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "# model = PixelCNN().to(device)\n",
    "# model(test_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0fb017-7685-488f-96a6-ac4dd361f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "#                                            train=True, \n",
    "#                                            transform=T.ToTensor(), \n",
    "#                                            download=True)\n",
    "\n",
    "# test_dataset = torchvision.datasets.MNIST(root='../datasets/', \n",
    "#                                            train=False, \n",
    "#                                            transform=T.ToTensor(), \n",
    "#                                            download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "265ab1e6-f142-4eb4-a43b-d5c293cc272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
    "# test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28992ff-e41f-4478-aeec-d2072adac04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PixelCNN().to(device)\n",
    "# # mse would most likely also work\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b2545fb-47fe-497f-979a-c2b99b4633f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     for epoch in range(1, num_epochs+1):\n",
    "#         train_loss = []\n",
    "#         for features, _ in train_dataloader:\n",
    "#             features = features.to(device)\n",
    "#             logits = model(features)\n",
    "#             loss = criterion(logits, features)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss.append(loss.cpu().item())\n",
    "        \n",
    "#         print(f'Epoch: {epoch}/{num_epochs}, Loss: {sum(train_loss)/len(train_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907049b4-a617-44d8-b662-33c59924c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3ba7ec-c6f8-4151-aa82-f1ed60d76ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the reconstruction by comparing original images with the reconstructed images\n",
    "# with torch.inference_mode():\n",
    "#     for idx in range(5):\n",
    "#         image, _ = test_dataset[idx]\n",
    "#         image = image.unsqueeze(0)\n",
    "#         generated_image = model(image.to(device))\n",
    "#         generated_image = torch.sigmoid(generated_image)\n",
    "#         generated_image = generated_image.squeeze()\n",
    "\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "#         plt.title(\"Original\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(generated_image.cpu(), cmap=\"gray\")\n",
    "#         plt.title(\"Reconstructed\")\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2f4bbd-25c5-4687-85ad-df98940ec041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, _ = test_dataset[0]\n",
    "# image = image.squeeze()\n",
    "# for row_idx, row in enumerate(image):\n",
    "#     for col_idx, _ in enumerate(image):\n",
    "#         if row_idx >= 14:\n",
    "#             image[row_idx][col_idx] = 0\n",
    "# plt.imshow(image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fba9a90-be3c-4e63-aee0-9d829b3eef1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image, _ = test_dataset[0]\n",
    "# image = image.squeeze()\n",
    "# for row_idx, row in enumerate(image):\n",
    "#     for col_idx, _ in enumerate(image):\n",
    "#         if row_idx >= 14:\n",
    "#             image[row_idx][col_idx] = 0\n",
    "\n",
    "# image = image.unsqueeze(0).unsqueeze(0).to(device) \n",
    "# with torch.inference_mode():\n",
    "#     for row_idx in range(28):\n",
    "#         for col_idx in range(28):\n",
    "#             generated_image = model(image)\n",
    "#             generated_image = torch.sigmoid(generated_image)\n",
    "#             generated_image = generated_image.squeeze()\n",
    "#             if row_idx >=14:\n",
    "#                 image[0][0][row_idx][col_idx] = generated_image[row_idx][col_idx]\n",
    "            \n",
    "# plt.imshow(image.cpu().squeeze(), cmap=\"gray\")\n",
    "# plt.title(\"Reconstructed\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cda0e-7112-4a0c-a064-9801e6d1d524",
   "metadata": {},
   "source": [
    "## PixelCNN with Gated Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf2ea6-fcac-48da-ab05-27fba9fd21dc",
   "metadata": {},
   "source": [
    "This part is highly inspired by [uvadlc-notebooks](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ab572b-4608-4903-bb1c-820343fe3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71412e61-1a4b-483c-81d7-b5ff1eed5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "NUM_CHANNELS = 64\n",
    "NUM_LAYERS = 7\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf3480b7-9b59-42b1-91f3-b90f737065ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='../datasets/', train=True, download=True, transform=T.ToTensor())\n",
    "test_dataset = MNIST(root='../datasets/', train=False, download=False, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97f5a45a-2d07-4dfb-8f41-d784fb35d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(dataset=train_dataset, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   num_workers=2,\n",
    "                                   drop_last=True)\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "205c6439-c6da-4e0c-86ff-bca07d8ae195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, mask):\n",
    "        super().__init__()\n",
    "        kernel_size = mask.shape\n",
    "        padding = tuple([(size-1)//2 for size in kernel_size])\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels, \n",
    "                              kernel_size=kernel_size,\n",
    "                              padding=padding)\n",
    "        \n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight = nn.Parameter(self.conv.weight * self.mask)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5052505-f7a0-44d9-a810-fb9b7a99b533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 28, 28])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.rand(3, 3)\n",
    "img = torch.randn(1, 1, 28, 28)\n",
    "mask_conv = MaskedConvolution(1, 32, mask)\n",
    "mask_conv(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e9259bb-18ac-406c-a2aa-c0bd72c2a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalStackConvolution(MaskedConvolution):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B'):\n",
    "        assert mask_type in ['A', 'B']\n",
    "        mask = torch.ones(kernel_size, kernel_size)\n",
    "        mask[kernel_size//2+1:,:] = 0\n",
    "        if mask_type=='A':\n",
    "            mask[kernel_size//2,:] = 0\n",
    "        super().__init__(in_channels, out_channels, mask)\n",
    "        \n",
    "class HorizontalStackConvolution(MaskedConvolution):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, mask_type='B'):\n",
    "        assert mask_type in ['A', 'B']\n",
    "        mask = torch.ones(1, kernel_size)\n",
    "        mask[0, kernel_size//2+1:] = 0\n",
    "        if mask_type=='A':\n",
    "            mask[0, kernel_size//2] = 0\n",
    "        super().__init__(in_channels, out_channels, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d4e9088-2fd7-436c-ad54-353be5133dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 0.]])\n",
      "tensor([[1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "test = VerticalStackConvolution(1, 32, 3)\n",
    "print(test.mask)\n",
    "test = VerticalStackConvolution(1, 32, 3, 'A')\n",
    "print(test.mask)\n",
    "test = HorizontalStackConvolution(1, 32, 3)\n",
    "print(test.mask)\n",
    "test = HorizontalStackConvolution(1, 32, 3, 'A')\n",
    "print(test.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d654477f-4001-4c80-835a-dc98a41f8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConvolution(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels=in_channels,\n",
    "                                                   out_channels=in_channels*2)\n",
    "        \n",
    "        self.h = HorizontalStackConvolution(in_channels=in_channels,\n",
    "                                                   out_channels=in_channels*2)\n",
    "        \n",
    "        self.v_to_h = nn.Conv2d(in_channels=in_channels*2,\n",
    "                                     out_channels=in_channels*2,\n",
    "                                     kernel_size=1)\n",
    "        \n",
    "        self.h_out = nn.Conv2d(in_channels=in_channels,\n",
    "                                      out_channels=in_channels,\n",
    "                                      kernel_size=1)\n",
    "        \n",
    "    def forward(self, v_prev, h_prev):\n",
    "        v = self.v(v_prev)\n",
    "        v_f, v_g = torch.chunk(v, chunks=2, dim=1)\n",
    "        v_out = torch.tanh(v_f) * torch.sigmoid(v_g)\n",
    "        \n",
    "        h = self.h(h_prev)\n",
    "        h += self.v_to_h(v)\n",
    "        h_f, h_g = torch.chunk(h, chunks=2, dim=1)\n",
    "        h_out = torch.tanh(h_f) * torch.sigmoid(h_g)\n",
    "        h_out = self.h_out(h_out)\n",
    "        h_out += h_prev\n",
    "        \n",
    "        return v_out, h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40460c19-a097-483a-a5d7-ef39118242df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 28, 28]) torch.Size([1, 64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# test gated convolution\n",
    "gated = GatedConvolution(NUM_CHANNELS)\n",
    "v = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
    "h = torch.randn(1, NUM_CHANNELS, 28, 28)\n",
    "v, h = gated(v, h)\n",
    "print(v.shape, h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f6115b45-b690-463f-be12-a8125b36c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedPixelCNN(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_gated):\n",
    "        super().__init__()\n",
    "        self.v = VerticalStackConvolution(in_channels=1,\n",
    "                                          out_channels=hidden_channels,\n",
    "                                          mask_type='A')\n",
    "        self.h = HorizontalStackConvolution(in_channels=1,\n",
    "                                          out_channels=hidden_channels,\n",
    "                                          mask_type='A')\n",
    "        \n",
    "        self.gated_convolutions = nn.ModuleList([GatedConvolution(hidden_channels) \n",
    "                                                 for _ in range(num_gated)])\n",
    "        # later we apply a 256 way softmax\n",
    "        self.output = nn.Conv2d(in_channels=hidden_channels, \n",
    "                                out_channels=256,\n",
    "                                kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        v = self.v(x)\n",
    "        h = self.h(x)\n",
    "        \n",
    "        for gated_layer in self.gated_convolutions:\n",
    "            v, h = gated_layer(v, h)\n",
    "        out = self.output(F.relu(h))\n",
    "        # from Batch, Classes, Height, Width to Batch, Classes, Channel, Height, Width\n",
    "        out = out.unsqueeze(dim=2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1e21dfff-75d0-4bef-b7bf-4521f065d813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1, 28, 28]) torch.Size([1, 1, 28, 28])\n",
      "tensor(5.5776, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test pixelcnn\n",
    "gated_pixel_cnn = GatedPixelCNN(NUM_CHANNELS, NUM_LAYERS)\n",
    "img = torch.randn(1, 1, 28, 28)\n",
    "labels = (torch.rand(1, 1, 28, 28) * 256).long()\n",
    "output = gated_pixel_cnn(img)\n",
    "print(output.shape, labels.shape)\n",
    "test = F.cross_entropy(output, labels, reduction='mean')\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
