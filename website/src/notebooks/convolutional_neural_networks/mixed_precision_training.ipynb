{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePkbfZFnjwTn"
      },
      "source": [
        "# Mixed Precision Traininig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DBhk7dbJkcF"
      },
      "source": [
        "So far when we trained neural networks, we utilized the `torch.float32` datatype. There are layers, like linear layers and convolutions, that can be executed much faster using the lower precision like `torch.float16`. Mixed precision training allows us to train a neural network, where operations will utilize different levels of precision. \n",
        "\n",
        "Mixed precision training has at least two advantages.\n",
        "\n",
        "1. Some layers are faster with `torch.float16` precision, therefore the whole training process will be significantly faster\n",
        "2. Operations using `torch.float16` require less memory than `torch.float32` operations. That will reduce the necessary vram requirements and will allow us to use a larger batch size.\n",
        "\n",
        "PyTorch provides a so called `automatic mixed precision` functionality, that automatically decides which of the operations will run with which precision. We do not have to make any of those decisions manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFP7geOuMvrm"
      },
      "source": [
        "We will demonstrate this performance boost using the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CtNn7_xufU79"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms as T\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fObhUTkdX8so"
      },
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "88kR5AJVkySh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "train_dataset = MNIST(root=\"../datasets\", train=True, download=True, transform=T.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XPp8hbzemziJ"
      },
      "outputs": [],
      "source": [
        "train_dataloader=DataLoader(dataset=train_dataset, \n",
        "                            batch_size=256, \n",
        "                            shuffle=True, \n",
        "                            drop_last=True,\n",
        "                            num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oln9GIzQM_uu"
      },
      "source": [
        "We use a much larger network, than what is required to get good performance for MINST. We do this in order to demonstrate the potential of mixed precision training. We use 14 convolutional layers and 3 fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2z2i7p9di_xg"
      },
      "outputs": [],
      "source": [
        "cfg = [[1, 32, 3, 1, 1],\n",
        "       [32, 64, 3, 1, 1],\n",
        "       [64, 64, 2, 2, 0],\n",
        "       [64, 128, 3, 1, 1],\n",
        "       [128, 128, 3, 1, 1],\n",
        "       [128, 128, 3, 1, 1],\n",
        "       [128, 128, 2, 2, 0],\n",
        "       [128, 256, 3, 1, 1],\n",
        "       [256, 256, 2, 1, 0],\n",
        "       [256, 512, 3, 1, 1],\n",
        "       [512, 512, 3, 1, 1],\n",
        "       [512, 512, 3, 1, 1],\n",
        "       [512, 512, 2, 2, 0],\n",
        "       [512, 1024, 3, 1, 1],\n",
        "]\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Conv2d(**kwargs),\n",
        "        nn.BatchNorm2d(num_features=kwargs['out_channels']),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.block(x)\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.features = self._build_layers(cfg)\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=1024, out_features=1000),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=1000, out_features=1000),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=1000, out_features=10),\n",
        "    )\n",
        "  \n",
        "  def _build_layers(self, cfg):\n",
        "    layers = []\n",
        "    for layer in cfg:\n",
        "      layers += [BasicBlock(in_channels=layer[0],\n",
        "                           out_channels=layer[1],\n",
        "                           kernel_size=layer[2],\n",
        "                           stride=layer[3],\n",
        "                           padding=layer[4])]\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KMpqebORUHt3"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS=10\n",
        "LR=0.0001\n",
        "DEVICE = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVT6L1VwD6SW"
      },
      "source": [
        "We start by training the neural network in a familiar manner, measuring the time an epoch takes and the reserved memory. We will use those values as a "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mQyqpeRAT1nF"
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, optimizer, criterion):\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    losses = []\n",
        "    for img, label in data_loader:\n",
        "      img = img.to(DEVICE)\n",
        "      label = label.to(DEVICE)\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(img)\n",
        "      loss = criterion(prediction, label)\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    s = f'Epoch: {epoch+1}, ' \\\n",
        "      f'Loss: {sum(losses)/len(losses):.4f}, ' \\\n",
        "      f'Elapsed Time: {end_time-start_time:.2f}sec, ' \\\n",
        "      f'Reserved Memory: {torch.cuda.memory_reserved() / 2**20:.2f}MB'\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bQNJEiI_IPem"
      },
      "outputs": [],
      "source": [
        "model = Model(cfg)\n",
        "model.to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zbkYLhL4PqMn"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yddo2V23XfiL"
      },
      "source": [
        "Each epoch takes slightly over 30 seconds to complete and we need roughtly 1GB of VRAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M82MAxWPdgnh",
        "outputId": "a46f80a1-a4a4-4b3f-c410-3d67666ffcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.2546, Elapsed Time: 33.29sec, Reserved Memory: 988.00MB\n",
            "Epoch: 2, Loss: 0.0325, Elapsed Time: 30.91sec, Reserved Memory: 988.00MB\n",
            "Epoch: 3, Loss: 0.0200, Elapsed Time: 31.16sec, Reserved Memory: 988.00MB\n",
            "Epoch: 4, Loss: 0.0144, Elapsed Time: 31.36sec, Reserved Memory: 988.00MB\n",
            "Epoch: 5, Loss: 0.0125, Elapsed Time: 31.56sec, Reserved Memory: 988.00MB\n",
            "Epoch: 6, Loss: 0.0140, Elapsed Time: 31.73sec, Reserved Memory: 988.00MB\n",
            "Epoch: 7, Loss: 0.0078, Elapsed Time: 31.71sec, Reserved Memory: 988.00MB\n",
            "Epoch: 8, Loss: 0.0082, Elapsed Time: 31.67sec, Reserved Memory: 988.00MB\n",
            "Epoch: 9, Loss: 0.0105, Elapsed Time: 31.73sec, Reserved Memory: 988.00MB\n",
            "Epoch: 10, Loss: 0.0070, Elapsed Time: 31.86sec, Reserved Memory: 988.00MB\n"
          ]
        }
      ],
      "source": [
        "train(train_dataloader, model, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eeKSZ_XRtv9"
      },
      "source": [
        "We repeat the training procedure, only this time we use mixed precision training. For that we will utilize the `torch.amp` module (automatic mixed precision). Look at the [official documentation](https://pytorch.org/docs/stable/amp.html), if you need more information.\n",
        "\n",
        "- The `torch.amp.autocast` context manager runs the region below the context manager in mixed precision. For our purposes the forward pass and the loss calculation is calculated using mixed precision.\n",
        "\n",
        "- We use `torch.cuda.amp.GradScalar` object in order to scale the gradients of the loss. If the forward pass of a layer uses 16 bit precision, so will the backward pass. For some of the calculations the gradients will be relatively small and the precision of torch.float16 will not be sufficient to hold those small values. The values will underflow. In order to remedy the problem, the loss is scaled and we let the scaler deal with backprop and gradient descent. At the end we reset the scaler object for the next batch.\n",
        "\n",
        "The three lines from below do exactly that.\n",
        "```\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cdhSPOKJins1"
      },
      "outputs": [],
      "source": [
        "def optimized_train(data_loader, model, optimizer, criterion):\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    losses = []\n",
        "    for img, label in data_loader:\n",
        "      img = img.to(DEVICE)\n",
        "      label = label.to(DEVICE)\n",
        "      optimizer.zero_grad()\n",
        "      with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "        prediction = model(img)\n",
        "        loss = criterion(prediction, label)\n",
        "      losses.append(loss.item())\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "    end_time = time.time()\n",
        "    s = f'Epoch: {epoch+1}, ' \\\n",
        "      f'Loss: {sum(losses)/len(losses):.4f}, ' \\\n",
        "      f'Elapsed Time: {end_time-start_time:.2f}sec, ' \\\n",
        "      f'Reserved Memory: {torch.cuda.memory_reserved() / 2**20:.2f}MB'\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4KgMNMjpjFCT"
      },
      "outputs": [],
      "source": [
        "model = Model(cfg)\n",
        "model.to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jZcoaw--PzIS"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TFTesLEXqtf"
      },
      "source": [
        "We improve the training speed by a factor of at least 2 and reduce the memory footpring significantly as well. The overhead to use automatic mixed precision is inconsequential compared to the benefits of amp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g8WTe_N8jMHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f701a4-ce08-4294-ccc8-157fec13912e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.2636, Elapsed Time: 13.82sec, Reserved Memory: 622.00MB\n",
            "Epoch: 2, Loss: 0.0307, Elapsed Time: 13.73sec, Reserved Memory: 622.00MB\n",
            "Epoch: 3, Loss: 0.0185, Elapsed Time: 14.48sec, Reserved Memory: 622.00MB\n",
            "Epoch: 4, Loss: 0.0146, Elapsed Time: 13.81sec, Reserved Memory: 622.00MB\n",
            "Epoch: 5, Loss: 0.0113, Elapsed Time: 13.78sec, Reserved Memory: 622.00MB\n",
            "Epoch: 6, Loss: 0.0105, Elapsed Time: 14.08sec, Reserved Memory: 622.00MB\n",
            "Epoch: 7, Loss: 0.0104, Elapsed Time: 13.75sec, Reserved Memory: 622.00MB\n",
            "Epoch: 8, Loss: 0.0077, Elapsed Time: 13.74sec, Reserved Memory: 622.00MB\n",
            "Epoch: 9, Loss: 0.0077, Elapsed Time: 13.79sec, Reserved Memory: 622.00MB\n",
            "Epoch: 10, Loss: 0.0080, Elapsed Time: 13.73sec, Reserved Memory: 622.00MB\n"
          ]
        }
      ],
      "source": [
        "optimized_train(train_dataloader, model, optimizer, criterion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mixed_precision_training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODKQkVU7hkXl83A8vh2ihB"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}