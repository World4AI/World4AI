{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7fcb937-9af2-4207-ab71-e9aaf4070810",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14def286-15b8-48c2-9d76-9ee282cb8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3418fe5-d1df-4f50-8d1d-215bb246c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_dataset = MNIST(root=\"../datasets/\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST(root=\"../datasets\", train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9d9d89-d1ea-45ca-b0bb-aecc98ea5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stratify = train_validation_dataset.targets.numpy()\n",
    "train_idxs, val_idxs = train_test_split(\n",
    "                                range(len(train_validation_dataset)),\n",
    "                                stratify=stratify,\n",
    "                                test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce617b4-0c22-4e13-8c37-022922fef8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(train_validation_dataset, train_idxs)\n",
    "val_dataset = Subset(train_validation_dataset, val_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3560414-07b6-4664-a471-002030d6b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "DEVICE = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS=10\n",
    "BATCH_SIZE=32\n",
    "\n",
    "NUM_LABELS = 10\n",
    "NUM_FEATURES = 28*28\n",
    "HIDDEN_SIZE_1 = 100\n",
    "HIDDEN_SIZE_2 = 50\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc2906b-1d16-49d3-8fc4-59f69b5f4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True,\n",
    "                              num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              drop_last=False,\n",
    "                              num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              drop_last=False,\n",
    "                              num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a650bf6-ab27-4026-8304-c667bb324a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Linear(NUM_FEATURES, HIDDEN_SIZE_1),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(HIDDEN_SIZE_1, HIDDEN_SIZE_2),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(HIDDEN_SIZE_2, NUM_LABELS),\n",
    "                nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643afa2f-3301-42b4-9b32-ecbc836ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        num_samples = 0\n",
    "        num_correct = 0\n",
    "        loss_sum = 0\n",
    "        batch_nums = 0\n",
    "\n",
    "        for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "            features = features.view(-1, NUM_FEATURES).to(DEVICE)\n",
    "            labels = labels.to(DEVICE) \n",
    "            # ------ FORWARD PASS --------\n",
    "            # first linear transformation\n",
    "            probs = model(features)\n",
    "            loss = criterion(probs, labels)\n",
    "            batch_nums+=1\n",
    "            loss_sum+=loss.item()\n",
    "\n",
    "            predictions = probs.argmax(dim=1)\n",
    "            num_samples+=len(features)\n",
    "            num_correct+=(labels == predictions).sum().detach().cpu().item()\n",
    "            \n",
    "        accuracy = num_correct / num_samples\n",
    "        avg_loss = loss_sum / batch_nums\n",
    "        return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e4edec-6834-4bf8-a7fb-49dddcaabe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    batch_nums = 0\n",
    "    for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        # reshape features and move to gpu\n",
    "        features = features.view(-1, NUM_FEATURES).to(DEVICE)\n",
    "        # move label to GPU\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # ------ FORWARD PASS --------\n",
    "        # first linear transformation\n",
    "        probs = model(features)\n",
    "\n",
    "        # ------CALCULATE LOSS --------\n",
    "        #cross-entropy loss\n",
    "        loss = criterion(probs, labels)\n",
    "\n",
    "        # ------BACKPROPAGATION --------\n",
    "        loss.backward()\n",
    "\n",
    "        # ------GRADIENT DESCENT --------\n",
    "        optimizer.step()\n",
    "\n",
    "        # ------CLEAR GRADIENTS --------\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ------TRACK LOSS --------\n",
    "        loss_sum += loss.detach().cpu().item()\n",
    "        batch_nums += 1\n",
    "    return loss_sum / batch_nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cb5574-52da-499a-8bac-8f748d5d17db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Train Loss: 1.5840126217718864 | Val Loss: 0.6833014472367915 | Val Acc: 0.8026666666666666\n",
      "Epoch: 2/10 | Train Loss: 0.49275893600526194 | Val Loss: 0.4023380399622182 | Val Acc: 0.8871666666666667\n",
      "Epoch: 3/10 | Train Loss: 0.34781832572803667 | Val Loss: 0.32630129273426023 | Val Acc: 0.9093333333333333\n",
      "Epoch: 4/10 | Train Loss: 0.2878535793521168 | Val Loss: 0.2761212297457647 | Val Acc: 0.9205\n",
      "Epoch: 5/10 | Train Loss: 0.2442894723955332 | Val Loss: 0.24669151771021017 | Val Acc: 0.9283333333333333\n",
      "Epoch: 6/10 | Train Loss: 0.211113075137792 | Val Loss: 0.21764235855377417 | Val Acc: 0.9381666666666667\n",
      "Epoch: 7/10 | Train Loss: 0.1856451758956973 | Val Loss: 0.19449451223927292 | Val Acc: 0.9443333333333334\n",
      "Epoch: 8/10 | Train Loss: 0.16617056687771567 | Val Loss: 0.18181321852186577 | Val Acc: 0.9478333333333333\n",
      "Epoch: 9/10 | Train Loss: 0.14985739022093672 | Val Loss: 0.16728815835643004 | Val Acc: 0.951\n",
      "Epoch: 10/10 | Train Loss: 0.13664886989936048 | Val Loss: 0.15513378205372297 | Val Acc: 0.9543333333333334\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=ALPHA)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_dataloader)\n",
    "    val_loss, val_accuracy = validate_epoch(model, val_dataloader)\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss} | Val Loss: {val_loss} | Val Acc: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac17b24-c8a7-4a19-8777-27a22e95f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.14062306454208806 Test Acc: 0.9582\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = validate_epoch(model, test_dataloader)\n",
    "print(f'Test Loss: {test_loss} Test Acc: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bb6774-57d6-4564-a0b8-ab65280779c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Train Loss: 1.6074911988763747 | Val Loss: 0.7364835358680563 | Val Acc: 0.8001666666666667\n",
      "Epoch: 2/10 | Train Loss: 0.5345603316209694 | Val Loss: 0.43251650598137936 | Val Acc: 0.8858333333333334\n",
      "Epoch: 3/10 | Train Loss: 0.3913343894451627 | Val Loss: 0.37032234383390306 | Val Acc: 0.9\n",
      "Epoch: 4/10 | Train Loss: 0.3441987585168162 | Val Loss: 0.33873798226580976 | Val Acc: 0.9088333333333334\n",
      "Epoch: 5/10 | Train Loss: 0.3147267649070568 | Val Loss: 0.3153703131495004 | Val Acc: 0.9138333333333334\n",
      "Epoch: 6/10 | Train Loss: 0.2928180080691102 | Val Loss: 0.30447521132040534 | Val Acc: 0.9176666666666666\n",
      "Epoch: 7/10 | Train Loss: 0.2751405779680841 | Val Loss: 0.2792716518520041 | Val Acc: 0.9268333333333333\n",
      "Epoch: 8/10 | Train Loss: 0.2601328149225914 | Val Loss: 0.26731096897670564 | Val Acc: 0.9296666666666666\n",
      "Epoch: 9/10 | Train Loss: 0.2485600982654109 | Val Loss: 0.26043352203324754 | Val Acc: 0.9306666666666666\n",
      "Epoch: 10/10 | Train Loss: 0.23963006790318853 | Val Loss: 0.25025850086611634 | Val Acc: 0.9355\n"
     ]
    }
   ],
   "source": [
    "# using weight decay -> L2 loss\n",
    "model = Model().to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=ALPHA, weight_decay=0.001)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_dataloader)\n",
    "    val_loss, val_accuracy = validate_epoch(model, val_dataloader)\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss} | Val Loss: {val_loss} | Val Acc: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf6a5fd-1f0a-4950-ac22-b6b931e6ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout to model\n",
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Linear(NUM_FEATURES, HIDDEN_SIZE_1),\n",
    "                nn.Dropout(p=0.5, inplace=True),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(HIDDEN_SIZE_1, HIDDEN_SIZE_2),\n",
    "                nn.Dropout(p=0.5, inplace=True),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(HIDDEN_SIZE_2, NUM_LABELS),\n",
    "                nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c95f0a0-978a-4ca4-a195-87027867ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Train Loss: 1.753309700922918 | Val Loss: 0.8019436944989448 | Val Acc: 0.7166666666666667\n",
      "Epoch: 2/10 | Train Loss: 0.7418865424221482 | Val Loss: 0.47760056315901434 | Val Acc: 0.86\n",
      "Epoch: 3/10 | Train Loss: 0.5100500681252612 | Val Loss: 0.40739288022543524 | Val Acc: 0.89\n",
      "Epoch: 4/10 | Train Loss: 0.4333171890722836 | Val Loss: 0.3901899809889654 | Val Acc: 0.8966666666666666\n",
      "Epoch: 5/10 | Train Loss: 0.3958948800460644 | Val Loss: 0.37063586936292653 | Val Acc: 0.9081666666666667\n",
      "Epoch: 6/10 | Train Loss: 0.36788900220842846 | Val Loss: 0.3519753341047846 | Val Acc: 0.914\n",
      "Epoch: 7/10 | Train Loss: 0.3485153307492343 | Val Loss: 0.3573096214615284 | Val Acc: 0.9148333333333334\n",
      "Epoch: 8/10 | Train Loss: 0.329779184120307 | Val Loss: 0.33445950937358304 | Val Acc: 0.9218333333333333\n",
      "Epoch: 9/10 | Train Loss: 0.31913269574631936 | Val Loss: 0.3317088654829546 | Val Acc: 0.9211666666666667\n",
      "Epoch: 10/10 | Train Loss: 0.3100332437854903 | Val Loss: 0.3316231705050202 | Val Acc: 0.9251666666666667\n"
     ]
    }
   ],
   "source": [
    "# using weight decay -> L2 loss\n",
    "model = DropoutModel().to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=ALPHA)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_dataloader)\n",
    "    val_loss, val_accuracy = validate_epoch(model, val_dataloader)\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss} | Val Loss: {val_loss} | Val Acc: {val_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
