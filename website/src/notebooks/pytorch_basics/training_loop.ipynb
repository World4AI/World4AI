{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88f300d-9347-4471-bfa9-c5e7982a3a25",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aff750-f65c-47d2-8841-c3e91dddf863",
   "metadata": {},
   "source": [
    "In this chapter we will finally learn to classify the MNIST dataset. We will implement the full training loop: we will loop over the number of epoch, get a batch of data from the dataset, perform a forward pass, utilize the backpropagation algorithm and use gradient descent. In this section we will implement many of those steps manually. In the next section we will utilize some helpful PyTorch classes and show how we can improve our efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6adc6e63-1f3c-4e68-bc51-1e8e91ad7af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6780a-125e-47e0-8684-958729d6ee8f",
   "metadata": {},
   "source": [
    "Below we download the MNIST dataset. We utilize so called `transforms` from the torchvision library for the first time, when we download the data. Transforms allow us to process images in a predetermined way. We will cover transforms in more detail later. For now you should now, that the `ToTensor()` transform automatically transforms the image into the PyTorch `Tensor` and rescales the pixel values between 0 and 1 (from originally between 0 and 255). This rescaling is important to make sure that training of the neural network actually works. We will discuss in the next section, why this technique, called `feature scaling`, is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2db8ee2-3dbd-40ce-8fbc-bb2eacfc5a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"../datasets\", train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ced5f3-bca2-4826-ad72-6f9cce39bcee",
   "metadata": {},
   "source": [
    "Usually it is good practice to save the parameters that were used in the training process. There are more convenient and efficient ways to do that, but a simple list is good enought at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a904ee49-55e2-4d24-83f2-ec4c37730e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "DEVICE = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS=10\n",
    "BATCH_SIZE=32\n",
    "\n",
    "#number of hidden units in the first and second hidden layer\n",
    "HIDDEN_SIZE_1 = 100\n",
    "HIDDEN_SIZE_2 = 50\n",
    "NUM_LABELS = 10\n",
    "NUM_FEATURES = 28*28\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc7fdd-8b63-4a2b-b503-f7582695b886",
   "metadata": {},
   "source": [
    "We create a vanilla DataLoader, where we shuffle the data with each epoch, drop the last batch if has less than 32 samples and use several processes to get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abf2a83-30f3-4679-875f-d609447a3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True,\n",
    "                              num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11045229-b3fb-4f42-a1b1-d66f480e6e66",
   "metadata": {},
   "source": [
    "We need to somehow initialize our weighs. PyTorch allows us to initialize a Tensor with random numbers. \n",
    "\n",
    "`torch.rand(size)` for example initializes a tensor of shape `size` with uniform random values between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b1ab47-3aea-4043-bfd3-435880b80a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8546, 0.8391, 0.5011],\n",
       "        [0.3893, 0.3010, 0.3057]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size determines the shape of the tensor\n",
    "torch.rand(size=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b2b0e-084e-4799-938a-701112357a4d",
   "metadata": {},
   "source": [
    "`torch.randn(size)` on the other hand initialize the tensor with the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258fc8ea-6fdb-4880-8f5d-58ac6e5ac4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0298, -0.9004, -0.8039],\n",
       "        [ 0.1111, -1.7512, -0.9334]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(size=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268ddc1-c2fc-4dd9-a6f7-d0b958b7de22",
   "metadata": {},
   "source": [
    "Additionally we can use the normal distribution `torch.normal(mean, std, size)`. This is the method we will utilize to initialize our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd10642-2417-49c2-b549-8ff1e9b143a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6043,  1.4194, -0.1698],\n",
       "        [-1.3896, -1.0160, -2.1488]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=0, std=1, size=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502ae3c-e60e-4893-9ca5-8850b41f8c9d",
   "metadata": {},
   "source": [
    "Our neural network has two hidden layers. That means that we need three sets of weights and biases. \n",
    "\n",
    "`features -> hidden_1 -> hidden_2 -> outputs`\n",
    "\n",
    "We initialize the weights with the normal distribution and set the biases to 0. Because we want autodiff to track those variables we set `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8671e7-7347-4696-a30b-dab7d5f77946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a set of weights and biases\n",
    "W_1 = torch.normal(mean=0, std=0.1, size=(HIDDEN_SIZE_1, NUM_FEATURES), requires_grad=True, device=DEVICE)\n",
    "b_1 = torch.zeros(1, HIDDEN_SIZE_1, requires_grad=True, device=DEVICE)\n",
    "\n",
    "W_2 = torch.normal(mean=0, std=0.1, size=(HIDDEN_SIZE_2, HIDDEN_SIZE_1), requires_grad=True, device=DEVICE)\n",
    "b_2 = torch.zeros(1, HIDDEN_SIZE_2, requires_grad=True, device=DEVICE)\n",
    "\n",
    "W_3 = torch.normal(mean=0, std=0.1, size=(NUM_LABELS, HIDDEN_SIZE_2), requires_grad=True, device=DEVICE)\n",
    "b_3 = torch.zeros(1, NUM_LABELS, requires_grad=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baafb17-afab-4a5d-ad3e-88b3c280e8d4",
   "metadata": {},
   "source": [
    "In the training loop below we will use some PyTorch functionality we have not covered yet. To demonstrate those new methods we create a simle Tensor object with shape (3, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f7d598-bab1-44ea-8735-38d5c492c657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [6., 5.]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2], [3, 4], [6, 5]], dtype=torch.float32)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11292bd-ef3e-4913-8517-e20806fdc3c3",
   "metadata": {},
   "source": [
    "Often we want to reshape a tensor. In PyTorch this is done with the method `view`. `a.view(1, 6)` for example takes the tensor of form (3, 2) and returns a tensor with the same data but, one that has 1 row and 6  columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04e10878-a56e-4628-a2f7-7e5c37ac27bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 6., 5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(1, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e57865-44aa-4c5f-aa23-b57cfb30948c",
   "metadata": {},
   "source": [
    "If we use `-1` for one of the dimensions, PyTorch will try to automatically infer the dimensionality. `a.view(-1, 3)` returns a (2, 3) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf22845-cf69-40ce-a9f9-068885ecd7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 6., 5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2902922-25f7-49f2-b2f6-6c267c510ce7",
   "metadata": {},
   "source": [
    "We will often apply so called reduction operations, like `sum()`, `mean()` or `max()`. Those methods reduce the dimensionality of the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35880c6-eb2a-408e-81a9-b3baab06faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.)\n",
      "tensor(3.5000)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "print(a.sum())\n",
    "print(a.mean())\n",
    "print(a.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f3f52-f556-4786-8786-53fd62d65de1",
   "metadata": {},
   "source": [
    "Those operations are rarely applied to the whole tensor. Usually you want to apply those functions to a certain dimension. Our tensor has the shape (3, 2). The 0 dimension is the batch dimension and 1st dimension is the feature dimension. For the most part you would want to do some per batch calculation. If you set `dim=0`, you would do a calculation per feature and average over the batch. If you set `dim=1` you average over all features within a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254b358f-f914-4a52-aef7-4671ecfc5f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  7., 11.])\n",
      "tensor([1.5000, 3.5000, 5.5000])\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4., 6.]),\n",
      "indices=tensor([1, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(a.sum(dim=1))\n",
    "print(a.mean(dim=1))\n",
    "# max returns a tuple\n",
    "# the first position contains actual max values\n",
    "# the second position contains the indices of max values\n",
    "print(a.max(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60281902-f521-42b7-b59a-6e22d5fef09d",
   "metadata": {},
   "source": [
    "Finally, we often apply mathematical functions like `exp()` or `log()` to all elements within a batch. Think how we would like to apply an activation function to all elements simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8d5d180-70df-4f27-a0cc-4ea27b506439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.7183,   7.3891],\n",
      "        [ 20.0855,  54.5981],\n",
      "        [403.4288, 148.4132]])\n",
      "tensor([[0.0000, 0.6931],\n",
      "        [1.0986, 1.3863],\n",
      "        [1.7918, 1.6094]])\n"
     ]
    }
   ],
   "source": [
    "print(a.exp())\n",
    "print(a.log())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c0a73-ba76-4bd6-b6bb-5c41a64ab270",
   "metadata": {},
   "source": [
    "The code for the actual training loop below is somewhat lengthy, but is actually relatively straightforward. You can find the corresponding numbers in the code below.\n",
    "\n",
    "1. We iterate over the number of epochs. In our case we train for 10 epochs\n",
    "2. In each epoch we iterate over the whole dataset, batch by batch\n",
    "3. Each batch contains 32 images and 32 labels.\n",
    "4. We reshape the features from shape (32, 1, 28, 28) into (32, 784), because our neural network needs a (batch_size, num_features) shape as input. The original features shape has 32 samples, 1 channel, a height and width of 28 pixels. The channel 1 means that the image is black and white. Colored images have 3 channels (red, green and blue). \n",
    "5. The labels that we receive contain the correct class of the handwritten digit, numbers from 0 to 1. In order to calculate the cross-entropy loss at a later step, we have to transform those classes into one hot vectors. That means that the labels are transformed from (32, 1) to (32, 10). The tensor contains the number 1 in the slot that corresponds to the correct class of the sample and 0 elsewhere.\n",
    "6. Run the forward pass by multiplying the input vector $\\mathbf{X}$ (or $\\mathbf{A^{<l-1>}}$) with the weight matrix $\\mathbf{W}$ and add the bias vector $\\mathbf{b}$. In the first two layers we apply the sigmoid activation function. In the last layer we use the softmax activation.\n",
    "7. We calculate the cross-entropy loss\n",
    "8. We use the backpropagation algorithm\n",
    "9. We apply gradient descent\n",
    "10. Finally we clear the gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e417500a-7244-43d2-97f8-bff4d7879ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.22500242292881012\n",
      "Epoch: 2 Loss: 0.20238369703292847\n",
      "Epoch: 3 Loss: 0.15146227180957794\n",
      "Epoch: 4 Loss: 0.10374166071414948\n",
      "Epoch: 5 Loss: 0.07694655656814575\n",
      "Epoch: 6 Loss: 0.06303787231445312\n",
      "Epoch: 7 Loss: 0.05480561777949333\n",
      "Epoch: 8 Loss: 0.049296267330646515\n",
      "Epoch: 9 Loss: 0.04533310979604721\n",
      "Epoch: 10 Loss: 0.042320940643548965\n"
     ]
    }
   ],
   "source": [
    "# 1. Iterate over epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # variables to track progress\n",
    "    loss_sum = 0\n",
    "    batch_nums = 0\n",
    "    # 2. Iterate over the dataset\n",
    "    # 3. And receive features and labels tensors\n",
    "    for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "        # 4. Reshape features and move tensor to gpu\n",
    "        features = features.view(-1, NUM_FEATURES).to(DEVICE)\n",
    "        \n",
    "        # 5. Create one hot labels and move to GPU\n",
    "        one_hot_labels = torch.zeros(BATCH_SIZE, NUM_LABELS).to(DEVICE)\n",
    "        for sample_idx, label in enumerate(labels):\n",
    "            one_hot_labels[sample_idx][label] = 1\n",
    "        \n",
    "        # 6. Forward pass\n",
    "        # first linear transformation\n",
    "        hidden_1 = features @ W_1.T + b_1\n",
    "        # sigmoid activation\n",
    "        hidden_1 = 1 / (1 + torch.exp(-hidden_1))\n",
    "        # second linear transformation\n",
    "        hidden_2 = hidden_1 @ W_2.T + b_2\n",
    "        # sigmoid activation\n",
    "        hidden_2 = 1 / (1 + torch.exp(-hidden_2))\n",
    "        # third linear transformation\n",
    "        logits = hidden_2 @ W_3.T + b_3\n",
    "        # softmax activation\n",
    "        numerator = torch.exp(logits)\n",
    "        denominator = numerator.sum(dim=1, keepdim=True)\n",
    "        softmax = numerator / denominator\n",
    "    \n",
    "        # 7. Calcualte the cross-entropy loss\n",
    "        loss = -(one_hot_labels * torch.log(softmax)).mean()\n",
    "\n",
    "        # 8. Apply Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # 9. Gradient Descent\n",
    "        with torch.inference_mode():\n",
    "            W_1.sub_(ALPHA * W_1.grad)\n",
    "            b_1.sub_(ALPHA * b_1.grad)\n",
    "            W_2.sub_(ALPHA * W_2.grad)\n",
    "            b_2.sub_(ALPHA * b_2.grad)\n",
    "            W_3.sub_(ALPHA * W_3.grad)\n",
    "            b_3.sub_(ALPHA * b_3.grad)\n",
    "            \n",
    "\n",
    "        # 10. Clear Gradients\n",
    "        W_1.grad.zero_()\n",
    "        W_2.grad.zero_()\n",
    "        W_3.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        b_2.grad.zero_()\n",
    "        b_3.grad.zero_()\n",
    "\n",
    "        \n",
    "        # ------TRACK LOSS --------\n",
    "        batch_nums += 1\n",
    "        loss_sum += loss.detach().cpu()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} Loss: {loss_sum / batch_nums}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88e200-5d6c-4c7f-abc6-3366b576f1cd",
   "metadata": {},
   "source": [
    "The cross-entropy loss decreases significantly, but we are also interested in the classification accuracy. We will calculate that below. To calculate accuracy we first determine the class, that the model predicst. This is relatively straightforward, all we have to do is to look up the category with the highest probability from the softmax activation function. When you look through the code below, you will notice that we do not actually work with the probabilities from softmax, but with the logits (the hidden features that are used as input into softmax activation are called logits). We do that, because the softmax is not necessary for the actual predictions. The class with the highest logit is also the class with the highest probability. We apply the `argmax()` function to the logits and end up with the predicted class (armax returns the index of the max value). When we use the expression `labels == predictions`, PyTorch returns a tensor, that contains `True` if the prediction equals the actual label and `False` otherwise. The `sum()` method treats a true value as 1 and a false value as 0, thereby essentially calculating the number of correct predictions. The `cpu()` method moves the tensor to the cpu and the `item()` method turns a Tensor object into a simple Python value (only works if a Tensor has one single value and not a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73e52787-c885-43a2-9ce7-3f7feba4395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 88.848%\n"
     ]
    }
   ],
   "source": [
    "# test acccuracy\n",
    "num_samples = 0\n",
    "num_correct = 0\n",
    "for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "    with torch.inference_mode():\n",
    "        features = features.view(-1, NUM_FEATURES).to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "    \n",
    "        # ------ FORWARD PASS --------\n",
    "        # first linear transformation\n",
    "        hidden_1 = features @ W_1.T + b_1\n",
    "        # sigmoid activation\n",
    "        hidden_1 = 1 / (1 + torch.exp(-hidden_1))\n",
    "        # second linear transformation\n",
    "        hidden_2 = hidden_1 @ W_2.T + b_2\n",
    "        # sigmoid activation\n",
    "        hidden_2 = 1 / (1 + torch.exp(-hidden_2))\n",
    "        # third linear transformation\n",
    "        logits = hidden_2 @ W_3.T + b_3\n",
    "        \n",
    "        predictions = logits.argmax(dim=1)\n",
    "        num_samples+=len(features)\n",
    "        num_correct+=(labels == predictions).sum().detach().cpu().item()\n",
    "        \n",
    "accuracy = num_correct / num_samples\n",
    "print(f'The accuracy is {accuracy*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3ba53-d9c3-41e2-9787-e8e3b4287b10",
   "metadata": {},
   "source": [
    "The accuracy is close to 90%. While this is not too bad, we will learn techniques over the next chapters that will bring us much closer to 100%. In the next section we will essentially redo the same calculations, using built in PyTorch classes. This will save us a lot of time in the chapters to come."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
