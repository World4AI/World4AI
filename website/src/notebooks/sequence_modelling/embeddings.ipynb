{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c7b010-4fbc-4d8b-ba1c-4b284fe96bc5",
   "metadata": {},
   "source": [
    "# Word Embeddings in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52f943-5a59-4749-93ad-934ce7f8d7c2",
   "metadata": {},
   "source": [
    "In PyTorch word embeddings are implemented as a lookup table (see [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)). The table has as many rows as there are tokens in the vocabulary. The number of columns correspond to the dimensionality of the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8533e931-8f99-4c5e-bfa0-0cb6ece3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7b3c4-45ef-45d8-94f5-2d2c50a22b73",
   "metadata": {},
   "source": [
    "In our example we assume that we have 10 tokens and we would like to create an embedding of lenght 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c900be3-7f5d-4dd9-a0b7-2b37341c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE=10\n",
    "EMBEDDING_DIM=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a07079-f6bb-4562-8ea0-4deeb5eca09b",
   "metadata": {},
   "source": [
    "We create a sequence that simulates 5 sentences, each consisting of 3 tokens. We initialize the sequence randomly with integer values between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c88811-2172-4a98-9135-c034bb1a0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=5\n",
    "SEQ_LEN=3\n",
    "sequence = torch.randint(low=0, high=VOCABULARY_SIZE, size=(BATCH_SIZE, SEQ_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b948720-469b-4ab8-a482-136779cd2583",
   "metadata": {},
   "source": [
    "The sequence is essentially a set of keys that will be used to lookup values in the embedding table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f82988-8d67-4051-9e32-393b85004c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 6, 7],\n",
       "        [3, 2, 2],\n",
       "        [2, 5, 2],\n",
       "        [0, 8, 3],\n",
       "        [4, 6, 3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e238ec-9427-4148-96d4-bf8c99d23ee6",
   "metadata": {},
   "source": [
    "The embedding itself is a PyTorch module. The layer `nn.Embedding` requires two arguments:\n",
    "\n",
    "- num_embeddings: This value corresponds to the number of tokens and is the number of rows in the embedding matrix.\n",
    "- embedding_dim: This is the vector length of the word embeddings and is the number of columns in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4c7232-f733-4823-8b22-85472707f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings=VOCABULARY_SIZE, embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0403fe-b0cf-43bf-b937-8870093de26d",
   "metadata": {},
   "source": [
    "The embedding layer is obviously not static, but is a set of weights. Those weights are learned jointly with other weights through the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed9df9a-3ecc-4111-8e3f-6dda50473199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.4610, -0.8082, -0.7816, -0.3135],\n",
      "        [ 0.1052, -0.0626, -0.7115, -1.1456],\n",
      "        [-0.3115,  1.5333,  0.6740, -1.8035],\n",
      "        [-0.1831, -2.3549,  1.2970, -0.4006],\n",
      "        [ 1.8532,  0.2546,  0.4889,  0.4106],\n",
      "        [ 1.0876,  1.1219, -0.5330,  0.1026],\n",
      "        [-0.4453, -0.8550, -0.0360,  2.3146],\n",
      "        [ 0.7443, -0.3514,  0.6963, -0.9283],\n",
      "        [-1.3944,  0.0908,  0.6961,  1.0211],\n",
      "        [-0.2815,  2.1397, -2.8740, -0.4085]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab141c7-ad37-40d9-add5-13ecfa389f85",
   "metadata": {},
   "source": [
    "When we input a value x into the embedding layer, we get back the x'th row of the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1b97e4-8c71-4ca8-bcdb-bd7fc4376ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4610, -0.8082, -0.7816, -0.3135])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    print(embedding(torch.tensor(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddbe96-33b3-4a11-acbe-a7802f8a3be9",
   "metadata": {},
   "source": [
    "And when we provide a tensor, we get the embeddings that correspond to the index values of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7850b376-e83f-4b0a-8a32-09b37f0f7c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4610, -0.8082, -0.7816, -0.3135],\n",
      "         [-0.4453, -0.8550, -0.0360,  2.3146],\n",
      "         [ 0.7443, -0.3514,  0.6963, -0.9283]],\n",
      "\n",
      "        [[-0.1831, -2.3549,  1.2970, -0.4006],\n",
      "         [-0.3115,  1.5333,  0.6740, -1.8035],\n",
      "         [-0.3115,  1.5333,  0.6740, -1.8035]],\n",
      "\n",
      "        [[-0.3115,  1.5333,  0.6740, -1.8035],\n",
      "         [ 1.0876,  1.1219, -0.5330,  0.1026],\n",
      "         [-0.3115,  1.5333,  0.6740, -1.8035]],\n",
      "\n",
      "        [[ 1.4610, -0.8082, -0.7816, -0.3135],\n",
      "         [-1.3944,  0.0908,  0.6961,  1.0211],\n",
      "         [-0.1831, -2.3549,  1.2970, -0.4006]],\n",
      "\n",
      "        [[ 1.8532,  0.2546,  0.4889,  0.4106],\n",
      "         [-0.4453, -0.8550, -0.0360,  2.3146],\n",
      "         [-0.1831, -2.3549,  1.2970, -0.4006]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    embeddings = embedding(sequence)\n",
    "    print(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
