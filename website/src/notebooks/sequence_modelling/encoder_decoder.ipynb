{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46bf153-29f9-4e41-93d8-aa4516252ca2",
   "metadata": {},
   "source": [
    "# Translation with Encoder-Decoder RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e22da-ee57-41bc-b038-a136ad34e36f",
   "metadata": {},
   "source": [
    "We will use this notebook to implement a simple encoder decoder architecture. The results will definetely not create the best possible model, but we hope that the general approach will become apparent. \n",
    "\n",
    "Some parts of this notebook were inspired by different official [PyTorch tutorials](https://pytorch.org/tutorials/). There are more tutorials to be discovered, if you need to deepen your knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa82834c-970e-4e04-8c8a-3d77f4d7ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a72721b-fe59-491f-84c2-1efcf59782a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ece00-1746-4e19-b66f-8169cd576bca",
   "metadata": {},
   "source": [
    "We are going to use very simple language pairs for our translation task, specifically the German-English. This dataset is taken from a an Anki dataset, a dataset that is used for flashcards in the spaced repition program Anki. There are many different languages available on the [website](https://www.manythings.org/anki/), if you would like to implement a similar model for a different language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625a622-367d-4d0d-9678-920d54a9fb69",
   "metadata": {},
   "source": [
    "Below we utilize a couple of bash commands to download and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2a006b-c673-4bd9-ac7b-9844cacf5fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9376k  100 9376k    0     0  1321k      0  0:00:07  0:00:07 --:--:-- 1809k\n",
      "/home/petruschka/repos/World4AI/website/src/notebooks/sequence_modelling\n"
     ]
    }
   ],
   "source": [
    "!cd ../datasets/ && { curl -O https://www.manythings.org/anki/deu-eng.zip ; cd -; }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b736cd28-baee-47d2-8290-79373465ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../datasets/deu-eng.zip\n",
      "  inflating: ../datasets/deu_eng/deu.txt  \n",
      "  inflating: ../datasets/deu_eng/_about.txt  \n"
     ]
    }
   ],
   "source": [
    "!rm -rf ../datasets/deu_eng/\n",
    "!unzip ../datasets/deu-eng.zip -d ../datasets/deu_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f02455d-6531-4fe3-8fa6-39439fc8e568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_about.txt  deu.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../datasets/deu_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86e611-17a8-4717-bd01-2db89b2a2d47",
   "metadata": {},
   "source": [
    "The starting language pairs are really simple, consisting only of a single word or expression. You should also notice that the pair is always followed by the licence, which we will need to remove at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88ac5b9-8045-44f0-b175-18db6d86d583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tGeh.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\n",
      "Hi.\tHallo!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)\n",
      "Hi.\tGrüß Gott!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)\n",
      "Run!\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)\n",
      "Run.\tLauf!\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #941078 (Fingerhut)\n",
      "Wow!\tPotzdonner!\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #2122382 (Pfirsichbaeumchen)\n",
      "Wow!\tDonnerwetter!\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #2122391 (Pfirsichbaeumchen)\n",
      "Duck!\tKopf runter!\tCC-BY 2.0 (France) Attribution: tatoeba.org #280158 (CM) & #9968521 (wolfgangth)\n",
      "Fire!\tFeuer!\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #1958697 (Tamy)\n",
      "Help!\tHilfe!\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #575889 (MUIRIEL)\n"
     ]
    }
   ],
   "source": [
    "!head ../datasets/deu_eng/deu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e670a-0077-4780-8f03-84c2747702ba",
   "metadata": {},
   "source": [
    "Later sentences consist fairly complex sentences, which are much harder to translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c532507-0c64-4f06-bcb5-e924336bb7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know that adding sentences only in your native or strongest language is probably not as much fun as practicing writing foreign languages, but please don't add sentences to the Tatoeba Corpus if you are not absolutely sure they are correct. If you want to practice languages that you are studying, please do so by using a website designed for that purpose such as www.lang-8.com.\tIch weiß wohl, dass das ausschließliche Beitragen von Sätzen in der Muttersprache – oder der am besten beherrschten Sprache – nicht ganz so viel Spaß macht, wie sich im Schreiben von Fremdsprachen zu üben; steuere beim Tatoeba-Korpus aber bitte trotzdem keine Sätze bei, über deren Korrektheit du dir nicht völlig im Klaren bist. Wenn du Sprachen, die du gerade lernst, üben möchtest, verwende dazu bitte Netzangebote, die eigens hierfür eingerichtet wurden, wie zum Beispiel www.lang-8.com.\tCC-BY 2.0 (France) Attribution: tatoeba.org #3847634 (CM) & #4878147 (Pfirsichbaeumchen)\n",
      "Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\tOhne Zweifel findet sich auf dieser Welt zu jedem Mann genau die richtige Ehefrau und umgekehrt; wenn man jedoch in Betracht zieht, dass ein Mensch nur Gelegenheit hat, mit ein paar hundert anderen bekannt zu sein, von denen ihm nur ein Dutzend oder weniger nahesteht, darunter höchstens ein oder zwei Freunde, dann erahnt man eingedenk der Millionen Einwohner dieser Welt leicht, dass seit Erschaffung ebenderselben wohl noch nie der richtige Mann der richtigen Frau begegnet ist.\tCC-BY 2.0 (France) Attribution: tatoeba.org #7697649 (RM) & #7729416 (Pfirsichbaeumchen)\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ../datasets/deu_eng/deu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d567c2-dc19-4cc3-a7bf-378cfa9234b6",
   "metadata": {},
   "source": [
    "Our tokenizer is extremely simple. We lowercase all words, strip unnecessary whitespace and put a space between the word and the .!? tokens. When also insert two special tokens `<sos>` to indicate the start of the sentence and `<eos>` to indicate the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0332a0a8-06ed-455a-93cf-b3eaa7299f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    return s\n",
    "\n",
    "def tokenizer(s):\n",
    "    s = normalize(s)\n",
    "    s = s.split(' ')\n",
    "    s.insert(0, '<sos>')\n",
    "    s.append('<eos>')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017fcc9-d583-4c88-b3e2-ff2ac328427f",
   "metadata": {},
   "source": [
    "We also remove sentences that have more than 20 tokens and the license. Eventually we return two lists with English and German sequene respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413bc681-ae4b-42c3-a507-20fbd07c0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(max_len=20):\n",
    "    print(\"Reading lines...\")\n",
    "    en_seq = []\n",
    "    de_seq = []\n",
    "    with open('../datasets/deu_eng/deu.txt', 'r', encoding='utf-8') as file:\n",
    "        print(f\"Tokenizing and removing sentences larger than {max_len}\")\n",
    "        for line in file:\n",
    "            pairs = line.split('\\t')\n",
    "            \n",
    "            en_sentence, de_sentence = tokenizer(pairs[0]), tokenizer(pairs[1])\n",
    "            \n",
    "            if len(en_sentence) <= max_len or len(de_sentence) <= max_len:\n",
    "                en_seq.append(en_sentence)\n",
    "                de_seq.append(de_sentence)\n",
    "        print(f\"The dataset has {len(en_seq)} pairs\")\n",
    "        return en_seq, de_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff624e79-6813-49ba-b9b5-b377638a0ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Tokenizing and removing sentences larger than 20\n",
      "The dataset has 255279 pairs\n"
     ]
    }
   ],
   "source": [
    "en_seq, de_seq = read_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a6463-e681-40f9-abf5-bb1ab4362db4",
   "metadata": {},
   "source": [
    "Below is an example what the tokenization process returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7422206-948d-44f0-a76c-3fefc69bb887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'go', '.', '<eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d4569-9b27-42d0-bf44-df7e6452b82a",
   "metadata": {},
   "source": [
    "We use our usual procedure to divide the dataset into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e2eb335-6871-4b64-b4e0-460589cc0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a09cbf-53d4-4d11-8d94-dea170dd3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into train test split\n",
    "# train_frac = 0.8\n",
    "# val_frac = 0.1\n",
    "# test_frac = 0.1\n",
    "train_en, test_val_en, train_de, test_val_de = train_test_split(en_seq, de_seq, test_size=0.2)\n",
    "val_en, test_en, val_de, test_de = train_test_split(test_val_en, test_val_de, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4cd9b-f352-4912-81f6-a3c4c3fc79aa",
   "metadata": {},
   "source": [
    "The PyTorch dataset is initialized with the two lists and returns the elements from the provided index. There is no magic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8507c0a9-ef46-415e-ac9e-67f0f32fb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, en, de):\n",
    "        assert len(en) == len(de)\n",
    "        self.en = en\n",
    "        self.de = de\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.en[idx], self.de[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4ff779-6838-415f-bbbf-279ebbe11b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PairDataset(train_en, train_de)\n",
    "val_dataset = PairDataset(val_en, val_de)\n",
    "test_dataset = PairDataset(test_en, test_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0029ff-49b2-42b8-b6a8-94608b573a0a",
   "metadata": {},
   "source": [
    "We need to create a corpus and to transform the tokens into indices. We use torchtext for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99763d07-e766-47c5-8188-d1f6340f8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67f5a425-969d-403c-acb2-272b5c0e83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_counter = Counter()\n",
    "de_counter = Counter()\n",
    "\n",
    "for line in train_en:\n",
    "    en_counter.update(line)\n",
    "\n",
    "for line in train_de:\n",
    "    de_counter.update(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcda074a-8a8f-49a2-a5b0-604fa7d61550",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sorted_by_freq_tuples = sorted(en_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "en_ordered_dict = OrderedDict(en_sorted_by_freq_tuples)\n",
    "\n",
    "de_sorted_by_freq_tuples = sorted(de_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "de_ordered_dict = OrderedDict(de_sorted_by_freq_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91dfc2-0d1c-416c-a757-7e7a75238dee",
   "metadata": {},
   "source": [
    "We have 4 special tokens. `<pad>` for zero-padding, `<unk>` for out-of-vocabulary tokens, `<sos>` to identify the start of the sentence and `<eos>` to identify the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af822cd1-11a2-4cca-a437-c8f23f7dc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "en_vocab = torchtext.vocab.vocab(en_ordered_dict, min_freq = 5, specials=['<pad>', '<unk>', '<sos>', '<eos>'], special_first = True)\n",
    "de_vocab = torchtext.vocab.vocab(de_ordered_dict, min_freq = 5, specials=['<pad>', '<unk>', '<sos>', '<eos>'], special_first = True)\n",
    "\n",
    "en_vocab.set_default_index(1)\n",
    "de_vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35805a6c-6256-420b-aae2-a645efff1178",
   "metadata": {},
   "source": [
    "Here is an example of what those vocabularies produce. The 2 and 3 tokens correspond to `<sos>` and `<eos>` repsectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45a27008-f281-4842-9b40-632f69fa1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 49, 4, 3]\n",
      "[2, 629, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab(en_seq[0]))\n",
    "print(de_vocab(de_seq[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23112b11-8085-4c14-b7c1-c8f36b7dbba8",
   "metadata": {},
   "source": [
    "In our collate function we pad the smaller sentences with 0 values to generate a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d6ac325-500d-426c-aea0-124dad650fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    en, de = [], []\n",
    "    for en_token, de_token in batch:\n",
    "        en.append(torch.tensor(en_vocab(en_token), dtype=torch.int64))\n",
    "        de.append(torch.tensor(de_vocab(de_token), dtype=torch.int64))\n",
    "    en_padded = nn.utils.rnn.pad_sequence(en, batch_first=True)\n",
    "    de_padded = nn.utils.rnn.pad_sequence(de, batch_first=True)\n",
    "    return en_padded, de_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f6003-2e3f-4f7a-a482-a2816b3f4e03",
   "metadata": {},
   "source": [
    "Finally we create the dataloaders we can loop over during training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d359ccf0-0c97-4ed5-bbe8-27d65a39cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=2,\n",
    "                              drop_last=True,\n",
    "                              collate_fn=collate)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              num_workers=2,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=collate)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              num_workers=2,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f3d4f-d100-4df8-85a7-1ab9add42bf3",
   "metadata": {},
   "source": [
    "The encoder is just an embedding layer and a two layer LSTM. At the end of the forward pass we return the hidden state h_n and the cell state c_n, that can later be used as an input into the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a510fffd-d4ed-4f8f-bbc6-b4a6d8787b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim=128, hidden_size=128, lstm_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, c_n) = self.lstm(x)\n",
    "        return h_n, c_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d50538-d739-4c73-a3fc-cd8c44e7432c",
   "metadata": {},
   "source": [
    "The decoder is slightly more involved. The `LSTMCell` is a single LSTM cell, that we can loop over. The reason why we not use a full LSTM layer, is our requirement to generate a word at each step and use that word as an input in the next step. For that we return the logits, that are used to greedily select the word. Then we use the word and invoke the forward pass of the decoder until the sequence is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9491323b-ce70-4782-8e10-c909e87a1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim=128, hidden_size=128, lstm_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm_cell_list = nn.ModuleList([nn.LSTMCell(input_size=embedding_dim, hidden_size=hidden_size) for i in range(lstm_layers)])\n",
    "        self.fc = nn.Linear(hidden_size, num_embeddings)\n",
    "    \n",
    "    def forward(self, x, h, c):\n",
    "        x = self.embedding(x)\n",
    "        h_n, c_n = torch.zeros_like(h, device=DEVICE), torch.zeros_like(c, device=DEVICE)\n",
    "        for i, lstm_cell in enumerate(self.lstm_cell_list):\n",
    "            (h_n[i], c_n[i]) = lstm_cell(x, (h[i], c[i]))\n",
    "            x = h_n[i].clone()\n",
    "        logits = self.fc(x)\n",
    "        return logits, h_n, c_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb439c9f-613c-4f0e-9542-4ba1befa1de4",
   "metadata": {},
   "source": [
    "The `EncoderDecoder` is combines the two. Here we use a technique called `teacher forcing`. Teacher forcing means that from time to time we do not use the words that our model generated as an input to the decoder, but words that are actually contained in the translation. That can stabilize training. We use a probability of 50% to decide if we use teacher forcing or not.\n",
    "\n",
    "The `Encoder` output is generated in a single run, but the `Decoder` is utilized in a loop, as the next word needs to be generated first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1813cbf7-3950-498e-a6c9-ea31ed38de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    \n",
    "    def forward(self, en_sequence, de_sequence):\n",
    "        batch_size, sequence_len, num_de_embeddings = de_sequence.size()[0], de_sequence.size()[1], self.decoder.embedding.num_embeddings\n",
    "        \n",
    "        # minus 1 due to fewer predictions as inputs, we don't predict <sos>\n",
    "        outputs = torch.zeros(batch_size, sequence_len-1, num_de_embeddings, device=DEVICE)\n",
    "\n",
    "        h_n, c_n = self.encoder(en_sequence)\n",
    "        inp = de_sequence[:, 0]\n",
    "        for i in range(1, sequence_len):\n",
    "            logits, h_n, c_n = decoder(inp, h_n, c_n)\n",
    "            outputs[:, i-1] = logits\n",
    "            \n",
    "            force = random.random() < self.teacher_forcing_ratio\n",
    "            if force:\n",
    "                inp = de_sequence[:, i]\n",
    "            else:\n",
    "                inp = logits.argmax(dim=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1104bc7-39e7-4043-8025-3ca27e08feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_performance(dataloader, model, criterion):\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    num_iterations = 0\n",
    "\n",
    "    # no need to calculate gradients\n",
    "    with torch.inference_mode():\n",
    "        for en_sequence, de_sequence in dataloader:\n",
    "            en_sequence = en_sequence.to(DEVICE)\n",
    "            de_sequence = de_sequence.to(DEVICE)\n",
    "\n",
    "            logits = model(en_sequence, de_sequence)\n",
    "            \n",
    "            # we don't actually predict the <sos> token\n",
    "            labels = de_sequence[:, 1:]\n",
    "            # we need to reshape in order to be able to use these tensors with CrossEntropyLoss\n",
    "            logits = logits.reshape(-1, logits.size()[2])\n",
    "            labels = labels.reshape(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_sum += loss.cpu().item()\n",
    "            num_iterations+=1\n",
    "\n",
    "    # we return the average loss\n",
    "    return loss_sum/num_iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ba4df-6218-4a3d-8a0d-344ce9edc4f7",
   "metadata": {},
   "source": [
    "The train function is mostly the same, that we used in all previous sections. \n",
    "\n",
    "The labels that are used as inputs as a decoder are sliced. Let's assume we have the following sentence as the output.\n",
    "\n",
    "`<sos>, what, is, your, name, <eos>`\n",
    "\n",
    "We use the following part as the input into the decoder\n",
    "`<sos>, what, is, your, name`\n",
    "\n",
    "And this is what we expect the model to generate\n",
    "`what, is, your, name, <eos>` \n",
    "Therefore only this part is used in the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eedecc2-1f89-4aa6-90b9-12f78bf885e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_dataloader, val_dataloader, model, optimizer, criterion, scheduler=None):\n",
    "    min_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "        for en_sequence, de_sequence in train_dataloader:\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            en_sequence = en_sequence.to(DEVICE)\n",
    "            de_sequence = de_sequence.to(DEVICE)\n",
    "\n",
    "            logits = model(en_sequence, de_sequence)\n",
    "            # we don't actually predict the <sos> token\n",
    "            labels = de_sequence[:, 1:]\n",
    "\n",
    "            # we need to reshape in order to be able to use these tensors with CrossEntropyLoss\n",
    "            logits = logits.reshape(-1, logits.size()[2])\n",
    "            labels = labels.reshape(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += loss.cpu().item()\n",
    "            num_iterations += 1\n",
    "        train_loss=loss_sum/num_iterations\n",
    "        val_loss = track_performance(val_dataloader, model, criterion)\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        print(f'Epoch: {epoch+1:>2}/{num_epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f}')\n",
    "        \n",
    "        if val_loss < min_loss:\n",
    "            print(\"Saving Weights!\")\n",
    "            min_loss = val_loss\n",
    "            torch.save({'encoder_weights': encoder.state_dict(), 'decoder_weights': decoder.state_dict()}, f='../temp/encoder_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f711e59-f852-4ed4-87f7-4149509c04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_embeddings=len(en_vocab), embedding_dim=128)\n",
    "decoder = Decoder(num_embeddings=len(de_vocab), embedding_dim=128)\n",
    "seq2seq = EncoderDecoder(encoder, decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "701cb41f-3f2f-4fce-a898-ad0944684f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='min',\n",
    "                                                       patience=2,\n",
    "                                                       verbose=True)\n",
    "\n",
    "num_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "579c5e78-8445-4a7e-be90-ec3354e7a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/25 | Train Loss: 4.77573 | Val Loss: 4.09706\n",
      "Saving Weights!\n",
      "Epoch:  2/25 | Train Loss: 3.74642 | Val Loss: 3.46187\n",
      "Saving Weights!\n",
      "Epoch:  3/25 | Train Loss: 3.25335 | Val Loss: 3.09955\n",
      "Saving Weights!\n",
      "Epoch:  4/25 | Train Loss: 2.93766 | Val Loss: 2.85532\n",
      "Saving Weights!\n",
      "Epoch:  5/25 | Train Loss: 2.68451 | Val Loss: 2.68323\n",
      "Saving Weights!\n",
      "Epoch:  6/25 | Train Loss: 2.49218 | Val Loss: 2.53332\n",
      "Saving Weights!\n",
      "Epoch:  7/25 | Train Loss: 2.33968 | Val Loss: 2.44510\n",
      "Saving Weights!\n",
      "Epoch:  8/25 | Train Loss: 2.21576 | Val Loss: 2.35043\n",
      "Saving Weights!\n",
      "Epoch:  9/25 | Train Loss: 2.10026 | Val Loss: 2.28960\n",
      "Saving Weights!\n",
      "Epoch: 10/25 | Train Loss: 2.02329 | Val Loss: 2.23866\n",
      "Saving Weights!\n",
      "Epoch: 11/25 | Train Loss: 1.94164 | Val Loss: 2.18903\n",
      "Saving Weights!\n",
      "Epoch: 12/25 | Train Loss: 1.87876 | Val Loss: 2.17249\n",
      "Saving Weights!\n",
      "Epoch: 13/25 | Train Loss: 1.82498 | Val Loss: 2.14898\n",
      "Saving Weights!\n",
      "Epoch: 14/25 | Train Loss: 1.77670 | Val Loss: 2.11008\n",
      "Saving Weights!\n",
      "Epoch: 15/25 | Train Loss: 1.73233 | Val Loss: 2.10481\n",
      "Saving Weights!\n",
      "Epoch: 16/25 | Train Loss: 1.68618 | Val Loss: 2.08480\n",
      "Saving Weights!\n",
      "Epoch: 17/25 | Train Loss: 1.64401 | Val Loss: 2.07315\n",
      "Saving Weights!\n",
      "Epoch: 18/25 | Train Loss: 1.61581 | Val Loss: 2.09271\n",
      "Epoch: 19/25 | Train Loss: 1.57346 | Val Loss: 2.05409\n",
      "Saving Weights!\n",
      "Epoch: 20/25 | Train Loss: 1.54611 | Val Loss: 2.05861\n",
      "Epoch: 21/25 | Train Loss: 1.51423 | Val Loss: 2.03507\n",
      "Saving Weights!\n",
      "Epoch: 22/25 | Train Loss: 1.48942 | Val Loss: 2.06015\n",
      "Epoch: 23/25 | Train Loss: 1.46976 | Val Loss: 2.02897\n",
      "Saving Weights!\n",
      "Epoch: 24/25 | Train Loss: 1.44596 | Val Loss: 2.05109\n",
      "Epoch: 25/25 | Train Loss: 1.42052 | Val Loss: 2.05540\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs, train_dataloader, val_dataloader, seq2seq, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16a35c-3f68-4bde-a29e-7733b6f08070",
   "metadata": {},
   "source": [
    "We load the model with the best weights for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b07358a-1e25-4d56-80f7-c99fc2ba35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('../temp/encoder_decoder.pt')\n",
    "encoder_weights = weights['encoder_weights']\n",
    "decoder_weights = weights['decoder_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21e58c7b-f938-47a2-beb7-46b2946f546d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(encoder_weights)\n",
    "decoder.load_state_dict(decoder_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32d22d-efad-4be1-b45c-ea486e9502d4",
   "metadata": {},
   "source": [
    "Here we use a provided English sentence and generate a translation. For that we generate words from the Decoder until the `<eos>` token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd4dee66-61d6-4d20-9e4e-df4707ab9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, vocab, encoder, decoder):\n",
    "    with torch.inference_mode():\n",
    "        outputs = []\n",
    "        \n",
    "        start_token = [\"<sos>\"]\n",
    "        end_token = [\"<eos>\"]\n",
    "        start_idx = vocab(start_token)[0]\n",
    "        end_idx = vocab(end_token)[0]\n",
    "                \n",
    "        h_n, c_n = encoder(sentence)\n",
    "        inp = torch.tensor([start_idx], device=DEVICE)\n",
    "        while True:\n",
    "            logits, h_n, c_n = decoder(inp, h_n, c_n)\n",
    "            inp = logits.argmax(dim=1)\n",
    "            outputs.append(inp.cpu().item())\n",
    "            if inp.item() == end_idx:\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7361c3-bd8f-4c98-a210-592d19e97bb6",
   "metadata": {},
   "source": [
    "Below we show examples for translations of 10 sentences. The quality of the translation is not state of the art, but given the small model and dataset and greedy search, this is an ok result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19010865-7d4f-402e-b4c6-7eb7de5d1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sequence, de_sequence = next(iter(test_dataloader))\n",
    "en_sequence = en_sequence.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4938fd1-44e6-4c3d-805a-f750e02f44ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'tom', 'did', 'only', 'what', 'he', 'had', 'to', 'do', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'tom', 'tat', 'nur', 'seine', 'pflicht', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['tom', 'hat', 'was', 'was', 'er', 'tun', 'muss', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', \"i'm\", 'sure', \"you'd\", 'feel', 'better', 'if', 'you', 'ate', 'something', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'es', 'ginge', 'dir', 'bestimmt', 'besser,', 'wenn', 'du', 'etwas', '<unk>', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['es', 'ist', 'mir', 'bestimmt', 'gefühl,', 'wenn', 'du', 'etwas', 'etwas', '<unk>', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'tom', \"shouldn't\", 'be', 'here', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'tom', 'sollte', 'nicht', 'hier', 'sein', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['tom', 'sollte', 'nicht', 'hier', 'sein', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'i', \"won't\", 'lose', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'ich', 'werde', 'nicht', 'verlieren', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['ich', 'werde', 'nicht', 'verlieren', '!', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'why', \"didn't\", 'you', 'just', 'do', 'what', 'we', 'asked', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'warum', 'hast', 'du', 'nicht', 'einfach', 'getan,', 'worum', 'wir', 'baten', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['warum', 'haben', 'sie', 'nicht', 'einfach', 'hier,', 'worum', 'wir', 'das', 'machen', '?', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'tom', 'has', 'no', 'brothers', 'or', 'sisters', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'tom', 'hat', 'weder', 'brüder', 'noch', 'schwestern', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['tom', 'hat', 'keine', 'geschwister', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', \"you're\", 'too', 'nervous', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'sie', 'sind', 'zu', 'nervös', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['du', 'bist', 'zu', 'nervös', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', \"i'll\", 'do', 'the', '<unk>', 'since', 'you', 'cooked', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'ich', 'wasche', 'ab,', 'weil', 'du', 'ja', 'gekocht', 'hast', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['ich', 'werde', 'das', '<unk>', 'tun,', 'was', 'du', '<unk>', 'hast', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', \"that's\", 'not', 'true', 'either', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'das', 'ist', 'auch', 'nicht', 'wahr', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['das', 'ist', 'auch', 'nicht', 'alles', '.', '<eos>']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: ['<sos>', 'tom', 'and', 'mary', 'started', 'walking', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "German Translation: ['<sos>', 'tom', 'und', 'maria', 'gingen', 'los', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Model Translation: ['tom', 'und', 'maria', 'fingen', 'an', 'zu', 'wandern', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    en_sentence = en_sequence[i].unsqueeze(0)\n",
    "    de_sentence = de_sequence[i].unsqueeze(0)\n",
    "    translation = translate_sentence(en_sentence, en_vocab, encoder, decoder)\n",
    "    print('-'*130)\n",
    "    print(f'English Sentence: {en_vocab.lookup_tokens(en_sentence[0].cpu().tolist())}')\n",
    "    print(f'German Translation: {de_vocab.lookup_tokens(de_sentence[0].cpu().tolist())}')\n",
    "    print(f'Model Translation: {de_vocab.lookup_tokens(translation)}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
