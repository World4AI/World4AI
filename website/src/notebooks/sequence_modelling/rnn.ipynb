{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501f4236-00ee-4fc5-b8e1-41a737700cd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recurrent Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13630bf4-fb10-4572-b263-b83a2c9ca407",
   "metadata": {},
   "source": [
    "In this section we will have a look at what a recurrent neural network does under the hood in PyTorch. We will conduct the same computations using `nn.RNN` on the one hand and manually by implementing matrix multiplications and additions on the other hand. This will give us the necessary intuition to work with more complex architectures in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6821049d-29ff-4679-adc8-c76b1f385488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d743a-1301-441a-b3f5-edb4c4190bb4",
   "metadata": {},
   "source": [
    "The parameters were provided with different values, in order to be able to understand the dimensionalities of tensors. If you work through the notebook and ask yourself why the output is shaped in a particular way, return to these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91d3770-edb4-4eaf-93dc-656c8ba02561",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "SEQUENCE_LENGTH=5\n",
    "INPUT_SIZE=1\n",
    "HIDDEN_SIZE=3\n",
    "NUM_LAYERS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc8bcc-6cf7-4f2c-97eb-e4574e7b712c",
   "metadata": {
    "tags": []
   },
   "source": [
    "A recurrent neural network in PyTorch is as expected a `nn.Module`. There are several important arguments that the module requires, you can read more on the official [PyTorch documentatoin](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).\n",
    "\n",
    "- input_size – This is the lenghts of the vectors in the sequence. If you have a sequence of size 10 and each piece of the sequence is a 4-dim vector, then input size is 4.\n",
    "\n",
    "- hidden_size – This is the number of output neurons\n",
    "\n",
    "- num_layers – The number of recurrent layers, defaults to 1\n",
    "\n",
    "- nonlinearity – Either 'tanh' or 'relu', defaults to 'tanh'\n",
    "\n",
    "- batch_first – This parameter can be tricky to grasp. So far when we used neural networks, the first dimensionality has always been the `batch size`. A recurrent neural network in PyTorch on the other hand, takes a shape of `(sequence length, batch size, features)` as the default. If you set this parameter to True, then you must provide the rnn with the shape the `(batch size, sequence length, features)`. Below we will use the default behaviour, but in our practical examples it will be convenient to set this to True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237ba2a-5ea4-4ff2-b556-08a92ef64b1d",
   "metadata": {},
   "source": [
    "Our recurrent neural network expects inputs of size 1, generates vectors of size three and stacks two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af487d1-c312-4299-928c-4a6bd990f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75805a-205e-4a74-acae-fefed5ce26e7",
   "metadata": {},
   "source": [
    "In order to be able to reconstruct the functionality of the `nn.RNN` module, we will extract the weights and biases that the module is initialized with. There are two sets of weights for each of the layer: `ih` is input-hidden and `hh` is hidden-hidden. The layers are marked with either `l0` or `l1`. If we created a three layer network, there would be a `l2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2b17ec-b017-41be-8bbb-e5ae0406cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- #\n",
    "# layer 1\n",
    "# ---------------------------- #\n",
    "\n",
    "# input to hidden weights and biases\n",
    "w_ih_l0 = rnn.weight_ih_l0\n",
    "b_ih_l0 = rnn.bias_ih_l0\n",
    "\n",
    "# hidden to hidden weights and biases\n",
    "w_hh_l0 = rnn.weight_hh_l0\n",
    "b_hh_l0 = rnn.bias_hh_l0\n",
    "\n",
    "# ---------------------------- #\n",
    "# layer 2\n",
    "# ---------------------------- #\n",
    "# input to hidden weights and biases\n",
    "w_ih_l1 = rnn.weight_ih_l1\n",
    "b_ih_l1 = rnn.bias_ih_l1\n",
    "\n",
    "# hidden to hidden weights and biases\n",
    "w_hh_l1 = rnn.weight_hh_l1\n",
    "b_hh_l1 = rnn.bias_hh_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c2984-0ba1-442d-bfbb-8525371907aa",
   "metadata": {},
   "source": [
    "We create a random sequence of dimensionality (5, 4, 1) and initial hidden state filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5a0913-6d9f-4587-aa89-ac6f207abd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = torch.randn(SEQUENCE_LENGTH, BATCH_SIZE, INPUT_SIZE)\n",
    "h_0 = torch.zeros(NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc58de-16e7-49fd-bb3d-95edc7c90296",
   "metadata": {},
   "source": [
    "The network returns the output and the last hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859e3a3d-2e10-4955-bc72-0e6d8e80cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output, h_n = rnn(sequence, h_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b17a0-369c-434c-943e-5a2361c58b2d",
   "metadata": {},
   "source": [
    "The `output` variable contains the hidden states that were produced for the top most layer (for each part of the sequence and each batch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56378444-b2c1-4c0c-9aae-db6dd1651d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56b1a5-c31e-4f06-9586-fd031e4ffbac",
   "metadata": {},
   "source": [
    "The `h_n` variable on the other hand is the last hidden state for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7616912a-9a41-43f7-a72b-10da239cc392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb1cce-3439-4fc0-acdf-a2eba4b8a2e2",
   "metadata": {},
   "source": [
    "The above explanations might not made a lot of sense to you, therefore you should work through this manual implementation below. Once you do, you will have a much better grasp at what the output and the h_n actually stand for. Don't skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbae8a3-8c61-42be-b151-d0da07ea6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_rnn():\n",
    "    hidden = h_0.clone()\n",
    "    output = torch.zeros(SEQUENCE_LENGTH, BATCH_SIZE, HIDDEN_SIZE)\n",
    "    with torch.inference_mode():\n",
    "        for idx, seq in enumerate(sequence):\n",
    "            for layer in range(NUM_LAYERS):\n",
    "                if layer == 0:\n",
    "                    hidden[0] = torch.tanh(seq @ w_ih_l0.T + b_ih_l0 + hidden[0] @ w_hh_l0.T + b_hh_l0)\n",
    "                elif layer == 1:\n",
    "                    hidden[1] = torch.tanh(hidden[0] @ w_ih_l1.T + b_ih_l1 + hidden[1] @ w_hh_l1.T + b_hh_l1)\n",
    "                    output[idx] = hidden[1]\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc847814-5a49-4d8a-aa19-79cae9b037f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_output, manual_h_n = manual_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f295726-f7a7-4bd4-8487-742053ad8bc9",
   "metadata": {},
   "source": [
    "The values that the `nn.RNN` module produced and those produced by our manual implementation are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eed5fe2-5600-4ddf-a184-631c59382ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1792e-02,  1.9047e-01,  1.6709e-01],\n",
       "         [ 7.2242e-02,  6.5224e-02,  2.0224e-01],\n",
       "         [ 1.0300e-01,  1.9305e-02,  2.1496e-01],\n",
       "         [ 1.0832e-02,  1.5682e-01,  1.7670e-01]],\n",
       "\n",
       "        [[ 1.0195e-01,  9.4162e-02,  1.3910e-01],\n",
       "         [ 1.3410e-01,  1.8453e-01,  1.5568e-01],\n",
       "         [ 8.6578e-02,  3.0272e-01,  1.3517e-01],\n",
       "         [ 1.2236e-01,  1.0065e-01,  1.4895e-01]],\n",
       "\n",
       "        [[ 1.2442e-01,  1.0543e-01,  1.7694e-01],\n",
       "         [ 2.4460e-01, -7.7739e-02,  2.1771e-01],\n",
       "         [ 1.3163e-01, -1.7330e-04,  1.5564e-01],\n",
       "         [ 2.3440e-01, -4.3322e-02,  2.2091e-01]],\n",
       "\n",
       "        [[ 2.3210e-01, -1.6176e-02,  1.7971e-01],\n",
       "         [ 1.3311e-01,  3.1258e-01,  1.3606e-01],\n",
       "         [ 8.3280e-02,  2.2146e-01,  1.3285e-01],\n",
       "         [ 2.1146e-01,  1.8934e-01,  1.6867e-01]],\n",
       "\n",
       "        [[ 2.1319e-01,  1.2671e-01,  2.0186e-01],\n",
       "         [ 1.5184e-01, -1.8205e-02,  1.5104e-01],\n",
       "         [ 1.5311e-01, -1.2818e-02,  1.5881e-01],\n",
       "         [ 3.1330e-01, -1.4031e-01,  2.3919e-01]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab54ed2b-d8b0-459e-acfd-95d50eace89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1792e-02,  1.9047e-01,  1.6709e-01],\n",
       "         [ 7.2242e-02,  6.5224e-02,  2.0224e-01],\n",
       "         [ 1.0300e-01,  1.9305e-02,  2.1496e-01],\n",
       "         [ 1.0832e-02,  1.5682e-01,  1.7670e-01]],\n",
       "\n",
       "        [[ 1.0195e-01,  9.4162e-02,  1.3910e-01],\n",
       "         [ 1.3410e-01,  1.8453e-01,  1.5568e-01],\n",
       "         [ 8.6578e-02,  3.0272e-01,  1.3517e-01],\n",
       "         [ 1.2236e-01,  1.0065e-01,  1.4895e-01]],\n",
       "\n",
       "        [[ 1.2442e-01,  1.0543e-01,  1.7694e-01],\n",
       "         [ 2.4460e-01, -7.7739e-02,  2.1771e-01],\n",
       "         [ 1.3163e-01, -1.7332e-04,  1.5564e-01],\n",
       "         [ 2.3440e-01, -4.3322e-02,  2.2091e-01]],\n",
       "\n",
       "        [[ 2.3210e-01, -1.6176e-02,  1.7971e-01],\n",
       "         [ 1.3310e-01,  3.1258e-01,  1.3606e-01],\n",
       "         [ 8.3280e-02,  2.2146e-01,  1.3285e-01],\n",
       "         [ 2.1146e-01,  1.8934e-01,  1.6867e-01]],\n",
       "\n",
       "        [[ 2.1319e-01,  1.2671e-01,  2.0186e-01],\n",
       "         [ 1.5184e-01, -1.8205e-02,  1.5104e-01],\n",
       "         [ 1.5311e-01, -1.2818e-02,  1.5881e-01],\n",
       "         [ 3.1330e-01, -1.4031e-01,  2.3919e-01]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c4c2e5-b15b-4fc3-a5fd-61b5fb6b661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0023, -0.4017,  0.0474],\n",
       "         [-0.2172,  0.0534,  0.1672],\n",
       "         [-0.2537, -0.1052,  0.1003],\n",
       "         [-0.3814, -0.2900, -0.2007]],\n",
       "\n",
       "        [[ 0.2132,  0.1267,  0.2019],\n",
       "         [ 0.1518, -0.0182,  0.1510],\n",
       "         [ 0.1531, -0.0128,  0.1588],\n",
       "         [ 0.3133, -0.1403,  0.2392]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ce7e99d-1a00-481d-8ad9-3f1b66d1c2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0023, -0.4017,  0.0474],\n",
       "         [-0.2172,  0.0534,  0.1672],\n",
       "         [-0.2537, -0.1052,  0.1003],\n",
       "         [-0.3814, -0.2900, -0.2007]],\n",
       "\n",
       "        [[ 0.2132,  0.1267,  0.2019],\n",
       "         [ 0.1518, -0.0182,  0.1510],\n",
       "         [ 0.1531, -0.0128,  0.1588],\n",
       "         [ 0.3133, -0.1403,  0.2392]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_h_n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
