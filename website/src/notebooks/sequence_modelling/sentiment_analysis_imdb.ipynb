{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35e798ab-5ec7-4985-895e-4d5d09b04118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import kaggle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e1692f2-8835-4780-93e9-81223a1be05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading imdb-dataset-of-50k-movie-reviews.zip to ../datasets\n",
      "100%|██████████████████████████████████████| 25.7M/25.7M [00:14<00:00, 2.37MB/s]\n",
      "100%|██████████████████████████████████████| 25.7M/25.7M [00:14<00:00, 1.92MB/s]\n"
     ]
    }
   ],
   "source": [
    "# donwload the data\n",
    "!kaggle datasets download -p ../datasets -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7331f635-b2d8-4a03-a8b5-deed5e348bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../datasets/imdb-dataset-of-50k-movie-reviews.zip\n",
      "  inflating: ../datasets/imdb/IMDB Dataset.csv  \n"
     ]
    }
   ],
   "source": [
    "# unzip the data\n",
    "!unzip -d ../datasets/imdb ../datasets/imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "375fedcc-086f-4e63-b72d-c1e5959924e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64672\n",
      "drwxrwxr-x 2 petruschka petruschka     4096 Sep  7 19:49  .\n",
      "drwxrwxr-x 5 petruschka petruschka     4096 Sep  7 19:49  ..\n",
      "-rw-rw-r-- 1 petruschka petruschka 66212309 Oct 19  2019 'IMDB Dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../datasets/imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0bc3a2d-23bb-4dde-b495-49d3f61aefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/imdb/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9209c4fa-89db-4666-a4be-b80f49896309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa85fc11-a2e6-4f2d-a25d-82cd56db935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:30000]\n",
    "val_df = df[30000:40000]\n",
    "test_df = df[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58899491-5ec4-4dbb-8536-2905f2c3f9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    15015\n",
       "negative    14985\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42456427-f7db-4849-919c-af5fcce5691a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    5022\n",
       "positive    4978\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9fa01ad-1789-4618-8da9-50e8c4f44a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    5007\n",
       "negative    4993\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eaaf1770-970a-4563-84d2-57c931048f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.length = len(df)\n",
    "        self.reviews = df[\"review\"].to_numpy()\n",
    "        self.sentiments = df[\"sentiment\"].to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.sentiments[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecc007a7-3b4b-47b9-b00b-ae0267a4439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_df)\n",
    "val_dataset = IMDBDataset(val_df)\n",
    "test_dataset = IMDBDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "217ff9aa-9b35-4ff0-a8a4-76b04492f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "83154aef-8c81-456a-8f3f-b1abdae72935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'are', 'you', 'doing', 'today', '?']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"How are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "523974b1-2a73-4621-9f78-bc53c688f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary\n",
    "# vocab expects with word\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "086e786c-b669-4492-910a-f4a1abac2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for review, _ in train_dataset:\n",
    "    counter.update(tokenizer(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "582038f4-85c9-4d7c-94fd-a0566092d31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112119\n"
     ]
    }
   ],
   "source": [
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "47715bc5-480a-4c1f-b3c7-450d50f12ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0d69b4be-f99a-4799-86b4-350b131ef8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_vocab = vocab(ordered_dict, min_freq=1, specials=[\"<pad>\", \"<unk>\"], special_first=True)\n",
    "imdb_vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d2bdb9b7-424f-4a29-a3e8-1af3e7738516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112121"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduced number of tokens\n",
    "len(imdb_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f05d9e97-3363-400e-9499-5e5b2a2baa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_vocab([\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6c92b96c-ffa4-45d6-8412-58ed30d6cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    token_ls, sentiment_ls, len_ls = [], [], []\n",
    "    for review, sentiment in batch:\n",
    "        tokens = [imdb_vocab[token] for token in tokenizer(review)]\n",
    "        sentiment_idx = 1 if sentiment == 'positive' else 0\n",
    "        token_ls.append(torch.tensor(tokens, dtype=torch.int64))\n",
    "        len_ls.append(len(tokens))\n",
    "        sentiment_ls.append(sentiment_idx)\n",
    "    return nn.utils.rnn.pad_sequence(token_ls, batch_first=True), torch.tensor(sentiment_ls), torch.tensor(len_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3e3572fb-d331-41e7-8576-d27963f8bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(train_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "4000d659-20a4-4588-bae3-fd393c01fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e3957097-8d57-43a3-b3b7-0b35f74dd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "NUM_EMBEDDINGS=len(imdb_vocab)\n",
    "EMBEDDING_DIM=20\n",
    "LSTM_HIDDEN_SIZE=64\n",
    "FC_HIDDEN_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "14c3edd8-4948-4847-bf14-177095dc0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "bc023302-d494-4f05-8c9c-4269b16bde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=NUM_EMBEDDINGS, embedding_dim=EMBEDDING_DIM, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=EMBEDDING_DIM, hidden_size=LSTM_HIDDEN_SIZE, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=LSTM_HIDDEN_SIZE, out_features=FC_HIDDEN_SIZE)\n",
    "        self.fc2 = nn.Linear(in_features=FC_HIDDEN_SIZE, out_features=1)\n",
    "    \n",
    "    def forward(self, x, sizes):\n",
    "        x = self.embedding(x)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, sizes.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = h_n[-1, ...]\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "69549b11-2257-4bb9-b805-59e61829fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "b76db1f9-8897-4ac7-b671-5348a5d4535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_performance(dataloader, model, criterion):\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    # no need to calculate gradients\n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (features, labels, sizes) in enumerate(dataloader):\n",
    "            features = features.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).view(-1, 1).float()\n",
    "            logits = model(features, sizes)\n",
    "            probs = torch.sigmoid(logits)\n",
    "                        \n",
    "            predictions = (probs > 0.5).float()\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            loss_sum += loss.cpu().item()\n",
    "            num_samples += len(features)\n",
    "    \n",
    "    # we return the average loss and the accuracy\n",
    "    return loss_sum/num_samples, num_correct/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "5ce140e6-24ee-4371-9500-7766b3e9ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_dataloader, val_dataloader, model, criterion, optimizer, scheduler=None):\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (features, labels, sizes) in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            features = features.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).view(-1, 1).float()\n",
    "            \n",
    "            # Empty the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            logits = model(features, sizes)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Descent\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss, train_acc = track_performance(train_dataloader, model, criterion)\n",
    "        val_loss, val_acc = track_performance(val_dataloader, model, criterion)\n",
    "\n",
    "        if scheduler:\n",
    "          scheduler.step(val_acc)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:>2}/{num_epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}')\n",
    "    return history            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "faadfee0-9203-4fe7-ba87-6e639d89b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       patience=2,\n",
    "                                                       verbose=True)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6a7eedd2-4fad-45c6-aef4-465dafa277ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/15 | Train Loss: 0.01862 | Val Loss: 0.01864 | Train Acc: 0.696 | Val Acc: 0.700\n",
      "Epoch:  2/15 | Train Loss: 0.01583 | Val Loss: 0.01656 | Train Acc: 0.752 | Val Acc: 0.735\n",
      "Epoch:  3/15 | Train Loss: 0.01139 | Val Loss: 0.01301 | Train Acc: 0.850 | Val Acc: 0.823\n",
      "Epoch:  4/15 | Train Loss: 0.01244 | Val Loss: 0.01455 | Train Acc: 0.822 | Val Acc: 0.785\n",
      "Epoch:  5/15 | Train Loss: 0.01405 | Val Loss: 0.01540 | Train Acc: 0.810 | Val Acc: 0.768\n",
      "Epoch:  6/15 | Train Loss: 0.00737 | Val Loss: 0.01065 | Train Acc: 0.910 | Val Acc: 0.859\n",
      "Epoch:  7/15 | Train Loss: 0.00614 | Val Loss: 0.00999 | Train Acc: 0.925 | Val Acc: 0.871\n",
      "Epoch:  8/15 | Train Loss: 0.00468 | Val Loss: 0.00964 | Train Acc: 0.948 | Val Acc: 0.878\n",
      "Epoch:  9/15 | Train Loss: 0.00349 | Val Loss: 0.00988 | Train Acc: 0.963 | Val Acc: 0.884\n",
      "Epoch: 10/15 | Train Loss: 0.00304 | Val Loss: 0.00997 | Train Acc: 0.968 | Val Acc: 0.881\n",
      "Epoch: 11/15 | Train Loss: 0.00219 | Val Loss: 0.01063 | Train Acc: 0.979 | Val Acc: 0.885\n",
      "Epoch: 12/15 | Train Loss: 0.00145 | Val Loss: 0.01172 | Train Acc: 0.988 | Val Acc: 0.884\n",
      "Epoch: 13/15 | Train Loss: 0.00109 | Val Loss: 0.01227 | Train Acc: 0.992 | Val Acc: 0.882\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 14/15 | Train Loss: 0.00093 | Val Loss: 0.01438 | Train Acc: 0.993 | Val Acc: 0.874\n",
      "Epoch: 15/15 | Train Loss: 0.00068 | Val Loss: 0.01487 | Train Acc: 0.996 | Val Acc: 0.882\n"
     ]
    }
   ],
   "source": [
    "history = train(15, train_dataloader, val_dataloader, model, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c08781-c6f3-47ce-823b-4afa8a536d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
