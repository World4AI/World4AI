{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e23e5a2-4579-4ea8-8807-0d34e6473a91",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba41f7d-3f90-4c55-9c60-51aa96614a55",
   "metadata": {},
   "source": [
    "In this section we are going to show how we can solve logistic regression using only NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f04c1b5-d6c6-4a53-ac67-e93e4ff05775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e6897-3ba5-4b4b-ae56-2d000255227f",
   "metadata": {},
   "source": [
    "We are dealing with a classification problem and are trying to predict the probability of a sample $i$ to belong to the class **1**, given the feature vector $\\mathbf{x}$. For that purpose we are going to fit the weights $\\mathbf{w}$ such that $\\sigma(z)$ is close to 1, when the true label $y$ is 1 and $\\sigma(z)$ is 0 when the $y$ is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed5722-470c-43f8-b434-0dfc16a93e64",
   "metadata": {},
   "source": [
    "$\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$, where $z = {\\mathbf{x}\\mathbf{w}^T+b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdfa7e-475c-4806-adf1-9e0083df20b1",
   "metadata": {},
   "source": [
    "We use the `make_classification` function from sklearn to create a binary classification dataset with 100 samples and 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee350c8-3109-48d8-9349-2760d673f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7999e181-2cb6-4333-b02e-6ed02d77d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reshape the data from (100,) to (100, 1)\n",
    "y = y.reshape(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d15e8-c742-46b9-8a99-0e58abd91c55",
   "metadata": {},
   "source": [
    "We initialize weights $\\mathbf{w}$ and the bias $b$ using the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7abac56-63cf-44eb-9475-772500b60fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(1, 10)\n",
    "b = np.random.randn(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af3f02-590c-482b-a340-119b9a492e99",
   "metadata": {},
   "source": [
    "Very few hyperparameters are required for logistic regression. The learning rate and the number of epochs are sufficient in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787b88dc-7d65-4c0a-884e-2334236ccd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263be856-556d-4cad-95c6-dd26bdc8e32a",
   "metadata": {},
   "source": [
    "Below we implement logistic regression.\n",
    "\n",
    "1. We start by calculating $\\sigma(\\mathbf{z})$, the probability vector. Each of $n$ elements calculates the probability that the element $i$ belongs to category 1.\n",
    "2. We calcualte the partial derivatives based on the cross-entropy loss $H = \\dfrac{1}{n} \\sum_i^n -\\Big[y^{(i)} \\ln\\big(\\sigma(z^{(i)})\\big) + \\big(1 - y^{(i)}\\big)\\ln\\big(1 - \\sigma(z^{(i)})\\big)\\Big] $\n",
    "\n",
    "$\\dfrac{\\partial H}{\\partial \\sigma^{(i)}} = - \\Big(y^{(i)} \\dfrac{1}{\\sigma^{(i)}} - (1 - y^{(i)}) \\dfrac{1}{1 - \\sigma^{(i)}} \\Big)$\n",
    "\n",
    "$\\dfrac{\\partial \\sigma^{(i)}}{\\partial z^{(i)}} = \\sigma^{(i)}(1 - \\sigma^{(i)})$\n",
    "\n",
    "$\\dfrac{\\partial z^{(i)}}{\\partial w_j^{(i)}} = x_j^{(i)}, \\dfrac{\\partial z^{(i)}}{\\partial b^{(i)}} = 1$\n",
    "\n",
    "3. And we apply the chain rule\n",
    "$\\dfrac{\\partial H}{\\partial w_j} = \\dfrac{1}{n}\\sum^n_i \\dfrac{\\partial H}{\\partial \\sigma^{(i)}} \\dfrac{\\partial \\sigma^{(i)}}{\\partial z^{(i)}} \\dfrac{\\partial z^{(i)}}{\\partial w^{(i)}_j}$\n",
    "\n",
    "3. Finally we apply batch gradient descent $\\mathbf{w} := \\mathbf{w} - \\alpha \\mathbf{\\nabla}$ and $b := b - \\alpha \\dfrac{\\partial L}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23331d22-4d7c-477f-ad54-32a39297f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Cross Entropy: 0.7766\n",
      "Epoch: 10 | Cross Entropy: 0.6268\n",
      "Epoch: 20 | Cross Entropy: 0.5355\n",
      "Epoch: 30 | Cross Entropy: 0.4750\n",
      "Epoch: 40 | Cross Entropy: 0.4319\n",
      "Epoch: 50 | Cross Entropy: 0.3996\n",
      "Epoch: 60 | Cross Entropy: 0.3743\n",
      "Epoch: 70 | Cross Entropy: 0.3541\n",
      "Epoch: 80 | Cross Entropy: 0.3375\n",
      "Epoch: 90 | Cross Entropy: 0.3238\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # 1. calculate predicted probabilities\n",
    "    z = X @ w.T + b\n",
    "    sigma = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        cross_entropy = -(y * np.log(sigma) + (1-y) * np.log(1 - sigma)).mean()\n",
    "        print(f\"Epoch: {epoch} | Cross Entropy: {cross_entropy:.4f}\",)\n",
    "    \n",
    "    # 2. calculate the partial derivatives \n",
    "    dH_dsigma = -(y * (1 / sigma) - (1 - y) * (1 / (1 - sigma)))\n",
    "    dsigma_dz = sigma * (1 - sigma)\n",
    "    dz_dx = X\n",
    "    dz_db = 1\n",
    "    \n",
    "    # 3. apply the chain rule\n",
    "    grad_w = (dH_dsigma * dsigma_dz * dz_dx).mean(axis=0)\n",
    "    grad_b = (dH_dsigma * dsigma_dz * dz_db).mean()\n",
    "    \n",
    "    # 4. apply batch gradient descent\n",
    "    w = w - alpha * grad_w\n",
    "    b = b - alpha * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7902ac-514b-4b7a-8d24-56c47c80a682",
   "metadata": {},
   "source": [
    "The loss decreases over the period of 100 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
