<script>
  import Question from "$lib/Question.svelte";
  import Latex from "$lib/Latex.svelte";
  import Code from "$lib/Code.svelte";

  let minEpsilon = 0.1;
  let currentEpsilon = 1;
  let epsilonStep = 0.001;
  let stepsToTake = 100;
  let stepsTaken = 0;

  function reduceEpsilon() {
    currentEpsilon = currentEpsilon - epsilonStep;
    if (currentEpsilon < minEpsilon) {
      currentEpsilon = minEpsilon;
    }
  }

  let Ns = [0, 0, 0];
  let Qs = [0, 0, 0];

  let model = [
    [0.2, 0.3, 0.5],
    [0.8, 0.1, 0.1],
    [0.9, 0.05, 0.05],
  ];
  let values = [
    [5, -2, -1],
    [1, -1, -5],
    [-2, -1, 100],
  ];

  let qs = [];

  let cumulativeProbabilities = [];
  model.forEach((probabilities) => {
    const cumulativeSum = (
      (sum) => (value) =>
        (sum += value)
    )(0);
    cumulativeProbabilities.push(probabilities.map(cumulativeSum));
  });

  model.forEach((distribution, idxOuter) => {
    let q = 0;
    distribution.forEach((probability, idxInner) => {
      q += probability * values[idxOuter][idxInner];
    });
    qs.push(q);
  });

  function takeSteps() {
    for (let step = 0; step < stepsToTake; step++) {
      stepsTaken += 1;
      let action;
      if (Math.random() < currentEpsilon) {
        action = Math.floor(Math.random() * model.length);
      } else {
        action = Qs.reduce((iMax, x, i, arr) => (x > arr[iMax] ? i : iMax), 0);
      }
      Ns[action] += 1;

      //make a draw from the distribution to get a reward
      let randomNumber = Math.random();
      let valueIndex = 0;
      for (let idx = 0; idx < cumulativeProbabilities[action].length; idx++) {
        if (randomNumber < cumulativeProbabilities[action][idx]) {
          valueIndex = idx;
          break;
        }
      }
      let value = values[action][valueIndex];

      Qs[action] = Qs[action] + (1 / Ns[action]) * (value - Qs[action]);
      Qs = [...Qs];
      Ns = [...Ns];

      reduceEpsilon();
    }
  }
</script>

<svelte:head>
  <title>World4AI | Reinforcement Learning | Epsilon-Greedy</title>
  <meta
    name="description"
    content="Epsilon-greedy is a strategy to balance exploration and exploitation. With the probability of epsilon the agent selects a random action and with probability of 1 - epsilon the agent selects the greedy action."
  />
</svelte:head>

<h1>Epsilon-Greedy</h1>
<Question
  >What strategy is commonly used to balance exploration and exploitation?</Question
>
<div class="separator" />

<p>
  Epsilon-Greedy is on the one hand the easiest strategy to balance exploration
  and exploitation and on the other hand probably the most common one.
</p>

<p>
  With the probability of <Latex>{String.raw`\epsilon`}</Latex> the agent selects
  a random action and with the probability of <Latex
    >{String.raw`1 - \epsilon`}</Latex
  > the agent selects the greedy action. Often <Latex>\epsilon</Latex> is not constant
  and starts with a relatively high value to encourage exploration when the agent
  knows nothing about the environment. The value is decreased over time and reaches
  a final minimal value after a designated number of steps. The minimal value often
  lies in the range between 0.01 and 0.1. Steps to take shows how many actions the
  agent is going to take the next time you click the button below.
</p>
<p>
  Below we see an interactive example of <Latex>\epsilon</Latex>-greedy using
  the example from the previous section. We start with an <Latex>\epsilon</Latex
  > of 1 and reduce it by 0.001 with each selected action until the value of 0.1.
</p>
<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Parameter Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><Latex>\epsilon</Latex></td>
      <td>{currentEpsilon}</td>
    </tr>
    <tr>
      <td>Min <Latex>\epsilon</Latex></td>
      <td>{minEpsilon}</td>
    </tr>
    <tr>
      <td>Linear Step <Latex>\epsilon</Latex></td>
      <td>{epsilonStep}</td>
    </tr>
    <tr>
      <td>Steps To Take</td>
      <td>{stepsToTake}</td>
    </tr>
    <tr>
      <td>Steps Taken <Latex>N</Latex></td>
      <td>{stepsTaken}</td>
    </tr>
  </tbody>
</table>
<p>
  <Latex>q_*</Latex> is the true action-value function, <Latex>N</Latex> is the number
  of action that were already take per action and <Latex>Q</Latex> is the estimated
  value for the action value function. The final row is the absolute difference beween
  the true and the estimated value.
</p>
<table>
  <thead>
    <tr>
      <th />
      <th>Action 0</th>
      <th>Action 1</th>
      <th>Action 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><Latex>q_*</Latex></td>
      {#each qs as q}
        <td>{q}</td>
      {/each}
    </tr>
    <tr>
      <td><Latex>N</Latex></td>
      {#each Ns as N}
        <td>{N}</td>
      {/each}
    </tr>
    <tr>
      <td><Latex>Q</Latex></td>
      {#each Qs as Q}
        <td>{Q}</td>
      {/each}
    </tr>
    <tr />
    <tr>
      <td><Latex>|q_* - Q|</Latex></td>
      {#each qs as q, idx}
        <td>{Math.abs(q - Qs[idx])}</td>
      {/each}
    </tr>
  </tbody>
</table>
<p>
  You can use the input to select the number of actions the agent is supposed to
  take.
</p>
<form>
  <input bind:value={stepsToTake} min="0" type="number" />
  <button on:click|preventDefault={takeSteps}>Take {stepsToTake} Steps</button>
</form>
<p>
  Below is a Python example, that shows a function that uses <Latex
    >\epsilon</Latex
  >-greedy to find the approximate action-value function.
</p>
<Code
  code={`
def bandit_algorithm(bandit, A, num_episodes=1000000, alpha=0.00001, epsilon=0.5):
    num_actions = len(A)
    Q = np.zeros(num_actions)
    
    for episode in range(num_episodes):
        
        if np.random.rand() < epsilon:
            action = np.random.choice(num_actions)
        else:
            action = Q.argmax()
        
        reward = bandit.step(action)
        
        Q[action] = Q[action] + alpha * (reward - Q[action])
        
        if episode % 100000 == 0:
            print(Q)
        
    return Q
  `}
/>
<div class="separator" />

<style>
  table {
    width: 100%;
  }

  th {
    text-transform: uppercase;
  }

  td,
  th {
    border: 1px double var(--text-color);
    padding: 7px;
    text-align: center;
  }
  td {
    font-style: italic;
  }
  input,
  button {
    background-color: var(--background-color);
    outline: none;
    border: 1px solid var(--text-color);
    color: var(--text-color);
    padding: 10px;
    font-size: 15px;
  }
  button {
    cursor: pointer;
  }

  button:hover {
    color: var(--main-color-1);
  }
</style>
