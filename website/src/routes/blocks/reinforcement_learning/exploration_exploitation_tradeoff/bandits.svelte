<script>
  import Question from "$lib/Question.svelte";
  import Math from "$lib/Math.svelte";
  import Code from "$lib/Code.svelte";
</script>

<svelte:head>
  <title>World4AI | Reinforcement Learning | Bandits</title>
  <meta
    name="description"
    content="Bandits are useful to study exploration strategies. Bandits posess one single state and a variaty of actions."
  />
</svelte:head>

<h1>Bandits</h1>
<Question
  >What are bandits and why are they useful in reinforcement learning?</Question
>
<div class="separator" />

<h2>Definition</h2>
<p>
  Usually the mathematical introduction into the topic of exploration vs
  exploitation is done by the means of so-called k-armed bandits. Unlike the
  full reinforcement learning problem in a bandits setting we face the same
  single state over and over again and have to choose one of the k actions. Once
  the action is chosen the agent receives the reward and the state is reset
  again.
</p>
<p>
  The name k-armed bandits comes from the casino game “One Armed Bandit”. In
  such a game the player has to push a single lever and after each push he
  either receives a reward or not with a certain probability. The outcome of the
  next pull does not depend on past actions. In k-armed bandits there are k
  levers. For each of the levers there is a corresponding probability for a
  certain reward. The idea is to find the lever that maximizes the expected
  reward.
</p>
<p>
  Unlike in an MDP in a bandit problem the action value function does not depend
  on the state.
</p>
<Math latex={String.raw`q_*(a) \doteq \mathbb{E}[R_t \mid A_t = a]`} />
<p>
  At the very first glance the solutions to k-armed bandits problems seem to be
  of limited use when faced with full reinforcement learning MDP tasks, but
  there is a particular reason to still cover the topic. We remember that
  reinforcement learning is learning through trial and error and delayed reward.
  Bandits basically remove the “delayed” part of the definition and deal only
  with trial and error and reward, as the reward is received for each of the
  actions directly and the environment is reset to the single not terminal
  state. That reduces complexity by not caring about the sequential problems but
  still allows us to discover different types of exploration-exploitation
  techniques, which eventually can be applied to full reinforcement learning
  tasks. We can say that bandits are a special case of an MDP.
</p>
<p>
  If you possess the true value function, then the solution is as simple as
  selecting the action with the highest value. There is of course always the
  option to get the model directly to calculate the expected reward for each of
  the actions, but this chapter is the precursor to full reinforcement learning
  solutions, therefore we will use interaction to approximate the value
  function.
</p>
<Math latex={String.raw`A_t \doteq \arg\max_a q_*(a)`} />
<div class="separator" />

<h2>Approximation Of The Values</h2>
<p>
  The intuitive way to estimate the action-value function is to select certain
  actions and build averages of the rewards. If you continue doing that, the
  approximation is going to get better and better, meaning it is going to get
  closer to the true action-value function.
</p>
<Math
  latex={String.raw`Q_t(a) \doteq \frac{\sum_{t-1}^{i=1} R_i \cdot 1_{A_i=a}}{\sum_{t-1}^{i=1} 1_{A_i=a}}`}
/>
<p>
  When estimating the value function using the above approach it is of course
  entirely possible that the value of a certain action appears only larger due
  to an approximation error while the actual true greedy action was not selected
  enough in the past to approach the true value. The problem gives rise to the
  necessity to balance exploration and exploitation. In the next sections we are
  going to discuss several techniques that are used to deal with the
  exploration-exploitation tradeoff. For the rest of this chapter we will cover
  techniques that make the computation of the approximation of the action-value
  function more efficient.
</p>
<div class="separator" />

<h2>Computational Efficiency</h2>
<p>
  If we follow the above definition of the calculation of the estimate for the
  action-value function then the natural approach would be to keep a list of all
  the actions taken and all the rewards received. The higher the number of
  actions taken the larger the list gets and the higher the memory requirements
  to keep the elements in memory and the higher the computational cost to loop
  through all the elements in the list to calculate the average.
</p>

<Math
  latex={String.raw`
        \begin{aligned}
        \mu_n & = \frac{1}{n} \sum_{i=1}^{n}x_i \\
        & = \frac{1}{n}(x_n + \sum_{i=1}^{n-1}x_i) \\
        & = \frac{1}{n}(x_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1}x_i) \\
        & = \frac{1}{n}(x_n + (n-1) \mu_{n-1}) \\
        & = \frac{1}{n}\mu_{n-1} + \frac{1}{n} n \mu_{n-1} - \frac{1}{n}x_n \\    
        & = \mu_{n-1} + \frac{1}{n}(x_n - \mu_{n-1})
        \end{aligned}`}
/>
<p>
  Using the expression above we can rewrite the calculation of the action-value
  for a particular action as follows.
</p>
<Math latex={String.raw`Q_{n+1} = Q_n  + \frac{1}{n}[R_n - Q_n]`} />
<p>
  The update rule requires only the last estimate of the action-value for a
  state a and the last reward received, no lists are necessary to track the
  average. More generally speaking the update can be described as follows.
</p>
<Math
  latex={String.raw`NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]`}
/>
<p>
  The target indicates the *true* value that was experienced and the idea is to
  push the new estimate towards the *true* value. The expression *Target -
  Estimate* is often described as *error*, as it is the difference between the
  true reward received after taking an action and the estimated reward. The
  *step size*, also called the learning rate, is the magnitude with which you
  change the estimate of the action-value function. From the expression above it
  follows that the *step size* is 1/n, which decreases with experience. In
  practice it is customary to either set the learning rate to a constant value
  or start with a relatively high value and reduce it until it reaches a minimum
  constant value.
</p>
<Math latex={String.raw`Q_{n+1} = Q_n  + \alpha[R_n - Q_n]`} />
<p>The greek letter :math:`\alpha` is often used for the learning rate.</p>
<div class="separator" />

<h2>Bandits Implementation</h2>
<p>
  The Bandit class takes two lists to initialize the environment. The first list
  contains the probabilities between 0 and 1 that indicate with what probability
  the agent receives a reward when the action is taken that corresponds with the
  position in the list. The second list contains the value of the reward that is
  given in case of success. The reward of 0 is given in the case of no success.
</p>
<Code
  code={`
# for a full implementation of bandits in gym look for the github repo by JKCooper2
# https://github.com/JKCooper2/gym-bandits
# this implementation is a simplified version

class Bandit():
    def __init__(self, probs=[], rewards=[]):

        self.probs = probs
        self.rewards = rewards
        # k as in k-armed bandits, number of arms
        k = len(self.probs)

        self.action_space = k
        self.observation_space = 1

    def step(self, action):
        if random.random() < self.probs[action]:
            reward = self.rewards[action]
        else:
            reward = 0

        return reward
  `}
/>
<p>
  The below example shows an environment that allows 2 actions. The first action
  gives with a probability of 50% a reward of 10 and the second action gives a
  reward of 1 with a probability of 100%.
</p>
<Code
  code={`
env = Bandit(probs=[0.5, 1], rewards=[10, 1])
  `}
/>
<div class="separator" />
