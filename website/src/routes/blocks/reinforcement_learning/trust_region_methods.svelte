<script>
  import Question from "$lib/Question.svelte";
  import Latex from "$lib/Latex.svelte";
</script>

<svelte:head>
  <title>World4AI | Reinforcement Learning | Trust Region Methods</title>
  <meta
    name="description"
    content="Trust Region Methods attempt to solve problems that are common to common policy gradient methods: sample inefficiency and a high chance of performance collapse"
  />
</svelte:head>

<h1>Trust Region Methods</h1>
<Question
  >What are trust region methods and what problems are they trying to solve?</Question
>
<div class="separator" />
<p>
  Trust region methods like TRPO and PPO were invented to solve problems that
  vanilla policy gradient methods and actor critic methods exhibit. Usually two
  major problems are discussed.
</p>
<p>
  The gradient of the policy tells us the direction in which we should change
  the weights of our policy. On the other hand the gradient does not tell us
  what step size we should take. When the gradient descent algorithm takes a
  relatively large step we can end with a bad performing policy. In
  reinforcement learning the data that is used for policy improvement is
  generated through the interaction between the agent and the environment. The
  bad policy that results from the large gradient descent step will lead to data
  that is generated by a bad policy which in turn will decrease the performance
  of the policy even more. Trust region methods define a "trust region" that
  determine a maximum step that can be taken with gradient descent.
</p>
<p>
  Unlike value based methods, which utilize the memory buffer to store memories
  and to utilize those many times for learning, policy gradient methods use
  experiences only once and throw them away.
</p>
<Latex
  >{String.raw`
    \nabla_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}}\Big[\nabla_{\theta} \log \pi_{\theta} A_{\theta}\Big]
  `}</Latex
>
<p>
  When we look at the definition of the gradient in the equation above, we
  realize that the reason for our inability to use old experiences is the need
  to calculate the expectation based on parameters <Latex>\theta</Latex>. Each
  time we take a gradient descent step we create a new policy with new
  parameters while the old experiences were collected with the old parameters.
  The trust region methods that we are going to cover will be able to utilize
  experiences several times and thus increase sample efficiency.
</p>
<div class="separator" />
