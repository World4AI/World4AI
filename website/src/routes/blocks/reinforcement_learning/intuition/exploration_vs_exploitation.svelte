<script>
    import Question from '$lib/Question.svelte';
    import { DeterministicAgent } from '$lib/reinforcement_learning/common/DeterministicAgent';
    import { GridEnvironment } from '$lib/reinforcement_learning/common/GridEnvironment';
    import { pickupGridMap, gridMap } from '$lib/reinforcement_learning/common/maps';

    import Grid from '$lib/reinforcement_learning/intuition/applications/Grid.svelte';

    let env_1 = new GridEnvironment(pickupGridMap);
    let agent_1 = new DeterministicAgent(env_1.getObservationSpace(), env_1.getActionSpace());    

    let env_2 = new GridEnvironment(gridMap, 0.8);
    let agent_2 = new DeterministicAgent(env_2.getObservationSpace(), env_2.getActionSpace()); 
</script>

<svelte:head>
    <title>World4AI | Reinforcement Learning - Exploration vs Exploitation</title>
    <meta name="description" content="In reinforcement learning the agent faces the so called exploration exploitation dilemma. At each step the agent has to decide to either explore or to exploit. Doing both at the same time is not possible.">
</svelte:head>

<h1>Exploration vs Exploitation</h1>
<Question>How can the agent discover good behaviour?</Question>
<div class="separator"></div>

<p>At each timestep the agent has to make the decision to either explore the environment or to exploit the current knowledge about the environment. The problem that the agent faces when deciding between the two options is the so called <strong>exploration exploitation dilemma (or exploration exploitation tradeoff)</strong>. On the one hand the agent wants to get the highest sum of rewards that is achievable based on the current knowledge - it wants to exploit. On the other hand in order to find a better sequence of actions, which lead to a higher sum of rewards, the agent needs to explore the environment. The dilemma is the fact that the agent can not do both at the same time. At each single step the agent either explores or exploits.</p>

<p class="info"><strong>Exploration:</strong> The agent exploits the available knowledge about the environment to get a best known sum of rewards.</p>
<p class="info"><strong>Exploitation:</strong> The agent explores the environment to gather new experience and knowledge about the environment.</p>

<div class="separator"></div>

<h2>Exploration in Deterministic Environments</h2>

<p>The grid environment we covered so far was deterministic. We assumed that there is no uncertainty and given the same circumstances the outcome would be the same. 
For example whenever the agent chose the action to go right in the first state for example the environment transitioned in such a way that the circle moved right. Each and every single time. No matter how often the action was repeated. Yet even in a deterministic environment the agent has to explore in order to find the optimal sequence of actions.</p>

<p class="info"><strong>Deterministic Environment</strong>: Given the same state of the environment and the same action by the agent the next state and the corresponding reward are always the same.</p>

<div class="flex-center">
    <Grid env={env_1} agent={agent_1} showAllRewards=true/>
</div>

<p>The above grid world shows an agent which has discovered the shortest route from the starting position to the goal position (triangle). At each timestep the agent earns a negative reward of -1. Once the agent reaches the goal, the environment gives a positive feedback of +1 reward and the game restarts. Generally the agent might keep taking the same path to reach the triangle, but if it kept exploring the environment it could discover that there is actually a big reward of +10 in the bottom right corner. The agent could pick up the reward first and then keep moving towards the goal. The high reward would make up for the additional few steps. In this deterministic example exploration would enable the agent to learn a strategy with a higher sum of rewards.</p>

<div class="separator"></div>

<h2>Exploration in Stochastic Environments</h2>

<p>Most of the environments (or the real world for that matter) are not deterministic, they are stochastic. That means that the next state and reward are calculated based on a probability distribution.</p>

<p class="info"><strong>Stochastic Environment</strong>: Given the same state of the environment and the same action by the agent, the next state and the corresponding reward are calculated using a probability distribution.</p>

<div class="flex-center">
    <Grid env={env_2} agent={agent_2}/>
</div>

<p>As in the previous examples the agent above is already trained and tries to follow the shortest route. The grid world on the other hand represents a stochastic environment. The environment transitions into the desired state of the agent with the probability of 20%. With the probability of 80% the direction is chosen randomly (this might also include the desired direction). For an untrained agent this makes the job of finding the shortest route a lot more complex. The agent does not know exactly how the distribution of the environment looks like. Therefore the agent has to explore and to determine the path that leads to the highest sum of rewards in <strong>expectation</strong>.</p>

<p>In stochastic environments the agent has to maximize the expected sum of rewards. Intuitively speaking that means that the agent has to choose the strategy that would give him the largest sum of rewards if the agent played an infinite number of games. This definition for the goal of the agent is going to be used for the rest of the course.</p>

<p class="info">The goal of the agent is to maximize the <strong>expected sum of rewards</strong>.</p>

<div class="separator"></div>

    
