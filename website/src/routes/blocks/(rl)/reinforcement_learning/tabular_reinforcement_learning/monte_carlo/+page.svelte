<script>
  import Container from "$lib/Container.svelte";
  import SvgContainer from "$lib/SvgContainer.svelte";
  import Latex from "$lib/Latex.svelte";
  import RandomSampling from "../_monte_carlo/RandomSampling.svelte";
  import Trajectory from "$lib/Trajectory.svelte";
  import Table from "$lib/Table.svelte";

  let header = ["S", "V(s)"];
  let data = [
    [0, 0],
    [1, 0],
    [2, 0],
    [3, 0],
  ];

  let qHeader = ["", "A_0", "A_1"];
  let qData = [
    ["S_0", "0", "0"],
    ["S_1", "0", "0"],
    ["S_2", "0", "0"],
    ["S_3", "0", "0"],
  ];

  let trajectory = [
    {
      type: "S",
      subscript: 0,
    },
    {
      type: "A",
      subscript: 0,
    },
    {
      type: "R",
      subscript: 1,
    },
    {
      type: "S",
      subscript: 1,
    },
    {
      type: "A",
      subscript: 1,
    },
    {
      type: "R",
      subscript: 2,
    },
    {
      type: "S",
      subscript: 2,
    },
    {
      type: "A",
      subscript: 2,
    },
    {
      type: "R",
      subscript: 3,
    },
    {
      type: "...",
      subscript: ".",
    },
    {
      type: "S",
      subscript: "T",
    },
  ];
</script>

<svelte:head>
  <title>World4AI | Reinforcement Learning | Monte Carlo</title>
  <meta
    name="description"
    content="Monte carlo methods sample full trajectories using random sampling. The collected sequences of states, actions and rewards are used to estimate the optimal value and policy functions."
  />
</svelte:head>

<h1>Monte Carlo Methods</h1>
<div class="separator" />

<Container>
  <p>
    If we look at any definition of Monte Carlo methods, there is a high chance
    that the definition contains random sampling.
  </p>
  <p class="info">
    Monte Carlo methods are a broad class of computational algorithms that rely
    on repeated random sampling to obtain numerical results.
  </p>
  <RandomSampling />
  <p>
    The chart above generates 100 random paths, that develop over a period of
    100 timesteps. In the first timestep all paths start out with a value of 0.
    At each timestep a random value between -0.5 and 0.5 is added to the value,
    therefore the expected value of the process is 0. Yet when we look at the
    chart there is quite a distribution of values at the end of the 100 period
    run. The average of 100 paths on the other hand is going to be relatively
    close to the expected value of 0. This is what the <strong
      >"Law of Large Numbers"</strong
    > is all about. If we take samples from a distribution and calculate the average,
    that average is going to converge to the true expected value. The more samples
    we have, the closer the average is going to be to the expected value.
  </p>
  <p>
    When we apply Monte Carlo methods to reinforcement learning we sample
    episode paths, also called trajectories. The agent interacts with the
    environment and collects experience tuples that consist of states, actions
    and rewards. Monte Carlo methods require the full trajectory from the
    starting state <Latex>S_0</Latex> to the terminal state <Latex>S_T</Latex> before
    an estimate can be improved, which means that Monte Carlo methods only work with
    episodic tasks.
  </p>
  <Trajectory {trajectory} />
  <p>
    The Monte Carlo algorithm uses general policy iteration. We alternate
    between policy evaluation and policy improvement to find the optimal policy.
  </p>

  <div class="separator" />
  <h2>Policy Evaluation</h2>
  <p>
    Policy estimation deals with finding the true value function of a given
    policy
    <Latex>{String.raw`\pi`}</Latex>. Mathematically speaking we are looking for
    the expected sum of discounted rewards (also called returns) when the agent
    follows the policy <Latex>{String.raw`\pi`}</Latex>.
  </p>
  <Latex>{String.raw`v_\pi(s) = \mathbb{E}[G_t \mid S_t = s]`}</Latex>
  <p>
    Using Monte Carlo we can estimate the expected value of a random variable by
    getting samples from a distribution and using the average of the drawn
    variables as the estimate of the expected value. Due to the <strong
      >"Law of Large Numbers"</strong
    >
    the average is going to approach the true expected value and the estimate is
    going to get more and more precise as we increase the number of samples. In reinforcement
    learning the trajectory and the corresponding return <Latex>G_t</Latex> are random
    variables. Therefore the agent can estimate the expected value of returns for
    a given policy <Latex>{String.raw`\pi`}</Latex> by interacting with the environment,
    generating trajectories over and over again and building averages over the returns
    of the trajectories.
  </p>
  <p>
    When the agent interacts with the environment, one single path out of all
    possible paths is generated by the Markov decision process. The more
    trajectories we generate the more exact our estimates of the value functions
    become. Those trajectories that are rare for a particular policy <Latex
      >\pi</Latex
    > will be drawn rarely. Trajectories with a higher likelihood will be drawn more
    often. Therefore the average of the return <Latex>G_t</Latex> will converge more
    and more to the true expected value.
  </p>
  <p>
    There are two methods to calculate the average of the return <Latex
      >G_t</Latex
    >. Each time the agent faces a state <Latex>s</Latex> during an episode is called
    a visit. In the <em>first-visit</em> Monte Carlo method only the return from
    the first visit to that state until the end of the episode is calculated. If
    the state is visited several times during an episode, the additional visits
    are not considered in the calculation. While in the <em>every-visit</em> method
    each visit is counted. The first-visit is generally more straightforward and
    is going to be covered in this section, but the algorithm can be easily adjusted
    to account for the every-visit method.
  </p>

  <p>
    Below we can see a stylized example of a trajectory and the application of
    the first-visit Monte Carlo estimation. The agent encounters the state 0 two
    times, but the value function is updated only once using the five rewards
    that follow from state 0 onwards. Similarly the state 1 is encountered 2
    times and the value function is updated using the four rewards that were
    achieved after the agent encountered the state. The state 2 is only
    encountered once and only the single reward that follows is used to update
    the value function. State 3 is a terminal state, which means that no rewards
    can be achieved after state 3 and therefore the value is 0.
  </p>
  <SvgContainer maxWidth="400px">
    <svg version="1.1" viewBox="0 0 500 120" xmlns="http://www.w3.org/2000/svg">
      <circle
        cx="30.183"
        cy="50"
        r="18.933"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.8934"
      />
      <text
        x="21.2463"
        y="60.233784"
        fill="#000000"
        font-family="sans-serif"
        font-size="28.114px"
        stroke-width=".70284"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="21.2463" y="60.233784" stroke-width=".70284">0</tspan></text
      >
      <circle
        cx="117.56"
        cy="50"
        r="18.933"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.8934"
      />
      <text
        x="108.62096"
        y="60.233784"
        fill="#000000"
        font-family="sans-serif"
        font-size="28.114px"
        stroke-width=".70284"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="108.62096" y="60.233784" stroke-width=".70284">1</tspan
        ></text
      >
      <circle
        cx="204.93"
        cy="50"
        r="18.933"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.8934"
      />
      <text
        x="195.99562"
        y="60.233784"
        fill="#000000"
        font-family="sans-serif"
        font-size="28.114px"
        stroke-width=".70284"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="195.99562" y="60.233784" stroke-width=".70284">0</tspan
        ></text
      >
      <circle
        cx="292.31"
        cy="50"
        r="18.933"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.8934"
      />
      <text
        x="283.3703"
        y="60.233784"
        fill="#000000"
        font-family="sans-serif"
        font-size="28.114px"
        stroke-width=".70284"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="283.3703" y="60.233784" stroke-width=".70284">1</tspan></text
      >
      <circle
        cx="379.68"
        cy="50"
        r="18.933"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.8934"
      />
      <text
        x="370.74496"
        y="60.233784"
        fill="#000000"
        font-family="sans-serif"
        font-size="28.114px"
        stroke-width=".70284"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="370.74496" y="60.233784" stroke-width=".70284">2</tspan
        ></text
      >
      <circle
        cx="477.06"
        cy="50"
        r="18.933"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.8934"
      />
      <text
        x="468.11963"
        y="60.233784"
        fill="#000000"
        font-family="sans-serif"
        font-size="28.114px"
        stroke-width=".70284"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="468.11963" y="60.233784" stroke-width=".70284">3</tspan
        ></text
      >
      <g fill="none" stroke="var(--text-color)">
        <g stroke-width="1px">
          <path d="m50 50h40" />
          <path d="m140 50h40" />
          <path d="m230 50h40" />
        </g>
        <path d="m315 50h40" stroke-width="1.1547px" />
        <path d="m405 50h40" stroke-width="1.1547px" />
      </g>
      <rect
        x="55"
        y="63.75"
        width="30"
        height="22.5"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.0607"
      />
      <text
        x="61.784096"
        y="82.837921"
        fill="#000000"
        font-family="sans-serif"
        font-size="21.503px"
        stroke-width=".53758"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="61.784096" y="82.837921" stroke-width=".53758">R</tspan
        ></text
      >
      <rect
        x="145"
        y="63.75"
        width="30"
        height="22.5"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.0607"
      />
      <text
        x="151.7841"
        y="82.837914"
        fill="#000000"
        font-family="sans-serif"
        font-size="21.503px"
        stroke-width=".53758"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="151.7841" y="82.837914" stroke-width=".53758">R</tspan></text
      >
      <rect
        x="235"
        y="63.75"
        width="30"
        height="22.5"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.0607"
      />
      <text
        x="241.78409"
        y="82.837921"
        fill="#000000"
        font-family="sans-serif"
        font-size="21.503px"
        stroke-width=".53758"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="241.78409" y="82.837921" stroke-width=".53758">R</tspan
        ></text
      >
      <rect
        x="321"
        y="63.75"
        width="30"
        height="22.5"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.0607"
      />
      <text
        x="327.78412"
        y="82.837921"
        fill="#000000"
        font-family="sans-serif"
        font-size="21.503px"
        stroke-width=".53758"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="327.78412" y="82.837921" stroke-width=".53758">R</tspan
        ></text
      >
      <rect
        x="410"
        y="63.75"
        width="30"
        height="22.5"
        fill="var(--main-color-3)"
        stroke="#000"
        stroke-width="1.0607"
      />
      <text
        x="416.78412"
        y="82.837921"
        fill="#000000"
        font-family="sans-serif"
        font-size="21.503px"
        stroke-width=".53758"
        style="line-height:1.25"
        xml:space="preserve"
        ><tspan x="416.78412" y="82.837921" stroke-width=".53758">R</tspan
        ></text
      >
      <g id="range" fill="none" stroke="var(--text-color)" stroke-width="0.5px">
        <rect x="5" y="5" width="450" height="110" />
        <rect x="90" y="10" width="360" height="100" />
        <rect x="355" y="15" width="90" height="90" />
      </g>
    </svg>
  </SvgContainer>
  <p>
    To make the calculations of the averages computationally efficient we are
    going to use the incremental implementation that we already used for n-armed
    bandits.
  </p>
  <Latex
    >{String.raw`NewEstimate \leftarrow OldEstimate + StepSize*[Target - OldEstimate]`}</Latex
  >
  <p>
    Practically the calculation of the estimates of the value function is done
    using a table that maps states to state values. The table is usually
    initialized using 0's. For a Markov decision process with 4 states (0-3) the
    table would look as below.
  </p>
  <Table {header} {data} />
  <p>
    The agent would interact with the environment and collect a full trajectory.
    At the end of the collection phase the agent goes through the whole state
    space and updates the estimation of values using the below update rule.
  </p>

  <Latex>{String.raw`V_{k+1} (s)= V_k (s)+ \alpha [G_t - V_k(s)]`}</Latex>
  <Latex>{String.raw`\text{where }G_t = \sum_{k=t}^T \gamma^{k-t}R_t`}</Latex>
  <p>
    A Python implementation of the monte carlo prediction algorithm could look
    the following way.
  </p>
</Container>

<div class="separator" />
