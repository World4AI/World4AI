<script>
  import Container from "$lib/Container.svelte";
  import SvgContainer from "$lib/SvgContainer.svelte";
  import Latex from "$lib/Latex.svelte";
  import Table from "$lib/reinforcement_learning/grid_world/Table.svelte";

  const modelHeader = ["Action 0", "Action 1", "Action 2"];
  const modelData = [
    [
      "Probability 20%: Reward 5",
      "Probability 80%: Reward 1",
      "Probability 90%: Reward -2",
    ],
    [
      "Probability 30%: Reward -2",
      "Probability 10%: Reward -1",
      "Probability 5%: Reward -1",
    ],
    [
      "Probability 50%: Reward -1",
      "Probability 10%: Reward -5",
      "Probability 5%: Reward 100",
    ],
  ];
</script>

<svelte:head>
  <title>World4AI | Reinforcement Learning | Bandits</title>
  <meta
    name="description"
    content="Bandits are useful to study exploration strategies. Bandits posess one single state and a variaty of actions."
  />
</svelte:head>

<h1>Bandits</h1>
<div class="separator" />

<Container>
  <h2>Definition</h2>
  <p>
    Usually the introduction into the topic of the exploration-exploitation
    trade-off is done by the means of so-called k-armed bandits. Unlike the full
    reinforcement learning problem in a bandits setting we face the same initial
    state over and over again and have to choose one of the k actions. Once the
    action is chosen the agent receives the reward and the state is reset again.
  </p>
  <SvgContainer maxWidth="400px">
    <svg version="1.1" viewBox="0 0 320 190" xmlns="http://www.w3.org/2000/svg">
      <rect
        x="5"
        y="5"
        width="280"
        height="180"
        fill="none"
        stroke="var(--text-color)"
      />
      <g id="boxes" fill="var(--background-color)" stroke="var(--text-color)">
        <rect x="15" y="15" width="80" height="120" />
        <rect x="105" y="15" width="80" height="120" />
        <rect x="195" y="15" width="80" height="120" />
      </g>
      <g
        id="numbers"
        font-family="sans-serif"
        font-size="99.917px"
        stroke="#000000"
        fill="var(--main-color-1)"
      >
        <text
          x="23.385542"
          y="111"
          style="line-height:1.25"
          xml:space="preserve"
          ><tspan x="23.385542" y="111.42005" stroke="#000000">7</tspan></text
        >
        <text
          x="113.38554"
          y="111.42005"
          style="line-height:1.25"
          xml:space="preserve"
          ><tspan x="113.38554" y="111.42005" stroke="#000000">7</tspan></text
        >
        <text
          x="203.38553"
          y="111.42005"
          style="line-height:1.25"
          xml:space="preserve"
          ><tspan x="203.38553" y="111.42005" stroke="#000000">7</tspan></text
        >
      </g>
      <g id="buttons" fill="var(--main-color-2)" stroke="#000">
        <rect x="25" y="155" width="20" height="20" />
        <rect x="55" y="155" width="20" height="20" />
        <rect x="85" y="155" width="20" height="20" />
      </g>
    </svg>
  </SvgContainer>
  <p>
    The name k-armed bandits comes from the casino game “One-Armed Bandit” (a
    slot machine). In such a game the player has to pull a single lever (the
    arm) and after each pull he receives a reward (which in most cases is 0). In
    k-armed bandits there are k such levers, where for each of the levers there
    is a corresponding probability distribution of rewards and the solution to a
    bandits problem is to find the lever that maximizes the expected reward.
  </p>
  <p>
    Unlike in a Markov decision process in a bandit problem the action value
    function does not depend on the state, because there is only one state: the
    initial state.
  </p>
  <Latex>{String.raw`q_*(a) \doteq \mathbb{E}[R_t \mid A_t = a]`}</Latex>
  <p>
    At the very first glance the solutions to k-armed bandits problems seem to
    be of limited use when faced with full reinforcement learning MDP tasks, but
    there is a particular reason to still cover the topic. We should remember
    that reinforcement learning is learning through trial and error and delayed
    rewards. Bandits remove the “delayed” part of the definition and deal only
    with trial and error and the immediate reward, as the reward is received for
    each of the actions directly and the environment is reset to the single not
    terminal state. That reduces complexity by not caring about the sequential
    problems but still allows us to discover different types of
    exploration-exploitation techniques, which eventually can be applied to full
    reinforcement learning tasks. In that sense we can say that bandits are a
    special case of the MDP.
  </p>
  <div class="separator" />

  <h2>Solution</h2>
  <Latex
    >{String.raw`
\begin{aligned}
    & q_*(a) \doteq \mathbb{E}[R_t \mid A_t = a] \\
    & A_t \doteq \arg\max_a q_*(a)
\end{aligned}
    `}</Latex
  >
  <p>
    If you have access to the probability distribution the solution is as simple
    as calculating the expected value and taking the action with the highest
    value.
  </p>
  <Table data={modelData} header={modelHeader} />
  <p>
    The expected value of rewards for the above bandit would look like as
    follows.
  </p>
  <Latex>q_*(0) = 0.2 * 5 + 0.3 * (-2) + 0.5 * (-1) = -0.1</Latex>
  <Latex>q_*(1) = 0.8 * 1 + 0.1 * (-1) + 0.1 * (-5) = 0.2</Latex>
  <Latex>q_*(2) = 0.9 * (-2) + 0.05 * (-1) + 0.05 * 100 = 3.15</Latex>
  <p>
    Based on those calculations the agent should always take the action Nr. 2.
  </p>
  <p>
    Yet we do not have access to the distribution of the rewards and have
    therefore to look for a different approach to estimate the action-value
    function. The intuitive way to estimate the action-value function is to
    select different actions over and over again and build averages of the
    rewards for a given action. If you continue doing that, the approximation is
    going to get better and better, meaning it is going to get closer to the
    true action-value function. Therefore the estimation can be done using <Latex
      >{String.raw`Q(a) = \frac{\sum_N R(a)}{N(a)}`}</Latex
    >, where <Latex>N(a)</Latex> is the number of times the action <Latex
      >a</Latex
    > has been selected and <Latex>R(a)</Latex> is a reward from selecting <Latex
      >a</Latex
    >.
  </p>
  <p>
    If we follow the above approach to estimate the action-value function, we
    are necessarily required to keep a list of the number of times an has been
    taken and all the corresponding received rewards. The higher the number of
    actions the larger the list gets and the higher the memory requirements and
    the computational cost become. In practice we can apply a mathematical trick
    for the calculation of the average to reduce the computational complexity.
  </p>
  <Latex
    >{String.raw`
        \begin{aligned}
        \mu_n & = \frac{1}{n} \sum_{i=1}^{n}x_i \\
        & = \frac{1}{n}(x_n + \sum_{i=1}^{n-1}x_i) \\
        & = \frac{1}{n}(x_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1}x_i) \\
        & = \frac{1}{n}(x_n + (n-1) \mu_{n-1}) \\
        & = \frac{1}{n}\mu_{n-1} + \frac{1}{n} n \mu_{n-1} - \frac{1}{n}x_n \\    
        & = \mu_{n-1} + \frac{1}{n}(x_n - \mu_{n-1})
        \end{aligned}`}
  </Latex>
  <p>
    The above update rule only requires the last estimate, therefore no lists
    are necessary to track the average. Using the expression above we can
    rewrite the calculation of the action-value function as below.
  </p>
  <Latex>{String.raw`Q_{n+1}(a) = Q_n(a)  + \frac{1}{n}[R_n - Q_n(a)]`}</Latex>
  <p>This update rule can also be expressed in more general terms.</p>
  <Latex
    >{String.raw`NewEstimate \leftarrow OldEstimate + StepSize\bigg[\underbrace{Target - OldEstimate}_\text{error}\bigg]`}</Latex
  >
  <p>
    The target indicates the <em>true</em> value that was experienced and the
    idea is to push the new estimate towards the target value. The expression
    <em>Target - Estimate</em> is often described as <em>error</em>, because it
    is the difference between the true received reward and the estimated reward.
    The
    <em>step size</em> (or the learning rate), is the magnitude with which you
    change the estimate of the action-value function. From the expression above
    it follows that the <em>step size</em> is <Latex
      >{String.raw`\frac{1}{n}`}</Latex
    >, which decreases over time. In practice it is customary to either set the
    learning rate to a constant value or start with a relatively high value and
    reduce it until it reaches a minimum constant value. In that case the greek
    letter <Latex>\alpha</Latex> is used to denote the learning rate.
  </p>
  <Latex>{String.raw`Q_{n+1}(a) = Q_n(a)  + \alpha[R_n - Q_n(a)]`}</Latex>
  <p>
    This update rule can be applied independent of the exploration-exploitation
    strategies, that we are going to cover in the next following sections.
  </p>
  <div class="separator" />
</Container>
