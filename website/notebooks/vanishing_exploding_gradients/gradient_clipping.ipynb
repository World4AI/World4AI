{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecba22f-765d-43df-bc7a-1a3ad7c589cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a10f73e7-c8bf-4b65-a6ca-1f0c6b092697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "DEVICE = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 10\n",
    "ALPHA = 0.01\n",
    "VAL_SIZE = 10_000\n",
    "BATCH_SIZE=32\n",
    "\n",
    "NUM_FEATURES = 28*28\n",
    "HIDDEN_1 = 100\n",
    "HIDDEN_2 = 50\n",
    "NUM_LABELS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3088068a-3e70-49cf-92b6-0d9d43eae24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = MNIST(root='../datasets/', train=True, download=True, transform=T.ToTensor())\n",
    "test_dataset = MNIST(root='../datasets/', train=False, download=False, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce0e91b-f446-4f82-8bfc-402294a2a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify = train_val_dataset.targets.numpy()\n",
    "train_idxs, val_idxs = train_test_split(\n",
    "                                range(len(train_val_dataset)),\n",
    "                                stratify=stratify,\n",
    "                                test_size=VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d53acd-d38d-4641-ba46-b1b16722a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(dataset=train_val_dataset, indices=train_idxs)\n",
    "val_dataset = Subset(dataset=train_val_dataset, indices=val_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e34fb08-a441-48e0-8d89-b298aa29c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=4,\n",
    "                              drop_last=True)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62588ad-63bb-4203-9477-e49c3ab12e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(NUM_FEATURES, HIDDEN_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_1, HIDDEN_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_2, NUM_LABELS)\n",
    "        )\n",
    "        \n",
    "    def forward(self, features):\n",
    "        return self.layers(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4168d3f7-c6b5-4567-a6be-a4e964d7aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "            features = features.view(-1, NUM_FEATURES).to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(features)\n",
    "            \n",
    "            predictions = outputs.max(dim=1)[1]\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_sum += loss.cpu().item()\n",
    "            num_samples += len(features)\n",
    "    return loss_sum/num_samples, num_correct/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c52f0553-7565-4345-a9cd-953bd5c5f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=ALPHA)\n",
    "    # combine softmax with cross entropy loss simultaneously, no need to attach softmax to the model\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (features, labels) in enumerate(train_dataloader):\n",
    "            features = features.view(-1, NUM_FEATURES).to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # empty the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(features)\n",
    "            # calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # clip gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_acc = calculate_performance(model, criterion, val_dataloader)\n",
    "        print(f'Epoch: {epoch+1} | Validation Loss: {val_loss} | Validation Accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d8860c-777e-448d-b3c6-f29e67af7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a3ace8-49e9-4e17-a05f-e5cdd896121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Validation Loss: 0.5509428356170655 | Validation Accuracy: 0.8536\n",
      "Epoch: 2 | Validation Loss: 0.41197297353744505 | Validation Accuracy: 0.8851\n",
      "Epoch: 3 | Validation Loss: 0.36639982678890226 | Validation Accuracy: 0.8954\n",
      "Epoch: 4 | Validation Loss: 0.3403232001900673 | Validation Accuracy: 0.9037\n",
      "Epoch: 5 | Validation Loss: 0.32071058940887454 | Validation Accuracy: 0.9076\n",
      "Epoch: 6 | Validation Loss: 0.3040276431441307 | Validation Accuracy: 0.9148\n",
      "Epoch: 7 | Validation Loss: 0.29033055518865586 | Validation Accuracy: 0.9179\n",
      "Epoch: 8 | Validation Loss: 0.275537036550045 | Validation Accuracy: 0.9198\n",
      "Epoch: 9 | Validation Loss: 0.26166385180950164 | Validation Accuracy: 0.9264\n",
      "Epoch: 10 | Validation Loss: 0.24922023360133172 | Validation Accuracy: 0.9283\n"
     ]
    }
   ],
   "source": [
    "# training goes slower due to gradient clipping\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
