{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddbbb9f-039c-4af8-8b29-f7d5833d99db",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73579883-1a92-4ed8-868d-83f17a431dc6",
   "metadata": {},
   "source": [
    "Machine learning, especially deep learning is dependent on lots and lots of data. That depnedence can put a high strain on researchers and practicioners. We need to be able to store, manage and retrieve high amounts of data and when we retrieve the data we need to make sure, that we do go beyond the capacity of our RAM or VRAM. PyTorch gives us a flexible way to deal with our data problems the way we see fit and provides the ```Dataset``` and the ```DataLoader``` classes to manage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad52b401-19bf-48f9-ac13-fffafc58ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bc7c6-a1ec-4b2d-92c6-f9a86deeda4b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0c01b-5332-4875-ac18-80a0c24c9a20",
   "metadata": {},
   "source": [
    "The Dataset object is the PyTorch representation of data. In PyTorch any class that implements the __getitem__ and the __len__ magic methods are considered to be Datasets. That means that theoretically ```[1, 2, 3]``` is a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b03643-4c9d-4c60-a828-a2b394b30be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dataset = [1,2,3]\n",
    "# a simple list has the __len__ method\n",
    "print(len(dataset))\n",
    "# a simple list has the __getitem__ method\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b96d6-e32c-4ab7-9fc0-b849be5ddc96",
   "metadata": {},
   "source": [
    "When we are dealing with real world data we will actually subclass the ```Dataset``` method and overwrite the ```__getitem__``` and the ```__len__``` methods. Below we create a dataset that contains a list of numbers, the size of which depends on a parameter in the ```__init___``` method. The ```__getitem__``` method implements the logic, which determines how the individual element of our data should be returned given only the index of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9bf67b-a8ca-43ae-b41b-e1d1be684872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.data = list(range(size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b509bf-1fcc-4583-89d3-fa7f9a0fcd3a",
   "metadata": {},
   "source": [
    "We can use the dataset the way we could use a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9570658-2718-4d3b-9305-c98cb094f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ListDataset(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c62c0ee-184e-49f7-bc2d-3d9cbc918934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abc6d2a-4905-4703-bcd3-1a385742ff8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(dataset[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d28c4-9acb-4697-a0cc-479fa91f6dd4",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a58777-16da-42d4-9a5d-8ea5b9e7befc",
   "metadata": {},
   "source": [
    "During the training process we  interact with the ```DataLoader``` object and never with the ```Dataset``` directly. The goal of the ```DataLoader``` is to return data in batch sized pieces, that are utilized for training or testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190d8ad-dea3-4a17-8524-f32c809d2b10",
   "metadata": {},
   "source": [
    "The DataLoaser class has many parameters, we will start with the most important ones.\n",
    "\n",
    "- ```dataset```: The Dataset object that implements the ```__len__``` and ```__getitem__``` interface\n",
    "- ```batch_size```: size of the mini-batch used in training/testing, defaults to 1\n",
    "- ```shuffle```: determines if the data is shuffeled in each epoch, defaults to False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3ac5e-d504-41c5-8e99-bdab3b720eb1",
   "metadata": {},
   "source": [
    "Generate a Dataset with 5 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24ccb3f-1ad4-4e1d-9c8d-bedc66c61d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ListDataset(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2d1e2-e1bf-4a25-b146-51f5a6ec0d3a",
   "metadata": {},
   "source": [
    "Generate a dataloader that shuffles the dataset object and returns 2 elements at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a12749c-517e-4949-8649-bc0d8a62b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5da2f-4da7-4591-b08d-14ed3aa56f16",
   "metadata": {},
   "source": [
    "We iterate through the dataloader to receive a batch at a time. Once only one object remains, a single element is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9584059a-7707-45c8-a822-9ae69d069cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH Nr. 1\n",
      "---------------------------------------------\n",
      "Batch Nr: 1 Data: tensor([2, 4])\n",
      "Batch Nr: 2 Data: tensor([0, 3])\n",
      "Batch Nr: 3 Data: tensor([1])\n",
      "\n",
      "EPOCH Nr. 2\n",
      "---------------------------------------------\n",
      "Batch Nr: 1 Data: tensor([1, 3])\n",
      "Batch Nr: 2 Data: tensor([4, 2])\n",
      "Batch Nr: 3 Data: tensor([0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print(f'EPOCH Nr. {epoch+1}')\n",
    "    print('-' * 45)\n",
    "    for batch_num, data in enumerate(dataloader):\n",
    "        print(f'Batch Nr: {batch_num+1} Data: {data}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80000e1d-b70d-4291-9bbe-87035328a085",
   "metadata": {},
   "source": [
    "Often we want our batches to be of equal size. If a batch is too small the calculation of the gradient might be too noisy. To avoid that we can the following argument.\n",
    "\n",
    "- ```drop_last```: does not include if the last batch is less than ```batch_size```, defaults to False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82930f-02eb-42a7-b01e-0a0d46c82319",
   "metadata": {},
   "source": [
    "Below we see that out of 5 samples only 4 are included in the loop if we set the ```drop_last``` variable to ```True```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57c2298-9c7e-40bc-86be-91b98ceace13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=2, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b467cf5-33f6-4c32-93d9-e16abeb4551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH Nr. 1\n",
      "---------------------------------------------\n",
      "Batch Nr: 1 Data: tensor([4, 3])\n",
      "Batch Nr: 2 Data: tensor([1, 0])\n",
      "\n",
      "EPOCH Nr. 2\n",
      "---------------------------------------------\n",
      "Batch Nr: 1 Data: tensor([0, 3])\n",
      "Batch Nr: 2 Data: tensor([1, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print(f'EPOCH Nr. {epoch+1}')\n",
    "    print('-' * 45)\n",
    "    for batch_num, data in enumerate(dataloader):\n",
    "        print(f'Batch Nr: {batch_num+1} Data: {data}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a6150-8211-45d6-8d59-bfb3fa7cd0c5",
   "metadata": {},
   "source": [
    "PyTorch gives us the ability to get the data in parallel by using subprocesses.\n",
    "\n",
    "- ```num_workers```: integer value that determines the number of workers that get the data in parallel. The default is 0, which means that only the main process is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50247a57-4742-4ec8-808b-3138a8df7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=2, shuffle=True, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9dabd7-bd82-4672-afaa-b2476ec854f8",
   "metadata": {},
   "source": [
    "We won't notice the speed difference using such a simple example, but the speedup with large datasets might be noticable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fc147-819f-437d-9bfd-8bf3c9a4ddfa",
   "metadata": {},
   "source": [
    "Behind the scene PyTorch does a load of work. In most cases the default way things are processed behind the scene are sufficient, but sometimes you might need more control. We are not going to cover those details just yet, because for the most part the default ```DataLoader``` is sufficient and we will cover the special cases when the need arises. If you are faced with a problem that requires more control, you can look at the [PyTorch documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
