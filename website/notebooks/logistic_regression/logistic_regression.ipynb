{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e23e5a2-4579-4ea8-8807-0d34e6473a91",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba41f7d-3f90-4c55-9c60-51aa96614a55",
   "metadata": {},
   "source": [
    "In this chapter we will solve a logistic regression problem, by manually implementing the gradient descent algorithm. In logistic regression we are faced with an equation of the below form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed5722-470c-43f8-b434-0dfc16a93e64",
   "metadata": {},
   "source": [
    "$\\sigma = p(1|\\mathbf{x}) = \\dfrac{1}{1 + e^{-z}}$, where $z = {\\mathbf{x}\\mathbf{w}^T+b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e6897-3ba5-4b4b-ae56-2d000255227f",
   "metadata": {},
   "source": [
    "Essentially we are trying to predict the probability of a sample to belong to the class **1**, given the feature vector $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03f6ed-1d02-4780-9391-bb2310d5101b",
   "metadata": {},
   "source": [
    "We use only NumPy to code up the algorithm, but we additianally utilize the sklearn package to create the classification dataset that is solvable by logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f04c1b5-d6c6-4a53-ac67-e93e4ff05775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44504dc2-db00-48bb-af4c-a78291cff8d3",
   "metadata": {},
   "source": [
    "Loss function $H = -\\dfrac{1}{n} \\sum_i^n y^{(i)} \\ln(p(y^{(i)})) + (1 - y^{(i)})\\ln(1 - p(y^{(i)})) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fee700-a252-43e1-9441-e591f6c311b6",
   "metadata": {},
   "source": [
    "Loss function $H = -\\dfrac{1}{n} \\sum_i^n y^{(i)} \\ln(a) + (1 - y^{(i)})\\ln(1 - a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb28e01-c8b1-4f33-8330-d67476564ad0",
   "metadata": {},
   "source": [
    "From simple calculus we know, that the derivative of the sum is the sum of derivatives, therefore we calculate derivative for a single sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170b9cc-ede2-4b4a-80cf-83a578f6be94",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial H}{\\partial a} = - \\Big(y \\dfrac{1}{a} - (1 - y) \\dfrac{1}{1 - a} \\Big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39627169-4ab1-49fe-a391-5e34fcdd8cef",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial a}{\\partial z} = a(1 - a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc90189-a198-4619-bccb-2b3aedbcaf48",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial z}{\\partial w_j} = x_j, \\dfrac{\\partial z}{\\partial b_j} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee350c8-3109-48d8-9349-2760d673f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=100, n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f40feac-8420-4c0d-a7d5-d21cbeb2ed16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39e4ffc-2cb3-40b3-9b50-3f14b5aa4807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7999e181-2cb6-4333-b02e-6ed02d77d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db48b6dc-ad41-4c4d-ab88-1f376289017f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7abac56-63cf-44eb-9475-772500b60fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(1, 10)\n",
    "b = np.random.randn(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787b88dc-7d65-4c0a-884e-2334236ccd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23331d22-4d7c-477f-ad54-32a39297f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cross Entropy: 0.17955183447101436\n",
      "Epoch: 10, Cross Entropy: 0.17943801665518452\n",
      "Epoch: 20, Cross Entropy: 0.17933201749625755\n",
      "Epoch: 30, Cross Entropy: 0.17923321714191442\n",
      "Epoch: 40, Cross Entropy: 0.17914105197583072\n",
      "Epoch: 50, Cross Entropy: 0.17905500892214296\n",
      "Epoch: 60, Cross Entropy: 0.17897462037432846\n",
      "Epoch: 70, Cross Entropy: 0.17889945967659032\n",
      "Epoch: 80, Cross Entropy: 0.17882913709430956\n",
      "Epoch: 90, Cross Entropy: 0.17876329621759723\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    z = X @ w.T + b\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        cross_entropy = -(y * np.log(a) + (1-y) * np.log(1 - a)).mean()\n",
    "        print(f\"Epoch: {epoch}, Cross Entropy: {cross_entropy}\",)\n",
    "    \n",
    "    # calculate the gradients \n",
    "    dH_da = -(y * 1 / a - (1-y)*(1 / (1-a)))\n",
    "    da_dz = a * (1 - a)\n",
    "    dH_dz = dH_da * da_dz\n",
    "    dH_dw = dH_dz * X\n",
    "    grad_w = (dH_dw).mean(axis=0) \n",
    "    grad_b = (dH_dz).mean()\n",
    "    \n",
    "    # apply batch gradient descent\n",
    "    w = w - alpha * grad_w\n",
    "    b = b - alpha * grad_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
