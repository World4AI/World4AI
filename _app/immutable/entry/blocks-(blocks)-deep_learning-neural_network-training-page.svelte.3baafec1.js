import{S as Is,i as As,s as Xs,k as T,a as x,q as $,y as m,W as Ds,l as W,h as s,c as y,m as S,r as i,z as _,n as G,N as v,b as r,A as c,g as w,d as h,B as g,L as Os,C as A}from"../chunks/index.4d92b023.js";import{C as Cs}from"../chunks/Container.b0705c7b.js";import{L as z}from"../chunks/Latex.e0b308c0.js";import{N as Ss}from"../chunks/NeuralNetwork.9b1e2957.js";import{A as Ns}from"../chunks/Alert.25a852b3.js";import{P as Xt}from"../chunks/PythonCode.212ba7a6.js";import{B as zs}from"../chunks/BackpropGraph.6a7a3666.js";import{B as Fs}from"../chunks/ButtonContainer.e9aac418.js";import{P as Vs}from"../chunks/PlayButton.85103c5a.js";import{P as ds,T as vs}from"../chunks/Ticks.45eca5c5.js";import{X as bs,Y as xs}from"../chunks/YLabel.182e66a3.js";import{C as dn}from"../chunks/Circle.f281e92b.js";import{P as Bs}from"../chunks/Path.7e6df014.js";import{R as Ls}from"../chunks/Rectangle.45d8140d.js";import{V as H,M as js}from"../chunks/Network.03de8e4c.js";const Zs=""+new URL("../assets/circular.f455c9c7.webp",import.meta.url).href;function Ms(o){let n;return{c(){n=$("a_1")},l(t){n=i(t,"a_1")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function qs(o){let n;return{c(){n=$("a_2")},l(t){n=i(t,"a_2")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Gs(o){let n;return{c(){n=$("a_1")},l(t){n=i(t,"a_1")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Us(o){let n;return{c(){n=$("a_2")},l(t){n=i(t,"a_2")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ys(o){let n,t,a,u,N,P,E,L,p,k;return n=new vs({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),a=new dn({props:{data:o[6][0]}}),N=new dn({props:{data:o[6][1],color:"var(--main-color-2)"}}),E=new bs({props:{text:"Feature 1",fontSize:15}}),p=new xs({props:{text:"Feature 2",fontSize:15}}),{c(){m(n.$$.fragment),t=x(),m(a.$$.fragment),u=x(),m(N.$$.fragment),P=x(),m(E.$$.fragment),L=x(),m(p.$$.fragment)},l(f){_(n.$$.fragment,f),t=y(f),_(a.$$.fragment,f),u=y(f),_(N.$$.fragment,f),P=y(f),_(E.$$.fragment,f),L=y(f),_(p.$$.fragment,f)},m(f,b){c(n,f,b),r(f,t,b),c(a,f,b),r(f,u,b),c(N,f,b),r(f,P,b),c(E,f,b),r(f,L,b),c(p,f,b),k=!0},p:A,i(f){k||(w(n.$$.fragment,f),w(a.$$.fragment,f),w(N.$$.fragment,f),w(E.$$.fragment,f),w(p.$$.fragment,f),k=!0)},o(f){h(n.$$.fragment,f),h(a.$$.fragment,f),h(N.$$.fragment,f),h(E.$$.fragment,f),h(p.$$.fragment,f),k=!1},d(f){g(n,f),f&&s(t),g(a,f),f&&s(u),g(N,f),f&&s(P),g(E,f),f&&s(L),g(p,f)}}}function Rs(o){let n,t;return n=new Vs({props:{f:o[7],delta:0}}),{c(){m(n.$$.fragment)},l(a){_(n.$$.fragment,a)},m(a,u){c(n,a,u),t=!0},p:A,i(a){t||(w(n.$$.fragment,a),t=!0)},o(a){h(n.$$.fragment,a),t=!1},d(a){g(n,a)}}}function Hs(o){let n,t,a,u,N,P,E,L,p,k,f,b,F,j;return n=new vs({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),a=new Ls({props:{data:o[2][0],size:9,color:"var(--main-color-3)"}}),N=new Ls({props:{data:o[2][1],size:9,color:"var(--main-color-4)"}}),E=new dn({props:{data:o[6][0]}}),p=new dn({props:{data:o[6][1],color:"var(--main-color-2)"}}),f=new bs({props:{text:"Feature 1",fontSize:15}}),F=new xs({props:{text:"Feature 2",fontSize:15}}),{c(){m(n.$$.fragment),t=x(),m(a.$$.fragment),u=x(),m(N.$$.fragment),P=x(),m(E.$$.fragment),L=x(),m(p.$$.fragment),k=x(),m(f.$$.fragment),b=x(),m(F.$$.fragment)},l(d){_(n.$$.fragment,d),t=y(d),_(a.$$.fragment,d),u=y(d),_(N.$$.fragment,d),P=y(d),_(E.$$.fragment,d),L=y(d),_(p.$$.fragment,d),k=y(d),_(f.$$.fragment,d),b=y(d),_(F.$$.fragment,d)},m(d,I){c(n,d,I),r(d,t,I),c(a,d,I),r(d,u,I),c(N,d,I),r(d,P,I),c(E,d,I),r(d,L,I),c(p,d,I),r(d,k,I),c(f,d,I),r(d,b,I),c(F,d,I),j=!0},p(d,I){const C={};I[0]&4&&(C.data=d[2][0]),a.$set(C);const it={};I[0]&4&&(it.data=d[2][1]),N.$set(it)},i(d){j||(w(n.$$.fragment,d),w(a.$$.fragment,d),w(N.$$.fragment,d),w(E.$$.fragment,d),w(p.$$.fragment,d),w(f.$$.fragment,d),w(F.$$.fragment,d),j=!0)},o(d){h(n.$$.fragment,d),h(a.$$.fragment,d),h(N.$$.fragment,d),h(E.$$.fragment,d),h(p.$$.fragment,d),h(f.$$.fragment,d),h(F.$$.fragment,d),j=!1},d(d){g(n,d),d&&s(t),g(a,d),d&&s(u),g(N,d),d&&s(P),g(E,d),d&&s(L),g(p,d),d&&s(k),g(f,d),d&&s(b),g(F,d)}}}function Js(o){let n,t,a,u,N,P,E,L;return n=new vs({props:{xTicks:[0,2e3,4e3,6e3,8e3,1e4,12e3],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),a=new Bs({props:{data:o[1]}}),N=new bs({props:{text:"Number of Steps",fontSize:15}}),E=new xs({props:{text:"Cross-Entropy Loss",fontSize:15}}),{c(){m(n.$$.fragment),t=x(),m(a.$$.fragment),u=x(),m(N.$$.fragment),P=x(),m(E.$$.fragment)},l(p){_(n.$$.fragment,p),t=y(p),_(a.$$.fragment,p),u=y(p),_(N.$$.fragment,p),P=y(p),_(E.$$.fragment,p)},m(p,k){c(n,p,k),r(p,t,k),c(a,p,k),r(p,u,k),c(N,p,k),r(p,P,k),c(E,p,k),L=!0},p(p,k){const f={};k[0]&2&&(f.data=p[1]),a.$set(f)},i(p){L||(w(n.$$.fragment,p),w(a.$$.fragment,p),w(N.$$.fragment,p),w(E.$$.fragment,p),L=!0)},o(p){h(n.$$.fragment,p),h(a.$$.fragment,p),h(N.$$.fragment,p),h(E.$$.fragment,p),L=!1},d(p){g(n,p),p&&s(t),g(a,p),p&&s(u),g(N,p),p&&s(P),g(E,p)}}}function Ks(o){let n;return{c(){n=$("a")},l(t){n=i(t,"a")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Qs(o){let n;return{c(){n=$("z")},l(t){n=i(t,"z")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function el(o){let n=String.raw`\mathbf{x}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function tl(o){let n=String.raw`\mathbf{w}^T`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function al(o){let n;return{c(){n=$("b")},l(t){n=i(t,"b")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function nl(o){let n;return{c(){n=$("f")},l(t){n=i(t,"f")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function sl(o){let n=String.raw`
            \mathbf{x} =
            \begin{bmatrix}
              x_1 & x_2 & x_3 & \cdots & x_m 
            \end{bmatrix}
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function ll(o){let n=String.raw`
            \mathbf{w} =
            \begin{bmatrix}
              w_1 & w_2 & w_3 & \cdots & w_m 
            \end{bmatrix}
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function rl(o){let n;return{c(){n=$("a")},l(t){n=i(t,"a")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function fl(o){let n=String.raw`
      z = \mathbf{x}\mathbf{w}^T + b \\
      a = f(z)
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function ol(o){let n,t,a,u,N,P,E,L,p,k;return t=new z({props:{$$slots:{default:[sl]},$$scope:{ctx:o}}}),u=new z({props:{$$slots:{default:[ll]},$$scope:{ctx:o}}}),P=new z({props:{$$slots:{default:[rl]},$$scope:{ctx:o}}}),p=new z({props:{$$slots:{default:[fl]},$$scope:{ctx:o}}}),{c(){n=$(`Given that we have a features vector
    `),m(t.$$.fragment),a=$(` and a weight vector
    `),m(u.$$.fragment),N=$(" we can calculate the output of the neuron "),m(P.$$.fragment),E=$(` in a two step procedure:
    `),L=T("div"),m(p.$$.fragment)},l(f){n=i(f,`Given that we have a features vector
    `),_(t.$$.fragment,f),a=i(f,` and a weight vector
    `),_(u.$$.fragment,f),N=i(f," we can calculate the output of the neuron "),_(P.$$.fragment,f),E=i(f,` in a two step procedure:
    `),L=W(f,"DIV",{});var b=S(L);_(p.$$.fragment,b),b.forEach(s)},m(f,b){r(f,n,b),c(t,f,b),r(f,a,b),c(u,f,b),r(f,N,b),c(P,f,b),r(f,E,b),r(f,L,b),c(p,L,null),k=!0},p(f,b){const F={};b[1]&32768&&(F.$$scope={dirty:b,ctx:f}),t.$set(F);const j={};b[1]&32768&&(j.$$scope={dirty:b,ctx:f}),u.$set(j);const d={};b[1]&32768&&(d.$$scope={dirty:b,ctx:f}),P.$set(d);const I={};b[1]&32768&&(I.$$scope={dirty:b,ctx:f}),p.$set(I)},i(f){k||(w(t.$$.fragment,f),w(u.$$.fragment,f),w(P.$$.fragment,f),w(p.$$.fragment,f),k=!0)},o(f){h(t.$$.fragment,f),h(u.$$.fragment,f),h(P.$$.fragment,f),h(p.$$.fragment,f),k=!1},d(f){f&&s(n),g(t,f),f&&s(a),g(u,f),f&&s(N),g(P,f),f&&s(E),f&&s(L),g(p)}}}function $l(o){let n=String.raw`\mathbf{X}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function il(o){let n=String.raw`\mathbf{X}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function ul(o){let n;return{c(){n=$("n \\times m")},l(t){n=i(t,"n \\times m")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function pl(o){let n;return{c(){n=$("n")},l(t){n=i(t,"n")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function ml(o){let n;return{c(){n=$("m")},l(t){n=i(t,"m")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function _l(o){let n=String.raw`
      \mathbf{X} =
      \begin{bmatrix}
      x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & \cdots & x_m^{(1)} \\
      x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & \cdots & x_m^{(2)} \\
      x_1^{(3)} & x_2^{(3)} & x_3^{(3)} & \cdots & x_m^{(3)} \\
      \vdots & \vdots & \vdots & \cdots & \vdots \\
      x_1^{(n)} & x_2^{(n)} & x_3^{(n)} & \cdots & x_m^{(n)} \\
      \end{bmatrix}
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function cl(o){let n=String.raw`\mathbf{w}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function wl(o){let n;return{c(){n=$("b")},l(t){n=i(t,"b")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function hl(o){let n=String.raw`\mathbf{W}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function gl(o){let n=String.raw`\mathbf{b}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function dl(o){let n=String.raw`
      \mathbf{W} =
      \begin{bmatrix}
      w_1^{[1]} & w_2^{[1]} & w_3^{[1]} & \cdots & w_m^{[1]} \\
      w_1^{[2]} & w_2^{[2]} & w_3^{[2]} & \cdots & w_m^{[2]} \\
      w_1^{[3]} & w_2^{[3]} & w_3^{[3]} & \cdots & w_m^{[3]} \\
      \vdots & \vdots & \vdots & \cdots & \vdots \\
      w_1^{[d]} & w_2^{[d]} & w_3^{[d]} & \cdots & w_m^{[d]} \\
      \end{bmatrix}
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function vl(o){let n=String.raw`\mathbf{W}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function bl(o){let n;return{c(){n=$("d \\times m")},l(t){n=i(t,"d \\times m")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function xl(o){let n;return{c(){n=$("m")},l(t){n=i(t,"m")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function yl(o){let n;return{c(){n=$("d")},l(t){n=i(t,"d")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function kl(o){let n=String.raw`
      \mathbf{b} =
      \begin{bmatrix}
      b^{[1]}  &
      b^{[2]} & 
      b^{[3]} & 
      \dots &
      b^{[d]}  
      \end{bmatrix}
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function El(o){let n=String.raw`\mathbf{b}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Tl(o){let n;return{c(){n=$("1 \\times d")},l(t){n=i(t,"1 \\times d")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Wl(o){let n=String.raw`\mathbf{Z}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Pl(o){let n=String.raw`\mathbf{A}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Sl(o){let n=String.raw`\mathbf{X}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Nl(o){let n=String.raw`\mathbf{W}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function zl(o){let n=String.raw`\mathbf{A}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Ll(o){let n=String.raw`
      \mathbf{Z} = \mathbf{X}\mathbf{W}^T + \mathbf{b} \\
      \mathbf{A} = f(\mathbf{Z})
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Il(o){let n,t,a,u,N,P,E,L,p,k;return t=new z({props:{$$slots:{default:[Sl]},$$scope:{ctx:o}}}),u=new z({props:{$$slots:{default:[Nl]},$$scope:{ctx:o}}}),P=new z({props:{$$slots:{default:[zl]},$$scope:{ctx:o}}}),p=new z({props:{$$slots:{default:[Ll]},$$scope:{ctx:o}}}),{c(){n=$(`Given that we have a features matrix
    `),m(t.$$.fragment),a=$(` and a weight matrix
    `),m(u.$$.fragment),N=$(` we can calculate the activations matrix
    `),m(P.$$.fragment),E=$(` in a two step procedure:
    `),L=T("div"),m(p.$$.fragment)},l(f){n=i(f,`Given that we have a features matrix
    `),_(t.$$.fragment,f),a=i(f,` and a weight matrix
    `),_(u.$$.fragment,f),N=i(f,` we can calculate the activations matrix
    `),_(P.$$.fragment,f),E=i(f,` in a two step procedure:
    `),L=W(f,"DIV",{});var b=S(L);_(p.$$.fragment,b),b.forEach(s)},m(f,b){r(f,n,b),c(t,f,b),r(f,a,b),c(u,f,b),r(f,N,b),c(P,f,b),r(f,E,b),r(f,L,b),c(p,L,null),k=!0},p(f,b){const F={};b[1]&32768&&(F.$$scope={dirty:b,ctx:f}),t.$set(F);const j={};b[1]&32768&&(j.$$scope={dirty:b,ctx:f}),u.$set(j);const d={};b[1]&32768&&(d.$$scope={dirty:b,ctx:f}),P.$set(d);const I={};b[1]&32768&&(I.$$scope={dirty:b,ctx:f}),p.$set(I)},i(f){k||(w(t.$$.fragment,f),w(u.$$.fragment,f),w(P.$$.fragment,f),w(p.$$.fragment,f),k=!0)},o(f){h(t.$$.fragment,f),h(u.$$.fragment,f),h(P.$$.fragment,f),h(p.$$.fragment,f),k=!1},d(f){f&&s(n),g(t,f),f&&s(a),g(u,f),f&&s(N),g(P,f),f&&s(E),f&&s(L),g(p)}}}function Al(o){let n;return{c(){n=$("n \\times d")},l(t){n=i(t,"n \\times d")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Xl(o){let n=String.raw`
      \mathbf{A} =
      \begin{bmatrix}
      a_1^{(1)} & a_2^{(1)} & a_3^{(1)} & \cdots & a_d^{(1)} \\
      a_1^{(2)} & a_2^{(2)} & a_3^{(2)} & \cdots & a_d^{(2)} \\
      a_1^{(3)} & a_2^{(3)} & a_3^{(3)} & \cdots & a_d^{(3)} \\
      \vdots & \vdots & \vdots & \cdots & \vdots \\
      a_1^{(n)} & a_2^{(n)} & a_3^{(n)} & \cdots & a_d^{(n)} \\
      \end{bmatrix}
    `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Dl(o){let n=String.raw`l`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Ol(o){let n=String.raw`\mathbf{W}^{<1>}`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Cl(o){let n=String.raw`
    \mathbf{Z^{<1>}} = \mathbf{X}\mathbf{W^{<1>T}} + \mathbf{b} \\ 
    \mathbf{A^{<1>}} = f( \mathbf{Z^{<l>}})
      `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Fl(o){let n=String.raw`
    \mathbf{Z^{<l>}} = \mathbf{A^{<l-1>}}\mathbf{W}^{<l>T} + \mathbf{b} \\ 
    \mathbf{A^{<l>}} = f(\mathbf{Z^{<l>}})
      `+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Vl(o){let n=String.raw`\mathbf{\hat{y}} = f(\cdots f(f(\mathbf{X}\mathbf{W}^{<1>T})\mathbf{W}^{<2>T})  \cdots \mathbf{W}^{<L>T})`+"",t;return{c(){t=$(n)},l(a){t=i(a,n)},m(a,u){r(a,t,u)},p:A,d(a){a&&s(t)}}}function Bl(o){let n;return{c(){n=$("L")},l(t){n=i(t,"L")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function jl(o){let n,t,a,u,N,P,E,L,p,k,f,b,F,j,d,I,C,it,me,ut,ae,Dt,ne,Rt,fe,La,Ht,ee,Ot,ve,Jt,pt,_e,be,xe,se,Ct,oe,ye,Ft,ce,Kt,mt,Ia,Qt,we,ea,$e,X,O,Z,le,M,ie,D,_t,Vt,V,vn,ke,bn,Ee,xn,Te,yn,We,kn,Pe,En,Se,Tn,Aa,Ne,Xa,q,Wn,ze,Pn,Le,Sn,Ie,Nn,Ae,zn,Xe,Ln,Da,ct,De,Oa,Y,In,Oe,An,Ce,Xn,Fe,Dn,Ve,On,Ca,wt,Be,Fa,R,Cn,je,Fn,Ze,Vn,Me,Bn,qe,jn,Va,ht,Ge,Ba,he,Ue,Zn,Ye,Mn,ja,ue,qn,Re,Gn,He,Un,Za,Je,Ma,Ke,Yn,Qe,Rn,qa,gt,et,Ga,pe,Hn,tt,Jn,at,Kn,Ua,Bt,Qn,Ya,dt,nt,Ra,jt,es,Ha,vt,st,Ja,Zt,ts,Ka,bt,lt,Qa,rt,as,ft,ns,en,Mt,ss,tn,xt,an,qt,ls,nn,yt,sn,kt,ln,Et,rn,Tt,ys,fn,ot,rs,ta,fs,os,on,Wt,$n,te,$s,aa,is,us,na,ps,ms,sa,_s,cs,un,Pt,pn,Gt,ws,mn,St,_n,Nt,hs,cn,Ut,gs,wn,Yt,hn;return E=new Ss({props:{layers:o[4],height:80,padding:{left:0,right:10}}}),d=new zs({props:{graph:o[0],width:580,height:900,maxWidth:400}}),me=new z({props:{$$slots:{default:[Ms]},$$scope:{ctx:o}}}),ae=new z({props:{$$slots:{default:[qs]},$$scope:{ctx:o}}}),ne=new z({props:{$$slots:{default:[Gs]},$$scope:{ctx:o}}}),fe=new z({props:{$$slots:{default:[Us]},$$scope:{ctx:o}}}),ee=new zs({props:{graph:o[3],width:1200,height:1800,maxWidth:700}}),se=new ds({props:{width:500,height:500,maxWidth:500,domain:[0,1],range:[0,1],$$slots:{default:[Ys]},$$scope:{ctx:o}}}),ce=new Ss({props:{layers:o[5],height:150,padding:{left:0,right:10}}}),we=new Fs({props:{$$slots:{default:[Rs]},$$scope:{ctx:o}}}),X=new ds({props:{width:500,height:500,maxWidth:600,domain:[0,1],range:[0,1],$$slots:{default:[Hs]},$$scope:{ctx:o}}}),Z=new ds({props:{width:500,height:500,maxWidth:600,domain:[0,12e3],range:[0,1],$$slots:{default:[Js]},$$scope:{ctx:o}}}),ke=new z({props:{$$slots:{default:[Ks]},$$scope:{ctx:o}}}),Ee=new z({props:{$$slots:{default:[Qs]},$$scope:{ctx:o}}}),Te=new z({props:{$$slots:{default:[el]},$$scope:{ctx:o}}}),We=new z({props:{$$slots:{default:[tl]},$$scope:{ctx:o}}}),Pe=new z({props:{$$slots:{default:[al]},$$scope:{ctx:o}}}),Se=new z({props:{$$slots:{default:[nl]},$$scope:{ctx:o}}}),Ne=new Ns({props:{type:"info",$$slots:{default:[ol]},$$scope:{ctx:o}}}),ze=new z({props:{$$slots:{default:[$l]},$$scope:{ctx:o}}}),Le=new z({props:{$$slots:{default:[il]},$$scope:{ctx:o}}}),Ie=new z({props:{$$slots:{default:[ul]},$$scope:{ctx:o}}}),Ae=new z({props:{$$slots:{default:[pl]},$$scope:{ctx:o}}}),Xe=new z({props:{$$slots:{default:[ml]},$$scope:{ctx:o}}}),De=new z({props:{$$slots:{default:[_l]},$$scope:{ctx:o}}}),Oe=new z({props:{$$slots:{default:[cl]},$$scope:{ctx:o}}}),Ce=new z({props:{$$slots:{default:[wl]},$$scope:{ctx:o}}}),Fe=new z({props:{$$slots:{default:[hl]},$$scope:{ctx:o}}}),Ve=new z({props:{$$slots:{default:[gl]},$$scope:{ctx:o}}}),Be=new z({props:{$$slots:{default:[dl]},$$scope:{ctx:o}}}),je=new z({props:{$$slots:{default:[vl]},$$scope:{ctx:o}}}),Ze=new z({props:{$$slots:{default:[bl]},$$scope:{ctx:o}}}),Me=new z({props:{$$slots:{default:[xl]},$$scope:{ctx:o}}}),qe=new z({props:{$$slots:{default:[yl]},$$scope:{ctx:o}}}),Ge=new z({props:{$$slots:{default:[kl]},$$scope:{ctx:o}}}),Ue=new z({props:{$$slots:{default:[El]},$$scope:{ctx:o}}}),Ye=new z({props:{$$slots:{default:[Tl]},$$scope:{ctx:o}}}),Re=new z({props:{$$slots:{default:[Wl]},$$scope:{ctx:o}}}),He=new z({props:{$$slots:{default:[Pl]},$$scope:{ctx:o}}}),Je=new Ns({props:{type:"info",$$slots:{default:[Il]},$$scope:{ctx:o}}}),Qe=new z({props:{$$slots:{default:[Al]},$$scope:{ctx:o}}}),et=new z({props:{$$slots:{default:[Xl]},$$scope:{ctx:o}}}),tt=new z({props:{$$slots:{default:[Dl]},$$scope:{ctx:o}}}),at=new z({props:{$$slots:{default:[Ol]},$$scope:{ctx:o}}}),nt=new z({props:{$$slots:{default:[Cl]},$$scope:{ctx:o}}}),st=new z({props:{$$slots:{default:[Fl]},$$scope:{ctx:o}}}),lt=new z({props:{$$slots:{default:[Vl]},$$scope:{ctx:o}}}),ft=new z({props:{$$slots:{default:[Bl]},$$scope:{ctx:o}}}),xt=new Xt({props:{code:o[8]}}),yt=new Xt({props:{code:o[9]}}),kt=new Xt({props:{code:o[10]}}),Et=new Xt({props:{code:o[11]}}),Wt=new Xt({props:{code:o[12]}}),Pt=new Xt({props:{code:o[13]}}),St=new Xt({props:{code:o[14]}}),{c(){n=T("p"),t=$(`Training a neural network is not much different from training logistic
    regression. We have to construct a computational graph first, which will
    allow us to apply the chain rule while propagating the the gradients from
    the loss function all the way to the weights and biases.`),a=x(),u=T("p"),N=$(`To emphasise this idea again, we are going to use an example of a neural
    network with two neurons in the hidden layer and a single output neuron. As
    usual we will assume a single training sample to avoid overcomplicated
    computational graphs.`),P=x(),m(E.$$.fragment),L=x(),p=T("p"),k=$(`Let's first zoom into the output neuron of the neural network. We disregard
    the loss function for the moment to keep things simple, but keep in mind,
    that the full graph would contain cross-entropy or the mean squared error.`),f=x(),b=T("p"),F=$(`We use the (L)ayer (N)euron (W)eight/(B)ias notation for weights and biases.
    L2 N1 W2 for example stands for weight 2 of the first neuron in the second
    layer of the neural network.`),j=x(),m(d.$$.fragment),I=x(),C=T("p"),it=$(`If you look at the above graph, you should notice, that this neuron is not
    different from a plain vanilla logistic regression graph. Yet instead of
    using the input features to calculate the output, we use the hidden features `),m(me.$$.fragment),ut=$(" and "),m(ae.$$.fragment),Dt=$(`. Each of the hidden features is based on a
    different logistic regression with its own set of weights and a bias. So
    when we use backpropagation we do not stop at `),m(ne.$$.fragment),Rt=$(" or "),m(fe.$$.fragment),La=$(", but keep moving towards the earlier weights and biases."),Ht=x(),m(ee.$$.fragment),Ot=x(),ve=T("p"),Jt=$(`The above graph only includes two hidden sigmoid neurons, but theoretically
    a graph can contains hundreds of layers with hundreds of neurons each.
    Automatic differentiation libraries will automatically construct a
    computational graph and calculate the gradients, no matter the size of the
    neural network.`),pt=x(),_e=T("p"),be=$(`Now let's remember that our original goal is to solve a non linear problem
    of the below kind.`),xe=x(),m(se.$$.fragment),Ct=x(),oe=T("p"),ye=$(`Our neural network will take the two features as input, process them through
    the hidden layer with four neurons and finally produce the output neuron,
    which contains the probability to belong to one of the two categories. This
    probability is used to measure the cross-entropy loss.`),Ft=x(),m(ce.$$.fragment),Kt=x(),mt=T("p"),Ia=$(`In the example below you can observe how the decision boundary moves when
    you use backpropagation. Usually 10000 steps are sufficient to find weights
    for a good decision boundary, this might take a couple of minutes. Try to
    observe how the cross-entropy and the shape of the decision boundary change
    over time. At a certain point you will most likely see a sharp drop in cross
    entropy, this is when things will start to improve significantly.`),Qt=x(),m(we.$$.fragment),ea=x(),$e=T("div"),m(X.$$.fragment),O=x(),m(Z.$$.fragment),le=x(),M=T("div"),ie=x(),D=T("p"),_t=$(`While the example above provides an intuitive introduction into the world of
    neural networks we need a way to formalize these calculations through
    mathematical notation.`),Vt=x(),V=T("p"),vn=$("As we have covered in previous chapters can calculate the value of a neuron "),m(ke.$$.fragment),bn=$(` in a two step process. In the first step we calculate the net input
    `),m(Ee.$$.fragment),xn=$(" by multiplying the feature vector "),m(Te.$$.fragment),yn=$(" with the transpose of the weight vector "),m(We.$$.fragment),kn=$(" and adding the bias scalar "),m(Pe.$$.fragment),En=$(`. In the second step we apply
    an activation function `),m(Se.$$.fragment),Tn=$(" to the net input."),Aa=x(),m(Ne.$$.fragment),Xa=x(),q=T("p"),Wn=$("In practice we utilize a dataset "),m(ze.$$.fragment),Pn=$(` consisting
    of many samples. . As usual `),m(Le.$$.fragment),Sn=$(" is an "),m(Ie.$$.fragment),Nn=$(" matrix, where "),m(Ae.$$.fragment),zn=$(" (rows) is the number of samples and "),m(Xe.$$.fragment),Ln=$(" (columns) is the number of input features."),Da=x(),ct=T("div"),m(De.$$.fragment),Oa=x(),Y=T("p"),In=$(`Similarly we need to be able to calculate several neurons in a layer. Each
    neuron in a particular layer has the same set of inputs, but has its own set
    of weights `),m(Oe.$$.fragment),An=$(" and its own bias "),m(Ce.$$.fragment),Xn=$(". For convenience it makes sence to collect the weights in a matrix "),m(Fe.$$.fragment),Dn=$(" and the biases in the vector "),m(Ve.$$.fragment),On=$("."),Ca=x(),wt=T("div"),m(Be.$$.fragment),Fa=x(),R=T("p"),Cn=$("The weight matrix "),m(je.$$.fragment),Fn=$(" is a "),m(Ze.$$.fragment),Vn=$(" matrix, where "),m(Me.$$.fragment),Bn=$(` is the number of features from the previous
    layer and `),m(qe.$$.fragment),jn=$(` is the number of neurons (hidden features) we want
    to calculate for the next layer.`),Va=x(),ht=T("div"),m(Ge.$$.fragment),Ba=x(),he=T("p"),m(Ue.$$.fragment),Zn=$(" is a "),m(Ye.$$.fragment),Mn=$(` vector
    of biases.`),ja=x(),ue=T("p"),qn=$("We can calculate the net input matrix "),m(Re.$$.fragment),Gn=$(" and the activation matrix "),m(He.$$.fragment),Un=$(` using the
    exact same operations we used before.`),Za=x(),m(Je.$$.fragment),Ma=x(),Ke=T("p"),Yn=$("The result is an "),m(Qe.$$.fragment),Rn=$(" matrix."),qa=x(),gt=T("div"),m(et.$$.fragment),Ga=x(),pe=T("p"),Hn=$(`Usually we deal with more that a single layer. We distinguish between layers
    by using the superscript `),m(tt.$$.fragment),Jn=$(". The matrix "),m(at.$$.fragment),Kn=$(" for example contains weights that are multiplied with the input features."),Ua=x(),Bt=T("p"),Qn=$("For the first layer we get:"),Ya=x(),dt=T("div"),m(nt.$$.fragment),Ra=x(),jt=T("p"),es=$("For all the other layers we get:"),Ha=x(),vt=T("div"),m(st.$$.fragment),Ja=x(),Zt=T("p"),ts=$("We can represent the same idea as a deeply nested function composition."),Ka=x(),bt=T("div"),m(lt.$$.fragment),Qa=x(),rt=T("p"),as=$(`We keep iterating over matrix multiplications and activation functions,
    until we reach the output layer `),m(ft.$$.fragment),ns=$(`, that is used as input into
    a loss function.`),en=x(),Mt=T("p"),ss=$("We can implement a neural network relatively easy using PyTorch."),tn=x(),m(xt.$$.fragment),an=x(),qt=T("p"),ls=$("We first create a circular dataset and plot the results."),nn=x(),m(yt.$$.fragment),sn=x(),m(kt.$$.fragment),ln=x(),m(Et.$$.fragment),rn=x(),Tt=T("img"),fn=x(),ot=T("p"),rs=$("We implement the logic of the neural network by creating a "),ta=T("code"),fs=$("NeuralNetwork"),os=$(` object. WE assume a network with two input neurons, two hidden layers with
    4 and 2 neurons respectively and an output neuron.`),on=x(),m(Wt.$$.fragment),$n=x(),te=T("p"),$s=$("The code is relatively self explanatory. The "),aa=T("code"),is=$("forward()"),us=$(` method
    multiplies the weight matrix of a layer with the features matrix from the
    previous layer and add the bias vector. The `),na=T("code"),ps=$("loss()"),ms=$(` method
    calculates the binary cross-entropy and the `),sa=T("code"),_s=$("step()"),cs=$(` method applies
    gradient descent and zeroes out the gradients.`),un=x(),m(Pt.$$.fragment),pn=x(),Gt=T("p"),ws=$(`Finally we run the forward pass, the backward pass and the gradient descent
    steps in a loop of 50,000 iterations.`),mn=x(),m(St.$$.fragment),_n=x(),Nt=T("pre"),hs=$(`tensor(0.8799)
tensor(0.6817)
tensor(0.0222)
tensor(0.0038)
tensor(0.0019)
  `),cn=x(),Ut=T("p"),gs=$(`The cross-entropy loss reduces drastically and unlike our custom
    implementation above, the PyTorch implementation runs only for a couple of
    seconds.`),wn=x(),Yt=T("div"),this.h()},l(e){n=W(e,"P",{});var l=S(n);t=i(l,`Training a neural network is not much different from training logistic
    regression. We have to construct a computational graph first, which will
    allow us to apply the chain rule while propagating the the gradients from
    the loss function all the way to the weights and biases.`),l.forEach(s),a=y(e),u=W(e,"P",{});var la=S(u);N=i(la,`To emphasise this idea again, we are going to use an example of a neural
    network with two neurons in the hidden layer and a single output neuron. As
    usual we will assume a single training sample to avoid overcomplicated
    computational graphs.`),la.forEach(s),P=y(e),_(E.$$.fragment,e),L=y(e),p=W(e,"P",{});var ra=S(p);k=i(ra,`Let's first zoom into the output neuron of the neural network. We disregard
    the loss function for the moment to keep things simple, but keep in mind,
    that the full graph would contain cross-entropy or the mean squared error.`),ra.forEach(s),f=y(e),b=W(e,"P",{});var fa=S(b);F=i(fa,`We use the (L)ayer (N)euron (W)eight/(B)ias notation for weights and biases.
    L2 N1 W2 for example stands for weight 2 of the first neuron in the second
    layer of the neural network.`),fa.forEach(s),j=y(e),_(d.$$.fragment,e),I=y(e),C=W(e,"P",{});var J=S(C);it=i(J,`If you look at the above graph, you should notice, that this neuron is not
    different from a plain vanilla logistic regression graph. Yet instead of
    using the input features to calculate the output, we use the hidden features `),_(me.$$.fragment,J),ut=i(J," and "),_(ae.$$.fragment,J),Dt=i(J,`. Each of the hidden features is based on a
    different logistic regression with its own set of weights and a bias. So
    when we use backpropagation we do not stop at `),_(ne.$$.fragment,J),Rt=i(J," or "),_(fe.$$.fragment,J),La=i(J,", but keep moving towards the earlier weights and biases."),J.forEach(s),Ht=y(e),_(ee.$$.fragment,e),Ot=y(e),ve=W(e,"P",{});var oa=S(ve);Jt=i(oa,`The above graph only includes two hidden sigmoid neurons, but theoretically
    a graph can contains hundreds of layers with hundreds of neurons each.
    Automatic differentiation libraries will automatically construct a
    computational graph and calculate the gradients, no matter the size of the
    neural network.`),oa.forEach(s),pt=y(e),_e=W(e,"P",{});var $a=S(_e);be=i($a,`Now let's remember that our original goal is to solve a non linear problem
    of the below kind.`),$a.forEach(s),xe=y(e),_(se.$$.fragment,e),Ct=y(e),oe=W(e,"P",{});var ia=S(oe);ye=i(ia,`Our neural network will take the two features as input, process them through
    the hidden layer with four neurons and finally produce the output neuron,
    which contains the probability to belong to one of the two categories. This
    probability is used to measure the cross-entropy loss.`),ia.forEach(s),Ft=y(e),_(ce.$$.fragment,e),Kt=y(e),mt=W(e,"P",{});var ua=S(mt);Ia=i(ua,`In the example below you can observe how the decision boundary moves when
    you use backpropagation. Usually 10000 steps are sufficient to find weights
    for a good decision boundary, this might take a couple of minutes. Try to
    observe how the cross-entropy and the shape of the decision boundary change
    over time. At a certain point you will most likely see a sharp drop in cross
    entropy, this is when things will start to improve significantly.`),ua.forEach(s),Qt=y(e),_(we.$$.fragment,e),ea=y(e),$e=W(e,"DIV",{class:!0});var zt=S($e);_(X.$$.fragment,zt),O=y(zt),_(Z.$$.fragment,zt),zt.forEach(s),le=y(e),M=W(e,"DIV",{class:!0}),S(M).forEach(s),ie=y(e),D=W(e,"P",{});var pa=S(D);_t=i(pa,`While the example above provides an intuitive introduction into the world of
    neural networks we need a way to formalize these calculations through
    mathematical notation.`),pa.forEach(s),Vt=y(e),V=W(e,"P",{});var B=S(V);vn=i(B,"As we have covered in previous chapters can calculate the value of a neuron "),_(ke.$$.fragment,B),bn=i(B,` in a two step process. In the first step we calculate the net input
    `),_(Ee.$$.fragment,B),xn=i(B," by multiplying the feature vector "),_(Te.$$.fragment,B),yn=i(B," with the transpose of the weight vector "),_(We.$$.fragment,B),kn=i(B," and adding the bias scalar "),_(Pe.$$.fragment,B),En=i(B,`. In the second step we apply
    an activation function `),_(Se.$$.fragment,B),Tn=i(B," to the net input."),B.forEach(s),Aa=y(e),_(Ne.$$.fragment,e),Xa=y(e),q=W(e,"P",{});var U=S(q);Wn=i(U,"In practice we utilize a dataset "),_(ze.$$.fragment,U),Pn=i(U,` consisting
    of many samples. . As usual `),_(Le.$$.fragment,U),Sn=i(U," is an "),_(Ie.$$.fragment,U),Nn=i(U," matrix, where "),_(Ae.$$.fragment,U),zn=i(U," (rows) is the number of samples and "),_(Xe.$$.fragment,U),Ln=i(U," (columns) is the number of input features."),U.forEach(s),Da=y(e),ct=W(e,"DIV",{class:!0});var ma=S(ct);_(De.$$.fragment,ma),ma.forEach(s),Oa=y(e),Y=W(e,"P",{});var K=S(Y);In=i(K,`Similarly we need to be able to calculate several neurons in a layer. Each
    neuron in a particular layer has the same set of inputs, but has its own set
    of weights `),_(Oe.$$.fragment,K),An=i(K," and its own bias "),_(Ce.$$.fragment,K),Xn=i(K,". For convenience it makes sence to collect the weights in a matrix "),_(Fe.$$.fragment,K),Dn=i(K," and the biases in the vector "),_(Ve.$$.fragment,K),On=i(K,"."),K.forEach(s),Ca=y(e),wt=W(e,"DIV",{class:!0});var _a=S(wt);_(Be.$$.fragment,_a),_a.forEach(s),Fa=y(e),R=W(e,"P",{});var Q=S(R);Cn=i(Q,"The weight matrix "),_(je.$$.fragment,Q),Fn=i(Q," is a "),_(Ze.$$.fragment,Q),Vn=i(Q," matrix, where "),_(Me.$$.fragment,Q),Bn=i(Q,` is the number of features from the previous
    layer and `),_(qe.$$.fragment,Q),jn=i(Q,` is the number of neurons (hidden features) we want
    to calculate for the next layer.`),Q.forEach(s),Va=y(e),ht=W(e,"DIV",{class:!0});var ca=S(ht);_(Ge.$$.fragment,ca),ca.forEach(s),Ba=y(e),he=W(e,"P",{});var $t=S(he);_(Ue.$$.fragment,$t),Zn=i($t," is a "),_(Ye.$$.fragment,$t),Mn=i($t,` vector
    of biases.`),$t.forEach(s),ja=y(e),ue=W(e,"P",{});var ge=S(ue);qn=i(ge,"We can calculate the net input matrix "),_(Re.$$.fragment,ge),Gn=i(ge," and the activation matrix "),_(He.$$.fragment,ge),Un=i(ge,` using the
    exact same operations we used before.`),ge.forEach(s),Za=y(e),_(Je.$$.fragment,e),Ma=y(e),Ke=W(e,"P",{});var Lt=S(Ke);Yn=i(Lt,"The result is an "),_(Qe.$$.fragment,Lt),Rn=i(Lt," matrix."),Lt.forEach(s),qa=y(e),gt=W(e,"DIV",{class:!0});var wa=S(gt);_(et.$$.fragment,wa),wa.forEach(s),Ga=y(e),pe=W(e,"P",{});var de=S(pe);Hn=i(de,`Usually we deal with more that a single layer. We distinguish between layers
    by using the superscript `),_(tt.$$.fragment,de),Jn=i(de,". The matrix "),_(at.$$.fragment,de),Kn=i(de," for example contains weights that are multiplied with the input features."),de.forEach(s),Ua=y(e),Bt=W(e,"P",{});var ha=S(Bt);Qn=i(ha,"For the first layer we get:"),ha.forEach(s),Ya=y(e),dt=W(e,"DIV",{class:!0});var ga=S(dt);_(nt.$$.fragment,ga),ga.forEach(s),Ra=y(e),jt=W(e,"P",{});var da=S(jt);es=i(da,"For all the other layers we get:"),da.forEach(s),Ha=y(e),vt=W(e,"DIV",{class:!0});var va=S(vt);_(st.$$.fragment,va),va.forEach(s),Ja=y(e),Zt=W(e,"P",{});var ba=S(Zt);ts=i(ba,"We can represent the same idea as a deeply nested function composition."),ba.forEach(s),Ka=y(e),bt=W(e,"DIV",{class:!0});var xa=S(bt);_(lt.$$.fragment,xa),xa.forEach(s),Qa=y(e),rt=W(e,"P",{});var It=S(rt);as=i(It,`We keep iterating over matrix multiplications and activation functions,
    until we reach the output layer `),_(ft.$$.fragment,It),ns=i(It,`, that is used as input into
    a loss function.`),It.forEach(s),en=y(e),Mt=W(e,"P",{});var ya=S(Mt);ss=i(ya,"We can implement a neural network relatively easy using PyTorch."),ya.forEach(s),tn=y(e),_(xt.$$.fragment,e),an=y(e),qt=W(e,"P",{});var ka=S(qt);ls=i(ka,"We first create a circular dataset and plot the results."),ka.forEach(s),nn=y(e),_(yt.$$.fragment,e),sn=y(e),_(kt.$$.fragment,e),ln=y(e),_(Et.$$.fragment,e),rn=y(e),Tt=W(e,"IMG",{src:!0,alt:!0}),fn=y(e),ot=W(e,"P",{});var At=S(ot);rs=i(At,"We implement the logic of the neural network by creating a "),ta=W(At,"CODE",{});var Ea=S(ta);fs=i(Ea,"NeuralNetwork"),Ea.forEach(s),os=i(At,` object. WE assume a network with two input neurons, two hidden layers with
    4 and 2 neurons respectively and an output neuron.`),At.forEach(s),on=y(e),_(Wt.$$.fragment,e),$n=y(e),te=W(e,"P",{});var re=S(te);$s=i(re,"The code is relatively self explanatory. The "),aa=W(re,"CODE",{});var Ta=S(aa);is=i(Ta,"forward()"),Ta.forEach(s),us=i(re,` method
    multiplies the weight matrix of a layer with the features matrix from the
    previous layer and add the bias vector. The `),na=W(re,"CODE",{});var Wa=S(na);ps=i(Wa,"loss()"),Wa.forEach(s),ms=i(re,` method
    calculates the binary cross-entropy and the `),sa=W(re,"CODE",{});var Pa=S(sa);_s=i(Pa,"step()"),Pa.forEach(s),cs=i(re,` method applies
    gradient descent and zeroes out the gradients.`),re.forEach(s),un=y(e),_(Pt.$$.fragment,e),pn=y(e),Gt=W(e,"P",{});var Sa=S(Gt);ws=i(Sa,`Finally we run the forward pass, the backward pass and the gradient descent
    steps in a loop of 50,000 iterations.`),Sa.forEach(s),mn=y(e),_(St.$$.fragment,e),_n=y(e),Nt=W(e,"PRE",{class:!0});var Na=S(Nt);hs=i(Na,`tensor(0.8799)
tensor(0.6817)
tensor(0.0222)
tensor(0.0038)
tensor(0.0019)
  `),Na.forEach(s),cn=y(e),Ut=W(e,"P",{});var za=S(Ut);gs=i(za,`The cross-entropy loss reduces drastically and unlike our custom
    implementation above, the PyTorch implementation runs only for a couple of
    seconds.`),za.forEach(s),wn=y(e),Yt=W(e,"DIV",{class:!0}),S(Yt).forEach(s),this.h()},h(){G($e,"class","flex flex-col md:flex-row"),G(M,"class","separator"),G(ct,"class","flex justify-center"),G(wt,"class","flex justify-center"),G(ht,"class","flex justify-center"),G(gt,"class","flex justify-center"),G(dt,"class","flex justify-center"),G(vt,"class","flex justify-center"),G(bt,"class","flex justify-center"),Os(Tt.src,ys=Zs)||G(Tt,"src",ys),G(Tt,"alt","circular data"),G(Nt,"class","text-sm"),G(Yt,"class","separator")},m(e,l){r(e,n,l),v(n,t),r(e,a,l),r(e,u,l),v(u,N),r(e,P,l),c(E,e,l),r(e,L,l),r(e,p,l),v(p,k),r(e,f,l),r(e,b,l),v(b,F),r(e,j,l),c(d,e,l),r(e,I,l),r(e,C,l),v(C,it),c(me,C,null),v(C,ut),c(ae,C,null),v(C,Dt),c(ne,C,null),v(C,Rt),c(fe,C,null),v(C,La),r(e,Ht,l),c(ee,e,l),r(e,Ot,l),r(e,ve,l),v(ve,Jt),r(e,pt,l),r(e,_e,l),v(_e,be),r(e,xe,l),c(se,e,l),r(e,Ct,l),r(e,oe,l),v(oe,ye),r(e,Ft,l),c(ce,e,l),r(e,Kt,l),r(e,mt,l),v(mt,Ia),r(e,Qt,l),c(we,e,l),r(e,ea,l),r(e,$e,l),c(X,$e,null),v($e,O),c(Z,$e,null),r(e,le,l),r(e,M,l),r(e,ie,l),r(e,D,l),v(D,_t),r(e,Vt,l),r(e,V,l),v(V,vn),c(ke,V,null),v(V,bn),c(Ee,V,null),v(V,xn),c(Te,V,null),v(V,yn),c(We,V,null),v(V,kn),c(Pe,V,null),v(V,En),c(Se,V,null),v(V,Tn),r(e,Aa,l),c(Ne,e,l),r(e,Xa,l),r(e,q,l),v(q,Wn),c(ze,q,null),v(q,Pn),c(Le,q,null),v(q,Sn),c(Ie,q,null),v(q,Nn),c(Ae,q,null),v(q,zn),c(Xe,q,null),v(q,Ln),r(e,Da,l),r(e,ct,l),c(De,ct,null),r(e,Oa,l),r(e,Y,l),v(Y,In),c(Oe,Y,null),v(Y,An),c(Ce,Y,null),v(Y,Xn),c(Fe,Y,null),v(Y,Dn),c(Ve,Y,null),v(Y,On),r(e,Ca,l),r(e,wt,l),c(Be,wt,null),r(e,Fa,l),r(e,R,l),v(R,Cn),c(je,R,null),v(R,Fn),c(Ze,R,null),v(R,Vn),c(Me,R,null),v(R,Bn),c(qe,R,null),v(R,jn),r(e,Va,l),r(e,ht,l),c(Ge,ht,null),r(e,Ba,l),r(e,he,l),c(Ue,he,null),v(he,Zn),c(Ye,he,null),v(he,Mn),r(e,ja,l),r(e,ue,l),v(ue,qn),c(Re,ue,null),v(ue,Gn),c(He,ue,null),v(ue,Un),r(e,Za,l),c(Je,e,l),r(e,Ma,l),r(e,Ke,l),v(Ke,Yn),c(Qe,Ke,null),v(Ke,Rn),r(e,qa,l),r(e,gt,l),c(et,gt,null),r(e,Ga,l),r(e,pe,l),v(pe,Hn),c(tt,pe,null),v(pe,Jn),c(at,pe,null),v(pe,Kn),r(e,Ua,l),r(e,Bt,l),v(Bt,Qn),r(e,Ya,l),r(e,dt,l),c(nt,dt,null),r(e,Ra,l),r(e,jt,l),v(jt,es),r(e,Ha,l),r(e,vt,l),c(st,vt,null),r(e,Ja,l),r(e,Zt,l),v(Zt,ts),r(e,Ka,l),r(e,bt,l),c(lt,bt,null),r(e,Qa,l),r(e,rt,l),v(rt,as),c(ft,rt,null),v(rt,ns),r(e,en,l),r(e,Mt,l),v(Mt,ss),r(e,tn,l),c(xt,e,l),r(e,an,l),r(e,qt,l),v(qt,ls),r(e,nn,l),c(yt,e,l),r(e,sn,l),c(kt,e,l),r(e,ln,l),c(Et,e,l),r(e,rn,l),r(e,Tt,l),r(e,fn,l),r(e,ot,l),v(ot,rs),v(ot,ta),v(ta,fs),v(ot,os),r(e,on,l),c(Wt,e,l),r(e,$n,l),r(e,te,l),v(te,$s),v(te,aa),v(aa,is),v(te,us),v(te,na),v(na,ps),v(te,ms),v(te,sa),v(sa,_s),v(te,cs),r(e,un,l),c(Pt,e,l),r(e,pn,l),r(e,Gt,l),v(Gt,ws),r(e,mn,l),c(St,e,l),r(e,_n,l),r(e,Nt,l),v(Nt,hs),r(e,cn,l),r(e,Ut,l),v(Ut,gs),r(e,wn,l),r(e,Yt,l),hn=!0},p(e,l){const la={};l[0]&1&&(la.graph=e[0]),d.$set(la);const ra={};l[1]&32768&&(ra.$$scope={dirty:l,ctx:e}),me.$set(ra);const fa={};l[1]&32768&&(fa.$$scope={dirty:l,ctx:e}),ae.$set(fa);const J={};l[1]&32768&&(J.$$scope={dirty:l,ctx:e}),ne.$set(J);const oa={};l[1]&32768&&(oa.$$scope={dirty:l,ctx:e}),fe.$set(oa);const $a={};l[1]&32768&&($a.$$scope={dirty:l,ctx:e}),se.$set($a);const ia={};l[1]&32768&&(ia.$$scope={dirty:l,ctx:e}),we.$set(ia);const ua={};l[0]&4|l[1]&32768&&(ua.$$scope={dirty:l,ctx:e}),X.$set(ua);const zt={};l[0]&2|l[1]&32768&&(zt.$$scope={dirty:l,ctx:e}),Z.$set(zt);const pa={};l[1]&32768&&(pa.$$scope={dirty:l,ctx:e}),ke.$set(pa);const B={};l[1]&32768&&(B.$$scope={dirty:l,ctx:e}),Ee.$set(B);const U={};l[1]&32768&&(U.$$scope={dirty:l,ctx:e}),Te.$set(U);const ma={};l[1]&32768&&(ma.$$scope={dirty:l,ctx:e}),We.$set(ma);const K={};l[1]&32768&&(K.$$scope={dirty:l,ctx:e}),Pe.$set(K);const _a={};l[1]&32768&&(_a.$$scope={dirty:l,ctx:e}),Se.$set(_a);const Q={};l[1]&32768&&(Q.$$scope={dirty:l,ctx:e}),Ne.$set(Q);const ca={};l[1]&32768&&(ca.$$scope={dirty:l,ctx:e}),ze.$set(ca);const $t={};l[1]&32768&&($t.$$scope={dirty:l,ctx:e}),Le.$set($t);const ge={};l[1]&32768&&(ge.$$scope={dirty:l,ctx:e}),Ie.$set(ge);const Lt={};l[1]&32768&&(Lt.$$scope={dirty:l,ctx:e}),Ae.$set(Lt);const wa={};l[1]&32768&&(wa.$$scope={dirty:l,ctx:e}),Xe.$set(wa);const de={};l[1]&32768&&(de.$$scope={dirty:l,ctx:e}),De.$set(de);const ha={};l[1]&32768&&(ha.$$scope={dirty:l,ctx:e}),Oe.$set(ha);const ga={};l[1]&32768&&(ga.$$scope={dirty:l,ctx:e}),Ce.$set(ga);const da={};l[1]&32768&&(da.$$scope={dirty:l,ctx:e}),Fe.$set(da);const va={};l[1]&32768&&(va.$$scope={dirty:l,ctx:e}),Ve.$set(va);const ba={};l[1]&32768&&(ba.$$scope={dirty:l,ctx:e}),Be.$set(ba);const xa={};l[1]&32768&&(xa.$$scope={dirty:l,ctx:e}),je.$set(xa);const It={};l[1]&32768&&(It.$$scope={dirty:l,ctx:e}),Ze.$set(It);const ya={};l[1]&32768&&(ya.$$scope={dirty:l,ctx:e}),Me.$set(ya);const ka={};l[1]&32768&&(ka.$$scope={dirty:l,ctx:e}),qe.$set(ka);const At={};l[1]&32768&&(At.$$scope={dirty:l,ctx:e}),Ge.$set(At);const Ea={};l[1]&32768&&(Ea.$$scope={dirty:l,ctx:e}),Ue.$set(Ea);const re={};l[1]&32768&&(re.$$scope={dirty:l,ctx:e}),Ye.$set(re);const Ta={};l[1]&32768&&(Ta.$$scope={dirty:l,ctx:e}),Re.$set(Ta);const Wa={};l[1]&32768&&(Wa.$$scope={dirty:l,ctx:e}),He.$set(Wa);const Pa={};l[1]&32768&&(Pa.$$scope={dirty:l,ctx:e}),Je.$set(Pa);const Sa={};l[1]&32768&&(Sa.$$scope={dirty:l,ctx:e}),Qe.$set(Sa);const Na={};l[1]&32768&&(Na.$$scope={dirty:l,ctx:e}),et.$set(Na);const za={};l[1]&32768&&(za.$$scope={dirty:l,ctx:e}),tt.$set(za);const ks={};l[1]&32768&&(ks.$$scope={dirty:l,ctx:e}),at.$set(ks);const Es={};l[1]&32768&&(Es.$$scope={dirty:l,ctx:e}),nt.$set(Es);const Ts={};l[1]&32768&&(Ts.$$scope={dirty:l,ctx:e}),st.$set(Ts);const Ws={};l[1]&32768&&(Ws.$$scope={dirty:l,ctx:e}),lt.$set(Ws);const Ps={};l[1]&32768&&(Ps.$$scope={dirty:l,ctx:e}),ft.$set(Ps)},i(e){hn||(w(E.$$.fragment,e),w(d.$$.fragment,e),w(me.$$.fragment,e),w(ae.$$.fragment,e),w(ne.$$.fragment,e),w(fe.$$.fragment,e),w(ee.$$.fragment,e),w(se.$$.fragment,e),w(ce.$$.fragment,e),w(we.$$.fragment,e),w(X.$$.fragment,e),w(Z.$$.fragment,e),w(ke.$$.fragment,e),w(Ee.$$.fragment,e),w(Te.$$.fragment,e),w(We.$$.fragment,e),w(Pe.$$.fragment,e),w(Se.$$.fragment,e),w(Ne.$$.fragment,e),w(ze.$$.fragment,e),w(Le.$$.fragment,e),w(Ie.$$.fragment,e),w(Ae.$$.fragment,e),w(Xe.$$.fragment,e),w(De.$$.fragment,e),w(Oe.$$.fragment,e),w(Ce.$$.fragment,e),w(Fe.$$.fragment,e),w(Ve.$$.fragment,e),w(Be.$$.fragment,e),w(je.$$.fragment,e),w(Ze.$$.fragment,e),w(Me.$$.fragment,e),w(qe.$$.fragment,e),w(Ge.$$.fragment,e),w(Ue.$$.fragment,e),w(Ye.$$.fragment,e),w(Re.$$.fragment,e),w(He.$$.fragment,e),w(Je.$$.fragment,e),w(Qe.$$.fragment,e),w(et.$$.fragment,e),w(tt.$$.fragment,e),w(at.$$.fragment,e),w(nt.$$.fragment,e),w(st.$$.fragment,e),w(lt.$$.fragment,e),w(ft.$$.fragment,e),w(xt.$$.fragment,e),w(yt.$$.fragment,e),w(kt.$$.fragment,e),w(Et.$$.fragment,e),w(Wt.$$.fragment,e),w(Pt.$$.fragment,e),w(St.$$.fragment,e),hn=!0)},o(e){h(E.$$.fragment,e),h(d.$$.fragment,e),h(me.$$.fragment,e),h(ae.$$.fragment,e),h(ne.$$.fragment,e),h(fe.$$.fragment,e),h(ee.$$.fragment,e),h(se.$$.fragment,e),h(ce.$$.fragment,e),h(we.$$.fragment,e),h(X.$$.fragment,e),h(Z.$$.fragment,e),h(ke.$$.fragment,e),h(Ee.$$.fragment,e),h(Te.$$.fragment,e),h(We.$$.fragment,e),h(Pe.$$.fragment,e),h(Se.$$.fragment,e),h(Ne.$$.fragment,e),h(ze.$$.fragment,e),h(Le.$$.fragment,e),h(Ie.$$.fragment,e),h(Ae.$$.fragment,e),h(Xe.$$.fragment,e),h(De.$$.fragment,e),h(Oe.$$.fragment,e),h(Ce.$$.fragment,e),h(Fe.$$.fragment,e),h(Ve.$$.fragment,e),h(Be.$$.fragment,e),h(je.$$.fragment,e),h(Ze.$$.fragment,e),h(Me.$$.fragment,e),h(qe.$$.fragment,e),h(Ge.$$.fragment,e),h(Ue.$$.fragment,e),h(Ye.$$.fragment,e),h(Re.$$.fragment,e),h(He.$$.fragment,e),h(Je.$$.fragment,e),h(Qe.$$.fragment,e),h(et.$$.fragment,e),h(tt.$$.fragment,e),h(at.$$.fragment,e),h(nt.$$.fragment,e),h(st.$$.fragment,e),h(lt.$$.fragment,e),h(ft.$$.fragment,e),h(xt.$$.fragment,e),h(yt.$$.fragment,e),h(kt.$$.fragment,e),h(Et.$$.fragment,e),h(Wt.$$.fragment,e),h(Pt.$$.fragment,e),h(St.$$.fragment,e),hn=!1},d(e){e&&s(n),e&&s(a),e&&s(u),e&&s(P),g(E,e),e&&s(L),e&&s(p),e&&s(f),e&&s(b),e&&s(j),g(d,e),e&&s(I),e&&s(C),g(me),g(ae),g(ne),g(fe),e&&s(Ht),g(ee,e),e&&s(Ot),e&&s(ve),e&&s(pt),e&&s(_e),e&&s(xe),g(se,e),e&&s(Ct),e&&s(oe),e&&s(Ft),g(ce,e),e&&s(Kt),e&&s(mt),e&&s(Qt),g(we,e),e&&s(ea),e&&s($e),g(X),g(Z),e&&s(le),e&&s(M),e&&s(ie),e&&s(D),e&&s(Vt),e&&s(V),g(ke),g(Ee),g(Te),g(We),g(Pe),g(Se),e&&s(Aa),g(Ne,e),e&&s(Xa),e&&s(q),g(ze),g(Le),g(Ie),g(Ae),g(Xe),e&&s(Da),e&&s(ct),g(De),e&&s(Oa),e&&s(Y),g(Oe),g(Ce),g(Fe),g(Ve),e&&s(Ca),e&&s(wt),g(Be),e&&s(Fa),e&&s(R),g(je),g(Ze),g(Me),g(qe),e&&s(Va),e&&s(ht),g(Ge),e&&s(Ba),e&&s(he),g(Ue),g(Ye),e&&s(ja),e&&s(ue),g(Re),g(He),e&&s(Za),g(Je,e),e&&s(Ma),e&&s(Ke),g(Qe),e&&s(qa),e&&s(gt),g(et),e&&s(Ga),e&&s(pe),g(tt),g(at),e&&s(Ua),e&&s(Bt),e&&s(Ya),e&&s(dt),g(nt),e&&s(Ra),e&&s(jt),e&&s(Ha),e&&s(vt),g(st),e&&s(Ja),e&&s(Zt),e&&s(Ka),e&&s(bt),g(lt),e&&s(Qa),e&&s(rt),g(ft),e&&s(en),e&&s(Mt),e&&s(tn),g(xt,e),e&&s(an),e&&s(qt),e&&s(nn),g(yt,e),e&&s(sn),g(kt,e),e&&s(ln),g(Et,e),e&&s(rn),e&&s(Tt),e&&s(fn),e&&s(ot),e&&s(on),g(Wt,e),e&&s($n),e&&s(te),e&&s(un),g(Pt,e),e&&s(pn),e&&s(Gt),e&&s(mn),g(St,e),e&&s(_n),e&&s(Nt),e&&s(cn),e&&s(Ut),e&&s(wn),e&&s(Yt)}}}function Zl(o){let n,t,a,u,N,P,E,L,p;return L=new Cs({props:{$$slots:{default:[jl]},$$scope:{ctx:o}}}),{c(){n=T("meta"),t=x(),a=T("h1"),u=$("Neural Network Training"),N=x(),P=T("div"),E=x(),m(L.$$.fragment),this.h()},l(k){const f=Ds("svelte-1kt0og6",document.head);n=W(f,"META",{name:!0,content:!0}),f.forEach(s),t=y(k),a=W(k,"H1",{});var b=S(a);u=i(b,"Neural Network Training"),b.forEach(s),N=y(k),P=W(k,"DIV",{class:!0}),S(P).forEach(s),E=y(k),_(L.$$.fragment,k),this.h()},h(){document.title="Neural Network Training - World4AI",G(n,"name","description"),G(n,"content","We can train a neural network by switching between the forward pass, the backward pass and gradient descent for a number of iterations. The implementation is relatively straightforward if we utilize one of the deep learning packages like PyTorch."),G(P,"class","separator")},m(k,f){v(document.head,n),r(k,t,f),r(k,a,f),v(a,u),r(k,N,f),r(k,P,f),r(k,E,f),c(L,k,f),p=!0},p(k,f){const b={};f[0]&7|f[1]&32768&&(b.$$scope={dirty:f,ctx:k}),L.$set(b)},i(k){p||(w(L.$$.fragment,k),p=!0)},o(k){h(L.$$.fragment,k),p=!1},d(k){s(n),k&&s(t),k&&s(a),k&&s(N),k&&s(P),k&&s(E),g(L,k)}}}let Ml=.5,ql=.5,Gl=200,gn=50;const Ul=.8,Yl=2;function Rl(o,n,t){let a=new H(.9);a._name="Feature 1";let u=new H(.3);u._name="Feature 2";let N=new H(.2);N._name="L1 N1 W1";let P=new H(.5);P._name="L1 N1 W2";let E=new H(1);E._name="L1 N1 B";let L=new H(.3);L._name="L1 N2 W1";let p=new H(.7);p._name="L1 N2 W2";let k=new H(0);k._name="L1 N2 B";let f=a.mul(N),b=u.mul(P),d=f.add(b).add(E).sigmoid();d._name="a_1";let I=a.mul(L),C=u.mul(p),ut=I.add(C).add(k).sigmoid();ut._name="a_2";let ae=new H(.22);ae._name="L2 N1 W1";let Dt=new H(.42);Dt._name="L2 N1 W2";let ne=new H(.2);ne._name="L2 N1 B";let Rt=d.mul(ae),fe=ut.mul(Dt),ee=Rt.add(fe).add(ne).sigmoid();ee._name="O",ee.backward();let Ot=JSON.parse(JSON.stringify(ee));ut._prev=[],d._prev=[];const ve=[{title:"Input",nodes:[{value:"x_1",class:"fill-gray-300"},{value:"x_2",class:"fill-gray-300"}]},{title:"Hidden",nodes:[{value:"a_1",class:"fill-w4ai-yellow"},{value:"a_2",class:"fill-w4ai-yellow"}]},{title:"Output",nodes:[{value:"o",class:"fill-w4ai-blue"}]}],Jt=[{title:"Input",nodes:[{value:"x_1",class:"fill-gray-300"},{value:"x_2",class:"fill-gray-300"}]},{title:"Hidden",nodes:[{value:"a_1",class:"fill-w4ai-yellow"},{value:"a_2",class:"fill-w4ai-yellow"},{value:"a_3",class:"fill-w4ai-yellow"},{value:"a_4",class:"fill-w4ai-yellow"}]},{title:"Output",nodes:[{value:"o",class:"fill-w4ai-blue"}]},{title:"Loss",nodes:[{value:"L",class:"fill-w4ai-red"}]}];let pt=[[],[]],_e=[.45,.25],be=[],xe=[];for(let X=0;X<_e.length;X++)for(let O=0;O<Gl;O++){let Z=2*Math.PI*Math.random(),le=_e[X],M=le*Math.cos(Z)+Ml,ie=le*Math.sin(Z)+ql;pt[X].push({x:M,y:ie}),be.push([M,ie]),xe.push(X)}let se=[];for(let X=0;X<gn;X++)for(let O=0;O<gn;O++){let Z=X/gn,le=O/gn,M=[];M.push(Z),M.push(le),se.push(M)}const Ct=[4,1];let oe=[],ye=[[],[]];function Ft(){let X=new js(Yl,Ct),O=new H(0),Z=0;function le(){Z+=1;for(let D=0;D<be.length;D++){let _t=X.forward(be[D]);if(xe[D]===0){let Vt=new H(1);O=O.add(Vt.sub(_t).log())}else xe[D]===1&&(O=O.add(_t.log()))}O=O.neg().div(be.length),oe.push({x:Z,y:O.data}),t(1,oe),O.backward(),X.parameters().forEach(D=>{D.data-=Ul*D.grad}),X.zeroGrad(),O=new H(0);let M=[],ie=[];se.forEach(D=>{X.forward(D).data<.5?M.push({x:D[0],y:D[1]}):ie.push({x:D[0],y:D[1]})}),t(2,ye=[]),ye.push(M),ye.push(ie)}return le}let ce=Ft();return[ee,oe,ye,Ot,ve,Jt,pt,ce,`import torch
import numpy as np
import matplotlib.pyplot as plt`,`# create circular data
def circular_data():
    radii = [0.45, 0.25]
    center_x = 0.5
    center_y = 0.5
    num_points = 200
    X = []
    y = []
    
    for label, radius in enumerate(radii):
        for point in range(num_points):
            angle = 2 * np.pi * np.random.rand()
            feature_1 = radius * np.cos(angle) + center_x
            feature_2 = radius * np.sin(angle) + center_y
            
            X.append([feature_1, feature_2])     
            y.append([label])
            
    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)`,"X, y = circular_data()",`feature_1 = X.T[0]
feature_2 = X.T[1]
# plot the circle
plt.figure(figsize=(4,4))
plt.scatter(x=feature_1, y=feature_2, c=y)
plt.title("Circular Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()`,`class NeuralNetwork:
    
    def __init__(self, X, y, shape=[2, 4, 2, 1], alpha = 0.1):
        self.X = X
        self.y = y
        self.alpha = alpha
        self.weights = []
        self.biases = []
        
        # initialize weights and matrices with random numbers
        for num_features, num_neurons in zip(shape[:-1], shape[1:]):
            weight_matrix = torch.randn(num_neurons, num_features, requires_grad=True)
            self.weights.append(weight_matrix)
            bias_vector = torch.randn(1, num_neurons, requires_grad=True)
            self.biases.append(bias_vector)
        
    def forward(self):
        A = X
        for W, b in zip(self.weights, self.biases):
            Z = A @ W.T + b
            A = torch.sigmoid(Z)
        return A
                    
    def loss(self, y_hat):
        loss =  -(self.y * torch.log(y_hat) + (1 - self.y) * torch.log(1 - y_hat)).mean()
        return loss
                            
    # update weights and biases
    def step(self):
        with torch.inference_mode():
            for w, b in zip(self.weights, self.biases):
                # gradient descent
                w.data.sub_(w.grad * self.alpha)
                b.data.sub_(b.grad * self.alpha)

                # zero out the gradients
                w.grad.zero_()
                b.grad.zero_()`,"nn = NeuralNetwork(X, y)",`# training loop
for i in range(50_000):
    y_hat = nn.forward()
    loss = nn.loss(y_hat)
    if i % 10000 == 0:
        print(loss.data)
    loss.backward()
    nn.step()`]}class ur extends Is{constructor(n){super(),As(this,n,Rl,Zl,Xs,{},null,[-1,-1])}}export{ur as default};
