import{S as Xt,i as Jt,s as Kt,k as A,a as y,q as u,y as w,W as Qt,l as j,h as r,c as z,m as G,r as m,z as g,n as Ie,N as k,b as s,A as c,g as v,d as _,B as b,C as I}from"../chunks/index.4d92b023.js";import{C as Ft}from"../chunks/Container.b0705c7b.js";import{L as W}from"../chunks/Latex.e0b308c0.js";import{H as qt}from"../chunks/Highlight.b7c1de53.js";import{N as Rt}from"../chunks/NeuralNetwork.9b1e2957.js";import{A as Ut}from"../chunks/Alert.25a852b3.js";import{P as Bt,T as Mt}from"../chunks/Ticks.45eca5c5.js";import{P as Et}from"../chunks/Path.7e6df014.js";import{X as Zt,Y as ea}from"../chunks/YLabel.182e66a3.js";import{L as Ze}from"../chunks/Legend.de38c007.js";function ta(i){let n;return{c(){n=u("vanishing")},l(a){n=m(a,"vanishing")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function aa(i){let n;return{c(){n=u("exploding")},l(a){n=m(a,"exploding")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function na(i){let n,a,t,$,x,L,h,S,E,P;return t=new qt({props:{$$slots:{default:[ta]},$$scope:{ctx:i}}}),x=new qt({props:{$$slots:{default:[aa]},$$scope:{ctx:i}}}),{c(){n=A("p"),a=u(`We expect the performance of a neural network to improve when we add more
    layers to its architecture. A deep neural network has more degrees of
    freedom to fit to the data than a shallow neural network and should thereby
    perform much better. In the very least the neural network should be able to
    overfit to the training data and to display decent performance on the
    training dataset. Yet the opposite is the case. When you naively keep adding
    more and more layers to the neural network, the performance will start to
    deterioarate until the network is not able to learn anything at all. This
    has to do with the so called `),w(t.$$.fragment),$=u(" or "),w(x.$$.fragment),L=u(` gradients. The vanishing gradient problem especially plagued the machine learning
    community for a long period of time, but by now we have some excellent tools
    to deal with those problems.`),h=y(),S=A("p"),E=u(`To focus on the core idea of the problem, we are going to assume that each
    layer has just one neuron with one weight and no bias. While this is an
    unreasonable assumption, the ideas will hold for much more complex neural
    networks.`)},l(o){n=j(o,"P",{});var d=G(n);a=m(d,`We expect the performance of a neural network to improve when we add more
    layers to its architecture. A deep neural network has more degrees of
    freedom to fit to the data than a shallow neural network and should thereby
    perform much better. In the very least the neural network should be able to
    overfit to the training data and to display decent performance on the
    training dataset. Yet the opposite is the case. When you naively keep adding
    more and more layers to the neural network, the performance will start to
    deterioarate until the network is not able to learn anything at all. This
    has to do with the so called `),g(t.$$.fragment,d),$=m(d," or "),g(x.$$.fragment,d),L=m(d,` gradients. The vanishing gradient problem especially plagued the machine learning
    community for a long period of time, but by now we have some excellent tools
    to deal with those problems.`),d.forEach(r),h=z(o),S=j(o,"P",{});var f=G(S);E=m(f,`To focus on the core idea of the problem, we are going to assume that each
    layer has just one neuron with one weight and no bias. While this is an
    unreasonable assumption, the ideas will hold for much more complex neural
    networks.`),f.forEach(r)},m(o,d){s(o,n,d),k(n,a),c(t,n,null),k(n,$),c(x,n,null),k(n,L),s(o,h,d),s(o,S,d),k(S,E),P=!0},p(o,d){const f={};d&16&&(f.$$scope={dirty:d,ctx:o}),t.$set(f);const p={};d&16&&(p.$$scope={dirty:d,ctx:o}),x.$set(p)},i(o){P||(v(t.$$.fragment,o),v(x.$$.fragment,o),P=!0)},o(o){_(t.$$.fragment,o),_(x.$$.fragment,o),P=!1},d(o){o&&r(n),b(t),b(x),o&&r(h),o&&r(S)}}}function la(i){let n=String.raw`z^{<l>}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function ra(i){let n=String.raw`a^{<l>}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function sa(i){let n;return{c(){n=u("a^3")},l(a){n=m(a,"a^3")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function oa(i){let n;return{c(){n=u("L")},l(a){n=m(a,"L")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function ia(i){let n=String.raw`
    \begin{aligned}
    z^{<1>} &= x^{<1>}w^{<1>} \\
    a^{<1>} &= f(z^{<1>}) \\
    z^{<2>} &= a^{<1>}w^{<2>} \\
    a^{<2>} &= f(z^{<2>}) \\
    z^{<3>} &= a^{<2>}w^{<3>} \\
    a^{<3>} &= f(z^{<3>}) \\
    \end{aligned}
      `+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function fa(i){let n=String.raw`w^{<1>}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function $a(i){let n=String.raw`\dfrac{d}{dw^{<1>}} Loss = 
    \dfrac{dLoss}{da^{<3>}} 
    \boxed{
    \dfrac{da^{<3>}}{dz^{<3>}} 
    \dfrac{dz^{<3>}}{da^{<2>}} 
    \dfrac{da^{<2>}}{dz^{<2>}} 
    \dfrac{dz^{<2>}}{da^{<1>}} 
    \dfrac{da^{<1>}}{dz^{<1>}} 
    }
    \dfrac{dz^{<1>}}{dw^{<1>}} 
    `+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function ua(i){let n=String.raw`\dfrac{da}{dz}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function ma(i){let n=String.raw`\dfrac{dz}{da}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function pa(i){let n=String.raw`\dfrac{1}{1 + e^{-z}}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function ha(i){let n=String.raw`\dfrac{da^{<l>}}{dz^{<l>}}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function da(i){let n=String.raw`a^{<l>}(1-a^{<l>})`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function wa(i){let n,a,t,$,x,L,h,S,E,P;return n=new Et({props:{data:i[1]}}),t=new Et({props:{data:i[2],color:"var(--main-color-2)"}}),x=new Mt({props:{xTicks:[-10,-8,-6,-4,-2,0,2,4,6,8,10],yTicks:[0,.2,.4,.6,.8,1],xOffset:6,yOffset:5}}),h=new Ze({props:{text:"sigmoid",coordinates:{x:-9,y:.9}}}),E=new Ze({props:{text:"sigmoid derivative",coordinates:{x:-9,y:.83},legendColor:"var(--main-color-2)"}}),{c(){w(n.$$.fragment),a=y(),w(t.$$.fragment),$=y(),w(x.$$.fragment),L=y(),w(h.$$.fragment),S=y(),w(E.$$.fragment)},l(o){g(n.$$.fragment,o),a=z(o),g(t.$$.fragment,o),$=z(o),g(x.$$.fragment,o),L=z(o),g(h.$$.fragment,o),S=z(o),g(E.$$.fragment,o)},m(o,d){c(n,o,d),s(o,a,d),c(t,o,d),s(o,$,d),c(x,o,d),s(o,L,d),c(h,o,d),s(o,S,d),c(E,o,d),P=!0},p:I,i(o){P||(v(n.$$.fragment,o),v(t.$$.fragment,o),v(x.$$.fragment,o),v(h.$$.fragment,o),v(E.$$.fragment,o),P=!0)},o(o){_(n.$$.fragment,o),_(t.$$.fragment,o),_(x.$$.fragment,o),_(h.$$.fragment,o),_(E.$$.fragment,o),P=!1},d(o){b(n,o),o&&r(a),b(t,o),o&&r($),b(x,o),o&&r(L),b(h,o),o&&r(S),b(E,o)}}}function ga(i){let n=String.raw`
\dfrac{da^{<l>}}{dz^{<l>}} 
    `+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function ca(i){let n=String.raw`\dfrac{d}{dw^{<1>}} Loss = 
    \dfrac{dLoss}{da^{<3>}} 
    \boxed{
    0.25
    \dfrac{dz^{<3>}}{da^{<2>}} 
    0.25
    \dfrac{dz^{<2>}}{da^{<1>}} 
    0.25
    }
    \dfrac{dz^{<1>}}{dw^{<1>}} 
    `+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function va(i){let n;return{c(){n=u("0")},l(a){n=m(a,"0")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function _a(i){let n,a,t,$,x,L,h,S,E,P,o,d;return n=new Et({props:{data:i[3]}}),t=new Mt({props:{xTicks:[0,1,2,3,4,5],yTicks:[.3,.25,.2,.15,.1,.05,0],xOffset:-18,yOffset:15}}),x=new Zt({props:{text:"Layers"}}),h=new ea({props:{text:"Factor"}}),E=new Ze({props:{text:"sigmoid",coordinates:{x:-9,y:.9}}}),o=new Ze({props:{text:"sigmoid derivative",coordinates:{x:-9,y:.83},legendColor:"var(--main-color-2)"}}),{c(){w(n.$$.fragment),a=y(),w(t.$$.fragment),$=y(),w(x.$$.fragment),L=y(),w(h.$$.fragment),S=y(),w(E.$$.fragment),P=y(),w(o.$$.fragment)},l(f){g(n.$$.fragment,f),a=z(f),g(t.$$.fragment,f),$=z(f),g(x.$$.fragment,f),L=z(f),g(h.$$.fragment,f),S=z(f),g(E.$$.fragment,f),P=z(f),g(o.$$.fragment,f)},m(f,p){c(n,f,p),s(f,a,p),c(t,f,p),s(f,$,p),c(x,f,p),s(f,L,p),c(h,f,p),s(f,S,p),c(E,f,p),s(f,P,p),c(o,f,p),d=!0},p:I,i(f){d||(v(n.$$.fragment,f),v(t.$$.fragment,f),v(x.$$.fragment,f),v(h.$$.fragment,f),v(E.$$.fragment,f),v(o.$$.fragment,f),d=!0)},o(f){_(n.$$.fragment,f),_(t.$$.fragment,f),_(x.$$.fragment,f),_(h.$$.fragment,f),_(E.$$.fragment,f),_(o.$$.fragment,f),d=!1},d(f){b(n,f),f&&r(a),b(t,f),f&&r($),b(x,f),f&&r(L),b(h,f),f&&r(S),b(E,f),f&&r(P),b(o,f)}}}function ba(i){let n=String.raw`\dfrac{da^{<l>}}{dz^{<l>}}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function xa(i){let n=String.raw`\dfrac{dL}{dw^{<1>}}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function ka(i){let n=String.raw`\dfrac{dz^{<l>}}{da^{<l-1>}}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function ya(i){let n=String.raw`w^{<l>}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function za(i){let n=String.raw`w^{<2>}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function Ea(i){let n=String.raw`w^{<3>}`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function Sa(i){let n=String.raw`\dfrac{d}{dw^{<1>}} Loss = 
    \dfrac{dLoss}{da^{<3>}} 
    \boxed{
    \dfrac{da^{<3>}}{dz^{<3>}} 
    0.95 
    \dfrac{da^{<2>}}{dz^{<2>}} 
    0.95 
    \dfrac{da^{<1>}}{dz^{<1>}} 
    }
    \dfrac{dz^{<1>}}{dw^{<1>}} 
    `+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function Ta(i){let n=String.raw`w > 1`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function La(i){let n=String.raw`w < - 1`+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function Pa(i){let n=String.raw`\dfrac{d}{dw^{<1>}} Loss = 
    \dfrac{dLoss}{da^{<3>}} 
    \boxed{
    \dfrac{da^{<3>}}{dz^{<3>}} 
    2
    \dfrac{da^{<2>}}{dz^{<2>}} 
    2
    \dfrac{da^{<1>}}{dz^{<1>}} 
    }
    \dfrac{dz^{<1>}}{dw^{<1>}} 
    `+"",a;return{c(){a=u(n)},l(t){a=m(t,n)},m(t,$){s(t,a,$)},p:I,d(t){t&&r(a)}}}function Wa(i){let n;return{c(){n=u(`Derivatives of activation functions and weights have a significant impact on
    whether we can train a deep neural network successfully or not.`)},l(a){n=m(a,`Derivatives of activation functions and weights have a significant impact on
    whether we can train a deep neural network successfully or not.`)},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function Ia(i){let n,a,t,$,x,L,h,S,E,P,o,d,f,p,T,q,O,ke,Ae,Q,je,V,et,R,tt,U,at,Ge,H,nt,Z,lt,ee,rt,te,st,He,ae,Oe,ne,ot,le,it,Ce,re,Ve,se,ft,oe,$t,De,ie,Ne,D,ut,fe,mt,$e,pt,Ye,N,ht,ue,dt,me,wt,Fe,Y,gt,pe,ct,he,vt,qe,de,Be,ye,_t,Me,F,bt,we,xt,ge,kt,Xe,ce,Je,ze,yt,Ke,ve,Qe,Ee,zt,Re,Se,Ue;return t=new W({props:{$$slots:{default:[la]},$$scope:{ctx:i}}}),x=new W({props:{$$slots:{default:[ra]},$$scope:{ctx:i}}}),h=new W({props:{$$slots:{default:[sa]},$$scope:{ctx:i}}}),E=new W({props:{$$slots:{default:[oa]},$$scope:{ctx:i}}}),f=new W({props:{$$slots:{default:[ia]},$$scope:{ctx:i}}}),O=new W({props:{$$slots:{default:[fa]},$$scope:{ctx:i}}}),Q=new W({props:{$$slots:{default:[$a]},$$scope:{ctx:i}}}),R=new W({props:{$$slots:{default:[ua]},$$scope:{ctx:i}}}),U=new W({props:{$$slots:{default:[ma]},$$scope:{ctx:i}}}),Z=new W({props:{$$slots:{default:[pa]},$$scope:{ctx:i}}}),ee=new W({props:{$$slots:{default:[ha]},$$scope:{ctx:i}}}),te=new W({props:{$$slots:{default:[da]},$$scope:{ctx:i}}}),ae=new Bt({props:{width:500,height:250,maxWidth:800,domain:[-10,10],range:[0,1],padding:{top:10,right:10,bottom:15,left:30},$$slots:{default:[wa]},$$scope:{ctx:i}}}),le=new W({props:{$$slots:{default:[ga]},$$scope:{ctx:i}}}),re=new W({props:{$$slots:{default:[ca]},$$scope:{ctx:i}}}),oe=new W({props:{$$slots:{default:[va]},$$scope:{ctx:i}}}),ie=new Bt({props:{width:500,height:250,maxWidth:800,domain:[1,5],range:[0,.3],$$slots:{default:[_a]},$$scope:{ctx:i}}}),fe=new W({props:{$$slots:{default:[ba]},$$scope:{ctx:i}}}),$e=new W({props:{$$slots:{default:[xa]},$$scope:{ctx:i}}}),ue=new W({props:{$$slots:{default:[ka]},$$scope:{ctx:i}}}),me=new W({props:{$$slots:{default:[ya]},$$scope:{ctx:i}}}),pe=new W({props:{$$slots:{default:[za]},$$scope:{ctx:i}}}),he=new W({props:{$$slots:{default:[Ea]},$$scope:{ctx:i}}}),de=new W({props:{$$slots:{default:[Sa]},$$scope:{ctx:i}}}),we=new W({props:{$$slots:{default:[Ta]},$$scope:{ctx:i}}}),ge=new W({props:{$$slots:{default:[La]},$$scope:{ctx:i}}}),ce=new W({props:{$$slots:{default:[Pa]},$$scope:{ctx:i}}}),ve=new Ut({props:{type:"info",$$slots:{default:[Wa]},$$scope:{ctx:i}}}),{c(){n=A("p"),a=u(`The forward pass is straighforward. We iterate between the calculation of
    the net value `),w(t.$$.fragment),$=u(" and the neuron output "),w(x.$$.fragment),L=u(" until we are able to calculate the final activation "),w(h.$$.fragment),S=u(` and
    the loss `),w(E.$$.fragment),P=u("."),o=y(),d=A("div"),w(f.$$.fragment),p=y(),T=A("p"),q=u(`In the backward pass we calculate the derivative of the loss with respect to
    weights of different layers by using the chain rule over and over again. For
    the first weigth `),w(O.$$.fragment),ke=u(` the calculation of the
    derivative would look as follows.`),Ae=y(),w(Q.$$.fragment),je=y(),V=A("p"),et=u(`If you look at the boxed calculations, you should notice that the same type
    of calculations are repeated over and over again: `),w(R.$$.fragment),tt=u(" and "),w(U.$$.fragment),at=u(`. We would encounter the
    same pattern even if we had to deal with 100 layers. If we can figure out
    the nature of those two derivatives we might understand what the value of
    the overall derivative looks like.`),Ge=y(),H=A("p"),nt=u("So far we have exclusively dealt with the sigmoid activation function "),w(Z.$$.fragment),lt=u(", therefore the derivative of "),w(ee.$$.fragment),rt=u(" is "),w(te.$$.fragment),st=u(`. When we draw both the
    activation functions and the derivative, we notice, that the derivative of
    the sigmoid approaches 0, when the net input gets too large or too small. At
    its peak the derivative is exactly 0.25.`),He=y(),w(ae.$$.fragment),Oe=y(),ne=A("p"),ot=u(`If we assume the best case scenario, we can replace
    `),w(le.$$.fragment),it=u(" by 0.25 and we end up with the following calculatoin of the derivative."),Ce=y(),w(re.$$.fragment),Ve=y(),se=A("p"),ft=u(`Each additional layer in the neural network forces the derivative to shrink
    by at least 4. With just 5 layers we are dealing with the factor close to `),w(oe.$$.fragment),$t=u("."),De=y(),w(ie.$$.fragment),Ne=y(),D=A("p"),ut=u("Given that the sigmoid derivative "),w(fe.$$.fragment),mt=u(` is always between 0.25 and 0, we have to assume, that the overall derivative
    `),w($e.$$.fragment),pt=u(` approaches 0 when the number
    of layers starts to grow. Layers that are close to the output layer are still
    able to change their respective weights appropriately, but the farther the layers
    are removed from the loss, the closer the multiplicator gets to 0 and the closer
    the derivative gets to 0. The weights of the first layers remain virtually unchanged
    from their initial values, preventing the neural network from learning. That
    is the vanishing gradient problem.`),Ye=y(),N=A("p"),ht=u("The derivative "),w(ue.$$.fragment),dt=u(` on the
    other hand is just the corresponding weight `),w(me.$$.fragment),wt=u("."),Fe=y(),Y=A("p"),gt=u("Assuming for example that "),w(pe.$$.fragment),ct=u(" and "),w(he.$$.fragment),vt=u(" are both 0.95, we would deal with the following gradient."),qe=y(),w(de.$$.fragment),Be=y(),ye=A("p"),_t=u(`Here we can make a similar argument that we did with the derivative of the
    sigmoid. When the derivatives of weights are between 0 and 1, the gradients
    in the first layers will approach 0.`),Me=y(),F=A("p"),bt=u(`Obviously unlike with the sigmoid, weights do not have any lower or higher
    bounds. All weights could therefore be in the range `),w(we.$$.fragment),xt=u(" and "),w(ge.$$.fragment),kt=u(`. If each weight corresponds to
    exactly 2, then the gradient will grow exponentially.`),Xe=y(),w(ce.$$.fragment),Je=y(),ze=A("p"),yt=u(`That could make the gradients in the first layers enormous, leading to the
    so called exploding gradient problem. Gradient descent will most likely
    start to diverge and at some point our program will throw an error, as the
    gradient will overflow.`),Ke=y(),w(ve.$$.fragment),Qe=y(),Ee=A("p"),zt=u(`The remedies to those problems will for the most part deal with adjustmens
    to weights and activation functions. This will be the topic of this chapter.`),Re=y(),Se=A("div"),this.h()},l(e){n=j(e,"P",{});var l=G(n);a=m(l,`The forward pass is straighforward. We iterate between the calculation of
    the net value `),g(t.$$.fragment,l),$=m(l," and the neuron output "),g(x.$$.fragment,l),L=m(l," until we are able to calculate the final activation "),g(h.$$.fragment,l),S=m(l,` and
    the loss `),g(E.$$.fragment,l),P=m(l,"."),l.forEach(r),o=z(e),d=j(e,"DIV",{class:!0});var Te=G(d);g(f.$$.fragment,Te),Te.forEach(r),p=z(e),T=j(e,"P",{});var _e=G(T);q=m(_e,`In the backward pass we calculate the derivative of the loss with respect to
    weights of different layers by using the chain rule over and over again. For
    the first weigth `),g(O.$$.fragment,_e),ke=m(_e,` the calculation of the
    derivative would look as follows.`),_e.forEach(r),Ae=z(e),g(Q.$$.fragment,e),je=z(e),V=j(e,"P",{});var B=G(V);et=m(B,`If you look at the boxed calculations, you should notice that the same type
    of calculations are repeated over and over again: `),g(R.$$.fragment,B),tt=m(B," and "),g(U.$$.fragment,B),at=m(B,`. We would encounter the
    same pattern even if we had to deal with 100 layers. If we can figure out
    the nature of those two derivatives we might understand what the value of
    the overall derivative looks like.`),B.forEach(r),Ge=z(e),H=j(e,"P",{});var C=G(H);nt=m(C,"So far we have exclusively dealt with the sigmoid activation function "),g(Z.$$.fragment,C),lt=m(C,", therefore the derivative of "),g(ee.$$.fragment,C),rt=m(C," is "),g(te.$$.fragment,C),st=m(C,`. When we draw both the
    activation functions and the derivative, we notice, that the derivative of
    the sigmoid approaches 0, when the net input gets too large or too small. At
    its peak the derivative is exactly 0.25.`),C.forEach(r),He=z(e),g(ae.$$.fragment,e),Oe=z(e),ne=j(e,"P",{});var be=G(ne);ot=m(be,`If we assume the best case scenario, we can replace
    `),g(le.$$.fragment,be),it=m(be," by 0.25 and we end up with the following calculatoin of the derivative."),be.forEach(r),Ce=z(e),g(re.$$.fragment,e),Ve=z(e),se=j(e,"P",{});var xe=G(se);ft=m(xe,`Each additional layer in the neural network forces the derivative to shrink
    by at least 4. With just 5 layers we are dealing with the factor close to `),g(oe.$$.fragment,xe),$t=m(xe,"."),xe.forEach(r),De=z(e),g(ie.$$.fragment,e),Ne=z(e),D=j(e,"P",{});var M=G(D);ut=m(M,"Given that the sigmoid derivative "),g(fe.$$.fragment,M),mt=m(M,` is always between 0.25 and 0, we have to assume, that the overall derivative
    `),g($e.$$.fragment,M),pt=m(M,` approaches 0 when the number
    of layers starts to grow. Layers that are close to the output layer are still
    able to change their respective weights appropriately, but the farther the layers
    are removed from the loss, the closer the multiplicator gets to 0 and the closer
    the derivative gets to 0. The weights of the first layers remain virtually unchanged
    from their initial values, preventing the neural network from learning. That
    is the vanishing gradient problem.`),M.forEach(r),Ye=z(e),N=j(e,"P",{});var X=G(N);ht=m(X,"The derivative "),g(ue.$$.fragment,X),dt=m(X,` on the
    other hand is just the corresponding weight `),g(me.$$.fragment,X),wt=m(X,"."),X.forEach(r),Fe=z(e),Y=j(e,"P",{});var J=G(Y);gt=m(J,"Assuming for example that "),g(pe.$$.fragment,J),ct=m(J," and "),g(he.$$.fragment,J),vt=m(J," are both 0.95, we would deal with the following gradient."),J.forEach(r),qe=z(e),g(de.$$.fragment,e),Be=z(e),ye=j(e,"P",{});var Le=G(ye);_t=m(Le,`Here we can make a similar argument that we did with the derivative of the
    sigmoid. When the derivatives of weights are between 0 and 1, the gradients
    in the first layers will approach 0.`),Le.forEach(r),Me=z(e),F=j(e,"P",{});var K=G(F);bt=m(K,`Obviously unlike with the sigmoid, weights do not have any lower or higher
    bounds. All weights could therefore be in the range `),g(we.$$.fragment,K),xt=m(K," and "),g(ge.$$.fragment,K),kt=m(K,`. If each weight corresponds to
    exactly 2, then the gradient will grow exponentially.`),K.forEach(r),Xe=z(e),g(ce.$$.fragment,e),Je=z(e),ze=j(e,"P",{});var Pe=G(ze);yt=m(Pe,`That could make the gradients in the first layers enormous, leading to the
    so called exploding gradient problem. Gradient descent will most likely
    start to diverge and at some point our program will throw an error, as the
    gradient will overflow.`),Pe.forEach(r),Ke=z(e),g(ve.$$.fragment,e),Qe=z(e),Ee=j(e,"P",{});var We=G(Ee);zt=m(We,`The remedies to those problems will for the most part deal with adjustmens
    to weights and activation functions. This will be the topic of this chapter.`),We.forEach(r),Re=z(e),Se=j(e,"DIV",{class:!0}),G(Se).forEach(r),this.h()},h(){Ie(d,"class","flex justify-center"),Ie(Se,"class","separator")},m(e,l){s(e,n,l),k(n,a),c(t,n,null),k(n,$),c(x,n,null),k(n,L),c(h,n,null),k(n,S),c(E,n,null),k(n,P),s(e,o,l),s(e,d,l),c(f,d,null),s(e,p,l),s(e,T,l),k(T,q),c(O,T,null),k(T,ke),s(e,Ae,l),c(Q,e,l),s(e,je,l),s(e,V,l),k(V,et),c(R,V,null),k(V,tt),c(U,V,null),k(V,at),s(e,Ge,l),s(e,H,l),k(H,nt),c(Z,H,null),k(H,lt),c(ee,H,null),k(H,rt),c(te,H,null),k(H,st),s(e,He,l),c(ae,e,l),s(e,Oe,l),s(e,ne,l),k(ne,ot),c(le,ne,null),k(ne,it),s(e,Ce,l),c(re,e,l),s(e,Ve,l),s(e,se,l),k(se,ft),c(oe,se,null),k(se,$t),s(e,De,l),c(ie,e,l),s(e,Ne,l),s(e,D,l),k(D,ut),c(fe,D,null),k(D,mt),c($e,D,null),k(D,pt),s(e,Ye,l),s(e,N,l),k(N,ht),c(ue,N,null),k(N,dt),c(me,N,null),k(N,wt),s(e,Fe,l),s(e,Y,l),k(Y,gt),c(pe,Y,null),k(Y,ct),c(he,Y,null),k(Y,vt),s(e,qe,l),c(de,e,l),s(e,Be,l),s(e,ye,l),k(ye,_t),s(e,Me,l),s(e,F,l),k(F,bt),c(we,F,null),k(F,xt),c(ge,F,null),k(F,kt),s(e,Xe,l),c(ce,e,l),s(e,Je,l),s(e,ze,l),k(ze,yt),s(e,Ke,l),c(ve,e,l),s(e,Qe,l),s(e,Ee,l),k(Ee,zt),s(e,Re,l),s(e,Se,l),Ue=!0},p(e,l){const Te={};l&16&&(Te.$$scope={dirty:l,ctx:e}),t.$set(Te);const _e={};l&16&&(_e.$$scope={dirty:l,ctx:e}),x.$set(_e);const B={};l&16&&(B.$$scope={dirty:l,ctx:e}),h.$set(B);const C={};l&16&&(C.$$scope={dirty:l,ctx:e}),E.$set(C);const be={};l&16&&(be.$$scope={dirty:l,ctx:e}),f.$set(be);const xe={};l&16&&(xe.$$scope={dirty:l,ctx:e}),O.$set(xe);const M={};l&16&&(M.$$scope={dirty:l,ctx:e}),Q.$set(M);const X={};l&16&&(X.$$scope={dirty:l,ctx:e}),R.$set(X);const J={};l&16&&(J.$$scope={dirty:l,ctx:e}),U.$set(J);const Le={};l&16&&(Le.$$scope={dirty:l,ctx:e}),Z.$set(Le);const K={};l&16&&(K.$$scope={dirty:l,ctx:e}),ee.$set(K);const Pe={};l&16&&(Pe.$$scope={dirty:l,ctx:e}),te.$set(Pe);const We={};l&16&&(We.$$scope={dirty:l,ctx:e}),ae.$set(We);const Tt={};l&16&&(Tt.$$scope={dirty:l,ctx:e}),le.$set(Tt);const Lt={};l&16&&(Lt.$$scope={dirty:l,ctx:e}),re.$set(Lt);const Pt={};l&16&&(Pt.$$scope={dirty:l,ctx:e}),oe.$set(Pt);const Wt={};l&16&&(Wt.$$scope={dirty:l,ctx:e}),ie.$set(Wt);const It={};l&16&&(It.$$scope={dirty:l,ctx:e}),fe.$set(It);const At={};l&16&&(At.$$scope={dirty:l,ctx:e}),$e.$set(At);const jt={};l&16&&(jt.$$scope={dirty:l,ctx:e}),ue.$set(jt);const Gt={};l&16&&(Gt.$$scope={dirty:l,ctx:e}),me.$set(Gt);const Ht={};l&16&&(Ht.$$scope={dirty:l,ctx:e}),pe.$set(Ht);const Ot={};l&16&&(Ot.$$scope={dirty:l,ctx:e}),he.$set(Ot);const Ct={};l&16&&(Ct.$$scope={dirty:l,ctx:e}),de.$set(Ct);const Vt={};l&16&&(Vt.$$scope={dirty:l,ctx:e}),we.$set(Vt);const Dt={};l&16&&(Dt.$$scope={dirty:l,ctx:e}),ge.$set(Dt);const Nt={};l&16&&(Nt.$$scope={dirty:l,ctx:e}),ce.$set(Nt);const Yt={};l&16&&(Yt.$$scope={dirty:l,ctx:e}),ve.$set(Yt)},i(e){Ue||(v(t.$$.fragment,e),v(x.$$.fragment,e),v(h.$$.fragment,e),v(E.$$.fragment,e),v(f.$$.fragment,e),v(O.$$.fragment,e),v(Q.$$.fragment,e),v(R.$$.fragment,e),v(U.$$.fragment,e),v(Z.$$.fragment,e),v(ee.$$.fragment,e),v(te.$$.fragment,e),v(ae.$$.fragment,e),v(le.$$.fragment,e),v(re.$$.fragment,e),v(oe.$$.fragment,e),v(ie.$$.fragment,e),v(fe.$$.fragment,e),v($e.$$.fragment,e),v(ue.$$.fragment,e),v(me.$$.fragment,e),v(pe.$$.fragment,e),v(he.$$.fragment,e),v(de.$$.fragment,e),v(we.$$.fragment,e),v(ge.$$.fragment,e),v(ce.$$.fragment,e),v(ve.$$.fragment,e),Ue=!0)},o(e){_(t.$$.fragment,e),_(x.$$.fragment,e),_(h.$$.fragment,e),_(E.$$.fragment,e),_(f.$$.fragment,e),_(O.$$.fragment,e),_(Q.$$.fragment,e),_(R.$$.fragment,e),_(U.$$.fragment,e),_(Z.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(le.$$.fragment,e),_(re.$$.fragment,e),_(oe.$$.fragment,e),_(ie.$$.fragment,e),_(fe.$$.fragment,e),_($e.$$.fragment,e),_(ue.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(he.$$.fragment,e),_(de.$$.fragment,e),_(we.$$.fragment,e),_(ge.$$.fragment,e),_(ce.$$.fragment,e),_(ve.$$.fragment,e),Ue=!1},d(e){e&&r(n),b(t),b(x),b(h),b(E),e&&r(o),e&&r(d),b(f),e&&r(p),e&&r(T),b(O),e&&r(Ae),b(Q,e),e&&r(je),e&&r(V),b(R),b(U),e&&r(Ge),e&&r(H),b(Z),b(ee),b(te),e&&r(He),b(ae,e),e&&r(Oe),e&&r(ne),b(le),e&&r(Ce),b(re,e),e&&r(Ve),e&&r(se),b(oe),e&&r(De),b(ie,e),e&&r(Ne),e&&r(D),b(fe),b($e),e&&r(Ye),e&&r(N),b(ue),b(me),e&&r(Fe),e&&r(Y),b(pe),b(he),e&&r(qe),b(de,e),e&&r(Be),e&&r(ye),e&&r(Me),e&&r(F),b(we),b(ge),e&&r(Xe),b(ce,e),e&&r(Je),e&&r(ze),e&&r(Ke),b(ve,e),e&&r(Qe),e&&r(Ee),e&&r(Re),e&&r(Se)}}}function Aa(i){let n,a,t,$,x,L,h,S,E,P,o,d,f;return S=new Ft({props:{$$slots:{default:[na]},$$scope:{ctx:i}}}),P=new Rt({props:{layers:i[0],height:50,maxWidth:"500px"}}),d=new Ft({props:{$$slots:{default:[Ia]},$$scope:{ctx:i}}}),{c(){n=A("meta"),a=y(),t=A("h1"),$=u("Vanishing and Exploding Gradients"),x=y(),L=A("div"),h=y(),w(S.$$.fragment),E=y(),w(P.$$.fragment),o=y(),w(d.$$.fragment),this.h()},l(p){const T=Qt("svelte-xmbmmp",document.head);n=j(T,"META",{name:!0,content:!0}),T.forEach(r),a=z(p),t=j(p,"H1",{});var q=G(t);$=m(q,"Vanishing and Exploding Gradients"),q.forEach(r),x=z(p),L=j(p,"DIV",{class:!0}),G(L).forEach(r),h=z(p),g(S.$$.fragment,p),E=z(p),g(P.$$.fragment,p),o=z(p),g(d.$$.fragment,p),this.h()},h(){document.title="Vanishing and Exploding Gradients - World4AI",Ie(n,"name","description"),Ie(n,"content","Exploding and vanishing gradients are two common problems in deep learning. By using the chain rule we constantly multiply values in order to calculate gradients. When we have many layers this multiplication procedure might lead to vanishing gradients if the values are between 0 and 1 or to exploding gradinents when the numbers are above 1."),Ie(L,"class","separator")},m(p,T){k(document.head,n),s(p,a,T),s(p,t,T),k(t,$),s(p,x,T),s(p,L,T),s(p,h,T),c(S,p,T),s(p,E,T),c(P,p,T),s(p,o,T),c(d,p,T),f=!0},p(p,[T]){const q={};T&16&&(q.$$scope={dirty:T,ctx:p}),S.$set(q);const O={};T&1&&(O.layers=p[0]),P.$set(O);const ke={};T&16&&(ke.$$scope={dirty:T,ctx:p}),d.$set(ke)},i(p){f||(v(S.$$.fragment,p),v(P.$$.fragment,p),v(d.$$.fragment,p),f=!0)},o(p){_(S.$$.fragment,p),_(P.$$.fragment,p),_(d.$$.fragment,p),f=!1},d(p){r(n),p&&r(a),p&&r(t),p&&r(x),p&&r(L),p&&r(h),b(S,p),p&&r(E),b(P,p),p&&r(o),b(d,p)}}}function St(i){return 1/(1+Math.exp(-i))}function ja(i){return St(i)*(1-St(i))}function Ga(i,n,a){let{layers:t=[{title:"Input",nodes:[{value:"x",class:"fill-white"}]},{title:"Hidden 1",nodes:[{value:"z",class:"fill-white"}]},{title:"",nodes:[{value:"a",class:"fill-white"}]},{title:"Hidden 2",nodes:[{value:"z",class:"fill-white"}]},{title:"",nodes:[{value:"a",class:"fill-white"}]},{title:"Output",nodes:[{value:"z",class:"fill-white"}]},{title:"",nodes:[{value:"a",class:"fill-white"}]},{title:"Loss",nodes:[{value:"L",class:"fill-white"}]}]}=n,$=[],x=[];for(let h=-10;h<=10;h+=.1)$.push({x:h,y:St(h)}),x.push({x:h,y:ja(h)});let L=[];for(let h=1;h<=5;h+=.1)L.push({x:h,y:.25**h});return i.$$set=h=>{"layers"in h&&a(0,t=h.layers)},[t,$,x,L]}class Ma extends Xt{constructor(n){super(),Jt(this,n,Ga,Aa,Kt,{layers:0})}}export{Ma as default};
