import{S as Ge,i as Me,s as je,y as d,z as w,A as v,g as p,d as g,B as b,Q as oe,R as le,m as I,h as l,n as E,b as $,C as Ft,a as x,c as S,V as Oe,N as T,q as k,r as y,k as A,l as q,u as vt,e as Lt,v as Ut,f as Xt,W as Yt}from"../chunks/index.4d92b023.js";import{C as Qt}from"../chunks/Container.b0705c7b.js";import{A as Ce}from"../chunks/Alert.25a852b3.js";import{H as _t}from"../chunks/Highlight.b7c1de53.js";import{L as Ne}from"../chunks/Latex.e0b308c0.js";import{R as Jt}from"../chunks/RandomAgent.2a3051e9.js";import{G as Kt,g as Zt,a as zt}from"../chunks/maps.0f079072.js";import{I as en}from"../chunks/Interaction.6c345f67.js";import{S as jt}from"../chunks/SvgContainer.f70b5745.js";import{B as Le}from"../chunks/Block.059eddcd.js";import{A as Ot}from"../chunks/Arrow.ae91874c.js";import{t as tn}from"../chunks/index.4de27e87.js";import{c as nn}from"../chunks/index.7e899070.js";import{T as Vt,a as Ct,b as Nt,R as Se,H as dt,D as wt}from"../chunks/HeaderEntry.2b6e8f51.js";function rn(o){let e,n,t,s,a,f,h,c,m;return n=new Le({props:{x:H/2,y:N+z/2,width:W,height:z,text:"Agent",fontSize:23,class:"fill-yellow-200"}}),t=new Le({props:{x:H/2,y:L-N-z/2,width:W,height:z,text:"Environment",fontSize:23,class:"fill-green-200"}}),s=new Ot({props:{data:[{x:H/2-W/2,y:L-N-z/2},{x:H/2-W/2-70,y:L-N-z/2},{x:H/2-W/2-70,y:N+z/2},{x:H/2-W/2-10,y:N+z/2}],dashed:!0,strokeWidth:2.5,strokeDashArray:"8 8",moving:!0,speed:100}}),a=new Ot({props:{data:[{x:H/2+W/2,y:N+z/2},{x:H/2+W/2+70,y:N+z/2},{x:H/2+W/2+70,y:L-N-z/2},{x:H/2+W/2+10,y:L-N-z/2}],dashed:!0,strokeWidth:2.5,strokeDashArray:"8 8",moving:!0,speed:100}}),f=new Le({props:{x:H/2+W/2+70,y:L/2,width:35,height:35,class:"fill-slate-200",type:"latex",text:"A",fontSize:22}}),h=new Le({props:{x:H/2-W/2-70-25,y:L/2,width:35,height:35,class:"fill-slate-200",type:"latex",text:"S",fontSize:22}}),c=new Le({props:{x:H/2-W/2-70+25,y:L/2,width:35,height:35,class:"fill-slate-200",type:"latex",text:"R",fontSize:22}}),{c(){e=oe("svg"),d(n.$$.fragment),d(t.$$.fragment),d(s.$$.fragment),d(a.$$.fragment),d(f.$$.fragment),d(h.$$.fragment),d(c.$$.fragment),this.h()},l(u){e=le(u,"svg",{version:!0,viewBox:!0});var _=I(e);w(n.$$.fragment,_),w(t.$$.fragment,_),w(s.$$.fragment,_),w(a.$$.fragment,_),w(f.$$.fragment,_),w(h.$$.fragment,_),w(c.$$.fragment,_),_.forEach(l),this.h()},h(){E(e,"version","1.1"),E(e,"viewBox","0 0 "+H+" "+L)},m(u,_){$(u,e,_),v(n,e,null),v(t,e,null),v(s,e,null),v(a,e,null),v(f,e,null),v(h,e,null),v(c,e,null),m=!0},p:Ft,i(u){m||(p(n.$$.fragment,u),p(t.$$.fragment,u),p(s.$$.fragment,u),p(a.$$.fragment,u),p(f.$$.fragment,u),p(h.$$.fragment,u),p(c.$$.fragment,u),m=!0)},o(u){g(n.$$.fragment,u),g(t.$$.fragment,u),g(s.$$.fragment,u),g(a.$$.fragment,u),g(f.$$.fragment,u),g(h.$$.fragment,u),g(c.$$.fragment,u),m=!1},d(u){u&&l(e),b(n),b(t),b(s),b(a),b(f),b(h),b(c)}}}function an(o){let e,n;return e=new jt({props:{maxWidth:"400px",$$slots:{default:[rn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,[s]){const a={};s&1&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}const H=400,L=400,W=160,z=60,N=5;class sn extends Ge{constructor(e){super(),Me(this,e,null,an,je,{})}}function Gt(o){let e,n;return{c(){e=oe("line"),this.h()},l(t){e=le(t,"line",{x1:!0,y1:!0,x2:!0,y2:!0,transform:!0,stroke:!0,"stroke-width":!0,"marker-end":!0}),I(e).forEach(l),this.h()},h(){E(e,"x1",o[4]),E(e,"y1",o[5]),E(e,"x2",o[6]),E(e,"y2",o[7]),E(e,"transform",n="rotate("+o[2]+", "+o[1]/2+", "+o[1]/2+")"),E(e,"stroke","black"),E(e,"stroke-width","2"),E(e,"marker-end","url(#arrowhead)")},m(t,s){$(t,e,s)},p(t,s){s&6&&n!==(n="rotate("+t[2]+", "+t[1]/2+", "+t[1]/2+")")&&E(e,"transform",n)},d(t){t&&l(e)}}}function on(o){let e,n,t,s,a,f,h,c,m=o[0]!==null&&Gt(o);return{c(){e=oe("svg"),n=oe("circle"),f=oe("defs"),h=oe("marker"),c=oe("polygon"),m&&m.c(),this.h()},l(u){e=le(u,"svg",{viewBox:!0});var _=I(e);n=le(_,"circle",{cx:!0,cy:!0,r:!0,fill:!0,stroke:!0,class:!0}),I(n).forEach(l),f=le(_,"defs",{});var R=I(f);h=le(R,"marker",{id:!0,markerWidth:!0,markerHeight:!0,refX:!0,refY:!0,orient:!0,fill:!0});var D=I(h);c=le(D,"polygon",{points:!0}),I(c).forEach(l),D.forEach(l),R.forEach(l),m&&m.l(_),_.forEach(l),this.h()},h(){E(n,"cx",t=o[1]/2),E(n,"cy",s=o[1]/2),E(n,"r",a=o[1]/2-5),E(n,"fill","none"),E(n,"stroke","black"),E(n,"class","fill-slate-300"),E(c,"points","0 0, 10 3.5, 0 7"),E(h,"id","arrowhead"),E(h,"markerWidth","10"),E(h,"markerHeight","7"),E(h,"refX","0"),E(h,"refY","3.5"),E(h,"orient","auto"),E(h,"fill","black"),E(e,"viewBox","0 0 150 150")},m(u,_){$(u,e,_),T(e,n),T(e,f),T(f,h),T(h,c),m&&m.m(e,null)},p(u,_){_&2&&t!==(t=u[1]/2)&&E(n,"cx",t),_&2&&s!==(s=u[1]/2)&&E(n,"cy",s),_&2&&a!==(a=u[1]/2-5)&&E(n,"r",a),u[0]!==null?m?m.p(u,_):(m=Gt(u),m.c(),m.m(e,null)):m&&(m.d(1),m=null)},d(u){u&&l(e),m&&m.d()}}}function ln(o){let e;return{c(){e=k("Action")},l(n){e=y(n,"Action")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function fn(o){let e,n;return e=new dt({props:{$$slots:{default:[ln]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&512&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function $n(o){let e,n;return e=new Se({props:{$$slots:{default:[fn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&512&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function un(o){let e,n;return{c(){e=A("span"),n=k(o[0]),this.h()},l(t){e=q(t,"SPAN",{class:!0});var s=I(e);n=y(s,o[0]),s.forEach(l),this.h()},h(){E(e,"class","bg-red-100 px-5 py-1 rounded-full")},m(t,s){$(t,e,s),T(e,n)},p(t,s){s&1&&vt(n,t[0])},d(t){t&&l(e)}}}function mn(o){let e,n;return e=new wt({props:{$$slots:{default:[un]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&513&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function cn(o){let e,n;return e=new Se({props:{$$slots:{default:[mn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&513&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function hn(o){let e,n,t,s;return e=new Ct({props:{$$slots:{default:[$n]},$$scope:{ctx:o}}}),t=new Nt({props:{$$slots:{default:[cn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment),n=x(),d(t.$$.fragment)},l(a){w(e.$$.fragment,a),n=S(a),w(t.$$.fragment,a)},m(a,f){v(e,a,f),$(a,n,f),v(t,a,f),s=!0},p(a,f){const h={};f&512&&(h.$$scope={dirty:f,ctx:a}),e.$set(h);const c={};f&513&&(c.$$scope={dirty:f,ctx:a}),t.$set(c)},i(a){s||(p(e.$$.fragment,a),p(t.$$.fragment,a),s=!0)},o(a){g(e.$$.fragment,a),g(t.$$.fragment,a),s=!1},d(a){b(e,a),a&&l(n),b(t,a)}}}function pn(o){let e,n,t,s;return e=new jt({props:{maxWidth:"100px",$$slots:{default:[on]},$$scope:{ctx:o}}}),t=new Vt({props:{$$slots:{default:[hn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment),n=x(),d(t.$$.fragment)},l(a){w(e.$$.fragment,a),n=S(a),w(t.$$.fragment,a)},m(a,f){v(e,a,f),$(a,n,f),v(t,a,f),s=!0},p(a,[f]){const h={};f&519&&(h.$$scope={dirty:f,ctx:a}),e.$set(h);const c={};f&513&&(c.$$scope={dirty:f,ctx:a}),t.$set(c)},i(a){s||(p(e.$$.fragment,a),p(t.$$.fragment,a),s=!0)},o(a){g(e.$$.fragment,a),g(t.$$.fragment,a),s=!1},d(a){b(e,a),a&&l(n),b(t,a)}}}function gn(o,e,n){let t;const s=tn(0,{duration:200,easing:nn});Oe(o,s,R=>n(2,t=R));let{action:a}=e,{size:f=150}=e,h={0:270,1:0,2:90,3:180},c=f*.2,m=f/2,u=f*.8,_=f/2;return o.$$set=R=>{"action"in R&&n(0,a=R.action),"size"in R&&n(1,f=R.size)},o.$$.update=()=>{o.$$.dirty&1&&a!=null&&s.set(h[a])},[a,f,t,s,c,m,u,_]}class _n extends Ge{constructor(e){super(),Me(this,e,gn,pn,je,{action:0,size:1})}}function dn(o){let e;return{c(){e=k("Column")},l(n){e=y(n,"Column")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function wn(o){let e;return{c(){e=k("Row")},l(n){e=y(n,"Row")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function vn(o){let e,n,t,s;return e=new dt({props:{$$slots:{default:[dn]},$$scope:{ctx:o}}}),t=new dt({props:{$$slots:{default:[wn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment),n=x(),d(t.$$.fragment)},l(a){w(e.$$.fragment,a),n=S(a),w(t.$$.fragment,a)},m(a,f){v(e,a,f),$(a,n,f),v(t,a,f),s=!0},p(a,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:a}),e.$set(h);const c={};f&2&&(c.$$scope={dirty:f,ctx:a}),t.$set(c)},i(a){s||(p(e.$$.fragment,a),p(t.$$.fragment,a),s=!0)},o(a){g(e.$$.fragment,a),g(t.$$.fragment,a),s=!1},d(a){b(e,a),a&&l(n),b(t,a)}}}function bn(o){let e,n;return e=new Se({props:{$$slots:{default:[vn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&2&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function Mt(o){let e,n;return e=new Se({props:{$$slots:{default:[En]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&3&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function kn(o){let e,n=o[0].c+"",t;return{c(){e=A("span"),t=k(n),this.h()},l(s){e=q(s,"SPAN",{class:!0});var a=I(e);t=y(a,n),a.forEach(l),this.h()},h(){E(e,"class","inline-block bg-red-100 px-5 py-1 rounded-full")},m(s,a){$(s,e,a),T(e,t)},p(s,a){a&1&&n!==(n=s[0].c+"")&&vt(t,n)},d(s){s&&l(e)}}}function yn(o){let e,n=o[0].r+"",t;return{c(){e=A("span"),t=k(n),this.h()},l(s){e=q(s,"SPAN",{class:!0});var a=I(e);t=y(a,n),a.forEach(l),this.h()},h(){E(e,"class","inline-block bg-blue-100 px-5 py-1 rounded-full")},m(s,a){$(s,e,a),T(e,t)},p(s,a){a&1&&n!==(n=s[0].r+"")&&vt(t,n)},d(s){s&&l(e)}}}function En(o){let e,n,t,s;return e=new wt({props:{$$slots:{default:[kn]},$$scope:{ctx:o}}}),t=new wt({props:{$$slots:{default:[yn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment),n=x(),d(t.$$.fragment)},l(a){w(e.$$.fragment,a),n=S(a),w(t.$$.fragment,a)},m(a,f){v(e,a,f),$(a,n,f),v(t,a,f),s=!0},p(a,f){const h={};f&3&&(h.$$scope={dirty:f,ctx:a}),e.$set(h);const c={};f&3&&(c.$$scope={dirty:f,ctx:a}),t.$set(c)},i(a){s||(p(e.$$.fragment,a),p(t.$$.fragment,a),s=!0)},o(a){g(e.$$.fragment,a),g(t.$$.fragment,a),s=!1},d(a){b(e,a),a&&l(n),b(t,a)}}}function xn(o){let e,n,t=o[0]&&Mt(o);return{c(){t&&t.c(),e=Lt()},l(s){t&&t.l(s),e=Lt()},m(s,a){t&&t.m(s,a),$(s,e,a),n=!0},p(s,a){s[0]?t?(t.p(s,a),a&1&&p(t,1)):(t=Mt(s),t.c(),p(t,1),t.m(e.parentNode,e)):t&&(Ut(),g(t,1,1,()=>{t=null}),Xt())},i(s){n||(p(t),n=!0)},o(s){g(t),n=!1},d(s){t&&t.d(s),s&&l(e)}}}function Sn(o){let e,n,t,s;return e=new Ct({props:{$$slots:{default:[bn]},$$scope:{ctx:o}}}),t=new Nt({props:{$$slots:{default:[xn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment),n=x(),d(t.$$.fragment)},l(a){w(e.$$.fragment,a),n=S(a),w(t.$$.fragment,a)},m(a,f){v(e,a,f),$(a,n,f),v(t,a,f),s=!0},p(a,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:a}),e.$set(h);const c={};f&3&&(c.$$scope={dirty:f,ctx:a}),t.$set(c)},i(a){s||(p(e.$$.fragment,a),p(t.$$.fragment,a),s=!0)},o(a){g(e.$$.fragment,a),g(t.$$.fragment,a),s=!1},d(a){b(e,a),a&&l(n),b(t,a)}}}function In(o){let e,n;return e=new Vt({props:{$$slots:{default:[Sn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,[s]){const a={};s&3&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function Tn(o,e,n){let{state:t}=e;return o.$$set=s=>{"state"in s&&n(0,t=s.state)},[t]}class An extends Ge{constructor(e){super(),Me(this,e,Tn,In,je,{state:0})}}function qn(o){let e;return{c(){e=k("Reward")},l(n){e=y(n,"Reward")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Rn(o){let e,n;return e=new dt({props:{$$slots:{default:[qn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&2&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function Pn(o){let e,n;return e=new Se({props:{$$slots:{default:[Rn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&2&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function Dn(o){let e,n;return{c(){e=A("span"),n=k(o[0]),this.h()},l(t){e=q(t,"SPAN",{class:!0});var s=I(e);n=y(s,o[0]),s.forEach(l),this.h()},h(){E(e,"class","bg-red-100 px-5 py-1 rounded-full")},m(t,s){$(t,e,s),T(e,n)},p(t,s){s&1&&vt(n,t[0])},d(t){t&&l(e)}}}function Bn(o){let e,n;return e=new wt({props:{$$slots:{default:[Dn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&3&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function Hn(o){let e,n;return e=new Se({props:{$$slots:{default:[Bn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,s){const a={};s&3&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function Wn(o){let e,n,t,s;return e=new Ct({props:{$$slots:{default:[Pn]},$$scope:{ctx:o}}}),t=new Nt({props:{$$slots:{default:[Hn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment),n=x(),d(t.$$.fragment)},l(a){w(e.$$.fragment,a),n=S(a),w(t.$$.fragment,a)},m(a,f){v(e,a,f),$(a,n,f),v(t,a,f),s=!0},p(a,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:a}),e.$set(h);const c={};f&3&&(c.$$scope={dirty:f,ctx:a}),t.$set(c)},i(a){s||(p(e.$$.fragment,a),p(t.$$.fragment,a),s=!0)},o(a){g(e.$$.fragment,a),g(t.$$.fragment,a),s=!1},d(a){b(e,a),a&&l(n),b(t,a)}}}function zn(o){let e,n;return e=new Vt({props:{$$slots:{default:[Wn]},$$scope:{ctx:o}}}),{c(){d(e.$$.fragment)},l(t){w(e.$$.fragment,t)},m(t,s){v(e,t,s),n=!0},p(t,[s]){const a={};s&3&&(a.$$scope={dirty:s,ctx:t}),e.$set(a)},i(t){n||(p(e.$$.fragment,t),n=!0)},o(t){g(e.$$.fragment,t),n=!1},d(t){b(e,t)}}}function Vn(o,e,n){let{reward:t}=e;return o.$$set=s=>{"reward"in s&&n(0,t=s.reward)},[t]}class Cn extends Ge{constructor(e){super(),Me(this,e,Vn,zn,je,{reward:0})}}function Nn(o){let e;return{c(){e=k(`In reinforcement learning the sequential information flow between the agent
    and the environment is called interaction.`)},l(n){e=y(n,`In reinforcement learning the sequential information flow between the agent
    and the environment is called interaction.`)},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Ln(o){let e;return{c(){e=k("timestep")},l(n){e=y(n,"timestep")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function On(o){let e;return{c(){e=k("states")},l(n){e=y(n,"states")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Gn(o){let e;return{c(){e=k("actions")},l(n){e=y(n,"actions")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Mn(o){let e;return{c(){e=k("rewards")},l(n){e=y(n,"rewards")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function jn(o){let e,n,t,s,a,f,h,c;return n=new _t({props:{$$slots:{default:[On]},$$scope:{ctx:o}}}),s=new _t({props:{$$slots:{default:[Gn]},$$scope:{ctx:o}}}),f=new _t({props:{$$slots:{default:[Mn]},$$scope:{ctx:o}}}),{c(){e=k(`In reinforcement learning there are just 3 types of data that need to be
    send between the agent and the environment: `),d(n.$$.fragment),t=k(`,
    `),d(s.$$.fragment),a=k(`
    and `),d(f.$$.fragment),h=k(".")},l(m){e=y(m,`In reinforcement learning there are just 3 types of data that need to be
    send between the agent and the environment: `),w(n.$$.fragment,m),t=y(m,`,
    `),w(s.$$.fragment,m),a=y(m,`
    and `),w(f.$$.fragment,m),h=y(m,".")},m(m,u){$(m,e,u),v(n,m,u),$(m,t,u),v(s,m,u),$(m,a,u),v(f,m,u),$(m,h,u),c=!0},p(m,u){const _={};u&32768&&(_.$$scope={dirty:u,ctx:m}),n.$set(_);const R={};u&32768&&(R.$$scope={dirty:u,ctx:m}),s.$set(R);const D={};u&32768&&(D.$$scope={dirty:u,ctx:m}),f.$set(D)},i(m){c||(p(n.$$.fragment,m),p(s.$$.fragment,m),p(f.$$.fragment,m),c=!0)},o(m){g(n.$$.fragment,m),g(s.$$.fragment,m),g(f.$$.fragment,m),c=!1},d(m){m&&l(e),b(n,m),m&&l(t),b(s,m),m&&l(a),b(f,m),m&&l(h)}}}function Fn(o){let e;return{c(){e=k("S_0")},l(n){e=y(n,"S_0")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Un(o){let e;return{c(){e=k("A_0")},l(n){e=y(n,"A_0")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Xn(o){let e;return{c(){e=k("S_1")},l(n){e=y(n,"S_1")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Yn(o){let e;return{c(){e=k("R_1")},l(n){e=y(n,"R_1")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Qn(o){let e;return{c(){e=k("A_1")},l(n){e=y(n,"A_1")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Jn(o){let e;return{c(){e=k("The state is the representation of the current condition of the environment.")},l(n){e=y(n,"The state is the representation of the current condition of the environment.")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Kn(o){let e;return{c(){e=k("The action is the representation of the decision of the agent.")},l(n){e=y(n,"The action is the representation of the decision of the agent.")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function Zn(o){let e;return{c(){e=k("The reward is the scalar signal to reinforce certain behaviour of the agent.")},l(n){e=y(n,"The reward is the scalar signal to reinforce certain behaviour of the agent.")},m(n,t){$(n,e,t)},d(n){n&&l(e)}}}function er(o){let e,n,t,s,a,f,h,c,m,u,_,R,D,te,P,fe,V,C,O,bt,G,kt,M,yt,j,Et,Fe,ie,Ue,$e,xt,Xe,F,Ye,ue,St,Qe,me,It,Je,ne,U,Ke,X,Ze,ce,Tt,et,he,tt,pe,At,nt,Y,rt,ge,qt,at,_e,Rt,st,re,Q,ot,J,lt,de,ft,we,Pt,it,K,$t,ve,Dt,ut,be,Bt,mt,ke,Ht,ct,ae,Z,ht,ee,pt,ye,gt;return s=new Ce({props:{type:"info",$$slots:{default:[Nn]},$$scope:{ctx:o}}}),c=new _t({props:{$$slots:{default:[Ln]},$$scope:{ctx:o}}}),_=new Ce({props:{type:"info",$$slots:{default:[jn]},$$scope:{ctx:o}}}),D=new sn({}),V=new Ne({props:{$$slots:{default:[Fn]},$$scope:{ctx:o}}}),O=new Ne({props:{$$slots:{default:[Un]},$$scope:{ctx:o}}}),G=new Ne({props:{$$slots:{default:[Xn]},$$scope:{ctx:o}}}),M=new Ne({props:{$$slots:{default:[Yn]},$$scope:{ctx:o}}}),j=new Ne({props:{$$slots:{default:[Qn]},$$scope:{ctx:o}}}),F=new Ce({props:{type:"info",$$slots:{default:[Jn]},$$scope:{ctx:o}}}),U=new An({props:{state:o[2]}}),X=new zt({props:{cells:o[3],player:o[2]}}),Y=new Ce({props:{type:"info",$$slots:{default:[Kn]},$$scope:{ctx:o}}}),Q=new _n({props:{action:o[1]}}),J=new zt({props:{cells:o[3],player:o[2]}}),K=new Ce({props:{type:"info",$$slots:{default:[Zn]},$$scope:{ctx:o}}}),Z=new Cn({props:{reward:o[0]}}),ee=new zt({props:{cells:o[3],player:o[2]}}),{c(){e=A("p"),n=k(`In reinforcement learning the agent and the environment interact with each
    other. In this context interaction means that signals flows sequentially
    between the two. The agent and the environment interact continuously, each
    reacting to the data sent by the other.`),t=x(),d(s.$$.fragment),a=x(),f=A("p"),h=k(`It is important to understand that this stream of data is exchanged in a
    strictly sequential way. When the environment sends a signal for example, it
    has to wait until it receives the response signal from the agent. Only then
    can the environment generate a new batch of data. Reinforcement learning
    works in discrete timesteps. Each iteration where the environment and the
    agent exchanged their data constitutes a `),d(c.$$.fragment),m=k("."),u=x(),d(_.$$.fragment),R=x(),d(D.$$.fragment),te=x(),P=A("p"),fe=k(`The interaction cycle starts with the the agent receiving the initial state
    `),d(V.$$.fragment),C=k(`
    from the environment. Based on that state the agent generates the action `),d(O.$$.fragment),bt=k(` it would like to take, which is transmitted to the environment. The environment
    transitions into the new state`),d(G.$$.fragment),kt=k(" and calculates the reward "),d(M.$$.fragment),yt=k(`. The new state and the reward are finally transmitted to the agent. The
    agent can use the reward as a feedback to learn, while the new state is used
    to generates the action `),d(j.$$.fragment),Et=k(` and the cycle keeps repeating, ponentially
    forever.`),Fe=x(),ie=A("div"),Ue=x(),$e=A("h2"),xt=k("State"),Xe=x(),d(F.$$.fragment),Ye=x(),ue=A("p"),St=k(`The state describes how the environment actually looks like. It is the
    condition that the agent is facing and the one parameter that the agent
    bases its decisions on.`),Qe=x(),me=A("p"),It=k(`In our simple gridworld example all the agent needs to know to make the
    decisions is the location of the circle in the environment. In the starting
    position the state would be row=0 and column=0. The state to the right of
    the starting position would be row equals to 0 and column equals to 1,
    meaning (0, 1). Based on the position the agent can choose the path towards
    the triangle.`),Je=x(),ne=A("div"),d(U.$$.fragment),Ke=x(),d(X.$$.fragment),Ze=x(),ce=A("p"),Tt=k(`This is not the only way to represent the the state of the environment. The
    state can be represented by a scalar, a vector, a matrix or a tensor and can
    be either discrete or continuous. In future chapters we will see more
    complex environments and learn how to deal with those. For now it is
    sufficient to know what role the state plays in the action-environment
    interaction.`),et=x(),he=A("div"),tt=x(),pe=A("h2"),At=k("Action"),nt=x(),d(Y.$$.fragment),rt=x(),ge=A("p"),qt=k(`The action is the behaviour the agent chooses based on the state of the
    environment. Like the state the action can be a scalar, a vector, a matrix
    or a tensor of discrete or continuous values.`),at=x(),_e=A("p"),Rt=k(`In the gridworld example the agent can move north, east, south and west.
    Each action is encoded by a discrete scalar value, where north equals 0,
    east equals 1, south equals 2 and west equals 3.`),st=x(),re=A("div"),d(Q.$$.fragment),ot=x(),d(J.$$.fragment),lt=x(),de=A("div"),ft=x(),we=A("h2"),Pt=k("Reward"),it=x(),d(K.$$.fragment),$t=x(),ve=A("p"),Dt=k(`The reward is what the agent receives from the environment for an action. It
    is the value that the environment uses to reinforce a behaviour and it is
    the value that the agent uses to improve it's behaviour.`),ut=x(),be=A("p"),Bt=k(`Unlike the action or the state the reward has to be a scalar, one single
    number, it is not possible for the reward to be a vector, matrix or tensor.
    As expected larger numbers represent larger or better rewards so that the
    reward of 1 is higher than the reward of -1.`),mt=x(),ke=A("p"),Ht=k(`In this gridworld example the agent receives a reward of -1 for each step
    taken with the exception of reaching the triangle, where the agent receives
    a reward of +1.`),ct=x(),ae=A("div"),d(Z.$$.fragment),ht=x(),d(ee.$$.fragment),pt=x(),ye=A("div"),this.h()},l(r){e=q(r,"P",{});var i=I(e);n=y(i,`In reinforcement learning the agent and the environment interact with each
    other. In this context interaction means that signals flows sequentially
    between the two. The agent and the environment interact continuously, each
    reacting to the data sent by the other.`),i.forEach(l),t=S(r),w(s.$$.fragment,r),a=S(r),f=q(r,"P",{});var se=I(f);h=y(se,`It is important to understand that this stream of data is exchanged in a
    strictly sequential way. When the environment sends a signal for example, it
    has to wait until it receives the response signal from the agent. Only then
    can the environment generate a new batch of data. Reinforcement learning
    works in discrete timesteps. Each iteration where the environment and the
    agent exchanged their data constitutes a `),w(c.$$.fragment,se),m=y(se,"."),se.forEach(l),u=S(r),w(_.$$.fragment,r),R=S(r),w(D.$$.fragment,r),te=S(r),P=q(r,"P",{});var B=I(P);fe=y(B,`The interaction cycle starts with the the agent receiving the initial state
    `),w(V.$$.fragment,B),C=y(B,`
    from the environment. Based on that state the agent generates the action `),w(O.$$.fragment,B),bt=y(B,` it would like to take, which is transmitted to the environment. The environment
    transitions into the new state`),w(G.$$.fragment,B),kt=y(B," and calculates the reward "),w(M.$$.fragment,B),yt=y(B,`. The new state and the reward are finally transmitted to the agent. The
    agent can use the reward as a feedback to learn, while the new state is used
    to generates the action `),w(j.$$.fragment,B),Et=y(B,` and the cycle keeps repeating, ponentially
    forever.`),B.forEach(l),Fe=S(r),ie=q(r,"DIV",{class:!0}),I(ie).forEach(l),Ue=S(r),$e=q(r,"H2",{});var Ie=I($e);xt=y(Ie,"State"),Ie.forEach(l),Xe=S(r),w(F.$$.fragment,r),Ye=S(r),ue=q(r,"P",{});var Te=I(ue);St=y(Te,`The state describes how the environment actually looks like. It is the
    condition that the agent is facing and the one parameter that the agent
    bases its decisions on.`),Te.forEach(l),Qe=S(r),me=q(r,"P",{});var Ae=I(me);It=y(Ae,`In our simple gridworld example all the agent needs to know to make the
    decisions is the location of the circle in the environment. In the starting
    position the state would be row=0 and column=0. The state to the right of
    the starting position would be row equals to 0 and column equals to 1,
    meaning (0, 1). Based on the position the agent can choose the path towards
    the triangle.`),Ae.forEach(l),Je=S(r),ne=q(r,"DIV",{class:!0});var qe=I(ne);w(U.$$.fragment,qe),qe.forEach(l),Ke=S(r),w(X.$$.fragment,r),Ze=S(r),ce=q(r,"P",{});var Re=I(ce);Tt=y(Re,`This is not the only way to represent the the state of the environment. The
    state can be represented by a scalar, a vector, a matrix or a tensor and can
    be either discrete or continuous. In future chapters we will see more
    complex environments and learn how to deal with those. For now it is
    sufficient to know what role the state plays in the action-environment
    interaction.`),Re.forEach(l),et=S(r),he=q(r,"DIV",{class:!0}),I(he).forEach(l),tt=S(r),pe=q(r,"H2",{});var Pe=I(pe);At=y(Pe,"Action"),Pe.forEach(l),nt=S(r),w(Y.$$.fragment,r),rt=S(r),ge=q(r,"P",{});var De=I(ge);qt=y(De,`The action is the behaviour the agent chooses based on the state of the
    environment. Like the state the action can be a scalar, a vector, a matrix
    or a tensor of discrete or continuous values.`),De.forEach(l),at=S(r),_e=q(r,"P",{});var Be=I(_e);Rt=y(Be,`In the gridworld example the agent can move north, east, south and west.
    Each action is encoded by a discrete scalar value, where north equals 0,
    east equals 1, south equals 2 and west equals 3.`),Be.forEach(l),st=S(r),re=q(r,"DIV",{class:!0});var Ee=I(re);w(Q.$$.fragment,Ee),Ee.forEach(l),ot=S(r),w(J.$$.fragment,r),lt=S(r),de=q(r,"DIV",{class:!0}),I(de).forEach(l),ft=S(r),we=q(r,"H2",{});var He=I(we);Pt=y(He,"Reward"),He.forEach(l),it=S(r),w(K.$$.fragment,r),$t=S(r),ve=q(r,"P",{});var We=I(ve);Dt=y(We,`The reward is what the agent receives from the environment for an action. It
    is the value that the environment uses to reinforce a behaviour and it is
    the value that the agent uses to improve it's behaviour.`),We.forEach(l),ut=S(r),be=q(r,"P",{});var xe=I(be);Bt=y(xe,`Unlike the action or the state the reward has to be a scalar, one single
    number, it is not possible for the reward to be a vector, matrix or tensor.
    As expected larger numbers represent larger or better rewards so that the
    reward of 1 is higher than the reward of -1.`),xe.forEach(l),mt=S(r),ke=q(r,"P",{});var ze=I(ke);Ht=y(ze,`In this gridworld example the agent receives a reward of -1 for each step
    taken with the exception of reaching the triangle, where the agent receives
    a reward of +1.`),ze.forEach(l),ct=S(r),ae=q(r,"DIV",{class:!0});var Ve=I(ae);w(Z.$$.fragment,Ve),Ve.forEach(l),ht=S(r),w(ee.$$.fragment,r),pt=S(r),ye=q(r,"DIV",{class:!0}),I(ye).forEach(l),this.h()},h(){E(ie,"class","separator"),E(ne,"class","mx-auto mb-4 max-w-sm"),E(he,"class","separator"),E(re,"class","mx-auto mb-4 max-w-xs"),E(de,"class","separator"),E(ae,"class","mx-auto mb-4 max-w-xs"),E(ye,"class","separator")},m(r,i){$(r,e,i),T(e,n),$(r,t,i),v(s,r,i),$(r,a,i),$(r,f,i),T(f,h),v(c,f,null),T(f,m),$(r,u,i),v(_,r,i),$(r,R,i),v(D,r,i),$(r,te,i),$(r,P,i),T(P,fe),v(V,P,null),T(P,C),v(O,P,null),T(P,bt),v(G,P,null),T(P,kt),v(M,P,null),T(P,yt),v(j,P,null),T(P,Et),$(r,Fe,i),$(r,ie,i),$(r,Ue,i),$(r,$e,i),T($e,xt),$(r,Xe,i),v(F,r,i),$(r,Ye,i),$(r,ue,i),T(ue,St),$(r,Qe,i),$(r,me,i),T(me,It),$(r,Je,i),$(r,ne,i),v(U,ne,null),$(r,Ke,i),v(X,r,i),$(r,Ze,i),$(r,ce,i),T(ce,Tt),$(r,et,i),$(r,he,i),$(r,tt,i),$(r,pe,i),T(pe,At),$(r,nt,i),v(Y,r,i),$(r,rt,i),$(r,ge,i),T(ge,qt),$(r,at,i),$(r,_e,i),T(_e,Rt),$(r,st,i),$(r,re,i),v(Q,re,null),$(r,ot,i),v(J,r,i),$(r,lt,i),$(r,de,i),$(r,ft,i),$(r,we,i),T(we,Pt),$(r,it,i),v(K,r,i),$(r,$t,i),$(r,ve,i),T(ve,Dt),$(r,ut,i),$(r,be,i),T(be,Bt),$(r,mt,i),$(r,ke,i),T(ke,Ht),$(r,ct,i),$(r,ae,i),v(Z,ae,null),$(r,ht,i),v(ee,r,i),$(r,pt,i),$(r,ye,i),gt=!0},p(r,i){const se={};i&32768&&(se.$$scope={dirty:i,ctx:r}),s.$set(se);const B={};i&32768&&(B.$$scope={dirty:i,ctx:r}),c.$set(B);const Ie={};i&32768&&(Ie.$$scope={dirty:i,ctx:r}),_.$set(Ie);const Te={};i&32768&&(Te.$$scope={dirty:i,ctx:r}),V.$set(Te);const Ae={};i&32768&&(Ae.$$scope={dirty:i,ctx:r}),O.$set(Ae);const qe={};i&32768&&(qe.$$scope={dirty:i,ctx:r}),G.$set(qe);const Re={};i&32768&&(Re.$$scope={dirty:i,ctx:r}),M.$set(Re);const Pe={};i&32768&&(Pe.$$scope={dirty:i,ctx:r}),j.$set(Pe);const De={};i&32768&&(De.$$scope={dirty:i,ctx:r}),F.$set(De);const Be={};i&4&&(Be.state=r[2]),U.$set(Be);const Ee={};i&8&&(Ee.cells=r[3]),i&4&&(Ee.player=r[2]),X.$set(Ee);const He={};i&32768&&(He.$$scope={dirty:i,ctx:r}),Y.$set(He);const We={};i&2&&(We.action=r[1]),Q.$set(We);const xe={};i&8&&(xe.cells=r[3]),i&4&&(xe.player=r[2]),J.$set(xe);const ze={};i&32768&&(ze.$$scope={dirty:i,ctx:r}),K.$set(ze);const Ve={};i&1&&(Ve.reward=r[0]),Z.$set(Ve);const Wt={};i&8&&(Wt.cells=r[3]),i&4&&(Wt.player=r[2]),ee.$set(Wt)},i(r){gt||(p(s.$$.fragment,r),p(c.$$.fragment,r),p(_.$$.fragment,r),p(D.$$.fragment,r),p(V.$$.fragment,r),p(O.$$.fragment,r),p(G.$$.fragment,r),p(M.$$.fragment,r),p(j.$$.fragment,r),p(F.$$.fragment,r),p(U.$$.fragment,r),p(X.$$.fragment,r),p(Y.$$.fragment,r),p(Q.$$.fragment,r),p(J.$$.fragment,r),p(K.$$.fragment,r),p(Z.$$.fragment,r),p(ee.$$.fragment,r),gt=!0)},o(r){g(s.$$.fragment,r),g(c.$$.fragment,r),g(_.$$.fragment,r),g(D.$$.fragment,r),g(V.$$.fragment,r),g(O.$$.fragment,r),g(G.$$.fragment,r),g(M.$$.fragment,r),g(j.$$.fragment,r),g(F.$$.fragment,r),g(U.$$.fragment,r),g(X.$$.fragment,r),g(Y.$$.fragment,r),g(Q.$$.fragment,r),g(J.$$.fragment,r),g(K.$$.fragment,r),g(Z.$$.fragment,r),g(ee.$$.fragment,r),gt=!1},d(r){r&&l(e),r&&l(t),b(s,r),r&&l(a),r&&l(f),b(c),r&&l(u),b(_,r),r&&l(R),b(D,r),r&&l(te),r&&l(P),b(V),b(O),b(G),b(M),b(j),r&&l(Fe),r&&l(ie),r&&l(Ue),r&&l($e),r&&l(Xe),b(F,r),r&&l(Ye),r&&l(ue),r&&l(Qe),r&&l(me),r&&l(Je),r&&l(ne),b(U),r&&l(Ke),b(X,r),r&&l(Ze),r&&l(ce),r&&l(et),r&&l(he),r&&l(tt),r&&l(pe),r&&l(nt),b(Y,r),r&&l(rt),r&&l(ge),r&&l(at),r&&l(_e),r&&l(st),r&&l(re),b(Q),r&&l(ot),b(J,r),r&&l(lt),r&&l(de),r&&l(ft),r&&l(we),r&&l(it),b(K,r),r&&l($t),r&&l(ve),r&&l(ut),r&&l(be),r&&l(mt),r&&l(ke),r&&l(ct),r&&l(ae),b(Z),r&&l(ht),b(ee,r),r&&l(pt),r&&l(ye)}}}function tr(o){let e,n,t,s,a,f,h,c,m;return c=new Qt({props:{$$slots:{default:[er]},$$scope:{ctx:o}}}),{c(){e=A("meta"),n=x(),t=A("h1"),s=k("States, Actions, Rewards"),a=x(),f=A("div"),h=x(),d(c.$$.fragment),this.h()},l(u){const _=Yt("svelte-1epgo2s",document.head);e=q(_,"META",{name:!0,content:!0}),_.forEach(l),n=S(u),t=q(u,"H1",{});var R=I(t);s=y(R,"States, Actions, Rewards"),R.forEach(l),a=S(u),f=q(u,"DIV",{class:!0}),I(f).forEach(l),h=S(u),w(c.$$.fragment,u),this.h()},h(){document.title="States, Actions, Rewards - World4AI",E(e,"name","description"),E(e,"content","In reinforcement learning the agent and the environment exchange three distinct signals between each other: the state, the action and reward."),E(f,"class","separator")},m(u,_){T(document.head,e),$(u,n,_),$(u,t,_),T(t,s),$(u,a,_),$(u,f,_),$(u,h,_),v(c,u,_),m=!0},p(u,[_]){const R={};_&32783&&(R.$$scope={dirty:_,ctx:u}),c.$set(R)},i(u){m||(p(c.$$.fragment,u),m=!0)},o(u){g(c.$$.fragment,u),m=!1},d(u){l(e),u&&l(n),u&&l(t),u&&l(a),u&&l(f),u&&l(h),b(c,u)}}}function nr(o,e,n){let t,s,a,f,h,c,m,u,_=new Kt(Zt),R=new Jt(_.getObservationSpace(),_.getActionSpace()),D=new en(R,_,2),te=_.cellsStore;Oe(o,te,C=>n(11,u=C));let P=D.observationStore;Oe(o,P,C=>n(10,m=C));let fe=D.actionStore;Oe(o,fe,C=>n(9,c=C));let V=D.rewardStore;return Oe(o,V,C=>n(8,h=C)),o.$$.update=()=>{o.$$.dirty&2048&&n(3,t=u),o.$$.dirty&1024&&n(2,s=m),o.$$.dirty&512&&n(1,a=c),o.$$.dirty&256&&n(0,f=h)},[f,a,s,t,te,P,fe,V,h,c,m,u]}class _r extends Ge{constructor(e){super(),Me(this,e,nr,tr,je,{})}}export{_r as default};
