import{S as B,i as N,s as j,k as I,a as w,y as E,q as $,W as F,l as P,h as r,c as k,z as T,m as q,r as d,n as H,N as D,b as s,A as z,g as A,d as W,B as C}from"../chunks/index.4d92b023.js";import{C as G}from"../chunks/Container.b0705c7b.js";import{H as S}from"../chunks/Highlight.b7c1de53.js";import{A as V}from"../chunks/Alert.25a852b3.js";function J(h){let t;return{c(){t=$("The whole chapter is very early work in progress.")},l(a){t=d(a,"The whole chapter is very early work in progress.")},m(a,i){s(a,t,i)},d(a){a&&r(t)}}}function K(h){let t;return{c(){t=$("dynamic programming")},l(a){t=d(a,"dynamic programming")},m(a,i){s(a,t,i)},d(a){a&&r(t)}}}function L(h){let t;return{c(){t=$("planning")},l(a){t=d(a,"planning")},m(a,i){s(a,t,i)},d(a){a&&r(t)}}}function O(h){let t;return{c(){t=$("Planning utilizes a model of the environment to improve a policy.")},l(a){t=d(a,"Planning utilizes a model of the environment to improve a policy.")},m(a,i){s(a,t,i)},d(a){a&&r(t)}}}function Q(h){let t,a,i,_,m,y,g,f,u,c,v,e,o,p;return i=new S({props:{$$slots:{default:[K]},$$scope:{ctx:h}}}),m=new S({props:{$$slots:{default:[L]},$$scope:{ctx:h}}}),f=new V({props:{type:"info",$$slots:{default:[O]},$$scope:{ctx:h}}}),{c(){t=I("p"),a=$("The algorithms that we are going to cover in this section are known as "),E(i.$$.fragment),_=$(`. Dynamic programming is not commonly used to solve reinforcement learning
    tasks. In fact there is no learning involved at all. Instead of interacting
    with the environment to find an optimal policy, dynamic programming utilizes `),E(m.$$.fragment),y=$("."),g=w(),E(f.$$.fragment),u=w(),c=I("p"),v=$(`Dynamic programming requires the full knowledge of the model of the
    environment and calculates the optimal value function and optimal policy
    through the knowledge of that model. The interaction between the agent and
    the environment is not necessary. While the access to the model of the
    environment is an unrealistic assumtion, the knowledge tha you will gain by
    studying dynamic programming algorithms is transferable to reinforcement
    learning.`),e=w(),o=I("div"),this.h()},l(n){t=P(n,"P",{});var l=q(t);a=d(l,"The algorithms that we are going to cover in this section are known as "),T(i.$$.fragment,l),_=d(l,`. Dynamic programming is not commonly used to solve reinforcement learning
    tasks. In fact there is no learning involved at all. Instead of interacting
    with the environment to find an optimal policy, dynamic programming utilizes `),T(m.$$.fragment,l),y=d(l,"."),l.forEach(r),g=k(n),T(f.$$.fragment,n),u=k(n),c=P(n,"P",{});var b=q(c);v=d(b,`Dynamic programming requires the full knowledge of the model of the
    environment and calculates the optimal value function and optimal policy
    through the knowledge of that model. The interaction between the agent and
    the environment is not necessary. While the access to the model of the
    environment is an unrealistic assumtion, the knowledge tha you will gain by
    studying dynamic programming algorithms is transferable to reinforcement
    learning.`),b.forEach(r),e=k(n),o=P(n,"DIV",{class:!0}),q(o).forEach(r),this.h()},h(){H(o,"class","separator")},m(n,l){s(n,t,l),D(t,a),z(i,t,null),D(t,_),z(m,t,null),D(t,y),s(n,g,l),z(f,n,l),s(n,u,l),s(n,c,l),D(c,v),s(n,e,l),s(n,o,l),p=!0},p(n,l){const b={};l&1&&(b.$$scope={dirty:l,ctx:n}),i.$set(b);const x={};l&1&&(x.$$scope={dirty:l,ctx:n}),m.$set(x);const M={};l&1&&(M.$$scope={dirty:l,ctx:n}),f.$set(M)},i(n){p||(A(i.$$.fragment,n),A(m.$$.fragment,n),A(f.$$.fragment,n),p=!0)},o(n){W(i.$$.fragment,n),W(m.$$.fragment,n),W(f.$$.fragment,n),p=!1},d(n){n&&r(t),C(i),C(m),n&&r(g),C(f,n),n&&r(u),n&&r(c),n&&r(e),n&&r(o)}}}function R(h){let t,a,i,_,m,y,g,f,u,c,v;return i=new V({props:{type:"danger",$$slots:{default:[J]},$$scope:{ctx:h}}}),c=new G({props:{$$slots:{default:[Q]},$$scope:{ctx:h}}}),{c(){t=I("meta"),a=w(),E(i.$$.fragment),_=w(),m=I("h1"),y=$("Dynamic Programming"),g=w(),f=I("div"),u=w(),E(c.$$.fragment),this.h()},l(e){const o=F("svelte-1hw7pms",document.head);t=P(o,"META",{name:!0,content:!0}),o.forEach(r),a=k(e),T(i.$$.fragment,e),_=k(e),m=P(e,"H1",{});var p=q(m);y=d(p,"Dynamic Programming"),p.forEach(r),g=k(e),f=P(e,"DIV",{class:!0}),q(f).forEach(r),u=k(e),T(c.$$.fragment,e),this.h()},h(){document.title="Dynamic Programming - World4AI",H(t,"name","description"),H(t,"content","In dynamic programming we have access to the model of the finite Markov decision process and can use iterative planning techniques to find the optimal value function and optimal policy."),H(f,"class","separator")},m(e,o){D(document.head,t),s(e,a,o),z(i,e,o),s(e,_,o),s(e,m,o),D(m,y),s(e,g,o),s(e,f,o),s(e,u,o),z(c,e,o),v=!0},p(e,[o]){const p={};o&1&&(p.$$scope={dirty:o,ctx:e}),i.$set(p);const n={};o&1&&(n.$$scope={dirty:o,ctx:e}),c.$set(n)},i(e){v||(A(i.$$.fragment,e),A(c.$$.fragment,e),v=!0)},o(e){W(i.$$.fragment,e),W(c.$$.fragment,e),v=!1},d(e){r(t),e&&r(a),C(i,e),e&&r(_),e&&r(m),e&&r(g),e&&r(f),e&&r(u),C(c,e)}}}class ee extends B{constructor(t){super(),N(this,t,null,R,j,{})}}export{ee as default};
