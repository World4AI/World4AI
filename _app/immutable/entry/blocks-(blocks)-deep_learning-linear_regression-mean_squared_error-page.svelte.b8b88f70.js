import{S as Kt,i as Nt,s as Yt,k as M,a as x,y as w,W as Gt,l as I,h as s,c as y,z as _,n as Xe,N as q,b as o,A as d,g,d as c,B as v,w as Lt,a9 as Ct,q as p,m as F,r as h,aa as Vt,C as L,v as Jt,f as Qt,P as Ut}from"../chunks/index.4d92b023.js";import{C as Zt}from"../chunks/Container.b0705c7b.js";import{S as At}from"../chunks/Slider.93409d64.js";import{L as D}from"../chunks/Latex.e0b308c0.js";import{H as Re}from"../chunks/Highlight.b7c1de53.js";import{A as Bt}from"../chunks/Alert.25a852b3.js";import{M as ea}from"../chunks/Mse.e3b58c1f.js";import{P as wt,T as _t}from"../chunks/Ticks.45eca5c5.js";import{C as dt}from"../chunks/Circle.f281e92b.js";import{X as vt,Y as bt}from"../chunks/YLabel.182e66a3.js";import{P as xt}from"../chunks/Path.7e6df014.js";function Ht(i,r,a){const t=i.slice();return t[8]=r[a],t}function ta(i){let r;return{c(){r=p("loss function")},l(a){r=h(a,"loss function")},m(a,t){o(a,r,t)},d(a){a&&s(r)}}}function aa(i){let r;return{c(){r=p("error function")},l(a){r=h(a,"error function")},m(a,t){o(a,r,t)},d(a){a&&s(r)}}}function ra(i){let r;return{c(){r=p("cost function")},l(a){r=h(a,"cost function")},m(a,t){o(a,r,t)},d(a){a&&s(r)}}}function na(i){let r,a,t,f,u,b,S,W;return r=new _t({props:{xTicks:[0,10,20,30,40,50,60],yTicks:[0,10,20,30,40,50,60],xOffset:-15,yOffset:25,fontSize:20}}),t=new vt({props:{text:"Feature",fontSize:25}}),u=new bt({props:{text:"Target",fontSize:25}}),S=new dt({props:{data:i[2],radius:5}}),{c(){w(r.$$.fragment),a=x(),w(t.$$.fragment),f=x(),w(u.$$.fragment),b=x(),w(S.$$.fragment)},l(m){_(r.$$.fragment,m),a=y(m),_(t.$$.fragment,m),f=y(m),_(u.$$.fragment,m),b=y(m),_(S.$$.fragment,m)},m(m,T){d(r,m,T),o(m,a,T),d(t,m,T),o(m,f,T),d(u,m,T),o(m,b,T),d(S,m,T),W=!0},p:L,i(m){W||(g(r.$$.fragment,m),g(t.$$.fragment,m),g(u.$$.fragment,m),g(S.$$.fragment,m),W=!0)},o(m){c(r.$$.fragment,m),c(t.$$.fragment,m),c(u.$$.fragment,m),c(S.$$.fragment,m),W=!1},d(m){v(r,m),m&&s(a),v(t,m),m&&s(f),v(u,m),m&&s(b),v(S,m)}}}function sa(i){let r,a,t,f,u,b,S,W,m,T;return r=new _t({props:{xTicks:[0,10,20,30,40,50,60],yTicks:[0,10,20,30,40,50,60],xOffset:-15,yOffset:25,fontSize:20}}),t=new vt({props:{text:"Feature",fontSize:25}}),u=new bt({props:{text:"Target",fontSize:25}}),S=new dt({props:{data:i[2],radius:5}}),m=new xt({props:{data:i[3]}}),{c(){w(r.$$.fragment),a=x(),w(t.$$.fragment),f=x(),w(u.$$.fragment),b=x(),w(S.$$.fragment),W=x(),w(m.$$.fragment)},l($){_(r.$$.fragment,$),a=y($),_(t.$$.fragment,$),f=y($),_(u.$$.fragment,$),b=y($),_(S.$$.fragment,$),W=y($),_(m.$$.fragment,$)},m($,z){d(r,$,z),o($,a,z),d(t,$,z),o($,f,z),d(u,$,z),o($,b,z),d(S,$,z),o($,W,z),d(m,$,z),T=!0},p:L,i($){T||(g(r.$$.fragment,$),g(t.$$.fragment,$),g(u.$$.fragment,$),g(S.$$.fragment,$),g(m.$$.fragment,$),T=!0)},o($){c(r.$$.fragment,$),c(t.$$.fragment,$),c(u.$$.fragment,$),c(S.$$.fragment,$),c(m.$$.fragment,$),T=!1},d($){v(r,$),$&&s(a),v(t,$),$&&s(f),v(u,$),$&&s(b),v(S,$),$&&s(W),v(m,$)}}}function la(i){let r=String.raw`y^{(i)}`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function ia(i){let r=String.raw`\hat{y}^{(i)}=x^{(i)}w + b`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function fa(i){let r;return{c(){r=p("i")},l(a){r=h(a,"i")},m(a,t){o(a,r,t)},d(a){a&&s(r)}}}function oa(i){let r=String.raw`y^{(i)} - \hat{y}^{(i)}`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function Rt(i){let r,a;return r=new xt({props:{data:i[8]}}),{c(){w(r.$$.fragment)},l(t){_(r.$$.fragment,t)},m(t,f){d(r,t,f),a=!0},p:L,i(t){a||(g(r.$$.fragment,t),a=!0)},o(t){c(r.$$.fragment,t),a=!1},d(t){v(r,t)}}}function $a(i){let r,a,t,f,u,b,S,W,m,T,$;r=new _t({props:{xTicks:[0,10,20,30,40,50,60],yTicks:[0,10,20,30,40,50,60],xOffset:-15,yOffset:25,fontSize:20}}),t=new vt({props:{text:"Feature",fontSize:25}}),u=new bt({props:{text:"Target",fontSize:25}});let z=i[4],E=[];for(let l=0;l<z.length;l+=1)E[l]=Rt(Ht(i,z,l));const Q=l=>c(E[l],1,1,()=>{E[l]=null});return W=new dt({props:{data:i[2],radius:5}}),T=new xt({props:{data:i[3]}}),{c(){w(r.$$.fragment),a=x(),w(t.$$.fragment),f=x(),w(u.$$.fragment),b=x();for(let l=0;l<E.length;l+=1)E[l].c();S=x(),w(W.$$.fragment),m=x(),w(T.$$.fragment)},l(l){_(r.$$.fragment,l),a=y(l),_(t.$$.fragment,l),f=y(l),_(u.$$.fragment,l),b=y(l);for(let k=0;k<E.length;k+=1)E[k].l(l);S=y(l),_(W.$$.fragment,l),m=y(l),_(T.$$.fragment,l)},m(l,k){d(r,l,k),o(l,a,k),d(t,l,k),o(l,f,k),d(u,l,k),o(l,b,k);for(let P=0;P<E.length;P+=1)E[P]&&E[P].m(l,k);o(l,S,k),d(W,l,k),o(l,m,k),d(T,l,k),$=!0},p(l,k){if(k&16){z=l[4];let P;for(P=0;P<z.length;P+=1){const C=Ht(l,z,P);E[P]?(E[P].p(C,k),g(E[P],1)):(E[P]=Rt(C),E[P].c(),g(E[P],1),E[P].m(S.parentNode,S))}for(Jt(),P=z.length;P<E.length;P+=1)Q(P);Qt()}},i(l){if(!$){g(r.$$.fragment,l),g(t.$$.fragment,l),g(u.$$.fragment,l);for(let k=0;k<z.length;k+=1)g(E[k]);g(W.$$.fragment,l),g(T.$$.fragment,l),$=!0}},o(l){c(r.$$.fragment,l),c(t.$$.fragment,l),c(u.$$.fragment,l),E=E.filter(Boolean);for(let k=0;k<E.length;k+=1)c(E[k]);c(W.$$.fragment,l),c(T.$$.fragment,l),$=!1},d(l){v(r,l),l&&s(a),v(t,l),l&&s(f),v(u,l),l&&s(b),Ut(E,l),l&&s(S),v(W,l),l&&s(m),v(T,l)}}}function ua(i){let r=String.raw`\hat{y} = xw + b`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function ma(i){let r=String.raw`y^{(i)}`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function pa(i){let r=String.raw`\sum_i^n y^{(i)} - \hat{y}^{(i)}`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function ha(i){let r;return{c(){r=p("mean squared error (MSE)")},l(a){r=h(a,"mean squared error (MSE)")},m(a,t){o(a,r,t)},d(a){a&&s(r)}}}function ga(i){let r=String.raw`(y^{(i)} - \hat{y}^{(i)})^2`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function ca(i){let r=String.raw`MSE=\dfrac{1}{n}\sum_i^n (y^{(i)} - \hat{y}^{(i)} )^2`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function wa(i){let r,a;return r=new D({props:{$$slots:{default:[ca]},$$scope:{ctx:i}}}),{c(){w(r.$$.fragment)},l(t){_(r.$$.fragment,t)},m(t,f){d(r,t,f),a=!0},p(t,f){const u={};f&2048&&(u.$$scope={dirty:f,ctx:t}),r.$set(u)},i(t){a||(g(r.$$.fragment,t),a=!0)},o(t){c(r.$$.fragment,t),a=!1},d(t){v(r,t)}}}function _a(i){let r=String.raw`
\mathbf{\hat{y}} = \mathbf{Xw}^T \\
MSE=mean\Big(\big[\mathbf{y} - \mathbf{\hat{y}}\big]^2\Big) \\
`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function da(i){let r,a;return r=new D({props:{$$slots:{default:[_a]},$$scope:{ctx:i}}}),{c(){w(r.$$.fragment)},l(t){_(r.$$.fragment,t)},m(t,f){d(r,t,f),a=!0},p(t,f){const u={};f&2048&&(u.$$scope={dirty:f,ctx:t}),r.$set(u)},i(t){a||(g(r.$$.fragment,t),a=!0)},o(t){c(r.$$.fragment,t),a=!1},d(t){v(r,t)}}}function va(i){let r=String.raw`\mathbf{\hat{y}}`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function ba(i){let r=String.raw`\mathbf{y - \hat{y}}`+"",a;return{c(){a=p(r)},l(t){a=h(t,r)},m(t,f){o(t,a,f)},p:L,d(t){t&&s(a)}}}function xa(i){let r;return{c(){r=p("w")},l(a){r=h(a,"w")},m(a,t){o(a,r,t)},d(a){a&&s(r)}}}function ya(i){let r;return{c(){r=p("b")},l(a){r=h(a,"b")},m(a,t){o(a,r,t)},d(a){a&&s(r)}}}function Sa(i){let r,a,t,f,u,b,S,W,m,T,$,z,E,Q,l,k,P,C,we,U,je,Ee,Z,qe,O,Ke,ee,Ne,te,Ye,ae,Ge,re,Je,We,ne,ze,A,Qe,se,Ue,le,Ze,ie,et,Pe,j,tt,fe,at,oe,rt,Me,$e,Ie,_e,nt,Oe,ue,Fe,B,st,me,lt,pe,it,be,ft,ot,De,de,$t,Le,he,Ce,N,ut,Ve,Y,mt,Ae,K,pt,ge,ht,ce,gt,Be,ve,He;W=new Re({props:{$$slots:{default:[ta]},$$scope:{ctx:i}}}),T=new Re({props:{$$slots:{default:[aa]},$$scope:{ctx:i}}}),z=new Re({props:{$$slots:{default:[ra]},$$scope:{ctx:i}}}),C=new wt({props:{width:500,height:500,maxWidth:600,domain:[0,60],range:[0,60],padding:{top:40,right:15,bottom:50,left:60},$$slots:{default:[na]},$$scope:{ctx:i}}}),Z=new wt({props:{width:500,height:500,maxWidth:600,domain:[0,60],range:[0,60],padding:{top:40,right:15,bottom:50,left:60},$$slots:{default:[sa]},$$scope:{ctx:i}}}),ee=new D({props:{$$slots:{default:[la]},$$scope:{ctx:i}}}),te=new D({props:{$$slots:{default:[ia]},$$scope:{ctx:i}}}),ae=new D({props:{$$slots:{default:[fa]},$$scope:{ctx:i}}}),re=new D({props:{$$slots:{default:[oa]},$$scope:{ctx:i}}}),ne=new wt({props:{width:500,height:500,maxWidth:600,domain:[0,60],range:[0,60],padding:{top:40,right:15,bottom:50,left:60},$$slots:{default:[$a]},$$scope:{ctx:i}}}),se=new D({props:{$$slots:{default:[ua]},$$scope:{ctx:i}}}),le=new D({props:{$$slots:{default:[ma]},$$scope:{ctx:i}}}),ie=new D({props:{$$slots:{default:[pa]},$$scope:{ctx:i}}}),fe=new Re({props:{$$slots:{default:[ha]},$$scope:{ctx:i}}}),oe=new D({props:{$$slots:{default:[ga]},$$scope:{ctx:i}}}),$e=new Bt({props:{type:"info",$$slots:{default:[wa]},$$scope:{ctx:i}}}),ue=new Bt({props:{type:"info",$$slots:{default:[da]},$$scope:{ctx:i}}}),me=new D({props:{$$slots:{default:[va]},$$scope:{ctx:i}}}),pe=new D({props:{$$slots:{default:[ba]},$$scope:{ctx:i}}}),he=new ea({props:{data:i[2],w:i[0],b:i[1]}});function Xt(e){i[5](e)}let yt={label:"Weight",labelId:"weight",showValue:!0,min:-20,max:20,step:.1};i[0]!==void 0&&(yt.value=i[0]),N=new At({props:yt}),Lt.push(()=>Ct(N,"value",Xt));function jt(e){i[6](e)}let St={label:"Bias",labelId:"bias",showValue:!0,min:-50,max:50};return i[1]!==void 0&&(St.value=i[1]),Y=new At({props:St}),Lt.push(()=>Ct(Y,"value",jt)),ge=new D({props:{$$slots:{default:[xa]},$$scope:{ctx:i}}}),ce=new D({props:{$$slots:{default:[ya]},$$scope:{ctx:i}}}),{c(){r=M("h1"),a=p("Mean Squared Error"),t=x(),f=M("div"),u=x(),b=M("p"),S=p(`While we can intuitively tell how far away our line is from some optimal
    location, we need a quantitative measure that the linear regression
    algorithm can use for optimization purposes. In machine learning we use a
    measure called `),w(W.$$.fragment),m=p(" (also called "),w(T.$$.fragment),$=p(" or "),w(z.$$.fragment),E=p(`). The lower the loss function, the
    closer we are to our goal. We can tweak the weights and biases to find a
    minimum value of the loss function.`),Q=x(),l=M("p"),k=p(`To make our illustrations easier, we will use a dataset with only 4 samples,
    as shown in the illustration below.`),P=x(),w(C.$$.fragment),we=x(),U=M("p"),je=p(`We will start by drawing a 45 degree regression line from the (0,0) to the
    (60, 60) position. While this looks "OK", we do not have a way to compare
    that particular line with any other lines.`),Ee=x(),w(Z.$$.fragment),qe=x(),O=M("p"),Ke=p(`The first step is to calculate the error between the actual target
    `),w(ee.$$.fragment),Ne=p(" and the predicted value "),w(te.$$.fragment),Ye=p(` for each datapoint
    `),w(ae.$$.fragment),Ge=p(". We can define the difference "),w(re.$$.fragment),Je=p(` as the error. Visually we can draw that error as the vertical line that connects
    the regression line with the true target.`),We=x(),w(ne.$$.fragment),ze=x(),A=M("p"),Qe=p("Depending on the location and rotation of the regression line "),w(se.$$.fragment),Ue=p(" and the actual target "),w(le.$$.fragment),Ze=p(`, the target
    might be above or below the regression line and thus the error can either be
    positive or negative. If we tried to sum up all the errors in the dataset `),w(ie.$$.fragment),et=p(` the positive and the negative errors would offset each other. If the errors
    were symmetrical above and below the line, we could end up with a summed error
    of 0. This is not what we want. We want positive and negative errors to contribute
    equally to our loss measure, without offsetting each other.`),Pe=x(),j=M("p"),tt=p("The loss that is actually used in linear regression is called the "),w(fe.$$.fragment),at=p(". The MSE takes each of the individual errors to the power of 2, "),w(oe.$$.fragment),rt=p(`, to get rid of the negative sign. The average of the errors is defined as
    the mean squared error.`),Me=x(),w($e.$$.fragment),Ie=x(),_e=M("p"),nt=p(`Remember that we should try to express all operations in matrix notation in
    order to make use of parallelization. For the mean squared error that would
    look as follows.`),Oe=x(),w(ue.$$.fragment),Fe=x(),B=M("p"),st=p("In the first step we calculate many predictions "),w(me.$$.fragment),lt=p(" simultaneously and calculate the vector of errors "),w(pe.$$.fragment),it=p(`. When we square the resulting vector, that implies that each individual
    unit within the vector is multiplied by itself in parallel. Finally we
    calculate the mean of the vector. For that purpose deep learning libraries
    provide a `),be=M("em"),ft=p("mean()"),ot=p(` operation, that takes a vector as input and generates
    a mean from all individual scalars within that vector.`),De=x(),de=M("p"),$t=p(`We can visuallize the mean squared error by drawing actual squares. Each
    data point has a corresponding square and the larger the area of that
    square. Try to use the example below and move the weight and the bias.
    Observe how the mean squared error changes based on the parameters.`),Le=x(),w(he.$$.fragment),Ce=x(),w(N.$$.fragment),Ve=x(),w(Y.$$.fragment),Ae=x(),K=M("p"),pt=p("Different combination of the weight "),w(ge.$$.fragment),ht=p(" and the bias "),w(ce.$$.fragment),gt=p(` produce different losses and our job is to find the combination that minimizes
    that loss function. Obviously it makes no sense to search manually for those
    parameters. The next section is therefore dedicated to a procedure that is commonly
    used to find the mimimum loss.`),Be=x(),ve=M("div"),this.h()},l(e){r=I(e,"H1",{});var n=F(r);a=h(n,"Mean Squared Error"),n.forEach(s),t=y(e),f=I(e,"DIV",{class:!0}),F(f).forEach(s),u=y(e),b=I(e,"P",{});var H=F(b);S=h(H,`While we can intuitively tell how far away our line is from some optimal
    location, we need a quantitative measure that the linear regression
    algorithm can use for optimization purposes. In machine learning we use a
    measure called `),_(W.$$.fragment,H),m=h(H," (also called "),_(T.$$.fragment,H),$=h(H," or "),_(z.$$.fragment,H),E=h(H,`). The lower the loss function, the
    closer we are to our goal. We can tweak the weights and biases to find a
    minimum value of the loss function.`),H.forEach(s),Q=y(e),l=I(e,"P",{});var xe=F(l);k=h(xe,`To make our illustrations easier, we will use a dataset with only 4 samples,
    as shown in the illustration below.`),xe.forEach(s),P=y(e),_(C.$$.fragment,e),we=y(e),U=I(e,"P",{});var ye=F(U);je=h(ye,`We will start by drawing a 45 degree regression line from the (0,0) to the
    (60, 60) position. While this looks "OK", we do not have a way to compare
    that particular line with any other lines.`),ye.forEach(s),Ee=y(e),_(Z.$$.fragment,e),qe=y(e),O=I(e,"P",{});var V=F(O);Ke=h(V,`The first step is to calculate the error between the actual target
    `),_(ee.$$.fragment,V),Ne=h(V," and the predicted value "),_(te.$$.fragment,V),Ye=h(V,` for each datapoint
    `),_(ae.$$.fragment,V),Ge=h(V,". We can define the difference "),_(re.$$.fragment,V),Je=h(V,` as the error. Visually we can draw that error as the vertical line that connects
    the regression line with the true target.`),V.forEach(s),We=y(e),_(ne.$$.fragment,e),ze=y(e),A=I(e,"P",{});var R=F(A);Qe=h(R,"Depending on the location and rotation of the regression line "),_(se.$$.fragment,R),Ue=h(R," and the actual target "),_(le.$$.fragment,R),Ze=h(R,`, the target
    might be above or below the regression line and thus the error can either be
    positive or negative. If we tried to sum up all the errors in the dataset `),_(ie.$$.fragment,R),et=h(R,` the positive and the negative errors would offset each other. If the errors
    were symmetrical above and below the line, we could end up with a summed error
    of 0. This is not what we want. We want positive and negative errors to contribute
    equally to our loss measure, without offsetting each other.`),R.forEach(s),Pe=y(e),j=I(e,"P",{});var G=F(j);tt=h(G,"The loss that is actually used in linear regression is called the "),_(fe.$$.fragment,G),at=h(G,". The MSE takes each of the individual errors to the power of 2, "),_(oe.$$.fragment,G),rt=h(G,`, to get rid of the negative sign. The average of the errors is defined as
    the mean squared error.`),G.forEach(s),Me=y(e),_($e.$$.fragment,e),Ie=y(e),_e=I(e,"P",{});var Se=F(_e);nt=h(Se,`Remember that we should try to express all operations in matrix notation in
    order to make use of parallelization. For the mean squared error that would
    look as follows.`),Se.forEach(s),Oe=y(e),_(ue.$$.fragment,e),Fe=y(e),B=I(e,"P",{});var X=F(B);st=h(X,"In the first step we calculate many predictions "),_(me.$$.fragment,X),lt=h(X," simultaneously and calculate the vector of errors "),_(pe.$$.fragment,X),it=h(X,`. When we square the resulting vector, that implies that each individual
    unit within the vector is multiplied by itself in parallel. Finally we
    calculate the mean of the vector. For that purpose deep learning libraries
    provide a `),be=I(X,"EM",{});var ke=F(be);ft=h(ke,"mean()"),ke.forEach(s),ot=h(X,` operation, that takes a vector as input and generates
    a mean from all individual scalars within that vector.`),X.forEach(s),De=y(e),de=I(e,"P",{});var Te=F(de);$t=h(Te,`We can visuallize the mean squared error by drawing actual squares. Each
    data point has a corresponding square and the larger the area of that
    square. Try to use the example below and move the weight and the bias.
    Observe how the mean squared error changes based on the parameters.`),Te.forEach(s),Le=y(e),_(he.$$.fragment,e),Ce=y(e),_(N.$$.fragment,e),Ve=y(e),_(Y.$$.fragment,e),Ae=y(e),K=I(e,"P",{});var J=F(K);pt=h(J,"Different combination of the weight "),_(ge.$$.fragment,J),ht=h(J," and the bias "),_(ce.$$.fragment,J),gt=h(J,` produce different losses and our job is to find the combination that minimizes
    that loss function. Obviously it makes no sense to search manually for those
    parameters. The next section is therefore dedicated to a procedure that is commonly
    used to find the mimimum loss.`),J.forEach(s),Be=y(e),ve=I(e,"DIV",{class:!0}),F(ve).forEach(s),this.h()},h(){Xe(f,"class","separator"),Xe(ve,"class","separator")},m(e,n){o(e,r,n),q(r,a),o(e,t,n),o(e,f,n),o(e,u,n),o(e,b,n),q(b,S),d(W,b,null),q(b,m),d(T,b,null),q(b,$),d(z,b,null),q(b,E),o(e,Q,n),o(e,l,n),q(l,k),o(e,P,n),d(C,e,n),o(e,we,n),o(e,U,n),q(U,je),o(e,Ee,n),d(Z,e,n),o(e,qe,n),o(e,O,n),q(O,Ke),d(ee,O,null),q(O,Ne),d(te,O,null),q(O,Ye),d(ae,O,null),q(O,Ge),d(re,O,null),q(O,Je),o(e,We,n),d(ne,e,n),o(e,ze,n),o(e,A,n),q(A,Qe),d(se,A,null),q(A,Ue),d(le,A,null),q(A,Ze),d(ie,A,null),q(A,et),o(e,Pe,n),o(e,j,n),q(j,tt),d(fe,j,null),q(j,at),d(oe,j,null),q(j,rt),o(e,Me,n),d($e,e,n),o(e,Ie,n),o(e,_e,n),q(_e,nt),o(e,Oe,n),d(ue,e,n),o(e,Fe,n),o(e,B,n),q(B,st),d(me,B,null),q(B,lt),d(pe,B,null),q(B,it),q(B,be),q(be,ft),q(B,ot),o(e,De,n),o(e,de,n),q(de,$t),o(e,Le,n),d(he,e,n),o(e,Ce,n),d(N,e,n),o(e,Ve,n),d(Y,e,n),o(e,Ae,n),o(e,K,n),q(K,pt),d(ge,K,null),q(K,ht),d(ce,K,null),q(K,gt),o(e,Be,n),o(e,ve,n),He=!0},p(e,n){const H={};n&2048&&(H.$$scope={dirty:n,ctx:e}),W.$set(H);const xe={};n&2048&&(xe.$$scope={dirty:n,ctx:e}),T.$set(xe);const ye={};n&2048&&(ye.$$scope={dirty:n,ctx:e}),z.$set(ye);const V={};n&2048&&(V.$$scope={dirty:n,ctx:e}),C.$set(V);const R={};n&2048&&(R.$$scope={dirty:n,ctx:e}),Z.$set(R);const G={};n&2048&&(G.$$scope={dirty:n,ctx:e}),ee.$set(G);const Se={};n&2048&&(Se.$$scope={dirty:n,ctx:e}),te.$set(Se);const X={};n&2048&&(X.$$scope={dirty:n,ctx:e}),ae.$set(X);const ke={};n&2048&&(ke.$$scope={dirty:n,ctx:e}),re.$set(ke);const Te={};n&2048&&(Te.$$scope={dirty:n,ctx:e}),ne.$set(Te);const J={};n&2048&&(J.$$scope={dirty:n,ctx:e}),se.$set(J);const kt={};n&2048&&(kt.$$scope={dirty:n,ctx:e}),le.$set(kt);const Tt={};n&2048&&(Tt.$$scope={dirty:n,ctx:e}),ie.$set(Tt);const Et={};n&2048&&(Et.$$scope={dirty:n,ctx:e}),fe.$set(Et);const qt={};n&2048&&(qt.$$scope={dirty:n,ctx:e}),oe.$set(qt);const Wt={};n&2048&&(Wt.$$scope={dirty:n,ctx:e}),$e.$set(Wt);const zt={};n&2048&&(zt.$$scope={dirty:n,ctx:e}),ue.$set(zt);const Pt={};n&2048&&(Pt.$$scope={dirty:n,ctx:e}),me.$set(Pt);const Mt={};n&2048&&(Mt.$$scope={dirty:n,ctx:e}),pe.$set(Mt);const ct={};n&1&&(ct.w=e[0]),n&2&&(ct.b=e[1]),he.$set(ct);const It={};!ut&&n&1&&(ut=!0,It.value=e[0],Vt(()=>ut=!1)),N.$set(It);const Ot={};!mt&&n&2&&(mt=!0,Ot.value=e[1],Vt(()=>mt=!1)),Y.$set(Ot);const Ft={};n&2048&&(Ft.$$scope={dirty:n,ctx:e}),ge.$set(Ft);const Dt={};n&2048&&(Dt.$$scope={dirty:n,ctx:e}),ce.$set(Dt)},i(e){He||(g(W.$$.fragment,e),g(T.$$.fragment,e),g(z.$$.fragment,e),g(C.$$.fragment,e),g(Z.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(ae.$$.fragment,e),g(re.$$.fragment,e),g(ne.$$.fragment,e),g(se.$$.fragment,e),g(le.$$.fragment,e),g(ie.$$.fragment,e),g(fe.$$.fragment,e),g(oe.$$.fragment,e),g($e.$$.fragment,e),g(ue.$$.fragment,e),g(me.$$.fragment,e),g(pe.$$.fragment,e),g(he.$$.fragment,e),g(N.$$.fragment,e),g(Y.$$.fragment,e),g(ge.$$.fragment,e),g(ce.$$.fragment,e),He=!0)},o(e){c(W.$$.fragment,e),c(T.$$.fragment,e),c(z.$$.fragment,e),c(C.$$.fragment,e),c(Z.$$.fragment,e),c(ee.$$.fragment,e),c(te.$$.fragment,e),c(ae.$$.fragment,e),c(re.$$.fragment,e),c(ne.$$.fragment,e),c(se.$$.fragment,e),c(le.$$.fragment,e),c(ie.$$.fragment,e),c(fe.$$.fragment,e),c(oe.$$.fragment,e),c($e.$$.fragment,e),c(ue.$$.fragment,e),c(me.$$.fragment,e),c(pe.$$.fragment,e),c(he.$$.fragment,e),c(N.$$.fragment,e),c(Y.$$.fragment,e),c(ge.$$.fragment,e),c(ce.$$.fragment,e),He=!1},d(e){e&&s(r),e&&s(t),e&&s(f),e&&s(u),e&&s(b),v(W),v(T),v(z),e&&s(Q),e&&s(l),e&&s(P),v(C,e),e&&s(we),e&&s(U),e&&s(Ee),v(Z,e),e&&s(qe),e&&s(O),v(ee),v(te),v(ae),v(re),e&&s(We),v(ne,e),e&&s(ze),e&&s(A),v(se),v(le),v(ie),e&&s(Pe),e&&s(j),v(fe),v(oe),e&&s(Me),v($e,e),e&&s(Ie),e&&s(_e),e&&s(Oe),v(ue,e),e&&s(Fe),e&&s(B),v(me),v(pe),e&&s(De),e&&s(de),e&&s(Le),v(he,e),e&&s(Ce),v(N,e),e&&s(Ve),v(Y,e),e&&s(Ae),e&&s(K),v(ge),v(ce),e&&s(Be),e&&s(ve)}}}function ka(i){let r,a,t,f;return t=new Zt({props:{$$slots:{default:[Sa]},$$scope:{ctx:i}}}),{c(){r=M("meta"),a=x(),w(t.$$.fragment),this.h()},l(u){const b=Gt("svelte-u1dbvm",document.head);r=I(b,"META",{name:!0,content:!0}),b.forEach(s),a=y(u),_(t.$$.fragment,u),this.h()},h(){document.title="Mean Squared Error - World4AI",Xe(r,"name","description"),Xe(r,"content","Linear regression utilizes the mean squared error as a loss function. The goal of linear regression is therefore to minimize the MSE.")},m(u,b){q(document.head,r),o(u,a,b),d(t,u,b),f=!0},p(u,[b]){const S={};b&2051&&(S.$$scope={dirty:b,ctx:u}),t.$set(S)},i(u){f||(g(t.$$.fragment,u),f=!0)},o(u){c(t.$$.fragment,u),f=!1},d(u){s(r),u&&s(a),v(t,u)}}}function Ta(i,r,a){let t=[{x:5,y:20},{x:10,y:40},{x:35,y:15},{x:45,y:59}],f=1,u=0,b=[],S=[];function W(){let $=0,z=u+f*$,E=60,Q=u+f*E;b.push({x:$,y:z}),b.push({x:E,y:Q}),t.forEach(l=>{let k=l.x,P=l.x,C=l.y,we=u+f*P,U=[{x:k,y:C},{x:P,y:we}];S.push(U)})}W();function m($){f=$,a(0,f)}function T($){u=$,a(1,u)}return[f,u,t,b,S,m,T]}class Ca extends Kt{constructor(r){super(),Nt(this,r,Ta,ka,Yt,{})}}export{Ca as default};
