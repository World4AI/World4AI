import{S as xe,i as ke,s as Ee,k as $,a as g,y as W,W as Ae,l as _,h as a,c as d,z as q,n as w,N as p,b as u,A as I,g as S,d as C,B,q as h,m as v,r as m,C as ne,Q as we,R as be,P as ye,e as ge}from"../chunks/index.4d92b023.js";import{C as Pe}from"../chunks/Container.b0705c7b.js";import{S as Te}from"../chunks/SvgContainer.f70b5745.js";import{B as We}from"../chunks/ButtonContainer.e9aac418.js";import{P as qe}from"../chunks/PlayButton.85103c5a.js";import{L as pe}from"../chunks/Latex.e0b308c0.js";function de(f,n,o){const e=f.slice();return e[3]=n[o],e[5]=o,e}function ve(f,n,o){const e=f.slice();return e[3]=n[o],e[7]=o,e}function Ie(f){let n,o;return n=new qe({props:{f:f[2],delta:70}}),{c(){W(n.$$.fragment)},l(e){q(n.$$.fragment,e)},m(e,i){I(n,e,i),o=!0},p:ne,i(e){o||(S(n.$$.fragment,e),o=!0)},o(e){C(n.$$.fragment,e),o=!1},d(e){B(n,e)}}}function $e(f){let n,o;return{c(){n=we("rect"),this.h()},l(e){n=be(e,"rect",{x:!0,y:!0,width:!0,height:!0,class:!0}),v(n).forEach(a),this.h()},h(){w(n,"x",2+f[5]*(ie+5)),w(n,"y",2+f[7]*(ie+5)),w(n,"width",ie),w(n,"height",ie),w(n,"class",o=f[5]===f[1]&&f[7]===f[0]?"fill-red-500 stroke-black":f[7]<f[0]||f[7]===f[0]&&f[5]<f[1]?"fill-blue-100 stroke-black":"fill-white")},m(e,i){u(e,n,i)},p(e,i){i&3&&o!==(o=e[5]===e[1]&&e[7]===e[0]?"fill-red-500 stroke-black":e[7]<e[0]||e[7]===e[0]&&e[5]<e[1]?"fill-blue-100 stroke-black":"fill-white")&&w(n,"class",o)},d(e){e&&a(n)}}}function _e(f){let n,o=Array(N),e=[];for(let i=0;i<o.length;i+=1)e[i]=$e(ve(f,o,i));return{c(){for(let i=0;i<e.length;i+=1)e[i].c();n=ge()},l(i){for(let s=0;s<e.length;s+=1)e[s].l(i);n=ge()},m(i,s){for(let l=0;l<e.length;l+=1)e[l]&&e[l].m(i,s);u(i,n,s)},p(i,s){if(s&3){o=Array(N);let l;for(l=0;l<o.length;l+=1){const c=ve(i,o,l);e[l]?e[l].p(c,s):(e[l]=$e(c),e[l].c(),e[l].m(n.parentNode,n))}for(;l<e.length;l+=1)e[l].d(1);e.length=o.length}},d(i){ye(e,i),i&&a(n)}}}function Se(f){let n,o=Array(N),e=[];for(let i=0;i<o.length;i+=1)e[i]=_e(de(f,o,i));return{c(){n=we("svg");for(let i=0;i<e.length;i+=1)e[i].c();this.h()},l(i){n=be(i,"svg",{viewBox:!0});var s=v(n);for(let l=0;l<e.length;l+=1)e[l].l(s);s.forEach(a),this.h()},h(){w(n,"viewBox","0 0 300 300")},m(i,s){u(i,n,s);for(let l=0;l<e.length;l+=1)e[l]&&e[l].m(n,null)},p(i,s){if(s&3){o=Array(N);let l;for(l=0;l<o.length;l+=1){const c=de(i,o,l);e[l]?e[l].p(c,s):(e[l]=_e(c),e[l].c(),e[l].m(n,null))}for(;l<e.length;l+=1)e[l].d(1);e.length=o.length}},d(i){i&&a(n),ye(e,i)}}}function Ce(f){let n=String.raw`p(\mathbf{x})`+"",o;return{c(){o=h(n)},l(e){o=m(e,n)},m(e,i){u(e,o,i)},p:ne,d(e){e&&a(o)}}}function Be(f){let n=String.raw`\mathbf{x}`+"",o;return{c(){o=h(n)},l(e){o=m(e,n)},m(e,i){u(e,o,i)},p:ne,d(e){e&&a(o)}}}function Ge(f){let n=String.raw`p(\mathbf{x}) = p(x_1) * p(x_2 | x_1) * p(x_2 | x_1, x_2) * \cdots * p(x_n | x_1, \cdots, x_{n-1})`+"",o;return{c(){o=h(n)},l(e){o=m(e,n)},m(e,i){u(e,o,i)},p:ne,d(e){e&&a(o)}}}function ze(f){let n,o,e,i,s,l,c,V,G,oe,H,x,Q,k,J,z,le,K,b,ae,E,se,A,re,U,L,fe,X,P,Y,T,ue,j,he,me,Z,M,ee;return x=new We({props:{$$slots:{default:[Ie]},$$scope:{ctx:f}}}),k=new Te({props:{maxWidth:"300px",$$slots:{default:[Se]},$$scope:{ctx:f}}}),E=new pe({props:{$$slots:{default:[Ce]},$$scope:{ctx:f}}}),A=new pe({props:{$$slots:{default:[Be]},$$scope:{ctx:f}}}),P=new pe({props:{$$slots:{default:[Ge]},$$scope:{ctx:f}}}),{c(){n=$("h1"),o=h("Autoregressive Generative Models"),e=g(),i=$("div"),s=g(),l=$("p"),c=h(`We have already encountered autoregressive models, when we were dealing with
    decoder-based language models, like GPT. An autoregressive model relies on
    its previously generated values. If for example you want to produce the
    fifth word in a sentence, you provide the model with the previous four
    words.`),V=g(),G=$("p"),oe=h(`Autogenerative models can also be used to generate images. We simply
    generate one pixel at a time, while conditioning the model on previously
    generated pixels.`),H=g(),W(x.$$.fragment),Q=g(),W(k.$$.fragment),J=g(),z=$("p"),le=h(`The interactive example above shows how pixel generation looks like. To
    generate the next (red) pixel, the model looks at all previous (blue)
    pixels. As you can imagine this recursive process is quite slow and scales
    quite badly with increased image size. While autoregressive procedures are
    state of the art for text generation, for image generation there are more
    efficient techniques, that we will introduce in future chapters. This
    chapter is merely an intorduction into the world of generative models.`),K=g(),b=$("p"),ae=h("The goal of a generative model is to estimate the probability distribution "),W(E.$$.fragment),se=h(`.
    `),W(A.$$.fragment),re=h(` could by a piece of text, a song or in
    our case a full image. Once we have learned this distrubution, we can draw random
    images from it.`),U=g(),L=$("p"),fe=h(`For autoregressive generative models we rewrite this task by using the chain
    rule of probabilities and express the probability distribution as the
    product of conditional probabilities.`),X=g(),W(P.$$.fragment),Y=g(),T=$("p"),ue=h("Essentially we are answering the following question: "),j=$("em"),he=h(`'What is the probability of the next pixel, given that I have observed
      all those previous pixels?'`),me=h(`. Learning the conditional distribution is often much easier, than learning
    the joint distribution. Imagine you have been provided with a half finished
    image, filling in the blanks should be relatively straightforward. Even
    though you do not know how the image looks like exactly, you can manage this
    task much better than creating an image from scratch. Even if you are not an
    artist, if we give you an image, where only the last pixel is missing, you
    will be able to fill in the blanks.`),Z=g(),M=$("div"),this.h()},l(t){n=_(t,"H1",{});var r=v(n);o=m(r,"Autoregressive Generative Models"),r.forEach(a),e=d(t),i=_(t,"DIV",{class:!0}),v(i).forEach(a),s=d(t),l=_(t,"P",{});var D=v(l);c=m(D,`We have already encountered autoregressive models, when we were dealing with
    decoder-based language models, like GPT. An autoregressive model relies on
    its previously generated values. If for example you want to produce the
    fifth word in a sentence, you provide the model with the previous four
    words.`),D.forEach(a),V=d(t),G=_(t,"P",{});var R=v(G);oe=m(R,`Autogenerative models can also be used to generate images. We simply
    generate one pixel at a time, while conditioning the model on previously
    generated pixels.`),R.forEach(a),H=d(t),q(x.$$.fragment,t),Q=d(t),q(k.$$.fragment,t),J=d(t),z=_(t,"P",{});var F=v(z);le=m(F,`The interactive example above shows how pixel generation looks like. To
    generate the next (red) pixel, the model looks at all previous (blue)
    pixels. As you can imagine this recursive process is quite slow and scales
    quite badly with increased image size. While autoregressive procedures are
    state of the art for text generation, for image generation there are more
    efficient techniques, that we will introduce in future chapters. This
    chapter is merely an intorduction into the world of generative models.`),F.forEach(a),K=d(t),b=_(t,"P",{});var y=v(b);ae=m(y,"The goal of a generative model is to estimate the probability distribution "),q(E.$$.fragment,y),se=m(y,`.
    `),q(A.$$.fragment,y),re=m(y,` could by a piece of text, a song or in
    our case a full image. Once we have learned this distrubution, we can draw random
    images from it.`),y.forEach(a),U=d(t),L=_(t,"P",{});var O=v(L);fe=m(O,`For autoregressive generative models we rewrite this task by using the chain
    rule of probabilities and express the probability distribution as the
    product of conditional probabilities.`),O.forEach(a),X=d(t),q(P.$$.fragment,t),Y=d(t),T=_(t,"P",{});var te=v(T);ue=m(te,"Essentially we are answering the following question: "),j=_(te,"EM",{});var ce=v(j);he=m(ce,`'What is the probability of the next pixel, given that I have observed
      all those previous pixels?'`),ce.forEach(a),me=m(te,`. Learning the conditional distribution is often much easier, than learning
    the joint distribution. Imagine you have been provided with a half finished
    image, filling in the blanks should be relatively straightforward. Even
    though you do not know how the image looks like exactly, you can manage this
    task much better than creating an image from scratch. Even if you are not an
    artist, if we give you an image, where only the last pixel is missing, you
    will be able to fill in the blanks.`),te.forEach(a),Z=d(t),M=_(t,"DIV",{class:!0}),v(M).forEach(a),this.h()},h(){w(i,"class","separator"),w(M,"class","separator")},m(t,r){u(t,n,r),p(n,o),u(t,e,r),u(t,i,r),u(t,s,r),u(t,l,r),p(l,c),u(t,V,r),u(t,G,r),p(G,oe),u(t,H,r),I(x,t,r),u(t,Q,r),I(k,t,r),u(t,J,r),u(t,z,r),p(z,le),u(t,K,r),u(t,b,r),p(b,ae),I(E,b,null),p(b,se),I(A,b,null),p(b,re),u(t,U,r),u(t,L,r),p(L,fe),u(t,X,r),I(P,t,r),u(t,Y,r),u(t,T,r),p(T,ue),p(T,j),p(j,he),p(T,me),u(t,Z,r),u(t,M,r),ee=!0},p(t,r){const D={};r&256&&(D.$$scope={dirty:r,ctx:t}),x.$set(D);const R={};r&259&&(R.$$scope={dirty:r,ctx:t}),k.$set(R);const F={};r&256&&(F.$$scope={dirty:r,ctx:t}),E.$set(F);const y={};r&256&&(y.$$scope={dirty:r,ctx:t}),A.$set(y);const O={};r&256&&(O.$$scope={dirty:r,ctx:t}),P.$set(O)},i(t){ee||(S(x.$$.fragment,t),S(k.$$.fragment,t),S(E.$$.fragment,t),S(A.$$.fragment,t),S(P.$$.fragment,t),ee=!0)},o(t){C(x.$$.fragment,t),C(k.$$.fragment,t),C(E.$$.fragment,t),C(A.$$.fragment,t),C(P.$$.fragment,t),ee=!1},d(t){t&&a(n),t&&a(e),t&&a(i),t&&a(s),t&&a(l),t&&a(V),t&&a(G),t&&a(H),B(x,t),t&&a(Q),B(k,t),t&&a(J),t&&a(z),t&&a(K),t&&a(b),B(E),B(A),t&&a(U),t&&a(L),t&&a(X),B(P,t),t&&a(Y),t&&a(T),t&&a(Z),t&&a(M)}}}function Le(f){let n,o,e,i;return e=new Pe({props:{$$slots:{default:[ze]},$$scope:{ctx:f}}}),{c(){n=$("meta"),o=g(),W(e.$$.fragment),this.h()},l(s){const l=Ae("svelte-1cuz11j",document.head);n=_(l,"META",{name:!0,content:!0}),l.forEach(a),o=d(s),q(e.$$.fragment,s),this.h()},h(){document.title="Autoregressive Generative Models - World4AI",w(n,"name","description"),w(n,"content","Autoregressive generative models, like GPT or PixelRNN, generate one element at a time, where the next element depends on all previously generated elements. During the trianing process the model is not allowed to look at the future tokens, therefore autoregressive models usually implement some sort of masking.")},m(s,l){p(document.head,n),u(s,o,l),I(e,s,l),i=!0},p(s,[l]){const c={};l&259&&(c.$$scope={dirty:l,ctx:s}),e.$set(c)},i(s){i||(S(e.$$.fragment,s),i=!0)},o(s){C(e.$$.fragment,s),i=!1},d(s){a(n),s&&a(o),B(e,s)}}}const N=10,ie=25;function Me(f,n,o){let e=0,i=0;function s(){i<N-1?o(1,i+=1):e<N-1?(o(1,i=0),o(0,e+=1)):(o(0,e=0),o(1,i=0))}return[e,i,s]}class Ve extends xe{constructor(n){super(),ke(this,n,Me,Le,Ee,{})}}export{Ve as default};
