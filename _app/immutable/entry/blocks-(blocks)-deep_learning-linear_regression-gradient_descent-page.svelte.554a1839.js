import{S as Ds,i as Os,s as Fs,k as W,a as k,y as u,W as As,l as D,h as l,c as T,z as p,n as C,N as d,b as i,A as h,g,d as c,B as _,w as Cn,a9 as Gn,q as f,m as O,r as o,u as Nn,aa as Mn,C as U}from"../chunks/index.4d92b023.js";import{C as Ls}from"../chunks/Container.b0705c7b.js";import{L as I}from"../chunks/Latex.e0b308c0.js";import{H as Kt}from"../chunks/Highlight.b7c1de53.js";import{S as Xn}from"../chunks/Slider.93409d64.js";import{A as Bs}from"../chunks/Alert.25a852b3.js";import{P as Qt,T as Rt}from"../chunks/Ticks.45eca5c5.js";import{C as Zt}from"../chunks/Circle.f281e92b.js";import{X as ea,Y as ta}from"../chunks/YLabel.182e66a3.js";import{P as Pt}from"../chunks/Path.7e6df014.js";import{L as qs}from"../chunks/Legend.de38c007.js";import{B as ys}from"../chunks/ButtonContainer.e9aac418.js";import{S as Ps}from"../chunks/StepButton.2fb0289b.js";function Ys(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function js(r){let a;return{c(){a=f("f(x) = x^2")},l(t){a=o(t,"f(x) = x^2")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Vs(r){let a;return{c(){a=f("f(x) = x^2")},l(t){a=o(t,"f(x) = x^2")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Cs(r){let a;return{c(){a=f("f(x)")},l(t){a=o(t,"f(x)")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Gs(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Ns(r){let a,t,n,x,b,P,E,z,S,y;return a=new Rt({props:{xTicks:[-80,-60,-40,-20,0,20,40,60,80],yTicks:[0,500,1e3,1500,2e3,2500,3e3],xOffset:-10,yOffset:25}}),n=new ea({props:{text:"x",fontSize:15,type:"latex"}}),b=new ta({props:{text:"f(x)",fontSize:15,type:"latex",x:-3}}),E=new Pt({props:{data:r[7]}}),S=new Zt({props:{data:[{x:0,y:0}]}}),{c(){u(a.$$.fragment),t=k(),u(n.$$.fragment),x=k(),u(b.$$.fragment),P=k(),u(E.$$.fragment),z=k(),u(S.$$.fragment)},l($){p(a.$$.fragment,$),t=T($),p(n.$$.fragment,$),x=T($),p(b.$$.fragment,$),P=T($),p(E.$$.fragment,$),z=T($),p(S.$$.fragment,$)},m($,v){h(a,$,v),i($,t,v),h(n,$,v),i($,x,v),h(b,$,v),i($,P,v),h(E,$,v),i($,z,v),h(S,$,v),y=!0},p:U,i($){y||(g(a.$$.fragment,$),g(n.$$.fragment,$),g(b.$$.fragment,$),g(E.$$.fragment,$),g(S.$$.fragment,$),y=!0)},o($){c(a.$$.fragment,$),c(n.$$.fragment,$),c(b.$$.fragment,$),c(E.$$.fragment,$),c(S.$$.fragment,$),y=!1},d($){_(a,$),$&&l(t),_(n,$),$&&l(x),_(b,$),$&&l(P),_(E,$),$&&l(z),_(S,$)}}}function Ms(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Xs(r){let a,t,n,x,b,P,E,z,S,y;return a=new Rt({props:{xTicks:[-80,-60,-40,-20,0,20,40,60,80],yTicks:[0,500,1e3,1500,2e3,2500,3e3],xOffset:-10,yOffset:25}}),n=new ea({props:{text:"x",fontSize:15,type:"latex"}}),b=new ta({props:{text:"f(x)",fontSize:15,type:"latex",x:-3}}),E=new Pt({props:{data:r[7]}}),S=new Zt({props:{data:r[8]}}),{c(){u(a.$$.fragment),t=k(),u(n.$$.fragment),x=k(),u(b.$$.fragment),P=k(),u(E.$$.fragment),z=k(),u(S.$$.fragment)},l($){p(a.$$.fragment,$),t=T($),p(n.$$.fragment,$),x=T($),p(b.$$.fragment,$),P=T($),p(E.$$.fragment,$),z=T($),p(S.$$.fragment,$)},m($,v){h(a,$,v),i($,t,v),h(n,$,v),i($,x,v),h(b,$,v),i($,P,v),h(E,$,v),i($,z,v),h(S,$,v),y=!0},p:U,i($){y||(g(a.$$.fragment,$),g(n.$$.fragment,$),g(b.$$.fragment,$),g(E.$$.fragment,$),g(S.$$.fragment,$),y=!0)},o($){c(a.$$.fragment,$),c(n.$$.fragment,$),c(b.$$.fragment,$),c(E.$$.fragment,$),c(S.$$.fragment,$),y=!1},d($){_(a,$),$&&l(t),_(n,$),$&&l(x),_(b,$),$&&l(P),_(E,$),$&&l(z),_(S,$)}}}function Hs(r){let a;return{c(){a=f("f(x)")},l(t){a=o(t,"f(x)")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Us(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Js(r){let a=String.raw`\dfrac{\mathrm{d}}{\mathrm{d}x}f(x) = 2x`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function Ks(r){let a,t,n,x,b,P,E,z,S,y,$,v;return a=new Rt({props:{xTicks:[-80,-60,-40,-20,0,20,40,60,80],yTicks:[0,500,1e3,1500,2e3,2500,3e3],xOffset:-10,yOffset:25}}),n=new ea({props:{text:"x",fontSize:15,type:"latex"}}),b=new ta({props:{text:"f(x)",fontSize:15,type:"latex",x:-3}}),E=new Pt({props:{data:r[7]}}),S=new Pt({props:{data:r[3]}}),$=new Zt({props:{data:r[8]}}),{c(){u(a.$$.fragment),t=k(),u(n.$$.fragment),x=k(),u(b.$$.fragment),P=k(),u(E.$$.fragment),z=k(),u(S.$$.fragment),y=k(),u($.$$.fragment)},l(m){p(a.$$.fragment,m),t=T(m),p(n.$$.fragment,m),x=T(m),p(b.$$.fragment,m),P=T(m),p(E.$$.fragment,m),z=T(m),p(S.$$.fragment,m),y=T(m),p($.$$.fragment,m)},m(m,F){h(a,m,F),i(m,t,F),h(n,m,F),i(m,x,F),h(b,m,F),i(m,P,F),h(E,m,F),i(m,z,F),h(S,m,F),i(m,y,F),h($,m,F),v=!0},p(m,F){const w={};F&8&&(w.data=m[3]),S.$set(w)},i(m){v||(g(a.$$.fragment,m),g(n.$$.fragment,m),g(b.$$.fragment,m),g(E.$$.fragment,m),g(S.$$.fragment,m),g($.$$.fragment,m),v=!0)},o(m){c(a.$$.fragment,m),c(n.$$.fragment,m),c(b.$$.fragment,m),c(E.$$.fragment,m),c(S.$$.fragment,m),c($.$$.fragment,m),v=!1},d(m){_(a,m),m&&l(t),_(n,m),m&&l(x),_(b,m),m&&l(P),_(E,m),m&&l(z),_(S,m),m&&l(y),_($,m)}}}function Qs(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Rs(r){let a;return{c(){a=f("f(x)")},l(t){a=o(t,"f(x)")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Zs(r){let a;return{c(){a=f("gradient descent")},l(t){a=o(t,"gradient descent")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function el(r){let a=String.raw`\dfrac{\mathrm{d}}{\mathrm{d}x}f(x)`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function tl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function al(r){let a=String.raw`\dfrac{\mathrm{d}}{\mathrm{d}x}f(x)`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function nl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function sl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function ll(r){let a;return{c(){a=f("\\alpha")},l(t){a=o(t,"\\alpha")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function rl(r){let a;return{c(){a=f("learning rate")},l(t){a=o(t,"learning rate")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function $l(r){let a;return{c(){a=f("f(x)")},l(t){a=o(t,"f(x)")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function il(r){let a;return{c(){a=f("t")},l(t){a=o(t,"t")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function fl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function ol(r){let a;return{c(){a=f("f(x)")},l(t){a=o(t,"f(x)")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function ml(r){let a=String.raw`x_{t+1} \coloneqq x_t - \alpha \dfrac{\mathrm{d}}{\mathrm{d}x}f(x_t)`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function ul(r){let a,t,n,x,b,P,E,z,S,y,$;return n=new I({props:{$$slots:{default:[il]},$$scope:{ctx:r}}}),b=new I({props:{$$slots:{default:[fl]},$$scope:{ctx:r}}}),E=new I({props:{$$slots:{default:[ol]},$$scope:{ctx:r}}}),y=new I({props:{$$slots:{default:[ml]},$$scope:{ctx:r}}}),{c(){a=W("p"),t=f("At each time step "),u(n.$$.fragment),x=f(` of the gradient descent algorithm we update
      the variable `),u(b.$$.fragment),P=f(", until "),u(E.$$.fragment),z=f(" converges to the miminum."),S=k(),u(y.$$.fragment)},l(v){a=D(v,"P",{});var m=O(a);t=o(m,"At each time step "),p(n.$$.fragment,m),x=o(m,` of the gradient descent algorithm we update
      the variable `),p(b.$$.fragment,m),P=o(m,", until "),p(E.$$.fragment,m),z=o(m," converges to the miminum."),m.forEach(l),S=T(v),p(y.$$.fragment,v)},m(v,m){i(v,a,m),d(a,t),h(n,a,null),d(a,x),h(b,a,null),d(a,P),h(E,a,null),d(a,z),i(v,S,m),h(y,v,m),$=!0},p(v,m){const F={};m&1048576&&(F.$$scope={dirty:m,ctx:v}),n.$set(F);const w={};m&1048576&&(w.$$scope={dirty:m,ctx:v}),b.$set(w);const A={};m&1048576&&(A.$$scope={dirty:m,ctx:v}),E.$set(A);const G={};m&1048576&&(G.$$scope={dirty:m,ctx:v}),y.$set(G)},i(v){$||(g(n.$$.fragment,v),g(b.$$.fragment,v),g(E.$$.fragment,v),g(y.$$.fragment,v),$=!0)},o(v){c(n.$$.fragment,v),c(b.$$.fragment,v),c(E.$$.fragment,v),c(y.$$.fragment,v),$=!1},d(v){v&&l(a),_(n),_(b),_(E),v&&l(S),_(y,v)}}}function pl(r){let a,t;return a=new Ps({}),a.$on("click",r[10]),{c(){u(a.$$.fragment)},l(n){p(a.$$.fragment,n)},m(n,x){h(a,n,x),t=!0},p:U,i(n){t||(g(a.$$.fragment,n),t=!0)},o(n){c(a.$$.fragment,n),t=!1},d(n){_(a,n)}}}function hl(r){let a,t,n,x,b,P,E,z,S,y,$,v,m,F;return a=new Rt({props:{xTicks:[-80,-60,-40,-20,0,20,40,60,80],yTicks:[0,500,1e3,1500,2e3,2500,3e3],xOffset:-10,yOffset:25}}),n=new ea({props:{text:"x",fontSize:15,type:"latex"}}),b=new ta({props:{text:"f(x)",fontSize:15,type:"latex",x:-3}}),E=new Pt({props:{data:r[7]}}),S=new Pt({props:{data:r[3]}}),$=new qs({props:{text:"Derivative: "+r[6].toFixed(2),coordinates:{x:-30,y:2800},fontSize:20}}),m=new Zt({props:{data:r[2]}}),{c(){u(a.$$.fragment),t=k(),u(n.$$.fragment),x=k(),u(b.$$.fragment),P=k(),u(E.$$.fragment),z=k(),u(S.$$.fragment),y=k(),u($.$$.fragment),v=k(),u(m.$$.fragment)},l(w){p(a.$$.fragment,w),t=T(w),p(n.$$.fragment,w),x=T(w),p(b.$$.fragment,w),P=T(w),p(E.$$.fragment,w),z=T(w),p(S.$$.fragment,w),y=T(w),p($.$$.fragment,w),v=T(w),p(m.$$.fragment,w)},m(w,A){h(a,w,A),i(w,t,A),h(n,w,A),i(w,x,A),h(b,w,A),i(w,P,A),h(E,w,A),i(w,z,A),h(S,w,A),i(w,y,A),h($,w,A),i(w,v,A),h(m,w,A),F=!0},p(w,A){const G={};A&8&&(G.data=w[3]),S.$set(G);const ge={};A&64&&(ge.text="Derivative: "+w[6].toFixed(2)),$.$set(ge);const J={};A&4&&(J.data=w[2]),m.$set(J)},i(w){F||(g(a.$$.fragment,w),g(n.$$.fragment,w),g(b.$$.fragment,w),g(E.$$.fragment,w),g(S.$$.fragment,w),g($.$$.fragment,w),g(m.$$.fragment,w),F=!0)},o(w){c(a.$$.fragment,w),c(n.$$.fragment,w),c(b.$$.fragment,w),c(E.$$.fragment,w),c(S.$$.fragment,w),c($.$$.fragment,w),c(m.$$.fragment,w),F=!1},d(w){_(a,w),w&&l(t),_(n,w),w&&l(x),_(b,w),w&&l(P),_(E,w),w&&l(z),_(S,w),w&&l(y),_($,w),w&&l(v),_(m,w)}}}function gl(r){let a;return{c(){a=f("\\alpha")},l(t){a=o(t,"\\alpha")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function cl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function _l(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function dl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function wl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function vl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function xl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function bl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function kl(r){let a;return{c(){a=f("\\alpha")},l(t){a=o(t,"\\alpha")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Tl(r){let a;return{c(){a=f("\\alpha")},l(t){a=o(t,"\\alpha")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function El(r){let a;return{c(){a=f("hyperparameters")},l(t){a=o(t,"hyperparameters")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Sl(r){let a;return{c(){a=f("w")},l(t){a=o(t,"w")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function yl(r){let a;return{c(){a=f("b")},l(t){a=o(t,"b")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Pl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Il(r){let a;return{c(){a=f("convex")},l(t){a=o(t,"convex")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function zl(r){let a;return{c(){a=f("f(x) = x^3 - 5x^2 + 10")},l(t){a=o(t,"f(x) = x^3 - 5x^2 + 10")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Wl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Dl(r){let a,t;return a=new Ps({}),a.$on("click",r[11]),{c(){u(a.$$.fragment)},l(n){p(a.$$.fragment,n)},m(n,x){h(a,n,x),t=!0},p:U,i(n){t||(g(a.$$.fragment,n),t=!0)},o(n){c(a.$$.fragment,n),t=!1},d(n){_(a,n)}}}function Ol(r){let a,t,n,x,b,P,E,z,S,y;return a=new Rt({props:{xTicks:[-3,-2,-1,0,1,2,3,4,5,6],yTicks:[-40,-30,-20,-10,0,10,20,30,40,50],xOffset:-15,yOffset:25}}),n=new ea({props:{text:"x",fontSize:15,type:"latex"}}),b=new ta({props:{text:"f(x)",fontSize:15,type:"latex",x:0}}),E=new Pt({props:{data:r[9]}}),S=new Zt({props:{data:r[5]}}),{c(){u(a.$$.fragment),t=k(),u(n.$$.fragment),x=k(),u(b.$$.fragment),P=k(),u(E.$$.fragment),z=k(),u(S.$$.fragment)},l($){p(a.$$.fragment,$),t=T($),p(n.$$.fragment,$),x=T($),p(b.$$.fragment,$),P=T($),p(E.$$.fragment,$),z=T($),p(S.$$.fragment,$)},m($,v){h(a,$,v),i($,t,v),h(n,$,v),i($,x,v),h(b,$,v),i($,P,v),h(E,$,v),i($,z,v),h(S,$,v),y=!0},p($,v){const m={};v&32&&(m.data=$[5]),S.$set(m)},i($){y||(g(a.$$.fragment,$),g(n.$$.fragment,$),g(b.$$.fragment,$),g(E.$$.fragment,$),g(S.$$.fragment,$),y=!0)},o($){c(a.$$.fragment,$),c(n.$$.fragment,$),c(b.$$.fragment,$),c(E.$$.fragment,$),c(S.$$.fragment,$),y=!1},d($){_(a,$),$&&l(t),_(n,$),$&&l(x),_(b,$),$&&l(P),_(E,$),$&&l(z),_(S,$)}}}function Fl(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Al(r){let a=String.raw`f(x_1, x_2) = x_1^2 + x_2^2`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function Ll(r){let a;return{c(){a=f("x")},l(t){a=o(t,"x")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Bl(r){let a=String.raw`\dfrac{\mathrm{d}}{\mathrm{d}x}f(x)`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function ql(r){let a=String.raw`\dfrac{\mathrm{\partial}}{\mathrm{\partial}x_1}f`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function Yl(r){let a=String.raw`\dfrac{\mathrm{\partial}}{\mathrm{\partial}x_2}f`+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function jl(r){let a=String.raw`
      \mathbf{x} = 
      \begin{bmatrix}
      x_1 \\
      x_2
      \end{bmatrix}
  `+"",t,n;return{c(){t=f(a),n=f(",")},l(x){t=o(x,a),n=o(x,",")},m(x,b){i(x,t,b),i(x,n,b)},p:U,d(x){x&&l(t),x&&l(n)}}}function Vl(r){let a=String.raw`
      \mathbf{\nabla} = 
      \begin{bmatrix}
      \dfrac{\mathrm{\partial}}{\mathrm{\partial}x_1}f \\[8pt] 
      \dfrac{\mathrm{\partial}}{\mathrm{\partial}x_2}f
      \end{bmatrix}
  `+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function Cl(r){let a=String.raw`\mathbf{x}_{t+1} \coloneqq \mathbf{x}_t - \alpha \mathbf{\nabla} `+"",t;return{c(){t=f(a)},l(n){t=o(n,a)},m(n,x){i(n,t,x)},p:U,d(n){n&&l(t)}}}function Gl(r){let a;return{c(){a=f("\\nabla")},l(t){a=o(t,"\\nabla")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Nl(r){let a;return{c(){a=f("gradient")},l(t){a=o(t,"gradient")},m(t,n){i(t,a,n)},d(t){t&&l(a)}}}function Ml(r){let a,t,n,x,b,P,E,z,S,y,$,v,m,F,w,A,G,ge,J,qt,It,se,L,Y,_t,ce,Wa,aa,_e,na,K,Da,de,Oa,we,Fa,ve,Aa,sa,xe,la,B,La,be,Ba,ke,qa,Te,Ya,Ee,ja,Se,Va,ye,Ca,Pe,Ga,Ie,Na,ra,Q,Ma,ze,Xa,We,Ha,De,Ua,$a,Oe,ia,zt,Ja,fa,Fe,oa,Ae,ma,re,Le,Be,Ka,Wt,ua,Qa,$e,Ra,pa,ie,qe,Ye,Za,Dt,Yt=r[0].toFixed(2)+"",ha,en,fe,tn,ga,Ot,an,ca,Z,j,nn,je,sn,Ve,ln,Ce,rn,Ge,$n,Ne,fn,Me,on,mn,N,un,Xe,pn,He,hn,Ue,gn,Je,cn,Ke,_n,dn,Qe,wn,Re,vn,_a,Ze,xn,et,bn,da,le,kn,tt,Tn,at,En,wa,nt,va,st,xa,oe,lt,rt,Sn,Ft,jt=r[1].toFixed(2)+"",ba,yn,me,Pn,ka,At,In,Ta,M,zn,$t,Wn,it,Dn,ft,On,ot,Fn,mt,An,Ea,ut,Sa,pt,ya,Lt,Ln,Pa,ht,Ia,R,Bn,gt,qn,Vt,Yn,jn,ct,Vn,za,Bt,dt;z=new I({props:{$$slots:{default:[Ys]},$$scope:{ctx:r}}}),y=new I({props:{$$slots:{default:[js]},$$scope:{ctx:r}}}),w=new I({props:{$$slots:{default:[Vs]},$$scope:{ctx:r}}}),G=new I({props:{$$slots:{default:[Cs]},$$scope:{ctx:r}}}),J=new I({props:{$$slots:{default:[Gs]},$$scope:{ctx:r}}}),se=new Qt({props:{width:500,height:500,maxWidth:500,domain:[-80,80],range:[0,3e3],padding:{top:40,right:10,bottom:40,left:60},$$slots:{default:[Ns]},$$scope:{ctx:r}}}),ce=new I({props:{$$slots:{default:[Ms]},$$scope:{ctx:r}}}),_e=new Qt({props:{width:500,height:500,maxWidth:500,domain:[-80,80],range:[0,3e3],padding:{top:40,right:10,bottom:40,left:60},$$slots:{default:[Xs]},$$scope:{ctx:r}}}),de=new I({props:{$$slots:{default:[Hs]},$$scope:{ctx:r}}}),we=new I({props:{$$slots:{default:[Us]},$$scope:{ctx:r}}}),ve=new I({props:{$$slots:{default:[Js]},$$scope:{ctx:r}}}),xe=new Qt({props:{width:500,height:500,maxWidth:500,domain:[-80,80],range:[0,3e3],padding:{top:40,right:10,bottom:40,left:60},$$slots:{default:[Ks]},$$scope:{ctx:r}}}),be=new I({props:{$$slots:{default:[Qs]},$$scope:{ctx:r}}}),ke=new I({props:{$$slots:{default:[Rs]},$$scope:{ctx:r}}}),Te=new Kt({props:{$$slots:{default:[Zs]},$$scope:{ctx:r}}}),Ee=new I({props:{$$slots:{default:[el]},$$scope:{ctx:r}}}),Se=new I({props:{$$slots:{default:[tl]},$$scope:{ctx:r}}}),ye=new I({props:{$$slots:{default:[al]},$$scope:{ctx:r}}}),Pe=new I({props:{$$slots:{default:[nl]},$$scope:{ctx:r}}}),Ie=new I({props:{$$slots:{default:[sl]},$$scope:{ctx:r}}}),ze=new I({props:{$$slots:{default:[ll]},$$scope:{ctx:r}}}),We=new Kt({props:{$$slots:{default:[rl]},$$scope:{ctx:r}}}),De=new I({props:{$$slots:{default:[$l]},$$scope:{ctx:r}}}),Oe=new Bs({props:{type:"info",$$slots:{default:[ul]},$$scope:{ctx:r}}}),Fe=new ys({props:{$$slots:{default:[pl]},$$scope:{ctx:r}}}),Ae=new Qt({props:{width:500,height:500,maxWidth:500,domain:[-80,80],range:[0,3e3],padding:{top:40,right:10,bottom:40,left:60},$$slots:{default:[hl]},$$scope:{ctx:r}}}),Be=new I({props:{$$slots:{default:[gl]},$$scope:{ctx:r}}});function Is(e){r[13](e)}let Hn={min:.01,max:1.05,step:.01};r[4]!==void 0&&(Hn.value=r[4]),$e=new Xn({props:Hn}),Cn.push(()=>Gn($e,"value",Is)),Ye=new I({props:{$$slots:{default:[cl]},$$scope:{ctx:r}}});function zs(e){r[14](e)}let Un={min:-60,max:60,step:.1};r[0]!==void 0&&(Un.value=r[0]),fe=new Xn({props:Un}),Cn.push(()=>Gn(fe,"value",zs)),je=new I({props:{$$slots:{default:[_l]},$$scope:{ctx:r}}}),Ve=new I({props:{$$slots:{default:[dl]},$$scope:{ctx:r}}}),Ce=new I({props:{$$slots:{default:[wl]},$$scope:{ctx:r}}}),Ge=new I({props:{$$slots:{default:[vl]},$$scope:{ctx:r}}}),Ne=new I({props:{$$slots:{default:[xl]},$$scope:{ctx:r}}}),Me=new I({props:{$$slots:{default:[bl]},$$scope:{ctx:r}}}),Xe=new I({props:{$$slots:{default:[kl]},$$scope:{ctx:r}}}),He=new I({props:{$$slots:{default:[Tl]},$$scope:{ctx:r}}}),Ue=new Kt({props:{$$slots:{default:[El]},$$scope:{ctx:r}}}),Je=new I({props:{$$slots:{default:[Sl]},$$scope:{ctx:r}}}),Ke=new I({props:{$$slots:{default:[yl]},$$scope:{ctx:r}}}),Re=new I({props:{$$slots:{default:[Pl]},$$scope:{ctx:r}}}),et=new Kt({props:{$$slots:{default:[Il]},$$scope:{ctx:r}}}),tt=new I({props:{$$slots:{default:[zl]},$$scope:{ctx:r}}}),at=new I({props:{$$slots:{default:[Wl]},$$scope:{ctx:r}}}),nt=new ys({props:{$$slots:{default:[Dl]},$$scope:{ctx:r}}}),st=new Qt({props:{width:500,height:250,maxWidth:600,domain:[-3,6],range:[-40,50],padding:{top:40,right:10,bottom:40,left:50},$$slots:{default:[Ol]},$$scope:{ctx:r}}}),rt=new I({props:{$$slots:{default:[Fl]},$$scope:{ctx:r}}});function Ws(e){r[15](e)}let Jn={min:-3,max:6,step:.1};return r[1]!==void 0&&(Jn.value=r[1]),me=new Xn({props:Jn}),Cn.push(()=>Gn(me,"value",Ws)),$t=new I({props:{$$slots:{default:[Al]},$$scope:{ctx:r}}}),it=new I({props:{$$slots:{default:[Ll]},$$scope:{ctx:r}}}),ft=new I({props:{$$slots:{default:[Bl]},$$scope:{ctx:r}}}),ot=new I({props:{$$slots:{default:[ql]},$$scope:{ctx:r}}}),mt=new I({props:{$$slots:{default:[Yl]},$$scope:{ctx:r}}}),ut=new I({props:{$$slots:{default:[jl]},$$scope:{ctx:r}}}),pt=new I({props:{$$slots:{default:[Vl]},$$scope:{ctx:r}}}),ht=new I({props:{$$slots:{default:[Cl]},$$scope:{ctx:r}}}),gt=new I({props:{$$slots:{default:[Gl]},$$scope:{ctx:r}}}),ct=new Kt({props:{$$slots:{default:[Nl]},$$scope:{ctx:r}}}),{c(){a=W("h1"),t=f("Gradient Descent"),n=k(),x=W("div"),b=k(),P=W("p"),E=f(`Before we discuss how we can find the optimal weights and the optimal bias
    in a linear regression setting, let us take a step back and consider how we
    can find the value of variable `),u(z.$$.fragment),S=f(" that minimizes the function "),u(y.$$.fragment),$=f("."),v=k(),m=W("p"),F=f("The equation "),u(w.$$.fragment),A=f(` depicts a parabola. From visual inspection
    we can determine, that the `),u(G.$$.fragment),ge=f(" is lowest when "),u(J.$$.fragment),qt=f(". is exactly 0."),It=k(),u(se.$$.fragment),L=k(),Y=W("p"),_t=f(`In machine learning we rarely have the luxury of being able to visually find
    the optimal solution. Our function is usually dependend on thousands or
    millions of features and that is not something that we can visualize. We
    need to apply an algorithmic procedure, that finds the minimum
    automatically. We start the algorithm by assigning `),u(ce.$$.fragment),Wa=f(` a random
    value. In the example below we picked 55.`),aa=k(),u(_e.$$.fragment),na=k(),K=W("p"),Da=f("Next we calculate the derivative of "),u(de.$$.fragment),Oa=f(" with respect to "),u(we.$$.fragment),Fa=f(". Using the rules of basic calculus we derive "),u(ve.$$.fragment),Aa=f(`. The slope at our starting point is therefore 110. We can draw the tangent
    line at the starting point to visualize the derivative.`),sa=k(),u(xe.$$.fragment),la=k(),B=W("p"),La=f(`The derivative shows us the direction of steepest descent, or simply put the
    derivative tells us in what direction we have to change `),u(be.$$.fragment),Ba=f(`
    if we want to reduce `),u(ke.$$.fragment),qa=f(". The "),u(Te.$$.fragment),Ya=f(` algorithm utilizes that directions and simply subtract the derivative
    `),u(Ee.$$.fragment),ja=f(" from "),u(Se.$$.fragment),Va=f(`. Gradient descent is an iterative algorithm. That means that we keep
    calculating the derivative `),u(ye.$$.fragment),Ca=f(" and updating the variable "),u(Pe.$$.fragment),Ga=f(` until some criterion is met. For
    example once the change in `),u(Ie.$$.fragment),Na=f(` is below a certain threshhold, we
    can assume that we are very close to the minimum.`),ra=k(),Q=W("p"),Ma=f(`While the derivative gives us the direction in which should take a step, the
    derivative does not give us the size of the step. For that purpose we use a
    variable `),u(ze.$$.fragment),Xa=f(`, also called the
    `),u(We.$$.fragment),Ha=f(`. The learning rate scales the
    derivative by multiplying the direction with a value that usually lies
    between 0.1 and 0.001. Larger values of the learning rate could make the
    algorithm diverge. That would mean that `),u(De.$$.fragment),Ua=f(` would get larger
    and larger and never get close to the minimum. While too low values would slow
    down the trainig process dramatically.`),$a=k(),u(Oe.$$.fragment),ia=k(),zt=W("p"),Ja=f(`Below you can play with an interactive example to get some intuition
    regarding the gradient descent algorithm. Each click on the play button
    takes a single gradient descent step, based on the parameters that you can
    change with the sliders.`),fa=k(),u(Fe.$$.fragment),oa=k(),u(Ae.$$.fragment),ma=k(),re=W("div"),Le=W("div"),u(Be.$$.fragment),Ka=k(),Wt=W("p"),ua=f(r[4]),Qa=k(),u($e.$$.fragment),pa=k(),ie=W("div"),qe=W("div"),u(Ye.$$.fragment),Za=k(),Dt=W("p"),ha=f(Yt),en=k(),u(fe.$$.fragment),ga=k(),Ot=W("p"),an=f(`You can learn several things about gradient descent if you play with the
    example.`),ca=k(),Z=W("ol"),j=W("li"),nn=f("If you try positive and negative "),u(je.$$.fragment),sn=f(` values you will observe that
      the sign of the derivative changes based on the sign of the location of `),u(Ve.$$.fragment),ln=f(". That behaviour makes sure that we distract negative values from "),u(Ce.$$.fragment),rn=f(" when "),u(Ge.$$.fragment),$n=f(" is negative and we distract positive values from "),u(Ne.$$.fragment),fn=f(" when "),u(Me.$$.fragment),on=f(` is positive. No matter where we start, the algorithm
      always pushes the variable towards the minimum.`),mn=k(),N=W("li"),un=f("If you try gradient descent with an "),u(Xe.$$.fragment),pn=f(` of 1.01 you will
      observe that the algorithm starts to diverge. Picking the correct learning
      rate is an extremely usefull skill and is generally on of the first things
      to tweak when you want your algorithm to perform better. In fact `),u(He.$$.fragment),hn=f(` is one of the so called
      `),u(Ue.$$.fragment),gn=f(`. A hyperparamter is a parameter
      that is set by the programmer that influences the learning of the
      parameters that you are truly interested in (like `),u(Je.$$.fragment),cn=f(" and "),u(Ke.$$.fragment),_n=f(")."),dn=k(),Qe=W("li"),wn=f(`You should also notice the decrease of the magnitude of the derivative
      when we start getting closer and closer to the optimal value, whiel the
      slope of the tangent gets flatter and flatter. This natural behaviour
      makes sure that we take smaller and smaller steps as we start approaching
      the optimum. This also means that gradient descent does not find an
      optimal value for `),u(Re.$$.fragment),vn=f(` but an approximative one. In many cases
      it is sufficient to be close enough to the optimal value.`),_a=k(),Ze=W("p"),xn=f(`While the gradient descent algorithm is the de facto standard in deep
    learning, it has some limitations. Only when we are dealing with a `),u(et.$$.fragment),bn=f(` function, we have a guarantee that the algorithm will converge to the global
    optimum. A convex function is like the parabola above, a function that is shaped
    like a "bowl". Such a "bowl" shaped function allows the variable to move towards
    the minimum without any barriers.`),da=k(),le=W("p"),kn=f("Below is the graph for the function "),u(tt.$$.fragment),Tn=f(`, a
    non convex function. We start at the `),u(at.$$.fragment),En=f(` position with a value of
    6. If you apply gradient several times (arrow button) you will notice that the
    ball gets stuck in the local minimum and will thus never keep going into the
    direction of the global minimum. This is due to the fact, that at that local
    minimum point the derivative corresponds to 0 and the gradient descent algorithm
    breaks down. You could move the slider below the graph and place the ball to
    the left of 0 and observe that the ball will keep going and going further down.`),wa=k(),u(nt.$$.fragment),va=k(),u(st.$$.fragment),xa=k(),oe=W("div"),lt=W("div"),u(rt.$$.fragment),Sn=k(),Ft=W("p"),ba=f(jt),yn=k(),u(me.$$.fragment),ka=k(),At=W("p"),In=f(`This behaviour has several implications that we should discuss. First, the
    starting position of the variable matters and might have an impact on the
    performance. Second, the following question arises: "why do deep learning
    researchers and practicioners use gradient descent, if the neural network
    function is not convex and there is a chance that the algorithm will get
    stuck in a local minimum?". Simply put, because it works exceptionally well
    in practice. Additionally, we rarely use the "traditional" gradient descent
    algorithm in practice. Over time, researchers discovered that the algorithm
    can be improved by such ideas as "momentum", which keeps the speed of
    gradient descent over many iterations and might thus jump over the local
    minimum. We will cover those ideas later, for now lets focus on the basic
    algorithm.`),Ta=k(),M=W("p"),zn=f(`Before we move on to the part where we discuss how we can apply this
    algorithm to linear regression, let us discuss how we can deal with
    functions that have more than one variable, for example `),u($t.$$.fragment),Wn=f(`. The approach is actually very similar. Instead of calculating the
    derivative with respect to `),u(it.$$.fragment),Dn=k(),u(ft.$$.fragment),On=f(` we need to calculate
    the partial derivatives with respect to all variables, in our case
    `),u(ot.$$.fragment),Fn=f(`
    and `),u(mt.$$.fragment),An=f(`. For convenience we put the partial derivatives and the variables into
    their corresponding vectors.`),Ea=k(),u(ut.$$.fragment),Sa=k(),u(pt.$$.fragment),ya=k(),Lt=W("p"),Ln=f(`The gradient descent algorithm looks almost the same. The only difference is
    the substitution of scalars for vectors.`),Pa=k(),u(ht.$$.fragment),Ia=k(),R=W("p"),Bn=f(`The vector that is represented by
    `),u(gt.$$.fragment),qn=f(" (pronounced "),Vt=W("em"),Yn=f("nabla"),jn=f(") is called the "),u(ct.$$.fragment),Vn=f(", giving its name to the gradient descent algorithm."),za=k(),Bt=W("div"),this.h()},l(e){a=D(e,"H1",{});var s=O(a);t=o(s,"Gradient Descent"),s.forEach(l),n=T(e),x=D(e,"DIV",{class:!0}),O(x).forEach(l),b=T(e),P=D(e,"P",{});var ue=O(P);E=o(ue,`Before we discuss how we can find the optimal weights and the optimal bias
    in a linear regression setting, let us take a step back and consider how we
    can find the value of variable `),p(z.$$.fragment,ue),S=o(ue," that minimizes the function "),p(y.$$.fragment,ue),$=o(ue,"."),ue.forEach(l),v=T(e),m=D(e,"P",{});var ee=O(m);F=o(ee,"The equation "),p(w.$$.fragment,ee),A=o(ee,` depicts a parabola. From visual inspection
    we can determine, that the `),p(G.$$.fragment,ee),ge=o(ee," is lowest when "),p(J.$$.fragment,ee),qt=o(ee,". is exactly 0."),ee.forEach(l),It=T(e),p(se.$$.fragment,e),L=T(e),Y=D(e,"P",{});var wt=O(Y);_t=o(wt,`In machine learning we rarely have the luxury of being able to visually find
    the optimal solution. Our function is usually dependend on thousands or
    millions of features and that is not something that we can visualize. We
    need to apply an algorithmic procedure, that finds the minimum
    automatically. We start the algorithm by assigning `),p(ce.$$.fragment,wt),Wa=o(wt,` a random
    value. In the example below we picked 55.`),wt.forEach(l),aa=T(e),p(_e.$$.fragment,e),na=T(e),K=D(e,"P",{});var te=O(K);Da=o(te,"Next we calculate the derivative of "),p(de.$$.fragment,te),Oa=o(te," with respect to "),p(we.$$.fragment,te),Fa=o(te,". Using the rules of basic calculus we derive "),p(ve.$$.fragment,te),Aa=o(te,`. The slope at our starting point is therefore 110. We can draw the tangent
    line at the starting point to visualize the derivative.`),te.forEach(l),sa=T(e),p(xe.$$.fragment,e),la=T(e),B=D(e,"P",{});var q=O(B);La=o(q,`The derivative shows us the direction of steepest descent, or simply put the
    derivative tells us in what direction we have to change `),p(be.$$.fragment,q),Ba=o(q,`
    if we want to reduce `),p(ke.$$.fragment,q),qa=o(q,". The "),p(Te.$$.fragment,q),Ya=o(q,` algorithm utilizes that directions and simply subtract the derivative
    `),p(Ee.$$.fragment,q),ja=o(q," from "),p(Se.$$.fragment,q),Va=o(q,`. Gradient descent is an iterative algorithm. That means that we keep
    calculating the derivative `),p(ye.$$.fragment,q),Ca=o(q," and updating the variable "),p(Pe.$$.fragment,q),Ga=o(q,` until some criterion is met. For
    example once the change in `),p(Ie.$$.fragment,q),Na=o(q,` is below a certain threshhold, we
    can assume that we are very close to the minimum.`),q.forEach(l),ra=T(e),Q=D(e,"P",{});var ae=O(Q);Ma=o(ae,`While the derivative gives us the direction in which should take a step, the
    derivative does not give us the size of the step. For that purpose we use a
    variable `),p(ze.$$.fragment,ae),Xa=o(ae,`, also called the
    `),p(We.$$.fragment,ae),Ha=o(ae,`. The learning rate scales the
    derivative by multiplying the direction with a value that usually lies
    between 0.1 and 0.001. Larger values of the learning rate could make the
    algorithm diverge. That would mean that `),p(De.$$.fragment,ae),Ua=o(ae,` would get larger
    and larger and never get close to the minimum. While too low values would slow
    down the trainig process dramatically.`),ae.forEach(l),$a=T(e),p(Oe.$$.fragment,e),ia=T(e),zt=D(e,"P",{});var Ct=O(zt);Ja=o(Ct,`Below you can play with an interactive example to get some intuition
    regarding the gradient descent algorithm. Each click on the play button
    takes a single gradient descent step, based on the parameters that you can
    change with the sliders.`),Ct.forEach(l),fa=T(e),p(Fe.$$.fragment,e),oa=T(e),p(Ae.$$.fragment,e),ma=T(e),re=D(e,"DIV",{class:!0});var vt=O(re);Le=D(vt,"DIV",{class:!0});var xt=O(Le);p(Be.$$.fragment,xt),Ka=T(xt),Wt=D(xt,"P",{class:!0});var Gt=O(Wt);ua=o(Gt,r[4]),Gt.forEach(l),xt.forEach(l),Qa=T(vt),p($e.$$.fragment,vt),vt.forEach(l),pa=T(e),ie=D(e,"DIV",{class:!0});var bt=O(ie);qe=D(bt,"DIV",{class:!0});var kt=O(qe);p(Ye.$$.fragment,kt),Za=T(kt),Dt=D(kt,"P",{class:!0});var Nt=O(Dt);ha=o(Nt,Yt),Nt.forEach(l),kt.forEach(l),en=T(bt),p(fe.$$.fragment,bt),bt.forEach(l),ga=T(e),Ot=D(e,"P",{});var Mt=O(Ot);an=o(Mt,`You can learn several things about gradient descent if you play with the
    example.`),Mt.forEach(l),ca=T(e),Z=D(e,"OL",{class:!0});var pe=O(Z);j=D(pe,"LI",{class:!0});var V=O(j);nn=o(V,"If you try positive and negative "),p(je.$$.fragment,V),sn=o(V,` values you will observe that
      the sign of the derivative changes based on the sign of the location of `),p(Ve.$$.fragment,V),ln=o(V,". That behaviour makes sure that we distract negative values from "),p(Ce.$$.fragment,V),rn=o(V," when "),p(Ge.$$.fragment,V),$n=o(V," is negative and we distract positive values from "),p(Ne.$$.fragment,V),fn=o(V," when "),p(Me.$$.fragment,V),on=o(V,` is positive. No matter where we start, the algorithm
      always pushes the variable towards the minimum.`),V.forEach(l),mn=T(pe),N=D(pe,"LI",{class:!0});var X=O(N);un=o(X,"If you try gradient descent with an "),p(Xe.$$.fragment,X),pn=o(X,` of 1.01 you will
      observe that the algorithm starts to diverge. Picking the correct learning
      rate is an extremely usefull skill and is generally on of the first things
      to tweak when you want your algorithm to perform better. In fact `),p(He.$$.fragment,X),hn=o(X,` is one of the so called
      `),p(Ue.$$.fragment,X),gn=o(X,`. A hyperparamter is a parameter
      that is set by the programmer that influences the learning of the
      parameters that you are truly interested in (like `),p(Je.$$.fragment,X),cn=o(X," and "),p(Ke.$$.fragment,X),_n=o(X,")."),X.forEach(l),dn=T(pe),Qe=D(pe,"LI",{class:!0});var Tt=O(Qe);wn=o(Tt,`You should also notice the decrease of the magnitude of the derivative
      when we start getting closer and closer to the optimal value, whiel the
      slope of the tangent gets flatter and flatter. This natural behaviour
      makes sure that we take smaller and smaller steps as we start approaching
      the optimum. This also means that gradient descent does not find an
      optimal value for `),p(Re.$$.fragment,Tt),vn=o(Tt,` but an approximative one. In many cases
      it is sufficient to be close enough to the optimal value.`),Tt.forEach(l),pe.forEach(l),_a=T(e),Ze=D(e,"P",{});var Et=O(Ze);xn=o(Et,`While the gradient descent algorithm is the de facto standard in deep
    learning, it has some limitations. Only when we are dealing with a `),p(et.$$.fragment,Et),bn=o(Et,` function, we have a guarantee that the algorithm will converge to the global
    optimum. A convex function is like the parabola above, a function that is shaped
    like a "bowl". Such a "bowl" shaped function allows the variable to move towards
    the minimum without any barriers.`),Et.forEach(l),da=T(e),le=D(e,"P",{});var he=O(le);kn=o(he,"Below is the graph for the function "),p(tt.$$.fragment,he),Tn=o(he,`, a
    non convex function. We start at the `),p(at.$$.fragment,he),En=o(he,` position with a value of
    6. If you apply gradient several times (arrow button) you will notice that the
    ball gets stuck in the local minimum and will thus never keep going into the
    direction of the global minimum. This is due to the fact, that at that local
    minimum point the derivative corresponds to 0 and the gradient descent algorithm
    breaks down. You could move the slider below the graph and place the ball to
    the left of 0 and observe that the ball will keep going and going further down.`),he.forEach(l),wa=T(e),p(nt.$$.fragment,e),va=T(e),p(st.$$.fragment,e),xa=T(e),oe=D(e,"DIV",{class:!0});var St=O(oe);lt=D(St,"DIV",{class:!0});var yt=O(lt);p(rt.$$.fragment,yt),Sn=T(yt),Ft=D(yt,"P",{class:!0});var Xt=O(Ft);ba=o(Xt,jt),Xt.forEach(l),yt.forEach(l),yn=T(St),p(me.$$.fragment,St),St.forEach(l),ka=T(e),At=D(e,"P",{});var Ht=O(At);In=o(Ht,`This behaviour has several implications that we should discuss. First, the
    starting position of the variable matters and might have an impact on the
    performance. Second, the following question arises: "why do deep learning
    researchers and practicioners use gradient descent, if the neural network
    function is not convex and there is a chance that the algorithm will get
    stuck in a local minimum?". Simply put, because it works exceptionally well
    in practice. Additionally, we rarely use the "traditional" gradient descent
    algorithm in practice. Over time, researchers discovered that the algorithm
    can be improved by such ideas as "momentum", which keeps the speed of
    gradient descent over many iterations and might thus jump over the local
    minimum. We will cover those ideas later, for now lets focus on the basic
    algorithm.`),Ht.forEach(l),Ta=T(e),M=D(e,"P",{});var H=O(M);zn=o(H,`Before we move on to the part where we discuss how we can apply this
    algorithm to linear regression, let us discuss how we can deal with
    functions that have more than one variable, for example `),p($t.$$.fragment,H),Wn=o(H,`. The approach is actually very similar. Instead of calculating the
    derivative with respect to `),p(it.$$.fragment,H),Dn=T(H),p(ft.$$.fragment,H),On=o(H,` we need to calculate
    the partial derivatives with respect to all variables, in our case
    `),p(ot.$$.fragment,H),Fn=o(H,`
    and `),p(mt.$$.fragment,H),An=o(H,`. For convenience we put the partial derivatives and the variables into
    their corresponding vectors.`),H.forEach(l),Ea=T(e),p(ut.$$.fragment,e),Sa=T(e),p(pt.$$.fragment,e),ya=T(e),Lt=D(e,"P",{});var Ut=O(Lt);Ln=o(Ut,`The gradient descent algorithm looks almost the same. The only difference is
    the substitution of scalars for vectors.`),Ut.forEach(l),Pa=T(e),p(ht.$$.fragment,e),Ia=T(e),R=D(e,"P",{});var ne=O(R);Bn=o(ne,`The vector that is represented by
    `),p(gt.$$.fragment,ne),qn=o(ne," (pronounced "),Vt=D(ne,"EM",{});var Jt=O(Vt);Yn=o(Jt,"nabla"),Jt.forEach(l),jn=o(ne,") is called the "),p(ct.$$.fragment,ne),Vn=o(ne,", giving its name to the gradient descent algorithm."),ne.forEach(l),za=T(e),Bt=D(e,"DIV",{class:!0}),O(Bt).forEach(l),this.h()},h(){C(x,"class","separator"),C(Wt,"class","m-0 ml-2"),C(Le,"class","flex justify-center items-center w-28"),C(re,"class","flex justify-center items-center"),C(Dt,"class","m-0 ml-2"),C(qe,"class","flex justify-center items-center w-28"),C(ie,"class","flex justify-center items-center"),C(j,"class","mb-2"),C(N,"class","mb-2"),C(Qe,"class","mb-2"),C(Z,"class","list-decimal list-inside"),C(Ft,"class","m-0 ml-2"),C(lt,"class","flex justify-center items-center w-24"),C(oe,"class","flex justify-center items-center"),C(Bt,"class","separator")},m(e,s){i(e,a,s),d(a,t),i(e,n,s),i(e,x,s),i(e,b,s),i(e,P,s),d(P,E),h(z,P,null),d(P,S),h(y,P,null),d(P,$),i(e,v,s),i(e,m,s),d(m,F),h(w,m,null),d(m,A),h(G,m,null),d(m,ge),h(J,m,null),d(m,qt),i(e,It,s),h(se,e,s),i(e,L,s),i(e,Y,s),d(Y,_t),h(ce,Y,null),d(Y,Wa),i(e,aa,s),h(_e,e,s),i(e,na,s),i(e,K,s),d(K,Da),h(de,K,null),d(K,Oa),h(we,K,null),d(K,Fa),h(ve,K,null),d(K,Aa),i(e,sa,s),h(xe,e,s),i(e,la,s),i(e,B,s),d(B,La),h(be,B,null),d(B,Ba),h(ke,B,null),d(B,qa),h(Te,B,null),d(B,Ya),h(Ee,B,null),d(B,ja),h(Se,B,null),d(B,Va),h(ye,B,null),d(B,Ca),h(Pe,B,null),d(B,Ga),h(Ie,B,null),d(B,Na),i(e,ra,s),i(e,Q,s),d(Q,Ma),h(ze,Q,null),d(Q,Xa),h(We,Q,null),d(Q,Ha),h(De,Q,null),d(Q,Ua),i(e,$a,s),h(Oe,e,s),i(e,ia,s),i(e,zt,s),d(zt,Ja),i(e,fa,s),h(Fe,e,s),i(e,oa,s),h(Ae,e,s),i(e,ma,s),i(e,re,s),d(re,Le),h(Be,Le,null),d(Le,Ka),d(Le,Wt),d(Wt,ua),d(re,Qa),h($e,re,null),i(e,pa,s),i(e,ie,s),d(ie,qe),h(Ye,qe,null),d(qe,Za),d(qe,Dt),d(Dt,ha),d(ie,en),h(fe,ie,null),i(e,ga,s),i(e,Ot,s),d(Ot,an),i(e,ca,s),i(e,Z,s),d(Z,j),d(j,nn),h(je,j,null),d(j,sn),h(Ve,j,null),d(j,ln),h(Ce,j,null),d(j,rn),h(Ge,j,null),d(j,$n),h(Ne,j,null),d(j,fn),h(Me,j,null),d(j,on),d(Z,mn),d(Z,N),d(N,un),h(Xe,N,null),d(N,pn),h(He,N,null),d(N,hn),h(Ue,N,null),d(N,gn),h(Je,N,null),d(N,cn),h(Ke,N,null),d(N,_n),d(Z,dn),d(Z,Qe),d(Qe,wn),h(Re,Qe,null),d(Qe,vn),i(e,_a,s),i(e,Ze,s),d(Ze,xn),h(et,Ze,null),d(Ze,bn),i(e,da,s),i(e,le,s),d(le,kn),h(tt,le,null),d(le,Tn),h(at,le,null),d(le,En),i(e,wa,s),h(nt,e,s),i(e,va,s),h(st,e,s),i(e,xa,s),i(e,oe,s),d(oe,lt),h(rt,lt,null),d(lt,Sn),d(lt,Ft),d(Ft,ba),d(oe,yn),h(me,oe,null),i(e,ka,s),i(e,At,s),d(At,In),i(e,Ta,s),i(e,M,s),d(M,zn),h($t,M,null),d(M,Wn),h(it,M,null),d(M,Dn),h(ft,M,null),d(M,On),h(ot,M,null),d(M,Fn),h(mt,M,null),d(M,An),i(e,Ea,s),h(ut,e,s),i(e,Sa,s),h(pt,e,s),i(e,ya,s),i(e,Lt,s),d(Lt,Ln),i(e,Pa,s),h(ht,e,s),i(e,Ia,s),i(e,R,s),d(R,Bn),h(gt,R,null),d(R,qn),d(R,Vt),d(Vt,Yn),d(R,jn),h(ct,R,null),d(R,Vn),i(e,za,s),i(e,Bt,s),dt=!0},p(e,s){const ue={};s&1048576&&(ue.$$scope={dirty:s,ctx:e}),z.$set(ue);const ee={};s&1048576&&(ee.$$scope={dirty:s,ctx:e}),y.$set(ee);const wt={};s&1048576&&(wt.$$scope={dirty:s,ctx:e}),w.$set(wt);const te={};s&1048576&&(te.$$scope={dirty:s,ctx:e}),G.$set(te);const q={};s&1048576&&(q.$$scope={dirty:s,ctx:e}),J.$set(q);const ae={};s&1048576&&(ae.$$scope={dirty:s,ctx:e}),se.$set(ae);const Ct={};s&1048576&&(Ct.$$scope={dirty:s,ctx:e}),ce.$set(Ct);const vt={};s&1048576&&(vt.$$scope={dirty:s,ctx:e}),_e.$set(vt);const xt={};s&1048576&&(xt.$$scope={dirty:s,ctx:e}),de.$set(xt);const Gt={};s&1048576&&(Gt.$$scope={dirty:s,ctx:e}),we.$set(Gt);const bt={};s&1048576&&(bt.$$scope={dirty:s,ctx:e}),ve.$set(bt);const kt={};s&1048584&&(kt.$$scope={dirty:s,ctx:e}),xe.$set(kt);const Nt={};s&1048576&&(Nt.$$scope={dirty:s,ctx:e}),be.$set(Nt);const Mt={};s&1048576&&(Mt.$$scope={dirty:s,ctx:e}),ke.$set(Mt);const pe={};s&1048576&&(pe.$$scope={dirty:s,ctx:e}),Te.$set(pe);const V={};s&1048576&&(V.$$scope={dirty:s,ctx:e}),Ee.$set(V);const X={};s&1048576&&(X.$$scope={dirty:s,ctx:e}),Se.$set(X);const Tt={};s&1048576&&(Tt.$$scope={dirty:s,ctx:e}),ye.$set(Tt);const Et={};s&1048576&&(Et.$$scope={dirty:s,ctx:e}),Pe.$set(Et);const he={};s&1048576&&(he.$$scope={dirty:s,ctx:e}),Ie.$set(he);const St={};s&1048576&&(St.$$scope={dirty:s,ctx:e}),ze.$set(St);const yt={};s&1048576&&(yt.$$scope={dirty:s,ctx:e}),We.$set(yt);const Xt={};s&1048576&&(Xt.$$scope={dirty:s,ctx:e}),De.$set(Xt);const Ht={};s&1048576&&(Ht.$$scope={dirty:s,ctx:e}),Oe.$set(Ht);const H={};s&1048576&&(H.$$scope={dirty:s,ctx:e}),Fe.$set(H);const Ut={};s&1048652&&(Ut.$$scope={dirty:s,ctx:e}),Ae.$set(Ut);const ne={};s&1048576&&(ne.$$scope={dirty:s,ctx:e}),Be.$set(ne),(!dt||s&16)&&Nn(ua,e[4]);const Jt={};!Ra&&s&16&&(Ra=!0,Jt.value=e[4],Mn(()=>Ra=!1)),$e.$set(Jt);const Kn={};s&1048576&&(Kn.$$scope={dirty:s,ctx:e}),Ye.$set(Kn),(!dt||s&1)&&Yt!==(Yt=e[0].toFixed(2)+"")&&Nn(ha,Yt);const Qn={};!tn&&s&1&&(tn=!0,Qn.value=e[0],Mn(()=>tn=!1)),fe.$set(Qn);const Rn={};s&1048576&&(Rn.$$scope={dirty:s,ctx:e}),je.$set(Rn);const Zn={};s&1048576&&(Zn.$$scope={dirty:s,ctx:e}),Ve.$set(Zn);const es={};s&1048576&&(es.$$scope={dirty:s,ctx:e}),Ce.$set(es);const ts={};s&1048576&&(ts.$$scope={dirty:s,ctx:e}),Ge.$set(ts);const as={};s&1048576&&(as.$$scope={dirty:s,ctx:e}),Ne.$set(as);const ns={};s&1048576&&(ns.$$scope={dirty:s,ctx:e}),Me.$set(ns);const ss={};s&1048576&&(ss.$$scope={dirty:s,ctx:e}),Xe.$set(ss);const ls={};s&1048576&&(ls.$$scope={dirty:s,ctx:e}),He.$set(ls);const rs={};s&1048576&&(rs.$$scope={dirty:s,ctx:e}),Ue.$set(rs);const $s={};s&1048576&&($s.$$scope={dirty:s,ctx:e}),Je.$set($s);const is={};s&1048576&&(is.$$scope={dirty:s,ctx:e}),Ke.$set(is);const fs={};s&1048576&&(fs.$$scope={dirty:s,ctx:e}),Re.$set(fs);const os={};s&1048576&&(os.$$scope={dirty:s,ctx:e}),et.$set(os);const ms={};s&1048576&&(ms.$$scope={dirty:s,ctx:e}),tt.$set(ms);const us={};s&1048576&&(us.$$scope={dirty:s,ctx:e}),at.$set(us);const ps={};s&1048576&&(ps.$$scope={dirty:s,ctx:e}),nt.$set(ps);const hs={};s&1048608&&(hs.$$scope={dirty:s,ctx:e}),st.$set(hs);const gs={};s&1048576&&(gs.$$scope={dirty:s,ctx:e}),rt.$set(gs),(!dt||s&2)&&jt!==(jt=e[1].toFixed(2)+"")&&Nn(ba,jt);const cs={};!Pn&&s&2&&(Pn=!0,cs.value=e[1],Mn(()=>Pn=!1)),me.$set(cs);const _s={};s&1048576&&(_s.$$scope={dirty:s,ctx:e}),$t.$set(_s);const ds={};s&1048576&&(ds.$$scope={dirty:s,ctx:e}),it.$set(ds);const ws={};s&1048576&&(ws.$$scope={dirty:s,ctx:e}),ft.$set(ws);const vs={};s&1048576&&(vs.$$scope={dirty:s,ctx:e}),ot.$set(vs);const xs={};s&1048576&&(xs.$$scope={dirty:s,ctx:e}),mt.$set(xs);const bs={};s&1048576&&(bs.$$scope={dirty:s,ctx:e}),ut.$set(bs);const ks={};s&1048576&&(ks.$$scope={dirty:s,ctx:e}),pt.$set(ks);const Ts={};s&1048576&&(Ts.$$scope={dirty:s,ctx:e}),ht.$set(Ts);const Es={};s&1048576&&(Es.$$scope={dirty:s,ctx:e}),gt.$set(Es);const Ss={};s&1048576&&(Ss.$$scope={dirty:s,ctx:e}),ct.$set(Ss)},i(e){dt||(g(z.$$.fragment,e),g(y.$$.fragment,e),g(w.$$.fragment,e),g(G.$$.fragment,e),g(J.$$.fragment,e),g(se.$$.fragment,e),g(ce.$$.fragment,e),g(_e.$$.fragment,e),g(de.$$.fragment,e),g(we.$$.fragment,e),g(ve.$$.fragment,e),g(xe.$$.fragment,e),g(be.$$.fragment,e),g(ke.$$.fragment,e),g(Te.$$.fragment,e),g(Ee.$$.fragment,e),g(Se.$$.fragment,e),g(ye.$$.fragment,e),g(Pe.$$.fragment,e),g(Ie.$$.fragment,e),g(ze.$$.fragment,e),g(We.$$.fragment,e),g(De.$$.fragment,e),g(Oe.$$.fragment,e),g(Fe.$$.fragment,e),g(Ae.$$.fragment,e),g(Be.$$.fragment,e),g($e.$$.fragment,e),g(Ye.$$.fragment,e),g(fe.$$.fragment,e),g(je.$$.fragment,e),g(Ve.$$.fragment,e),g(Ce.$$.fragment,e),g(Ge.$$.fragment,e),g(Ne.$$.fragment,e),g(Me.$$.fragment,e),g(Xe.$$.fragment,e),g(He.$$.fragment,e),g(Ue.$$.fragment,e),g(Je.$$.fragment,e),g(Ke.$$.fragment,e),g(Re.$$.fragment,e),g(et.$$.fragment,e),g(tt.$$.fragment,e),g(at.$$.fragment,e),g(nt.$$.fragment,e),g(st.$$.fragment,e),g(rt.$$.fragment,e),g(me.$$.fragment,e),g($t.$$.fragment,e),g(it.$$.fragment,e),g(ft.$$.fragment,e),g(ot.$$.fragment,e),g(mt.$$.fragment,e),g(ut.$$.fragment,e),g(pt.$$.fragment,e),g(ht.$$.fragment,e),g(gt.$$.fragment,e),g(ct.$$.fragment,e),dt=!0)},o(e){c(z.$$.fragment,e),c(y.$$.fragment,e),c(w.$$.fragment,e),c(G.$$.fragment,e),c(J.$$.fragment,e),c(se.$$.fragment,e),c(ce.$$.fragment,e),c(_e.$$.fragment,e),c(de.$$.fragment,e),c(we.$$.fragment,e),c(ve.$$.fragment,e),c(xe.$$.fragment,e),c(be.$$.fragment,e),c(ke.$$.fragment,e),c(Te.$$.fragment,e),c(Ee.$$.fragment,e),c(Se.$$.fragment,e),c(ye.$$.fragment,e),c(Pe.$$.fragment,e),c(Ie.$$.fragment,e),c(ze.$$.fragment,e),c(We.$$.fragment,e),c(De.$$.fragment,e),c(Oe.$$.fragment,e),c(Fe.$$.fragment,e),c(Ae.$$.fragment,e),c(Be.$$.fragment,e),c($e.$$.fragment,e),c(Ye.$$.fragment,e),c(fe.$$.fragment,e),c(je.$$.fragment,e),c(Ve.$$.fragment,e),c(Ce.$$.fragment,e),c(Ge.$$.fragment,e),c(Ne.$$.fragment,e),c(Me.$$.fragment,e),c(Xe.$$.fragment,e),c(He.$$.fragment,e),c(Ue.$$.fragment,e),c(Je.$$.fragment,e),c(Ke.$$.fragment,e),c(Re.$$.fragment,e),c(et.$$.fragment,e),c(tt.$$.fragment,e),c(at.$$.fragment,e),c(nt.$$.fragment,e),c(st.$$.fragment,e),c(rt.$$.fragment,e),c(me.$$.fragment,e),c($t.$$.fragment,e),c(it.$$.fragment,e),c(ft.$$.fragment,e),c(ot.$$.fragment,e),c(mt.$$.fragment,e),c(ut.$$.fragment,e),c(pt.$$.fragment,e),c(ht.$$.fragment,e),c(gt.$$.fragment,e),c(ct.$$.fragment,e),dt=!1},d(e){e&&l(a),e&&l(n),e&&l(x),e&&l(b),e&&l(P),_(z),_(y),e&&l(v),e&&l(m),_(w),_(G),_(J),e&&l(It),_(se,e),e&&l(L),e&&l(Y),_(ce),e&&l(aa),_(_e,e),e&&l(na),e&&l(K),_(de),_(we),_(ve),e&&l(sa),_(xe,e),e&&l(la),e&&l(B),_(be),_(ke),_(Te),_(Ee),_(Se),_(ye),_(Pe),_(Ie),e&&l(ra),e&&l(Q),_(ze),_(We),_(De),e&&l($a),_(Oe,e),e&&l(ia),e&&l(zt),e&&l(fa),_(Fe,e),e&&l(oa),_(Ae,e),e&&l(ma),e&&l(re),_(Be),_($e),e&&l(pa),e&&l(ie),_(Ye),_(fe),e&&l(ga),e&&l(Ot),e&&l(ca),e&&l(Z),_(je),_(Ve),_(Ce),_(Ge),_(Ne),_(Me),_(Xe),_(He),_(Ue),_(Je),_(Ke),_(Re),e&&l(_a),e&&l(Ze),_(et),e&&l(da),e&&l(le),_(tt),_(at),e&&l(wa),_(nt,e),e&&l(va),_(st,e),e&&l(xa),e&&l(oe),_(rt),_(me),e&&l(ka),e&&l(At),e&&l(Ta),e&&l(M),_($t),_(it),_(ft),_(ot),_(mt),e&&l(Ea),_(ut,e),e&&l(Sa),_(pt,e),e&&l(ya),e&&l(Lt),e&&l(Pa),_(ht,e),e&&l(Ia),e&&l(R),_(gt),_(ct),e&&l(za),e&&l(Bt)}}}function Xl(r){let a,t,n,x;return n=new Ls({props:{$$slots:{default:[Ml]},$$scope:{ctx:r}}}),{c(){a=W("meta"),t=k(),u(n.$$.fragment),this.h()},l(b){const P=As("svelte-zwgicu",document.head);a=D(P,"META",{name:!0,content:!0}),P.forEach(l),t=T(b),p(n.$$.fragment,b),this.h()},h(){document.title="Gradient Descent - World4AI",C(a,"name","description"),C(a,"content","Gradient descent is the algorithm that utilizes caclulus to find the parameters, that minimize the function. The algorithm works in an iterative fashion, where we change the parameters until a certain stop criterion is met.")},m(b,P){d(document.head,a),i(b,t,P),h(n,b,P),x=!0},p(b,[P]){const E={};P&1048703&&(E.$$scope={dirty:P,ctx:b}),n.$set(E)},i(b){x||(g(n.$$.fragment,b),x=!0)},o(b){c(n.$$.fragment,b),x=!1},d(b){l(a),b&&l(t),_(n,b)}}}let Hl=.01;function Ul(r,a,t){let n,x,b,P=[],E=[],z=[],S=[],y=55,$=.01;z.push({x:55,y:55**2});let v=[],m=6;for(let L=-60;L<=60;L++){let Y=L,_t=Y**2;P.push({x:Y,y:_t})}let F=[];for(let L=-6;L<=7;L+=.1){let Y=L,_t=Y**3-5*Y**2+10;F.push({x:Y,y:_t})}function w(){t(2,E=[]),E.push({x:y,y:n})}function A(){t(5,v=[]),v.push({x:m,y:b})}function G(){t(3,S=[]);let L=2*y,Y=n-L*y;S.push({x:-100,y:L*-100+Y}),S.push({x:100,y:L*100+Y})}function ge(){t(0,y=y-$*2*y)}function J(){t(1,m=m-Hl*(3*m**2-10*m))}function qt(L){$=L,t(4,$)}function It(L){y=L,t(0,y)}function se(L){m=L,t(1,m)}return r.$$.update=()=>{r.$$.dirty&1&&(n=y**2),r.$$.dirty&1&&t(6,x=2*y),r.$$.dirty&2&&t(12,b=m**3-5*m**2+10),r.$$.dirty&1&&y&&w(),r.$$.dirty&1&&y&&G(),r.$$.dirty&4096&&b&&A()},[y,m,E,S,$,v,x,P,z,F,ge,J,b,qt,It,se]}class ir extends Ds{constructor(a){super(),Os(this,a,Ul,Xl,Fs,{})}}export{ir as default};
