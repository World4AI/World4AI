import{S as bt,i as vt,s as yt,k as B,a as v,q as L,y as _,W as kt,l as C,h as f,c as y,m as P,r as A,z as g,n as fe,N as T,b as p,A as w,g as m,d as h,B as b,Q as Et,R as xt,C as ye,e as _e,v as tt,f as nt,P as ot}from"../chunks/index.4d92b023.js";import{C as Vt}from"../chunks/Container.b0705c7b.js";import{F as Rt,I as Nt}from"../chunks/InternalLink.7deb899c.js";import{C as Lt}from"../chunks/Convolution.b73a3805.js";import{S as At}from"../chunks/SvgContainer.f70b5745.js";import{H as ut}from"../chunks/Highlight.b7c1de53.js";import{P as q}from"../chunks/PythonCode.212ba7a6.js";import{B as ie}from"../chunks/Block.059eddcd.js";import{A as O}from"../chunks/Arrow.ae91874c.js";import{P as Dt}from"../chunks/Plus.fc904b16.js";import{T as Tt,a as Bt,b as Ct,R as wt,H as Pt,D as zt}from"../chunks/HeaderEntry.2b6e8f51.js";function pt(c,t,a){const n=c.slice();return n[4]=t[a],n}function mt(c,t,a){const n=c.slice();return n[7]=t[a],n[9]=a,n}function ht(c,t,a){const n=c.slice();return n[10]=t[a],n}function It(c){let t;return{c(){t=L("ResNet")},l(a){t=A(a,"ResNet")},m(a,n){p(a,t,n)},d(a){a&&f(t)}}}function Ft(c){let t,a,n,o,l,r,i,s,R,D,I,$,E,z,S,F,W,H;return a=new ie({props:{x:V/2,y:x-k/2-Y,width:M,height:k,text:"Input"}}),n=new ie({props:{x:V/2,y:x-k/2-Y-c[3],width:M,height:k,text:"Conv2d"}}),o=new ie({props:{x:V/2,y:x-k/2-Y-c[3]*2,width:M,height:k,text:"BatchNorm2d"}}),l=new ie({props:{x:V/2,y:x-k/2-Y-c[3]*3,width:M,height:k,text:"ReLU"}}),r=new ie({props:{x:V/2,y:x-k/2-Y-c[3]*4,width:M,height:k,text:"Conv2d"}}),i=new ie({props:{x:V/2,y:x-k/2-Y-c[3]*5,width:M,height:k,text:"BatchNorm"}}),s=new Dt({props:{x:V/2,y:x-k/2-Y-c[3]*6,radius:5,offset:2}}),R=new ie({props:{x:V/2,y:x-k/2-Y-c[3]*7,width:M,height:k,text:"ReLU"}}),D=new O({props:{data:[{x:V/2,y:x-k},{x:V/2,y:x-c[3]+3}],dashed:"true",moving:"true"}}),I=new O({props:{data:[{x:V/2,y:x-k-c[3]},{x:V/2,y:x-c[3]*2+3}],dashed:"true",moving:"true"}}),$=new O({props:{data:[{x:V/2,y:x-k-c[3]*2},{x:V/2,y:x-c[3]*3+3}],dashed:"true",moving:"true"}}),E=new O({props:{data:[{x:V/2,y:x-k-c[3]*3},{x:V/2,y:x-c[3]*4+3}],dashed:"true",moving:"true"}}),z=new O({props:{data:[{x:V/2,y:x-k-c[3]*4},{x:V/2,y:x-c[3]*5+3}],dashed:"true",moving:"true"}}),S=new O({props:{data:[{x:V/2,y:x-k-c[3]*5},{x:V/2,y:x-c[3]*6}],dashed:"true",moving:"true"}}),F=new O({props:{data:[{x:V/2,y:x-k-c[3]*6},{x:V/2,y:x-c[3]*7+3}],dashed:"true",moving:"true"}}),W=new O({props:{data:[{x:V/2-M/2,y:x-k+k/2},{x:V/2-M/2-15,y:x-k+k/2},{x:V/2-M/2-15,y:x-k+k/2-c[3]*6},{x:V/2-10,y:x-k+k/2-c[3]*6}],dashed:"true",moving:"true"}}),{c(){t=Et("svg"),_(a.$$.fragment),_(n.$$.fragment),_(o.$$.fragment),_(l.$$.fragment),_(r.$$.fragment),_(i.$$.fragment),_(s.$$.fragment),_(R.$$.fragment),_(D.$$.fragment),_(I.$$.fragment),_($.$$.fragment),_(E.$$.fragment),_(z.$$.fragment),_(S.$$.fragment),_(F.$$.fragment),_(W.$$.fragment),this.h()},l(d){t=xt(d,"svg",{viewBox:!0});var N=P(t);g(a.$$.fragment,N),g(n.$$.fragment,N),g(o.$$.fragment,N),g(l.$$.fragment,N),g(r.$$.fragment,N),g(i.$$.fragment,N),g(s.$$.fragment,N),g(R.$$.fragment,N),g(D.$$.fragment,N),g(I.$$.fragment,N),g($.$$.fragment,N),g(E.$$.fragment,N),g(z.$$.fragment,N),g(S.$$.fragment,N),g(F.$$.fragment,N),g(W.$$.fragment,N),N.forEach(f),this.h()},h(){fe(t,"viewBox","0 0 "+V+" "+x)},m(d,N){p(d,t,N),w(a,t,null),w(n,t,null),w(o,t,null),w(l,t,null),w(r,t,null),w(i,t,null),w(s,t,null),w(R,t,null),w(D,t,null),w(I,t,null),w($,t,null),w(E,t,null),w(z,t,null),w(S,t,null),w(F,t,null),w(W,t,null),H=!0},p:ye,i(d){H||(m(a.$$.fragment,d),m(n.$$.fragment,d),m(o.$$.fragment,d),m(l.$$.fragment,d),m(r.$$.fragment,d),m(i.$$.fragment,d),m(s.$$.fragment,d),m(R.$$.fragment,d),m(D.$$.fragment,d),m(I.$$.fragment,d),m($.$$.fragment,d),m(E.$$.fragment,d),m(z.$$.fragment,d),m(S.$$.fragment,d),m(F.$$.fragment,d),m(W.$$.fragment,d),H=!0)},o(d){h(a.$$.fragment,d),h(n.$$.fragment,d),h(o.$$.fragment,d),h(l.$$.fragment,d),h(r.$$.fragment,d),h(i.$$.fragment,d),h(s.$$.fragment,d),h(R.$$.fragment,d),h(D.$$.fragment,d),h(I.$$.fragment,d),h($.$$.fragment,d),h(E.$$.fragment,d),h(z.$$.fragment,d),h(S.$$.fragment,d),h(F.$$.fragment,d),h(W.$$.fragment,d),H=!1},d(d){d&&f(t),b(a),b(n),b(o),b(l),b(r),b(i),b(s),b(R),b(D),b(I),b($),b(E),b(z),b(S),b(F),b(W)}}}function St(c){let t=c[10]+"",a;return{c(){a=L(t)},l(n){a=A(n,t)},m(n,o){p(n,a,o)},p:ye,d(n){n&&f(a)}}}function $t(c){let t,a;return t=new Pt({props:{$$slots:{default:[St]},$$scope:{ctx:c}}}),{c(){_(t.$$.fragment)},l(n){g(t.$$.fragment,n)},m(n,o){w(t,n,o),a=!0},p(n,o){const l={};o&8192&&(l.$$scope={dirty:o,ctx:n}),t.$set(l)},i(n){a||(m(t.$$.fragment,n),a=!0)},o(n){h(t.$$.fragment,n),a=!1},d(n){b(t,n)}}}function Wt(c){let t,a,n=c[0],o=[];for(let r=0;r<n.length;r+=1)o[r]=$t(ht(c,n,r));const l=r=>h(o[r],1,1,()=>{o[r]=null});return{c(){for(let r=0;r<o.length;r+=1)o[r].c();t=_e()},l(r){for(let i=0;i<o.length;i+=1)o[i].l(r);t=_e()},m(r,i){for(let s=0;s<o.length;s+=1)o[s]&&o[s].m(r,i);p(r,t,i),a=!0},p(r,i){if(i&1){n=r[0];let s;for(s=0;s<n.length;s+=1){const R=ht(r,n,s);o[s]?(o[s].p(R,i),m(o[s],1)):(o[s]=$t(R),o[s].c(),m(o[s],1),o[s].m(t.parentNode,t))}for(tt(),s=n.length;s<o.length;s+=1)l(s);nt()}},i(r){if(!a){for(let i=0;i<n.length;i+=1)m(o[i]);a=!0}},o(r){o=o.filter(Boolean);for(let i=0;i<o.length;i+=1)h(o[i]);a=!1},d(r){ot(o,r),r&&f(t)}}}function qt(c){let t,a;return t=new wt({props:{$$slots:{default:[Wt]},$$scope:{ctx:c}}}),{c(){_(t.$$.fragment)},l(n){g(t.$$.fragment,n)},m(n,o){w(t,n,o),a=!0},p(n,o){const l={};o&8192&&(l.$$scope={dirty:o,ctx:n}),t.$set(l)},i(n){a||(m(t.$$.fragment,n),a=!0)},o(n){h(t.$$.fragment,n),a=!1},d(n){b(t,n)}}}function Ht(c){let t=c[7]+"",a;return{c(){a=L(t)},l(n){a=A(n,t)},m(n,o){p(n,a,o)},p:ye,d(n){n&&f(a)}}}function Mt(c){let t,a=c[7]+"",n;return{c(){t=B("span"),n=L(a),this.h()},l(o){t=C(o,"SPAN",{class:!0});var l=P(t);n=A(l,a),l.forEach(f),this.h()},h(){fe(t,"class","inline-block bg-blue-100 px-3 py-1 rounded-full")},m(o,l){p(o,t,l),T(t,n)},p:ye,d(o){o&&f(t)}}}function Ut(c){let t,a=c[7]+"",n;return{c(){t=B("span"),n=L(a),this.h()},l(o){t=C(o,"SPAN",{class:!0});var l=P(t);n=A(l,a),l.forEach(f),this.h()},h(){fe(t,"class","inline-block bg-red-100 px-3 py-1 rounded-full")},m(o,l){p(o,t,l),T(t,n)},p:ye,d(o){o&&f(t)}}}function jt(c){let t;function a(l,r){return l[7]==="ResNet Block"?Ut:l[9]===1&&l[7]!==""?Mt:Ht}let o=a(c)(c);return{c(){o.c(),t=_e()},l(l){o.l(l),t=_e()},m(l,r){o.m(l,r),p(l,t,r)},p(l,r){o.p(l,r)},d(l){o.d(l),l&&f(t)}}}function dt(c){let t,a;return t=new zt({props:{$$slots:{default:[jt]},$$scope:{ctx:c}}}),{c(){_(t.$$.fragment)},l(n){g(t.$$.fragment,n)},m(n,o){w(t,n,o),a=!0},p(n,o){const l={};o&8192&&(l.$$scope={dirty:o,ctx:n}),t.$set(l)},i(n){a||(m(t.$$.fragment,n),a=!0)},o(n){h(t.$$.fragment,n),a=!1},d(n){b(t,n)}}}function Gt(c){let t,a,n=c[4],o=[];for(let r=0;r<n.length;r+=1)o[r]=dt(mt(c,n,r));const l=r=>h(o[r],1,1,()=>{o[r]=null});return{c(){for(let r=0;r<o.length;r+=1)o[r].c();t=v()},l(r){for(let i=0;i<o.length;i+=1)o[i].l(r);t=y(r)},m(r,i){for(let s=0;s<o.length;s+=1)o[s]&&o[s].m(r,i);p(r,t,i),a=!0},p(r,i){if(i&2){n=r[4];let s;for(s=0;s<n.length;s+=1){const R=mt(r,n,s);o[s]?(o[s].p(R,i),m(o[s],1)):(o[s]=dt(R),o[s].c(),m(o[s],1),o[s].m(t.parentNode,t))}for(tt(),s=n.length;s<o.length;s+=1)l(s);nt()}},i(r){if(!a){for(let i=0;i<n.length;i+=1)m(o[i]);a=!0}},o(r){o=o.filter(Boolean);for(let i=0;i<o.length;i+=1)h(o[i]);a=!1},d(r){ot(o,r),r&&f(t)}}}function _t(c){let t,a;return t=new wt({props:{$$slots:{default:[Gt]},$$scope:{ctx:c}}}),{c(){_(t.$$.fragment)},l(n){g(t.$$.fragment,n)},m(n,o){w(t,n,o),a=!0},p(n,o){const l={};o&8192&&(l.$$scope={dirty:o,ctx:n}),t.$set(l)},i(n){a||(m(t.$$.fragment,n),a=!0)},o(n){h(t.$$.fragment,n),a=!1},d(n){b(t,n)}}}function Ot(c){let t,a,n=c[1],o=[];for(let r=0;r<n.length;r+=1)o[r]=_t(pt(c,n,r));const l=r=>h(o[r],1,1,()=>{o[r]=null});return{c(){for(let r=0;r<o.length;r+=1)o[r].c();t=_e()},l(r){for(let i=0;i<o.length;i+=1)o[i].l(r);t=_e()},m(r,i){for(let s=0;s<o.length;s+=1)o[s]&&o[s].m(r,i);p(r,t,i),a=!0},p(r,i){if(i&2){n=r[1];let s;for(s=0;s<n.length;s+=1){const R=pt(r,n,s);o[s]?(o[s].p(R,i),m(o[s],1)):(o[s]=_t(R),o[s].c(),m(o[s],1),o[s].m(t.parentNode,t))}for(tt(),s=n.length;s<o.length;s+=1)l(s);nt()}},i(r){if(!a){for(let i=0;i<n.length;i+=1)m(o[i]);a=!0}},o(r){o=o.filter(Boolean);for(let i=0;i<o.length;i+=1)h(o[i]);a=!1},d(r){ot(o,r),r&&f(t)}}}function Yt(c){let t,a,n,o;return t=new Bt({props:{$$slots:{default:[qt]},$$scope:{ctx:c}}}),n=new Ct({props:{$$slots:{default:[Ot]},$$scope:{ctx:c}}}),{c(){_(t.$$.fragment),a=v(),_(n.$$.fragment)},l(l){g(t.$$.fragment,l),a=y(l),g(n.$$.fragment,l)},m(l,r){w(t,l,r),p(l,a,r),w(n,l,r),o=!0},p(l,r){const i={};r&8192&&(i.$$scope={dirty:r,ctx:l}),t.$set(i);const s={};r&8192&&(s.$$scope={dirty:r,ctx:l}),n.$set(s)},i(l){o||(m(t.$$.fragment,l),m(n.$$.fragment,l),o=!0)},o(l){h(t.$$.fragment,l),h(n.$$.fragment,l),o=!1},d(l){b(t,l),l&&f(a),b(n,l)}}}function Jt(c){let t;return{c(){t=L("83%")},l(a){t=A(a,"83%")},m(a,n){p(a,t,n)},d(a){a&&f(t)}}}function Kt(c){let t,a,n,o,l,r,i,s,R,D,I,$,E,z,S,F,W,H,d,N,ce,Ge,ke,J,Ee,ue,Oe,xe,U,Ve,pe,Ye,Re,me,Je,Ne,K,Le,Q,Ae,X,De,Z,Te,ee,Be,he,Ke,Ce,te,Pe,$e,Qe,ze,ne,Ie,de,Xe,Fe,oe,Se,re,We,ae,qe,j,Ze,G,et,He,se,Me,le,Ue;return r=new ut({props:{$$slots:{default:[It]},$$scope:{ctx:c}}}),i=new Nt({props:{type:"reference",id:1}}),F=new At({props:{maxWidth:Xt,$$slots:{default:[Ft]},$$scope:{ctx:c}}}),J=new Lt({props:{maxWidth:350,kernel:1,stride:2,padding:0,imageWidth:6,imageHeight:6,showOutput:!0,numChannels:2,numFilters:4}}),U=new Tt({props:{$$slots:{default:[Yt]},$$scope:{ctx:c}}}),K=new q({props:{code:`import time
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, Subset
from torchvision.datasets.cifar import CIFAR10
from torchvision import transforms as T

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`}}),Q=new q({props:{code:`train_transform = T.Compose([T.Resize((50, 50)), 
                             T.ToTensor(),
                             T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])`}}),X=new q({props:{code:"train_val_dataset = CIFAR10(root='../datasets', download=True, train=True, transform=train_transform)"}}),Z=new q({props:{code:`# split dataset into train and validate
indices = list(range(len(train_val_dataset)))
train_idxs, val_idxs = train_test_split(
    indices, test_size=0.1, stratify=train_val_dataset.targets
)

train_dataset = Subset(train_val_dataset, train_idxs)
val_dataset = Subset(train_val_dataset, val_idxs)`}}),ee=new q({props:{code:`batch_size=128
train_dataloader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4,
    drop_last=True,
)
val_dataloader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=4,
    drop_last=False,
)`}}),te=new q({props:{code:`class BasicBlock(nn.Module):
    def __init__(self,
               in_channels,
               out_channels):
        super().__init__()
    
        first_stride=1
        if out_channels != in_channels:
            first_stride=2

        self.residual = nn.Sequential(
            nn.Conv2d(in_channels, 
                      out_channels, 
                      kernel_size=3, 
                      stride=first_stride, 
                      padding=1, 
                      bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, 
                      out_channels, 
                      kernel_size=3, 
                      stride=1, 
                      padding=1, 
                      bias=False),
            nn.BatchNorm2d(out_channels))

        self.downsampling = None
        if out_channels != in_channels:
            self.downsampling = nn.Sequential(
              nn.Conv2d(in_channels, 
                        out_channels,
                        kernel_size=1,
                        stride=2,
                        bias=False),
              nn.BatchNorm2d(out_channels))
  
    def forward(self, x):
        identity = x
        if self.downsampling:
            identity = self.downsampling(identity)

        x = self.residual(x)
        return torch.relu(x + identity)`}}),ne=new q({props:{code:`cfg = [64, 64, 64,
       128, 128, 128, 128,
       256, 256, 256, 256, 256, 256, 
       512, 512, 512]`}}),oe=new q({props:{code:`class Model(nn.Module):
  
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)
        self.blocks = self._create_network()
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(cfg[-1], 10))

    def _create_network(self):
        blocks = []
        prev_channels = 64
        for channels in self.cfg:
            blocks += [BasicBlock(prev_channels, channels)]
            prev_channels = channels
        return nn.Sequential(*blocks)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)
        x = self.blocks(x)
        x = self.classifier(x)
        return x`}}),re=new q({props:{code:`def train(
    num_epochs,
    train_dataloader,
    val_dataloader,
    model,
    criterion,
    optimizer,
    scheduler=None,
):
    model.to(device)
    scaler = torch.cuda.amp.GradScaler()
    for epoch in range(num_epochs):
        start_time = time.time()
        for _, (features, labels) in enumerate(train_dataloader):
            model.train()
            features = features.to(device)
            labels = labels.to(device)

            # Empty the gradients
            optimizer.zero_grad()

            with torch.autocast(device_type="cuda", dtype=torch.float16):
                # Forward Pass
                logits = model(features)
                # Calculate Loss
                loss = criterion(logits, labels)

            # Backward Pass
            scaler.scale(loss).backward()

            # Gradient Descent
            scaler.step(optimizer)
            scaler.update()

        val_loss, val_acc = track_performance(val_dataloader, model, criterion)
        end_time = time.time()

        s = (
            f"Epoch: {epoch+1:>2}/{num_epochs} | "
            f"Epoch Duration: {end_time - start_time:.3f} sec | "
            f"Val Loss: {val_loss:.5f} | "
            f"Val Acc: {val_acc:.3f} |"
        )
        print(s)

        if scheduler:
            scheduler.step(val_loss)`}}),ae=new q({props:{code:`model = Model(cfg)
optimizer = optim.Adam(params=model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, factor=0.1, patience=2, verbose=True
)
criterion = nn.CrossEntropyLoss(reduction="sum")`}}),G=new ut({props:{$$slots:{default:[Jt]},$$scope:{ctx:c}}}),se=new q({props:{code:`train(
    num_epochs=30,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
)`}}),le=new q({props:{isOutput:!0,code:`Epoch:  1/30 | Epoch Duration: 12.656 sec | Val Loss: 1.18379 | Val Acc: 0.577 |
Epoch:  2/30 | Epoch Duration: 11.668 sec | Val Loss: 1.02777 | Val Acc: 0.643 |
Epoch:  3/30 | Epoch Duration: 11.689 sec | Val Loss: 0.80600 | Val Acc: 0.726 |
Epoch:  4/30 | Epoch Duration: 11.673 sec | Val Loss: 0.83877 | Val Acc: 0.719 |
Epoch:  5/30 | Epoch Duration: 11.810 sec | Val Loss: 0.70858 | Val Acc: 0.762 |
Epoch:  6/30 | Epoch Duration: 11.836 sec | Val Loss: 0.67763 | Val Acc: 0.779 |
Epoch:  7/30 | Epoch Duration: 11.989 sec | Val Loss: 0.69613 | Val Acc: 0.773 |
Epoch:  8/30 | Epoch Duration: 11.947 sec | Val Loss: 0.65614 | Val Acc: 0.793 |
Epoch:  9/30 | Epoch Duration: 11.867 sec | Val Loss: 0.72713 | Val Acc: 0.784 |
Epoch: 10/30 | Epoch Duration: 11.809 sec | Val Loss: 0.75445 | Val Acc: 0.791 |
Epoch: 11/30 | Epoch Duration: 12.118 sec | Val Loss: 0.80031 | Val Acc: 0.787 |
Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.
Epoch: 12/30 | Epoch Duration: 11.843 sec | Val Loss: 0.65141 | Val Acc: 0.832 |
Epoch: 13/30 | Epoch Duration: 11.893 sec | Val Loss: 0.68573 | Val Acc: 0.828 |
Epoch: 14/30 | Epoch Duration: 11.871 sec | Val Loss: 0.73215 | Val Acc: 0.832 |
Epoch: 15/30 | Epoch Duration: 12.073 sec | Val Loss: 0.77129 | Val Acc: 0.832 |
Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.
Epoch: 16/30 | Epoch Duration: 11.843 sec | Val Loss: 0.77902 | Val Acc: 0.833 |
Epoch: 17/30 | Epoch Duration: 12.498 sec | Val Loss: 0.78212 | Val Acc: 0.831 |
Epoch: 18/30 | Epoch Duration: 11.925 sec | Val Loss: 0.79099 | Val Acc: 0.834 |
Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.
Epoch: 19/30 | Epoch Duration: 11.876 sec | Val Loss: 0.78918 | Val Acc: 0.833 |
Epoch: 20/30 | Epoch Duration: 11.899 sec | Val Loss: 0.79028 | Val Acc: 0.831 |
Epoch: 21/30 | Epoch Duration: 11.871 sec | Val Loss: 0.79865 | Val Acc: 0.832 |
Epoch 00021: reducing learning rate of group 0 to 1.0000e-07.
Epoch: 22/30 | Epoch Duration: 12.021 sec | Val Loss: 0.79126 | Val Acc: 0.831 |
Epoch: 23/30 | Epoch Duration: 11.897 sec | Val Loss: 0.79015 | Val Acc: 0.832 |
Epoch: 24/30 | Epoch Duration: 11.919 sec | Val Loss: 0.78823 | Val Acc: 0.832 |
Epoch 00024: reducing learning rate of group 0 to 1.0000e-08.
Epoch: 25/30 | Epoch Duration: 11.945 sec | Val Loss: 0.78385 | Val Acc: 0.831 |
Epoch: 26/30 | Epoch Duration: 12.040 sec | Val Loss: 0.79242 | Val Acc: 0.831 |
Epoch: 27/30 | Epoch Duration: 11.758 sec | Val Loss: 0.78959 | Val Acc: 0.832 |
Epoch: 28/30 | Epoch Duration: 11.754 sec | Val Loss: 0.79259 | Val Acc: 0.830 |
Epoch: 29/30 | Epoch Duration: 11.838 sec | Val Loss: 0.79554 | Val Acc: 0.831 |
Epoch: 30/30 | Epoch Duration: 11.856 sec | Val Loss: 0.78739 | Val Acc: 0.833 |
`}}),{c(){t=B("p"),a=L("We have introduced and discussed "),n=B("a"),o=L("skip connections"),l=L(`
    in a previous chapter. This time around we will talk about `),_(r.$$.fragment),_(i.$$.fragment),s=L(`, the ConvNet which introduced
    skip connections to the world. Several ResNet variants were introduced in
    the original paper. From ResNet18 with just 18 layers all the way to
    ResNet152, with 152 layers. The 152 layer variant won the ILSVRC15
    classification challenge with a 3.57 top-5 error rate. Remember that just
    the year before GoogLeNet achieved 6.67.`),R=v(),D=B("p"),I=L(`In this section we will focus on the ResNet34 architecture. But if you are
    interested in implementing the 152 layer architecture, you should be able to
    extend the code below.`),$=v(),E=B("p"),z=L(`Similar to the architectures we studied before, ResNet34 is based on many
    basic building blocks, only this time the block is based on skip
    connections.`),S=v(),_(F.$$.fragment),W=v(),H=B("p"),d=L(`The block consists of two convolutions. The skip connection goes directly
    from the input to the block (output of the previous layer), past the two
    convolutions and is added to the usual path, before the ReLU is applied to
    the sum. Bear in mind, that this block is slightly different for larger
    ResNet architectures.`),N=v(),ce=B("p"),Ge=L(`The number of filters and the image resolution usually stays constant within
    the block. This makes the output size equal to the input size and we do not
    have any trouble adding the input to the output of the second convolution.
    Yet sometimes we reduce the resolution by 2 using a stride of 2 and we
    simultaneously increase the number of filters by two. If we have a 100x100x3
    image, we would end up with a 50x50x6 image. This procedure keeps the number
    of paramters constant, yet we can not apply the addition, because the
    dimensionality of the input and the output differ. In order to deal with the
    problem the input is also processed using a 1x1 kernel with a stride of 2.`),ke=v(),_(J.$$.fragment),Ee=v(),ue=B("p"),Oe=L(`The overall architecture looks as follows. We use the same building blocks
    over and over again. From time to time we halve the resolution and double
    the number of channels.`),xe=v(),_(U.$$.fragment),Ve=v(),pe=B("p"),Ye=L(`At the end we have a single fully connected layer, which produces 1,000
    logits (the number of ImageNet categories).`),Re=v(),me=B("p"),Je=L(`Let's implement the ResNet34 in PyTorch and train it on the CIFAR-10
    dataset.`),Ne=v(),_(K.$$.fragment),Le=v(),_(Q.$$.fragment),Ae=v(),_(X.$$.fragment),De=v(),_(Z.$$.fragment),Te=v(),_(ee.$$.fragment),Be=v(),he=B("p"),Ke=L(`Below we implement the skip connection block from the diagram above. If the
    number of input channels and the number of output channels is identical, we
    proceed by adding the calculated residual to the input. If on the other hand
    the number of channels is different, we first use the stride of 2, in order
    to downsample the image by a factor of 2 and we adjust the input through a
    1x1 convolution with stride 2, in order for the addition to work.`),Ce=v(),_(te.$$.fragment),Pe=v(),$e=B("p"),Qe=L(`The configuration list below, contains the number of channels for all of the
    ResNet basic blocks.`),ze=v(),_(ne.$$.fragment),Ie=v(),de=B("p"),Xe=L(`Finally we create a full ResNet34 architecture, using the configuration
    above.`),Fe=v(),_(oe.$$.fragment),Se=v(),_(re.$$.fragment),We=v(),_(ae.$$.fragment),qe=v(),j=B("p"),Ze=L("Using ResNet34 we achieve an accuracy of roughly "),_(G.$$.fragment),et=L(`.
    While we do not beat our previous implementations, do not underestimate skip
    connections. CIFAR-10 is a relatively small dataset, and ResNet34 is a
    relatively small neural network and we therefore can not generalize our
    results. Most modern architectures use skip connections, because this
    technique stood the test of time.`),He=v(),_(se.$$.fragment),Me=v(),_(le.$$.fragment),this.h()},l(e){t=C(e,"P",{});var u=P(t);a=A(u,"We have introduced and discussed "),n=C(u,"A",{href:!0});var ge=P(n);o=A(ge,"skip connections"),ge.forEach(f),l=A(u,`
    in a previous chapter. This time around we will talk about `),g(r.$$.fragment,u),g(i.$$.fragment,u),s=A(u,`, the ConvNet which introduced
    skip connections to the world. Several ResNet variants were introduced in
    the original paper. From ResNet18 with just 18 layers all the way to
    ResNet152, with 152 layers. The 152 layer variant won the ILSVRC15
    classification challenge with a 3.57 top-5 error rate. Remember that just
    the year before GoogLeNet achieved 6.67.`),u.forEach(f),R=y(e),D=C(e,"P",{});var we=P(D);I=A(we,`In this section we will focus on the ResNet34 architecture. But if you are
    interested in implementing the 152 layer architecture, you should be able to
    extend the code below.`),we.forEach(f),$=y(e),E=C(e,"P",{});var be=P(E);z=A(be,`Similar to the architectures we studied before, ResNet34 is based on many
    basic building blocks, only this time the block is based on skip
    connections.`),be.forEach(f),S=y(e),g(F.$$.fragment,e),W=y(e),H=C(e,"P",{});var ve=P(H);d=A(ve,`The block consists of two convolutions. The skip connection goes directly
    from the input to the block (output of the previous layer), past the two
    convolutions and is added to the usual path, before the ReLU is applied to
    the sum. Bear in mind, that this block is slightly different for larger
    ResNet architectures.`),ve.forEach(f),N=y(e),ce=C(e,"P",{});var rt=P(ce);Ge=A(rt,`The number of filters and the image resolution usually stays constant within
    the block. This makes the output size equal to the input size and we do not
    have any trouble adding the input to the output of the second convolution.
    Yet sometimes we reduce the resolution by 2 using a stride of 2 and we
    simultaneously increase the number of filters by two. If we have a 100x100x3
    image, we would end up with a 50x50x6 image. This procedure keeps the number
    of paramters constant, yet we can not apply the addition, because the
    dimensionality of the input and the output differ. In order to deal with the
    problem the input is also processed using a 1x1 kernel with a stride of 2.`),rt.forEach(f),ke=y(e),g(J.$$.fragment,e),Ee=y(e),ue=C(e,"P",{});var at=P(ue);Oe=A(at,`The overall architecture looks as follows. We use the same building blocks
    over and over again. From time to time we halve the resolution and double
    the number of channels.`),at.forEach(f),xe=y(e),g(U.$$.fragment,e),Ve=y(e),pe=C(e,"P",{});var st=P(pe);Ye=A(st,`At the end we have a single fully connected layer, which produces 1,000
    logits (the number of ImageNet categories).`),st.forEach(f),Re=y(e),me=C(e,"P",{});var lt=P(me);Je=A(lt,`Let's implement the ResNet34 in PyTorch and train it on the CIFAR-10
    dataset.`),lt.forEach(f),Ne=y(e),g(K.$$.fragment,e),Le=y(e),g(Q.$$.fragment,e),Ae=y(e),g(X.$$.fragment,e),De=y(e),g(Z.$$.fragment,e),Te=y(e),g(ee.$$.fragment,e),Be=y(e),he=C(e,"P",{});var it=P(he);Ke=A(it,`Below we implement the skip connection block from the diagram above. If the
    number of input channels and the number of output channels is identical, we
    proceed by adding the calculated residual to the input. If on the other hand
    the number of channels is different, we first use the stride of 2, in order
    to downsample the image by a factor of 2 and we adjust the input through a
    1x1 convolution with stride 2, in order for the addition to work.`),it.forEach(f),Ce=y(e),g(te.$$.fragment,e),Pe=y(e),$e=C(e,"P",{});var ft=P($e);Qe=A(ft,`The configuration list below, contains the number of channels for all of the
    ResNet basic blocks.`),ft.forEach(f),ze=y(e),g(ne.$$.fragment,e),Ie=y(e),de=C(e,"P",{});var ct=P(de);Xe=A(ct,`Finally we create a full ResNet34 architecture, using the configuration
    above.`),ct.forEach(f),Fe=y(e),g(oe.$$.fragment,e),Se=y(e),g(re.$$.fragment,e),We=y(e),g(ae.$$.fragment,e),qe=y(e),j=C(e,"P",{});var je=P(j);Ze=A(je,"Using ResNet34 we achieve an accuracy of roughly "),g(G.$$.fragment,je),et=A(je,`.
    While we do not beat our previous implementations, do not underestimate skip
    connections. CIFAR-10 is a relatively small dataset, and ResNet34 is a
    relatively small neural network and we therefore can not generalize our
    results. Most modern architectures use skip connections, because this
    technique stood the test of time.`),je.forEach(f),He=y(e),g(se.$$.fragment,e),Me=y(e),g(le.$$.fragment,e),this.h()},h(){fe(n,"href","/blocks/deep_learning/stability_speedup/skip_connections")},m(e,u){p(e,t,u),T(t,a),T(t,n),T(n,o),T(t,l),w(r,t,null),w(i,t,null),T(t,s),p(e,R,u),p(e,D,u),T(D,I),p(e,$,u),p(e,E,u),T(E,z),p(e,S,u),w(F,e,u),p(e,W,u),p(e,H,u),T(H,d),p(e,N,u),p(e,ce,u),T(ce,Ge),p(e,ke,u),w(J,e,u),p(e,Ee,u),p(e,ue,u),T(ue,Oe),p(e,xe,u),w(U,e,u),p(e,Ve,u),p(e,pe,u),T(pe,Ye),p(e,Re,u),p(e,me,u),T(me,Je),p(e,Ne,u),w(K,e,u),p(e,Le,u),w(Q,e,u),p(e,Ae,u),w(X,e,u),p(e,De,u),w(Z,e,u),p(e,Te,u),w(ee,e,u),p(e,Be,u),p(e,he,u),T(he,Ke),p(e,Ce,u),w(te,e,u),p(e,Pe,u),p(e,$e,u),T($e,Qe),p(e,ze,u),w(ne,e,u),p(e,Ie,u),p(e,de,u),T(de,Xe),p(e,Fe,u),w(oe,e,u),p(e,Se,u),w(re,e,u),p(e,We,u),w(ae,e,u),p(e,qe,u),p(e,j,u),T(j,Ze),w(G,j,null),T(j,et),p(e,He,u),w(se,e,u),p(e,Me,u),w(le,e,u),Ue=!0},p(e,u){const ge={};u&8192&&(ge.$$scope={dirty:u,ctx:e}),r.$set(ge);const we={};u&8192&&(we.$$scope={dirty:u,ctx:e}),F.$set(we);const be={};u&8192&&(be.$$scope={dirty:u,ctx:e}),U.$set(be);const ve={};u&8192&&(ve.$$scope={dirty:u,ctx:e}),G.$set(ve)},i(e){Ue||(m(r.$$.fragment,e),m(i.$$.fragment,e),m(F.$$.fragment,e),m(J.$$.fragment,e),m(U.$$.fragment,e),m(K.$$.fragment,e),m(Q.$$.fragment,e),m(X.$$.fragment,e),m(Z.$$.fragment,e),m(ee.$$.fragment,e),m(te.$$.fragment,e),m(ne.$$.fragment,e),m(oe.$$.fragment,e),m(re.$$.fragment,e),m(ae.$$.fragment,e),m(G.$$.fragment,e),m(se.$$.fragment,e),m(le.$$.fragment,e),Ue=!0)},o(e){h(r.$$.fragment,e),h(i.$$.fragment,e),h(F.$$.fragment,e),h(J.$$.fragment,e),h(U.$$.fragment,e),h(K.$$.fragment,e),h(Q.$$.fragment,e),h(X.$$.fragment,e),h(Z.$$.fragment,e),h(ee.$$.fragment,e),h(te.$$.fragment,e),h(ne.$$.fragment,e),h(oe.$$.fragment,e),h(re.$$.fragment,e),h(ae.$$.fragment,e),h(G.$$.fragment,e),h(se.$$.fragment,e),h(le.$$.fragment,e),Ue=!1},d(e){e&&f(t),b(r),b(i),e&&f(R),e&&f(D),e&&f($),e&&f(E),e&&f(S),b(F,e),e&&f(W),e&&f(H),e&&f(N),e&&f(ce),e&&f(ke),b(J,e),e&&f(Ee),e&&f(ue),e&&f(xe),b(U,e),e&&f(Ve),e&&f(pe),e&&f(Re),e&&f(me),e&&f(Ne),b(K,e),e&&f(Le),b(Q,e),e&&f(Ae),b(X,e),e&&f(De),b(Z,e),e&&f(Te),b(ee,e),e&&f(Be),e&&f(he),e&&f(Ce),b(te,e),e&&f(Pe),e&&f($e),e&&f(ze),b(ne,e),e&&f(Ie),e&&f(de),e&&f(Fe),b(oe,e),e&&f(Se),b(re,e),e&&f(We),b(ae,e),e&&f(qe),e&&f(j),b(G),e&&f(He),b(se,e),e&&f(Me),b(le,e)}}}function Qt(c){let t,a,n,o,l,r,i,s,R,D,I;return s=new Vt({props:{$$slots:{default:[Kt]},$$scope:{ctx:c}}}),D=new Rt({props:{references:c[2]}}),{c(){t=B("meta"),a=v(),n=B("h1"),o=L("ResNet"),l=v(),r=B("div"),i=v(),_(s.$$.fragment),R=v(),_(D.$$.fragment),this.h()},l($){const E=kt("svelte-13t6h3e",document.head);t=C(E,"META",{name:!0,content:!0}),E.forEach(f),a=y($),n=C($,"H1",{});var z=P(n);o=A(z,"ResNet"),z.forEach(f),l=y($),r=C($,"DIV",{class:!0}),P(r).forEach(f),i=y($),g(s.$$.fragment,$),R=y($),g(D.$$.fragment,$),this.h()},h(){document.title="ResNet - World4AI",fe(t,"name","description"),fe(t,"content","The ResNet convolutional neural network architecture introduced skip connections. Skip connections allowed to train very deep neural networks and the 152 layer ResNet won the 2015 ImageNet classification competition."),fe(r,"class","separator")},m($,E){T(document.head,t),p($,a,E),p($,n,E),T(n,o),p($,l,E),p($,r,E),p($,i,E),w(s,$,E),p($,R,E),w(D,$,E),I=!0},p($,[E]){const z={};E&8192&&(z.$$scope={dirty:E,ctx:$}),s.$set(z)},i($){I||(m(s.$$.fragment,$),m(D.$$.fragment,$),I=!0)},o($){h(s.$$.fragment,$),h(D.$$.fragment,$),I=!1},d($){f(t),$&&f(a),$&&f(n),$&&f(l),$&&f(r),$&&f(i),b(s,$),$&&f(R),b(D,$)}}}let Y=1,V=100,x=300,M=60,k=20,Xt="220px";const gt=9;function Zt(c){let t=["Type","Repeat","Parameters"],a=[["Convolution 2D","","7x7x64"],["BatchNorm2D","",""],["ReLU","",""],["Max Pooling","","Filter: 3x3, Stride: 2"],["ResNet Block","3","3x3x64"],["ResNet Block","4","3x3x128"],["ResNet Block","6","3x3x256"],["ResNet Block","3","3x3x512"],["Adaptive Avg. Pooling","","512"],["Fully Connected","","1000"],["Softmax","","1000"]];const n=[{author:"K. He, X. Zhang, S. Ren and J. Sun",title:"Deep Residual Learning for Image Recognition",journal:"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",year:"2016",pages:"770-778",volume:"",issue:""}],o=(x-gt-1*k)/(gt-2);return[t,a,n,o]}class pn extends bt{constructor(t){super(),vt(this,t,Zt,Qt,yt,{})}}export{pn as default};
