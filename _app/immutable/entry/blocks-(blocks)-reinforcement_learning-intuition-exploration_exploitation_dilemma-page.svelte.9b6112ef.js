import{S as De,i as Pe,s as Ce,k as v,a as u,q as p,y as S,W as Ge,l as b,h as a,c as g,m as x,r as d,z as A,n as ee,N as E,b as s,A as I,g as D,d as P,B as C,V as we}from"../chunks/index.4d92b023.js";import{C as Oe}from"../chunks/Container.b0705c7b.js";import{H as re}from"../chunks/Highlight.b7c1de53.js";import{A as ve}from"../chunks/Alert.25a852b3.js";import{D as Te}from"../chunks/DeterministicAgent.a1dd8b4b.js";import{G as Se,p as qe,g as We,a as Ae}from"../chunks/maps.0f079072.js";import{I as Ie}from"../chunks/Interaction.6c345f67.js";function ze(l){let n;return{c(){n=p("exploration-exploitation dilemma")},l(t){n=d(t,"exploration-exploitation dilemma")},m(t,o){s(t,n,o)},d(t){t&&a(n)}}}function He(l){let n;return{c(){n=p(`The agent can either explore the environment or exploit the already
    accumulated knowledge. The exploration-exploitation dilemma describes the
    fact that the agent can not do both at the same time.`)},l(t){n=d(t,`The agent can either explore the environment or exploit the already
    accumulated knowledge. The exploration-exploitation dilemma describes the
    fact that the agent can not do both at the same time.`)},m(t,o){s(t,n,o)},d(t){t&&a(n)}}}function Me(l){let n;return{c(){n=p("Deterministic Environment")},l(t){n=d(t,"Deterministic Environment")},m(t,o){s(t,n,o)},d(t){t&&a(n)}}}function Ve(l){let n,t,o;return n=new re({props:{$$slots:{default:[Me]},$$scope:{ctx:l}}}),{c(){S(n.$$.fragment),t=p(`: Given the same state of
    the environment and the same action by the agent the next state and the
    corresponding reward are always the same.`)},l(r){A(n.$$.fragment,r),t=d(r,`: Given the same state of
    the environment and the same action by the agent the next state and the
    corresponding reward are always the same.`)},m(r,h){I(n,r,h),s(r,t,h),o=!0},p(r,h){const f={};h&262144&&(f.$$scope={dirty:h,ctx:r}),n.$set(f)},i(r){o||(D(n.$$.fragment,r),o=!0)},o(r){P(n.$$.fragment,r),o=!1},d(r){C(n,r),r&&a(t)}}}function Fe(l){let n;return{c(){n=p("Stochastic Environment")},l(t){n=d(t,"Stochastic Environment")},m(t,o){s(t,n,o)},d(t){t&&a(n)}}}function je(l){let n,t,o;return n=new re({props:{$$slots:{default:[Fe]},$$scope:{ctx:l}}}),{c(){S(n.$$.fragment),t=p(`: Given the same state of the
    environment and the same action by the agent, the next state and the
    corresponding reward are calculated using a probability distribution.`)},l(r){A(n.$$.fragment,r),t=d(r,`: Given the same state of the
    environment and the same action by the agent, the next state and the
    corresponding reward are calculated using a probability distribution.`)},m(r,h){I(n,r,h),s(r,t,h),o=!0},p(r,h){const f={};h&262144&&(f.$$scope={dirty:h,ctx:r}),n.$set(f)},i(r){o||(D(n.$$.fragment,r),o=!0)},o(r){P(n.$$.fragment,r),o=!1},d(r){C(n,r),r&&a(t)}}}function Re(l){let n;return{c(){n=p("expectation")},l(t){n=d(t,"expectation")},m(t,o){s(t,n,o)},d(t){t&&a(n)}}}function Ye(l){let n;return{c(){n=p("expected sum of rewards")},l(t){n=d(t,"expected sum of rewards")},m(t,o){s(t,n,o)},d(t){t&&a(n)}}}function Be(l){let n,t,o,r;return t=new re({props:{$$slots:{default:[Ye]},$$scope:{ctx:l}}}),{c(){n=p("The goal of the agent is to maximize the "),S(t.$$.fragment),o=p(".")},l(h){n=d(h,"The goal of the agent is to maximize the "),A(t.$$.fragment,h),o=d(h,".")},m(h,f){s(h,n,f),I(t,h,f),s(h,o,f),r=!0},p(h,f){const w={};f&262144&&(w.$$scope={dirty:f,ctx:h}),t.$set(w)},i(h){r||(D(t.$$.fragment,h),r=!0)},o(h){P(t.$$.fragment,h),r=!1},d(h){h&&a(n),C(t,h),h&&a(o)}}}function Ne(l){let n,t,o,r,h,f,w,$,k,m,c,T,G,Y,F,y,B,O,N,j,_,be,se,q,he,J,le,K,_e,me,W,fe,L,xe,ce,z,pe,H,ye,M,Ee,de,V,ue,Q,ke,ge,U,$e;return o=new re({props:{$$slots:{default:[ze]},$$scope:{ctx:l}}}),f=new ve({props:{type:"info",$$slots:{default:[He]},$$scope:{ctx:l}}}),y=new ve({props:{type:"info",$$slots:{default:[Ve]},$$scope:{ctx:l}}}),q=new Ae({props:{cells:l[3],player:l[2],showColoredReward:!0,showReward:!0}}),W=new ve({props:{type:"info",$$slots:{default:[je]},$$scope:{ctx:l}}}),z=new Ae({props:{cells:l[1],player:l[0]}}),M=new re({props:{$$slots:{default:[Re]},$$scope:{ctx:l}}}),V=new ve({props:{type:"info",$$slots:{default:[Be]},$$scope:{ctx:l}}}),{c(){n=v("p"),t=p(`At each timestep the agent has to make the decision to either explore the
    environment or to exploit the current knowledge about the environment. The
    problem that the agent faces when deciding between the two options is the so
    called
    `),S(o.$$.fragment),r=p("."),h=u(),S(f.$$.fragment),w=u(),$=v("p"),k=p(`On the one hand the agent aims to get the highest sum of rewards that is
    achievable based on the current knowledge - it wants to exploit. On the
    other hand in order to find a sequence of actions which lead to a higher sum
    of rewards the agent needs to explore the environment. The dilemma is the
    fact that the agent can not do both at the same time. At each single step
    the agent either explores or exploits.`),m=u(),c=v("div"),T=u(),G=v("h2"),Y=p("Exploration in Deterministic Environments"),F=u(),S(y.$$.fragment),B=u(),O=v("p"),N=p(`The grid environment we covered so far was deterministic. We assumed that
    there is no uncertainty and given the same circumstances the outcome would
    be the same. For example whenever the agent chose the action to go right in
    the first state the environment transitioned in such a way that the circle
    moved actually move right. Each and every single time. Yet even in a
    deterministic environment the agent has to explore in order to find the
    optimal sequence of actions.`),j=u(),_=v("p"),be=p(`The grid world below shows an agent which has discovered the shortest route
    from the starting position to the goal position (triangle). At each timestep
    the agent earns a negative reward of -1. Once the agent reaches the goal,
    the environment gives a positive feedback of +1 reward and the game
    restarts. Generally the agent might keep taking the same path to reach the
    triangle, but if it kept exploring the environment it could discover that
    there is actually a big reward of +10 in the bottom right corner. The agent
    could pick up the reward first and then keep moving towards the goal. The
    high reward would make up for the additional few steps. In this
    deterministic example exploration would enable the agent to learn a strategy
    with a higher sum of rewards.`),se=u(),S(q.$$.fragment),he=u(),J=v("div"),le=u(),K=v("h2"),_e=p("Exploration in Stochastic Environments"),me=u(),S(W.$$.fragment),fe=u(),L=v("p"),xe=p(`Most of the environments (or the real world for that matter) are not
    deterministic, they are stochastic. That means that the next state and
    reward are calculated based on a probability distribution. That means that
    given the same state and action, the next state and rewards are not going to
    be consistent.`),ce=u(),S(z.$$.fragment),pe=u(),H=v("p"),ye=p(`As in the previous examples the agent above is already trained and tries to
    follow the shortest route. The grid world on the other hand represents a
    stochastic environment. The environment transitions into the desired state
    of the agent with the probability of 50%. With the probability of 50% the
    direction is chosen randomly (this might also include the desired
    direction). For an untrained agent this makes the job of finding the
    shortest route a lot more complex. The agent does not know exactly how the
    distribution of the environment looks like. Therefore the agent has to
    explore and to determine the path that leads to the highest sum of rewards
    in `),S(M.$$.fragment),Ee=p("."),de=u(),S(V.$$.fragment),ue=u(),Q=v("p"),ke=p(`In stochastic environments the agent has to maximize the expected sum of
    rewards. Intuitively speaking that means that the agent has to choose the
    strategy that would give him the largest sum of rewards if the agent played
    an infinite number of games.`),ge=u(),U=v("div"),this.h()},l(e){n=b(e,"P",{});var i=x(n);t=d(i,`At each timestep the agent has to make the decision to either explore the
    environment or to exploit the current knowledge about the environment. The
    problem that the agent faces when deciding between the two options is the so
    called
    `),A(o.$$.fragment,i),r=d(i,"."),i.forEach(a),h=g(e),A(f.$$.fragment,e),w=g(e),$=b(e,"P",{});var te=x($);k=d(te,`On the one hand the agent aims to get the highest sum of rewards that is
    achievable based on the current knowledge - it wants to exploit. On the
    other hand in order to find a sequence of actions which lead to a higher sum
    of rewards the agent needs to explore the environment. The dilemma is the
    fact that the agent can not do both at the same time. At each single step
    the agent either explores or exploits.`),te.forEach(a),m=g(e),c=b(e,"DIV",{class:!0}),x(c).forEach(a),T=g(e),G=b(e,"H2",{});var ne=x(G);Y=d(ne,"Exploration in Deterministic Environments"),ne.forEach(a),F=g(e),A(y.$$.fragment,e),B=g(e),O=b(e,"P",{});var ae=x(O);N=d(ae,`The grid environment we covered so far was deterministic. We assumed that
    there is no uncertainty and given the same circumstances the outcome would
    be the same. For example whenever the agent chose the action to go right in
    the first state the environment transitioned in such a way that the circle
    moved actually move right. Each and every single time. Yet even in a
    deterministic environment the agent has to explore in order to find the
    optimal sequence of actions.`),ae.forEach(a),j=g(e),_=b(e,"P",{});var X=x(_);be=d(X,`The grid world below shows an agent which has discovered the shortest route
    from the starting position to the goal position (triangle). At each timestep
    the agent earns a negative reward of -1. Once the agent reaches the goal,
    the environment gives a positive feedback of +1 reward and the game
    restarts. Generally the agent might keep taking the same path to reach the
    triangle, but if it kept exploring the environment it could discover that
    there is actually a big reward of +10 in the bottom right corner. The agent
    could pick up the reward first and then keep moving towards the goal. The
    high reward would make up for the additional few steps. In this
    deterministic example exploration would enable the agent to learn a strategy
    with a higher sum of rewards.`),X.forEach(a),se=g(e),A(q.$$.fragment,e),he=g(e),J=b(e,"DIV",{class:!0}),x(J).forEach(a),le=g(e),K=b(e,"H2",{});var ie=x(K);_e=d(ie,"Exploration in Stochastic Environments"),ie.forEach(a),me=g(e),A(W.$$.fragment,e),fe=g(e),L=b(e,"P",{});var Z=x(L);xe=d(Z,`Most of the environments (or the real world for that matter) are not
    deterministic, they are stochastic. That means that the next state and
    reward are calculated based on a probability distribution. That means that
    given the same state and action, the next state and rewards are not going to
    be consistent.`),Z.forEach(a),ce=g(e),A(z.$$.fragment,e),pe=g(e),H=b(e,"P",{});var R=x(H);ye=d(R,`As in the previous examples the agent above is already trained and tries to
    follow the shortest route. The grid world on the other hand represents a
    stochastic environment. The environment transitions into the desired state
    of the agent with the probability of 50%. With the probability of 50% the
    direction is chosen randomly (this might also include the desired
    direction). For an untrained agent this makes the job of finding the
    shortest route a lot more complex. The agent does not know exactly how the
    distribution of the environment looks like. Therefore the agent has to
    explore and to determine the path that leads to the highest sum of rewards
    in `),A(M.$$.fragment,R),Ee=d(R,"."),R.forEach(a),de=g(e),A(V.$$.fragment,e),ue=g(e),Q=b(e,"P",{});var oe=x(Q);ke=d(oe,`In stochastic environments the agent has to maximize the expected sum of
    rewards. Intuitively speaking that means that the agent has to choose the
    strategy that would give him the largest sum of rewards if the agent played
    an infinite number of games.`),oe.forEach(a),ge=g(e),U=b(e,"DIV",{class:!0}),x(U).forEach(a),this.h()},h(){ee(c,"class","separator"),ee(J,"class","separator"),ee(U,"class","separator")},m(e,i){s(e,n,i),E(n,t),I(o,n,null),E(n,r),s(e,h,i),I(f,e,i),s(e,w,i),s(e,$,i),E($,k),s(e,m,i),s(e,c,i),s(e,T,i),s(e,G,i),E(G,Y),s(e,F,i),I(y,e,i),s(e,B,i),s(e,O,i),E(O,N),s(e,j,i),s(e,_,i),E(_,be),s(e,se,i),I(q,e,i),s(e,he,i),s(e,J,i),s(e,le,i),s(e,K,i),E(K,_e),s(e,me,i),I(W,e,i),s(e,fe,i),s(e,L,i),E(L,xe),s(e,ce,i),I(z,e,i),s(e,pe,i),s(e,H,i),E(H,ye),I(M,H,null),E(H,Ee),s(e,de,i),I(V,e,i),s(e,ue,i),s(e,Q,i),E(Q,ke),s(e,ge,i),s(e,U,i),$e=!0},p(e,i){const te={};i&262144&&(te.$$scope={dirty:i,ctx:e}),o.$set(te);const ne={};i&262144&&(ne.$$scope={dirty:i,ctx:e}),f.$set(ne);const ae={};i&262144&&(ae.$$scope={dirty:i,ctx:e}),y.$set(ae);const X={};i&8&&(X.cells=e[3]),i&4&&(X.player=e[2]),q.$set(X);const ie={};i&262144&&(ie.$$scope={dirty:i,ctx:e}),W.$set(ie);const Z={};i&2&&(Z.cells=e[1]),i&1&&(Z.player=e[0]),z.$set(Z);const R={};i&262144&&(R.$$scope={dirty:i,ctx:e}),M.$set(R);const oe={};i&262144&&(oe.$$scope={dirty:i,ctx:e}),V.$set(oe)},i(e){$e||(D(o.$$.fragment,e),D(f.$$.fragment,e),D(y.$$.fragment,e),D(q.$$.fragment,e),D(W.$$.fragment,e),D(z.$$.fragment,e),D(M.$$.fragment,e),D(V.$$.fragment,e),$e=!0)},o(e){P(o.$$.fragment,e),P(f.$$.fragment,e),P(y.$$.fragment,e),P(q.$$.fragment,e),P(W.$$.fragment,e),P(z.$$.fragment,e),P(M.$$.fragment,e),P(V.$$.fragment,e),$e=!1},d(e){e&&a(n),C(o),e&&a(h),C(f,e),e&&a(w),e&&a($),e&&a(m),e&&a(c),e&&a(T),e&&a(G),e&&a(F),C(y,e),e&&a(B),e&&a(O),e&&a(j),e&&a(_),e&&a(se),C(q,e),e&&a(he),e&&a(J),e&&a(le),e&&a(K),e&&a(me),C(W,e),e&&a(fe),e&&a(L),e&&a(ce),C(z,e),e&&a(pe),e&&a(H),C(M),e&&a(de),C(V,e),e&&a(ue),e&&a(Q),e&&a(ge),e&&a(U)}}}function Je(l){let n,t,o,r,h,f,w,$,k;return $=new Oe({props:{$$slots:{default:[Ne]},$$scope:{ctx:l}}}),{c(){n=v("meta"),t=u(),o=v("h1"),r=p("Exploration-Exploitation Dilemma"),h=u(),f=v("div"),w=u(),S($.$$.fragment),this.h()},l(m){const c=Ge("svelte-11rceq5",document.head);n=b(c,"META",{name:!0,content:!0}),c.forEach(a),t=g(m),o=b(m,"H1",{});var T=x(o);r=d(T,"Exploration-Exploitation Dilemma"),T.forEach(a),h=g(m),f=b(m,"DIV",{class:!0}),x(f).forEach(a),w=g(m),A($.$$.fragment,m),this.h()},h(){document.title="Exploration-Exploitation Dilemma - World4AI",ee(n,"name","description"),ee(n,"content","In reinforcement learning the agent faces the so called exploration exploitation dilemma. At each step the agent has to decide to either explore or to exploit. Doing both at the same time is not possible."),ee(f,"class","separator")},m(m,c){E(document.head,n),s(m,t,c),s(m,o,c),E(o,r),s(m,h,c),s(m,f,c),s(m,w,c),I($,m,c),k=!0},p(m,[c]){const T={};c&262159&&(T.$$scope={dirty:c,ctx:m}),$.$set(T)},i(m){k||(D($.$$.fragment,m),k=!0)},o(m){P($.$$.fragment,m),k=!1},d(m){a(n),m&&a(t),m&&a(o),m&&a(h),m&&a(f),m&&a(w),C($,m)}}}function Ke(l,n,t){let o,r,h,f,w,$,k,m,c=new Se(qe),T=new Te(c.getObservationSpace(),c.getActionSpace()),G=new Ie(T,c,2),Y=c.cellsStore;we(l,Y,_=>t(11,m=_));let F=G.observationStore;we(l,F,_=>t(10,k=_));let y=new Se(We,!0),B=new Te(y.getObservationSpace(),y.getActionSpace()),O=new Ie(B,y,2),N=y.cellsStore;we(l,N,_=>t(9,$=_));let j=O.observationStore;return we(l,j,_=>t(8,w=_)),l.$$.update=()=>{l.$$.dirty&2048&&t(3,o=m),l.$$.dirty&1024&&t(2,r=k),l.$$.dirty&512&&t(1,h=$),l.$$.dirty&256&&t(0,f=w)},[f,h,r,o,Y,F,N,j,w,$,k,m]}class nt extends De{constructor(n){super(),Pe(this,n,Ke,Je,Ce,{})}}export{nt as default};
