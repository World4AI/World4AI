import{S as Le,i as We,s as Fe,k as L,a as v,q,y as h,W as Se,l as W,h as r,c as y,m as F,r as E,z as _,n as xe,N as I,b as o,A as k,g as x,d as z,B as T,C as ce}from"../chunks/index.4d92b023.js";import{C as Ce}from"../chunks/Container.b0705c7b.js";import{F as Me,I as Ne}from"../chunks/InternalLink.7deb899c.js";import{L as Oe}from"../chunks/Latex.e0b308c0.js";import{A as ze}from"../chunks/Alert.25a852b3.js";import{N as De}from"../chunks/NeuralNetwork.9b1e2957.js";import{P as Te,T as Pe}from"../chunks/Ticks.45eca5c5.js";import{X as Ae,Y as qe}from"../chunks/YLabel.182e66a3.js";import{C as K}from"../chunks/Circle.f281e92b.js";import{R as we}from"../chunks/Rectangle.45d8140d.js";function He(d){let a;return{c(){a=q(`Why do we need neural network when we can solve regression tasks using
    linear regression and classification tasks using logistic regression?`)},l(f){a=E(f,`Why do we need neural network when we can solve regression tasks using
    linear regression and classification tasks using logistic regression?`)},m(f,s){o(f,a,s)},d(f){f&&r(a)}}}function Be(d){let a,f,s,g,p,P,m,u,$,c;return a=new Pe({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),s=new K({props:{data:d[1][0]}}),p=new K({props:{data:d[1][1],color:"var(--main-color-2)"}}),m=new Ae({props:{text:"Feature 1",fontSize:15}}),$=new qe({props:{text:"Feature 2",fontSize:15}}),{c(){h(a.$$.fragment),f=v(),h(s.$$.fragment),g=v(),h(p.$$.fragment),P=v(),h(m.$$.fragment),u=v(),h($.$$.fragment)},l(n){_(a.$$.fragment,n),f=y(n),_(s.$$.fragment,n),g=y(n),_(p.$$.fragment,n),P=y(n),_(m.$$.fragment,n),u=y(n),_($.$$.fragment,n)},m(n,i){k(a,n,i),o(n,f,i),k(s,n,i),o(n,g,i),k(p,n,i),o(n,P,i),k(m,n,i),o(n,u,i),k($,n,i),c=!0},p:ce,i(n){c||(x(a.$$.fragment,n),x(s.$$.fragment,n),x(p.$$.fragment,n),x(m.$$.fragment,n),x($.$$.fragment,n),c=!0)},o(n){z(a.$$.fragment,n),z(s.$$.fragment,n),z(p.$$.fragment,n),z(m.$$.fragment,n),z($.$$.fragment,n),c=!1},d(n){T(a,n),n&&r(f),T(s,n),n&&r(g),T(p,n),n&&r(P),T(m,n),n&&r(u),T($,n)}}}function Xe(d){let a,f,s,g,p,P,m,u,$,c,n,i,w,A;return a=new Pe({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),s=new we({props:{data:d[2][0],size:9,color:"var(--main-color-3)"}}),p=new we({props:{data:d[2][1],size:9,color:"var(--main-color-4)"}}),m=new K({props:{data:d[1][0]}}),$=new K({props:{data:d[1][1],color:"var(--main-color-2)"}}),n=new Ae({props:{text:"Feature 1",fontSize:15}}),w=new qe({props:{text:"Feature 2",fontSize:15}}),{c(){h(a.$$.fragment),f=v(),h(s.$$.fragment),g=v(),h(p.$$.fragment),P=v(),h(m.$$.fragment),u=v(),h($.$$.fragment),c=v(),h(n.$$.fragment),i=v(),h(w.$$.fragment)},l(t){_(a.$$.fragment,t),f=y(t),_(s.$$.fragment,t),g=y(t),_(p.$$.fragment,t),P=y(t),_(m.$$.fragment,t),u=y(t),_($.$$.fragment,t),c=y(t),_(n.$$.fragment,t),i=y(t),_(w.$$.fragment,t)},m(t,b){k(a,t,b),o(t,f,b),k(s,t,b),o(t,g,b),k(p,t,b),o(t,P,b),k(m,t,b),o(t,u,b),k($,t,b),o(t,c,b),k(n,t,b),o(t,i,b),k(w,t,b),A=!0},p:ce,i(t){A||(x(a.$$.fragment,t),x(s.$$.fragment,t),x(p.$$.fragment,t),x(m.$$.fragment,t),x($.$$.fragment,t),x(n.$$.fragment,t),x(w.$$.fragment,t),A=!0)},o(t){z(a.$$.fragment,t),z(s.$$.fragment,t),z(p.$$.fragment,t),z(m.$$.fragment,t),z($.$$.fragment,t),z(n.$$.fragment,t),z(w.$$.fragment,t),A=!1},d(t){T(a,t),t&&r(f),T(s,t),t&&r(g),T(p,t),t&&r(P),T(m,t),t&&r(u),T($,t),t&&r(c),T(n,t),t&&r(i),T(w,t)}}}function Ye(d){let a,f,s,g,p,P,m,u,$,c,n,i,w,A;return a=new Pe({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),s=new we({props:{data:d[3][0],size:9,color:"var(--main-color-3)"}}),p=new we({props:{data:d[3][1],size:9,color:"var(--main-color-4)"}}),m=new K({props:{data:d[1][0]}}),$=new K({props:{data:d[1][1],color:"var(--main-color-2)"}}),n=new Ae({props:{text:"Feature 1",fontSize:15}}),w=new qe({props:{text:"Feature 2",fontSize:15}}),{c(){h(a.$$.fragment),f=v(),h(s.$$.fragment),g=v(),h(p.$$.fragment),P=v(),h(m.$$.fragment),u=v(),h($.$$.fragment),c=v(),h(n.$$.fragment),i=v(),h(w.$$.fragment)},l(t){_(a.$$.fragment,t),f=y(t),_(s.$$.fragment,t),g=y(t),_(p.$$.fragment,t),P=y(t),_(m.$$.fragment,t),u=y(t),_($.$$.fragment,t),c=y(t),_(n.$$.fragment,t),i=y(t),_(w.$$.fragment,t)},m(t,b){k(a,t,b),o(t,f,b),k(s,t,b),o(t,g,b),k(p,t,b),o(t,P,b),k(m,t,b),o(t,u,b),k($,t,b),o(t,c,b),k(n,t,b),o(t,i,b),k(w,t,b),A=!0},p:ce,i(t){A||(x(a.$$.fragment,t),x(s.$$.fragment,t),x(p.$$.fragment,t),x(m.$$.fragment,t),x($.$$.fragment,t),x(n.$$.fragment,t),x(w.$$.fragment,t),A=!0)},o(t){z(a.$$.fragment,t),z(s.$$.fragment,t),z(p.$$.fragment,t),z(m.$$.fragment,t),z($.$$.fragment,t),z(n.$$.fragment,t),z(w.$$.fragment,t),A=!1},d(t){T(a,t),t&&r(f),T(s,t),t&&r(g),T(p,t),t&&r(P),T(m,t),t&&r(u),T($,t),t&&r(c),T(n,t),t&&r(i),T(w,t)}}}function je(d){let a;return{c(){a=q(`What components and properties should a neural network exhibit to solve
    nonlinear problems?`)},l(f){a=E(f,`What components and properties should a neural network exhibit to solve
    nonlinear problems?`)},m(f,s){o(f,a,s)},d(f){f&&r(a)}}}function Re(d){let a=String.raw`\dfrac{1}{1+e^{-z}}`+"",f;return{c(){f=q(a)},l(s){f=E(s,a)},m(s,g){o(s,f,g)},p:ce,d(s){s&&r(f)}}}function Ve(d){let a;return{c(){a=q(`To deal with nonlinear problems we need a neural network with at least 1
    hidden layer.`)},l(f){a=E(f,`To deal with nonlinear problems we need a neural network with at least 1
    hidden layer.`)},m(f,s){o(f,a,s)},d(f){f&&r(a)}}}function Ge(d){let a,f,s,g,p,P,m,u,$,c,n,i,w,A,t,b,ne,Y,be,re,C,ie,j,ve,ae,R,ye,oe,M,le,N,ge,O,de,se,V,he,fe,D,$e,G,_e,me,H,ue,J,ke,pe;return g=new ze({props:{type:"info",$$slots:{default:[He]},$$scope:{ctx:d}}}),$=new Te({props:{width:500,height:500,maxWidth:600,domain:[0,1],range:[0,1],$$slots:{default:[Be]},$$scope:{ctx:d}}}),w=new Ne({props:{type:"note",id:"1"}}),b=new Te({props:{width:500,height:500,maxWidth:600,domain:[0,1],range:[0,1],padding:{top:10,right:40,bottom:45,left:45},$$slots:{default:[Xe]},$$scope:{ctx:d}}}),C=new Te({props:{width:500,height:500,maxWidth:600,domain:[0,1],range:[0,1],padding:{top:10,right:40,bottom:45,left:45},$$slots:{default:[Ye]},$$scope:{ctx:d}}}),M=new ze({props:{type:"info",$$slots:{default:[je]},$$scope:{ctx:d}}}),O=new Oe({props:{$$slots:{default:[Re]},$$scope:{ctx:d}}}),D=new ze({props:{type:"warning",$$slots:{default:[Ve]},$$scope:{ctx:d}}}),H=new De({props:{layers:d[4],height:150,padding:{left:0,right:10}}}),{c(){a=L("p"),f=q(`We have arrived at a point in our studies, where we can start to understand
    neural networks, but there are several questions we should ask ourselves
    before we move on to the technicalities of neural networks. Let's start with
    the most obvious question.`),s=v(),h(g.$$.fragment),p=v(),P=L("p"),m=q(`In the example below we have a classification problem with two classes and
    two features. By visually inspecting the dataset, the human brain can
    quickly separate the data by imagining a circle between the two classes.`),u=v(),h($.$$.fragment),c=v(),n=L("p"),i=q(`If you go back to the logistic regression lecture, you will remember that
    logistic regression produces a linear decision boundary`),h(w.$$.fragment),A=q(`. While this might be sufficient for some problems, in our case we would
    misclassify approximately half of the data. Logistic regression can only
    produce a linear decision boundary and therefore can only solve linear
    problems. The data below on the other hand clearly depicts a nonlinear
    problem.`),t=v(),h(b.$$.fragment),ne=v(),Y=L("p"),be=q(`A neural network on the other hand can theoretically generate an adequate
    decision boundary for nonlinear problems.`),re=v(),h(C.$$.fragment),ie=v(),j=L("p"),ve=q(`Most interesting problems in machine learning are nonlinear. Computer vision
    for example is highly nonlinear. Linear and logistic regression are
    therefore not sufficient and we have to utilize artificial neural networks.`),ae=v(),R=L("p"),ye=q("From our discussion above the next question follows naturally."),oe=v(),h(M.$$.fragment),le=v(),N=L("p"),ge=q(`A neural network must utilize nonlinear activation functions in order to
    solve nonlinear problems. If for example we used an identity function as our
    activation function, no matter how many layers our neural network would
    have, we would only be able to solve linear problems. A sigmoid activation
    function `),h(O.$$.fragment),de=q(` is nonlinear and is
    going to be used as an example in this lecture. That being said, there are many
    more nonlinear activation functions, which often provide much better properties
    than the sigmoid activation. Additional activation functions are going to be
    discussed in a separate lecture.`),se=v(),V=L("p"),he=q(`As you have probably already guessed, a nonlinear activation function by
    itself is not sufficient to solve nonlinear problems. Logistic regression
    for example produces a linear decision boundary, even though it is based on
    the sigmoid activation function.`),fe=v(),h(D.$$.fragment),$e=v(),G=L("p"),_e=q(`The below architecture with two inputs, one hidden layer with four neurons
    and the sigmoid activation function will be utilized to learn to solve the
    circular problem above.`),me=v(),h(H.$$.fragment),ue=v(),J=L("p"),ke=q(`How many hidden layers you eventually use and how many neurons are going to
    be used in a particular layer is up to you, but many problems will require a
    a particualar architecture to be solved efficiently.`)},l(e){a=W(e,"P",{});var l=F(a);f=E(l,`We have arrived at a point in our studies, where we can start to understand
    neural networks, but there are several questions we should ask ourselves
    before we move on to the technicalities of neural networks. Let's start with
    the most obvious question.`),l.forEach(r),s=y(e),_(g.$$.fragment,e),p=y(e),P=W(e,"P",{});var Q=F(P);m=E(Q,`In the example below we have a classification problem with two classes and
    two features. By visually inspecting the dataset, the human brain can
    quickly separate the data by imagining a circle between the two classes.`),Q.forEach(r),u=y(e),_($.$$.fragment,e),c=y(e),n=W(e,"P",{});var B=F(n);i=E(B,`If you go back to the logistic regression lecture, you will remember that
    logistic regression produces a linear decision boundary`),_(w.$$.fragment,B),A=E(B,`. While this might be sufficient for some problems, in our case we would
    misclassify approximately half of the data. Logistic regression can only
    produce a linear decision boundary and therefore can only solve linear
    problems. The data below on the other hand clearly depicts a nonlinear
    problem.`),B.forEach(r),t=y(e),_(b.$$.fragment,e),ne=y(e),Y=W(e,"P",{});var U=F(Y);be=E(U,`A neural network on the other hand can theoretically generate an adequate
    decision boundary for nonlinear problems.`),U.forEach(r),re=y(e),_(C.$$.fragment,e),ie=y(e),j=W(e,"P",{});var Z=F(j);ve=E(Z,`Most interesting problems in machine learning are nonlinear. Computer vision
    for example is highly nonlinear. Linear and logistic regression are
    therefore not sufficient and we have to utilize artificial neural networks.`),Z.forEach(r),ae=y(e),R=W(e,"P",{});var ee=F(R);ye=E(ee,"From our discussion above the next question follows naturally."),ee.forEach(r),oe=y(e),_(M.$$.fragment,e),le=y(e),N=W(e,"P",{});var X=F(N);ge=E(X,`A neural network must utilize nonlinear activation functions in order to
    solve nonlinear problems. If for example we used an identity function as our
    activation function, no matter how many layers our neural network would
    have, we would only be able to solve linear problems. A sigmoid activation
    function `),_(O.$$.fragment,X),de=E(X,` is nonlinear and is
    going to be used as an example in this lecture. That being said, there are many
    more nonlinear activation functions, which often provide much better properties
    than the sigmoid activation. Additional activation functions are going to be
    discussed in a separate lecture.`),X.forEach(r),se=y(e),V=W(e,"P",{});var te=F(V);he=E(te,`As you have probably already guessed, a nonlinear activation function by
    itself is not sufficient to solve nonlinear problems. Logistic regression
    for example produces a linear decision boundary, even though it is based on
    the sigmoid activation function.`),te.forEach(r),fe=y(e),_(D.$$.fragment,e),$e=y(e),G=W(e,"P",{});var Ee=F(G);_e=E(Ee,`The below architecture with two inputs, one hidden layer with four neurons
    and the sigmoid activation function will be utilized to learn to solve the
    circular problem above.`),Ee.forEach(r),me=y(e),_(H.$$.fragment,e),ue=y(e),J=W(e,"P",{});var Ie=F(J);ke=E(Ie,`How many hidden layers you eventually use and how many neurons are going to
    be used in a particular layer is up to you, but many problems will require a
    a particualar architecture to be solved efficiently.`),Ie.forEach(r)},m(e,l){o(e,a,l),I(a,f),o(e,s,l),k(g,e,l),o(e,p,l),o(e,P,l),I(P,m),o(e,u,l),k($,e,l),o(e,c,l),o(e,n,l),I(n,i),k(w,n,null),I(n,A),o(e,t,l),k(b,e,l),o(e,ne,l),o(e,Y,l),I(Y,be),o(e,re,l),k(C,e,l),o(e,ie,l),o(e,j,l),I(j,ve),o(e,ae,l),o(e,R,l),I(R,ye),o(e,oe,l),k(M,e,l),o(e,le,l),o(e,N,l),I(N,ge),k(O,N,null),I(N,de),o(e,se,l),o(e,V,l),I(V,he),o(e,fe,l),k(D,e,l),o(e,$e,l),o(e,G,l),I(G,_e),o(e,me,l),k(H,e,l),o(e,ue,l),o(e,J,l),I(J,ke),pe=!0},p(e,l){const Q={};l&64&&(Q.$$scope={dirty:l,ctx:e}),g.$set(Q);const B={};l&64&&(B.$$scope={dirty:l,ctx:e}),$.$set(B);const U={};l&64&&(U.$$scope={dirty:l,ctx:e}),b.$set(U);const Z={};l&64&&(Z.$$scope={dirty:l,ctx:e}),C.$set(Z);const ee={};l&64&&(ee.$$scope={dirty:l,ctx:e}),M.$set(ee);const X={};l&64&&(X.$$scope={dirty:l,ctx:e}),O.$set(X);const te={};l&64&&(te.$$scope={dirty:l,ctx:e}),D.$set(te)},i(e){pe||(x(g.$$.fragment,e),x($.$$.fragment,e),x(w.$$.fragment,e),x(b.$$.fragment,e),x(C.$$.fragment,e),x(M.$$.fragment,e),x(O.$$.fragment,e),x(D.$$.fragment,e),x(H.$$.fragment,e),pe=!0)},o(e){z(g.$$.fragment,e),z($.$$.fragment,e),z(w.$$.fragment,e),z(b.$$.fragment,e),z(C.$$.fragment,e),z(M.$$.fragment,e),z(O.$$.fragment,e),z(D.$$.fragment,e),z(H.$$.fragment,e),pe=!1},d(e){e&&r(a),e&&r(s),T(g,e),e&&r(p),e&&r(P),e&&r(u),T($,e),e&&r(c),e&&r(n),T(w),e&&r(t),T(b,e),e&&r(ne),e&&r(Y),e&&r(re),T(C,e),e&&r(ie),e&&r(j),e&&r(ae),e&&r(R),e&&r(oe),T(M,e),e&&r(le),e&&r(N),T(O),e&&r(se),e&&r(V),e&&r(fe),T(D,e),e&&r($e),e&&r(G),e&&r(me),T(H,e),e&&r(ue),e&&r(J)}}}function Je(d){let a,f,s,g,p,P,m,u,$,c,n;return u=new Ce({props:{$$slots:{default:[Ge]},$$scope:{ctx:d}}}),c=new Me({props:{notes:d[0]}}),{c(){a=L("meta"),f=v(),s=L("h1"),g=q("Nonlinear Problems"),p=v(),P=L("div"),m=v(),h(u.$$.fragment),$=v(),h(c.$$.fragment),this.h()},l(i){const w=Se("svelte-kftey",document.head);a=W(w,"META",{name:!0,content:!0}),w.forEach(r),f=y(i),s=W(i,"H1",{});var A=F(s);g=E(A,"Nonlinear Problems"),A.forEach(r),p=y(i),P=W(i,"DIV",{class:!0}),F(P).forEach(r),m=y(i),_(u.$$.fragment,i),$=y(i),_(c.$$.fragment,i),this.h()},h(){document.title="Nonlinear Problems - World4AI",xe(a,"name","description"),xe(a,"content","Most interesting problems in machine learning are highly nonlinear. Neural networks are capable of solving such problems, if the network uses nonlinear activation functions and at least 1 hidden layer."),xe(P,"class","separator")},m(i,w){I(document.head,a),o(i,f,w),o(i,s,w),I(s,g),o(i,p,w),o(i,P,w),o(i,m,w),k(u,i,w),o(i,$,w),k(c,i,w),n=!0},p(i,[w]){const A={};w&64&&(A.$$scope={dirty:w,ctx:i}),u.$set(A)},i(i){n||(x(u.$$.fragment,i),x(c.$$.fragment,i),n=!0)},o(i){z(u.$$.fragment,i),z(c.$$.fragment,i),n=!1},d(i){r(a),i&&r(f),i&&r(s),i&&r(p),i&&r(P),i&&r(m),T(u,i),i&&r($),T(c,i)}}}let S=50,Ke=.5,Qe=.5;function Ue(d){let a=[`The decision boundary in the visualisations is caclulated approximately by dividing the graph into ${S} rows and ${S} columns and drawing a box with the corresponding class. If the decision boundaries don't look smooth, this is due to the approximative nature of the calculation.`],f=[[],[]],s=[.45,.25];for(let m=0;m<s.length;m++)for(let u=0;u<200;u++){let $=2*Math.PI*Math.random(),c=s[m],n=c*Math.cos($)+Ke,i=c*Math.sin($)+Qe;f[m].push({x:n,y:i})}let g=[[],[]];for(let m=0;m<=S;m++)for(let u=0;u<=S;u++){let $=m/S,c=u/S,n={x:$,y:c};$+c>1?g[0].push(n):g[1].push(n)}let p=[[],[]];for(let m=0;m<=S;m++)for(let u=0;u<=S;u++){let $=m/S,c=u/S,n={x:$,y:c};($-.5)**2+(c-.5)**2>.12?p[0].push(n):p[1].push(n),p.push(n)}return[a,f,g,p,[{title:"Input",nodes:[{value:"x_1",class:"fill-gray-300"},{value:"x_2",class:"fill-gray-300"}]},{title:"Hidden 1",nodes:[{value:"a_1",class:"fill-w4ai-yellow"},{value:"a_2",class:"fill-w4ai-yellow"},{value:"a_3",class:"fill-w4ai-yellow"},{value:"a_4",class:"fill-w4ai-yellow"}]},{title:"Output",nodes:[{value:"o_1",class:"fill-w4ai-blue"}]}]]}class ft extends Le{constructor(a){super(),We(this,a,Ue,Je,Fe,{})}}export{ft as default};
