import{S as dt,i as pt,s as ct,k as G,a as E,y,W as $t,l as A,h as f,c as I,z as v,n as j,N as g,b as $,A as _,g as d,d as c,B as k,q as b,m as T,r as x,Q as Te,R as Pe,C as ne,P as Ge,v as mt,f as ut,e as xe}from"../chunks/index.4d92b023.js";import{C as gt}from"../chunks/Container.b0705c7b.js";import{F as wt,I as Qe}from"../chunks/InternalLink.7deb899c.js";import{H as yt}from"../chunks/Highlight.b7c1de53.js";import{P as vt}from"../chunks/PythonCode.212ba7a6.js";import{S as be}from"../chunks/SvgContainer.f70b5745.js";import{B as F}from"../chunks/Block.059eddcd.js";import{A as C}from"../chunks/Arrow.ae91874c.js";function lt(u,n,o){const r=u.slice();return r[2]=n[o],r[4]=o,r}function _t(u,n,o){const r=u.slice();return r[2]=n[o],r[6]=o,r}function st(u,n,o){const r=u.slice();return r[7]=n[o],r[4]=o,r}function kt(u,n,o){const r=u.slice();return r[2]=n[o],r[4]=o,r}function bt(u){let n;return{c(){n=b("GPT")},l(o){n=x(o,"GPT")},m(o,r){$(o,n,r)},d(o){o&&f(n)}}}function xt(u){let n,o;return n=new F({props:{x:100,y:30+u[4]*50,width:160,height:30,fontSize:15,class:"fill-blue-100",text:"Decoder Layer"}}),{c(){y(n.$$.fragment)},l(r){v(n.$$.fragment,r)},m(r,l){_(n,r,l),o=!0},p:ne,i(r){o||(d(n.$$.fragment,r),o=!0)},o(r){c(n.$$.fragment,r),o=!1},d(r){k(n,r)}}}function Tt(u){let n,o,r;o=new C({props:{data:[{x:100,y:310},{x:100,y:6}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4",moving:!0}});let l=Array(6),s=[];for(let i=0;i<l.length;i+=1)s[i]=xt(kt(u,l,i));return{c(){n=Te("svg"),y(o.$$.fragment);for(let i=0;i<s.length;i+=1)s[i].c();this.h()},l(i){n=Pe(i,"svg",{viewBox:!0});var t=T(n);v(o.$$.fragment,t);for(let a=0;a<s.length;a+=1)s[a].l(t);t.forEach(f),this.h()},h(){j(n,"viewBox","0 0 200 300")},m(i,t){$(i,n,t),_(o,n,null);for(let a=0;a<s.length;a+=1)s[a]&&s[a].m(n,null);r=!0},p:ne,i(i){if(!r){d(o.$$.fragment,i);for(let t=0;t<l.length;t+=1)d(s[t]);r=!0}},o(i){c(o.$$.fragment,i),s=s.filter(Boolean);for(let t=0;t<s.length;t+=1)c(s[t]);r=!1},d(i){i&&f(n),k(o),Ge(s,i)}}}function Pt(u){let n,o,r,l,s,i,t,a,h,P,O,W,M,D,S;return o=new F({props:{x:150,y:175,width:200,height:300,text:"",fontSize:20}}),r=new C({props:{data:[{x:150,y:350},{x:150,y:275}],strokeWidth:2,strokeDashArray:"4 4",dashed:!0}}),l=new C({props:{data:[{x:150,y:350},{x:150,y:290},{x:90,y:290},{x:90,y:275}],strokeWidth:2,strokeDashArray:"4 4",dashed:!0}}),s=new C({props:{data:[{x:150,y:350},{x:150,y:290},{x:210,y:290},{x:210,y:275}],strokeWidth:2,strokeDashArray:"4 4",dashed:!0}}),i=new C({props:{data:[{x:150,y:350},{x:150,y:310},{x:10,y:310},{x:10,y:200},{x:65,y:200}],strokeWidth:2,strokeDashArray:"4 4",dashed:!0}}),t=new C({props:{data:[{x:150,y:250},{x:150,y:125}],strokeWidth:2,strokeDashArray:"4 4",dashed:!0}}),a=new C({props:{data:[{x:150,y:170},{x:10,y:170},{x:10,y:50},{x:65,y:50}],strokeWidth:2,strokeDashArray:"4 4",dashed:!0}}),h=new C({props:{data:[{x:150,y:100},{x:150,y:10}],strokeWidth:2,strokeDashArray:"4 4",dashed:!0}}),P=new F({props:{x:150,y:50,width:150,height:30,text:"Add & Norm",fontSize:15,class:"fill-red-400"}}),O=new F({props:{x:150,y:100,width:150,height:30,text:"P.w. Feed Forward",fontSize:15,class:"fill-red-400"}}),W=new F({props:{x:150,y:200,width:150,height:30,text:"Add & Norm",fontSize:15,class:"fill-red-400"}}),M=new F({props:{x:150,y:250,width:150,height:30,text:"Multihead Atention",fontSize:15,class:"fill-red-400"}}),D=new F({props:{x:43,y:250,width:60,height:20,text:"Masked",fontSize:10,class:"fill-red-400"}}),{c(){n=Te("svg"),y(o.$$.fragment),y(r.$$.fragment),y(l.$$.fragment),y(s.$$.fragment),y(i.$$.fragment),y(t.$$.fragment),y(a.$$.fragment),y(h.$$.fragment),y(P.$$.fragment),y(O.$$.fragment),y(W.$$.fragment),y(M.$$.fragment),y(D.$$.fragment),this.h()},l(p){n=Pe(p,"svg",{viewBox:!0});var w=T(n);v(o.$$.fragment,w),v(r.$$.fragment,w),v(l.$$.fragment,w),v(s.$$.fragment,w),v(i.$$.fragment,w),v(t.$$.fragment,w),v(a.$$.fragment,w),v(h.$$.fragment,w),v(P.$$.fragment,w),v(O.$$.fragment,w),v(W.$$.fragment,w),v(M.$$.fragment,w),v(D.$$.fragment,w),w.forEach(f),this.h()},h(){j(n,"viewBox","0 0 260 360")},m(p,w){$(p,n,w),_(o,n,null),_(r,n,null),_(l,n,null),_(s,n,null),_(i,n,null),_(t,n,null),_(a,n,null),_(h,n,null),_(P,n,null),_(O,n,null),_(W,n,null),_(M,n,null),_(D,n,null),S=!0},p:ne,i(p){S||(d(o.$$.fragment,p),d(r.$$.fragment,p),d(l.$$.fragment,p),d(s.$$.fragment,p),d(i.$$.fragment,p),d(t.$$.fragment,p),d(a.$$.fragment,p),d(h.$$.fragment,p),d(P.$$.fragment,p),d(O.$$.fragment,p),d(W.$$.fragment,p),d(M.$$.fragment,p),d(D.$$.fragment,p),S=!0)},o(p){c(o.$$.fragment,p),c(r.$$.fragment,p),c(l.$$.fragment,p),c(s.$$.fragment,p),c(i.$$.fragment,p),c(t.$$.fragment,p),c(a.$$.fragment,p),c(h.$$.fragment,p),c(P.$$.fragment,p),c(O.$$.fragment,p),c(W.$$.fragment,p),c(M.$$.fragment,p),c(D.$$.fragment,p),S=!1},d(p){p&&f(n),k(o),k(r),k(l),k(s),k(i),k(t),k(a),k(h),k(P),k(O),k(W),k(M),k(D)}}}function ft(u){let n,o;return n=new F({props:{x:25+u[4]*50,y:20,width:40,height:20,text:u[7],fontSize:10,class:u[4]===3?"fill-black":"fill-lime-100"}}),{c(){y(n.$$.fragment)},l(r){v(n.$$.fragment,r)},m(r,l){_(n,r,l),o=!0},p:ne,i(r){o||(d(n.$$.fragment,r),o=!0)},o(r){c(n.$$.fragment,r),o=!1},d(r){k(n,r)}}}function Gt(u){let n,o,r=u[1],l=[];for(let i=0;i<r.length;i+=1)l[i]=ft(st(u,r,i));const s=i=>c(l[i],1,1,()=>{l[i]=null});return{c(){n=Te("svg");for(let i=0;i<l.length;i+=1)l[i].c();this.h()},l(i){n=Pe(i,"svg",{viewBox:!0});var t=T(n);for(let a=0;a<l.length;a+=1)l[a].l(t);t.forEach(f),this.h()},h(){j(n,"viewBox","0 0 200 40")},m(i,t){$(i,n,t);for(let a=0;a<l.length;a+=1)l[a]&&l[a].m(n,null);o=!0},p(i,t){if(t&2){r=i[1];let a;for(a=0;a<r.length;a+=1){const h=st(i,r,a);l[a]?(l[a].p(h,t),d(l[a],1)):(l[a]=ft(h),l[a].c(),d(l[a],1),l[a].m(n,null))}for(mt(),a=r.length;a<l.length;a+=1)s(a);ut()}},i(i){if(!o){for(let t=0;t<r.length;t+=1)d(l[t]);o=!0}},o(i){l=l.filter(Boolean);for(let t=0;t<l.length;t+=1)c(l[t]);o=!1},d(i){i&&f(n),Ge(l,i)}}}function At(u){let n,o;return n=new C({props:{data:[{x:25+u[4]*50,y:120},{x:25+u[6]*50,y:45}],showMarker:!1,dashed:!0,moving:!0,speed:50}}),{c(){y(n.$$.fragment)},l(r){v(n.$$.fragment,r)},m(r,l){_(n,r,l),o=!0},i(r){o||(d(n.$$.fragment,r),o=!0)},o(r){c(n.$$.fragment,r),o=!1},d(r){k(n,r)}}}function Et(u){let n,o,r=u[6]>=u[4]&&At(u);return{c(){r&&r.c(),n=xe()},l(l){r&&r.l(l),n=xe()},m(l,s){r&&r.m(l,s),$(l,n,s),o=!0},p:ne,i(l){o||(d(r),o=!0)},o(l){c(r),o=!1},d(l){r&&r.d(l),l&&f(n)}}}function ht(u){let n,o,r,l;n=new F({props:{x:25+u[4]*50,y:130,width:20,height:20,text:"T_"+(u[4]+1),class:"fill-red-300"}}),o=new F({props:{x:25+u[4]*50,y:35,width:20,height:20,text:"T_"+(u[4]+1),class:"fill-yellow-100"}});let s=Array(6),i=[];for(let t=0;t<s.length;t+=1)i[t]=Et(_t(u,s,t));return{c(){y(n.$$.fragment),y(o.$$.fragment);for(let t=0;t<i.length;t+=1)i[t].c();r=xe()},l(t){v(n.$$.fragment,t),v(o.$$.fragment,t);for(let a=0;a<i.length;a+=1)i[a].l(t);r=xe()},m(t,a){_(n,t,a),_(o,t,a);for(let h=0;h<i.length;h+=1)i[h]&&i[h].m(t,a);$(t,r,a),l=!0},p:ne,i(t){if(!l){d(n.$$.fragment,t),d(o.$$.fragment,t);for(let a=0;a<s.length;a+=1)d(i[a]);l=!0}},o(t){c(n.$$.fragment,t),c(o.$$.fragment,t),i=i.filter(Boolean);for(let a=0;a<i.length;a+=1)c(i[a]);l=!1},d(t){k(n,t),k(o,t),Ge(i,t),t&&f(r)}}}function It(u){let n,o,r;o=new F({props:{x:40,y:10,width:50,height:12,text:"GPT Family"}});let l=Array(6),s=[];for(let t=0;t<l.length;t+=1)s[t]=ht(lt(u,l,t));const i=t=>c(s[t],1,1,()=>{s[t]=null});return{c(){n=Te("svg"),y(o.$$.fragment);for(let t=0;t<s.length;t+=1)s[t].c();this.h()},l(t){n=Pe(t,"svg",{viewBox:!0});var a=T(n);v(o.$$.fragment,a);for(let h=0;h<s.length;h+=1)s[h].l(a);a.forEach(f),this.h()},h(){j(n,"viewBox","0 0 300 150")},m(t,a){$(t,n,a),_(o,n,null);for(let h=0;h<s.length;h+=1)s[h]&&s[h].m(n,null);r=!0},p(t,a){if(a&0){l=Array(6);let h;for(h=0;h<l.length;h+=1){const P=lt(t,l,h);s[h]?(s[h].p(P,a),d(s[h],1)):(s[h]=ht(P),s[h].c(),d(s[h],1),s[h].m(n,null))}for(mt(),h=l.length;h<s.length;h+=1)i(h);ut()}},i(t){if(!r){d(o.$$.fragment,t);for(let a=0;a<l.length;a+=1)d(s[a]);r=!0}},o(t){c(o.$$.fragment,t),s=s.filter(Boolean);for(let a=0;a<s.length;a+=1)c(s[a]);r=!1},d(t){t&&f(n),k(o),Ge(s,t)}}}function Wt(u){let n,o,r,l,s,i,t,a,h,P,O,W,M,D,S,p,w,Ae,re,Ee,Ie,ae,We,Be,oe,Se,ze,ie,Oe,De,me,H,ue,Q,Me,de,N,pe,V,Fe,ce,B,je,U,Le,X,Ce,Y,He,R,Ne,Re,$e,Z,qe,ge,J,we,q,Ue,le,Xe,Ye,ye,K,Je,ve,ee,Ke,_e;return t=new yt({props:{$$slots:{default:[bt]},$$scope:{ctx:u}}}),P=new be({props:{maxWidth:"250px",$$slots:{default:[Tt]},$$scope:{ctx:u}}}),S=new be({props:{maxWidth:"300px",$$slots:{default:[Pt]},$$scope:{ctx:u}}}),H=new be({props:{maxWidth:"400px",$$slots:{default:[Gt]},$$scope:{ctx:u}}}),N=new be({props:{maxWidth:"600px",$$slots:{default:[It]},$$scope:{ctx:u}}}),U=new Qe({props:{id:1,type:"reference"}}),X=new Qe({props:{id:2,type:"reference"}}),Y=new Qe({props:{id:3,type:"reference"}}),J=new vt({props:{code:`from transformers import pipeline

generator = pipeline("text-generation")
prompt = (
    "In the year 2035 humanity will have created human level artificial intelligence."
)
outputs = generator(prompt, max_length=100)`}}),{c(){n=G("h1"),o=b("GPT"),r=E(),l=G("div"),s=E(),i=G("p"),y(t.$$.fragment),a=b(`, short for generative pre-training , is a family
    of models developed by researchers at OpenAI. GPT is a decoder based
    transformer model, without any encoder interaction. We simply stack layers
    of decoders on top of each other.`),h=E(),y(P.$$.fragment),O=E(),W=G("p"),M=b(`Due to the lack of encoders in the GPT architecture we do not need any
    cross-attention. This simplifies the decoder layer to just two sublayers:
    masked multihead attention and position-wise feed-forward neural network.`),D=E(),y(S.$$.fragment),p=E(),w=G("p"),Ae=b(`The training objective is of GPT is quite simple. Given some tokens from a
    sentence, predict the next token. For example if the GPT is given the three
    words `),re=G("em"),Ee=b("what is your"),Ie=b(`, the stack of decores should predict words like
    `),ae=G("em"),We=b("name"),Be=b(", "),oe=G("em"),Se=b("age"),ze=b(" or "),ie=G("em"),Oe=b("weight"),De=b("."),me=E(),y(H.$$.fragment),ue=E(),Q=G("p"),Me=b(`Unline BERT, GPT is unidirectional and we need to maks out future words. The
    tokens must never pay attention to future tokens, as that would contaminate
    the training process by allowing the tokens to attend to tokens that they
    are actually trying to predict. So if we want to predict the fourth token
    based on the third embedding that was processed by layers of decoders, the
    third embedding can attend to all previous tokens, including itself, but not
    the token it tries to predict.`),de=E(),y(N.$$.fragment),pe=E(),V=G("p"),Fe=b(`Based on the pre-trainig task, it is obvious that GPT can be used for text
    generation. You provide the model with a starting text and the model
    generates word after word to finish your writing, using the previously
    generated words as input in a recursive manner.`),ce=E(),B=G("p"),je=b("GPT is not a single model, but a family of models. By now we have GPT-1"),y(U.$$.fragment),Le=b(", GPT-2"),y(X.$$.fragment),Ce=b(", GPT-3"),y(Y.$$.fragment),He=b(` and GPT-4. With each new iteration OpenAI increased the size of the models
    and the datasets that the models were trained on. It became clear that you could
    scale up transformer-like models through size and data and the performance would
    improve. Unfortunately for the deep-learning community OpenAI decided starting
    with GPT-2 not to release the pre-trained models to the public due to security
    concerns and profit considerations. While the weights of GPT-2 were released
    eventually, the public has no direct access to GPT-3 or GPT-4. Only throught
    the OpenAI api can you interract with newest GPT models. Luckily there are companies,
    like
    `),R=G("a"),Ne=b("EleutherAI"),Re=b(`, that attemt to replicate the OpenAI GPT-3/GPT-4 models. At the moment of
    writing their largest model, GPT-NeoX-20B, consists of 20 billion
    parameters, but they plan to train even larger models to match the
    performance of the newest models by OpenAI.`),$e=E(),Z=G("p"),qe=b(`We can use the transformers library by HuggingFace to interract with GPT-2.
    The easiest way to accomplish that is to use the text-generation pipeline. A
    pipeline abstracts away most of code running in the background. We do not
    need to take care of the tokenizer or the model, just by using
    'text-generation' as the input, HuggingFace downloads GPT-2 weights and
    allows you to generate text.`),ge=E(),y(J.$$.fragment),we=E(),q=G("p"),Ue=b("When we use the prompt "),le=G("em"),Xe=b(`"In the year 2035 humanity will have created human level artificial
      intelligence."`),Ye=b(`
    we might get the following result.`),ye=E(),K=G("p"),Je=b(`In the year 2035 humanity will have created human level artificial
    intelligence. Some experts believe that the first phase of AI could arrive
    around 2030 and be capable of being applied in a multitude of applications,
    including transportation, health or the arts.\\n\\nAs humans live longer and
    more efficiently and interact more quickly with computers, we will probably
    see a gradual step towards being a multi-systemed society. A society which
    will have more people involved and will focus on information management,
    education, marketing, governance,`),ve=E(),ee=G("p"),Ke=b(`This is relatively cohesive, but the results will depend on your initial
    prompt and will change each time you run the code.`),this.h()},l(e){n=A(e,"H1",{});var m=T(n);o=x(m,"GPT"),m.forEach(f),r=I(e),l=A(e,"DIV",{class:!0}),T(l).forEach(f),s=I(e),i=A(e,"P",{});var te=T(i);v(t.$$.fragment,te),a=x(te,`, short for generative pre-training , is a family
    of models developed by researchers at OpenAI. GPT is a decoder based
    transformer model, without any encoder interaction. We simply stack layers
    of decoders on top of each other.`),te.forEach(f),h=I(e),v(P.$$.fragment,e),O=I(e),W=A(e,"P",{});var se=T(W);M=x(se,`Due to the lack of encoders in the GPT architecture we do not need any
    cross-attention. This simplifies the decoder layer to just two sublayers:
    masked multihead attention and position-wise feed-forward neural network.`),se.forEach(f),D=I(e),v(S.$$.fragment,e),p=I(e),w=A(e,"P",{});var z=T(w);Ae=x(z,`The training objective is of GPT is quite simple. Given some tokens from a
    sentence, predict the next token. For example if the GPT is given the three
    words `),re=A(z,"EM",{});var fe=T(re);Ee=x(fe,"what is your"),fe.forEach(f),Ie=x(z,`, the stack of decores should predict words like
    `),ae=A(z,"EM",{});var he=T(ae);We=x(he,"name"),he.forEach(f),Be=x(z,", "),oe=A(z,"EM",{});var Ve=T(oe);Se=x(Ve,"age"),Ve.forEach(f),ze=x(z," or "),ie=A(z,"EM",{});var Ze=T(ie);Oe=x(Ze,"weight"),Ze.forEach(f),De=x(z,"."),z.forEach(f),me=I(e),v(H.$$.fragment,e),ue=I(e),Q=A(e,"P",{});var et=T(Q);Me=x(et,`Unline BERT, GPT is unidirectional and we need to maks out future words. The
    tokens must never pay attention to future tokens, as that would contaminate
    the training process by allowing the tokens to attend to tokens that they
    are actually trying to predict. So if we want to predict the fourth token
    based on the third embedding that was processed by layers of decoders, the
    third embedding can attend to all previous tokens, including itself, but not
    the token it tries to predict.`),et.forEach(f),de=I(e),v(N.$$.fragment,e),pe=I(e),V=A(e,"P",{});var tt=T(V);Fe=x(tt,`Based on the pre-trainig task, it is obvious that GPT can be used for text
    generation. You provide the model with a starting text and the model
    generates word after word to finish your writing, using the previously
    generated words as input in a recursive manner.`),tt.forEach(f),ce=I(e),B=A(e,"P",{});var L=T(B);je=x(L,"GPT is not a single model, but a family of models. By now we have GPT-1"),v(U.$$.fragment,L),Le=x(L,", GPT-2"),v(X.$$.fragment,L),Ce=x(L,", GPT-3"),v(Y.$$.fragment,L),He=x(L,` and GPT-4. With each new iteration OpenAI increased the size of the models
    and the datasets that the models were trained on. It became clear that you could
    scale up transformer-like models through size and data and the performance would
    improve. Unfortunately for the deep-learning community OpenAI decided starting
    with GPT-2 not to release the pre-trained models to the public due to security
    concerns and profit considerations. While the weights of GPT-2 were released
    eventually, the public has no direct access to GPT-3 or GPT-4. Only throught
    the OpenAI api can you interract with newest GPT models. Luckily there are companies,
    like
    `),R=A(L,"A",{href:!0,target:!0,rel:!0});var nt=T(R);Ne=x(nt,"EleutherAI"),nt.forEach(f),Re=x(L,`, that attemt to replicate the OpenAI GPT-3/GPT-4 models. At the moment of
    writing their largest model, GPT-NeoX-20B, consists of 20 billion
    parameters, but they plan to train even larger models to match the
    performance of the newest models by OpenAI.`),L.forEach(f),$e=I(e),Z=A(e,"P",{});var rt=T(Z);qe=x(rt,`We can use the transformers library by HuggingFace to interract with GPT-2.
    The easiest way to accomplish that is to use the text-generation pipeline. A
    pipeline abstracts away most of code running in the background. We do not
    need to take care of the tokenizer or the model, just by using
    'text-generation' as the input, HuggingFace downloads GPT-2 weights and
    allows you to generate text.`),rt.forEach(f),ge=I(e),v(J.$$.fragment,e),we=I(e),q=A(e,"P",{});var ke=T(q);Ue=x(ke,"When we use the prompt "),le=A(ke,"EM",{});var at=T(le);Xe=x(at,`"In the year 2035 humanity will have created human level artificial
      intelligence."`),at.forEach(f),Ye=x(ke,`
    we might get the following result.`),ke.forEach(f),ye=I(e),K=A(e,"P",{class:!0});var ot=T(K);Je=x(ot,`In the year 2035 humanity will have created human level artificial
    intelligence. Some experts believe that the first phase of AI could arrive
    around 2030 and be capable of being applied in a multitude of applications,
    including transportation, health or the arts.\\n\\nAs humans live longer and
    more efficiently and interact more quickly with computers, we will probably
    see a gradual step towards being a multi-systemed society. A society which
    will have more people involved and will focus on information management,
    education, marketing, governance,`),ot.forEach(f),ve=I(e),ee=A(e,"P",{});var it=T(ee);Ke=x(it,`This is relatively cohesive, but the results will depend on your initial
    prompt and will change each time you run the code.`),it.forEach(f),this.h()},h(){j(l,"class","separator"),j(R,"href","https://www.eleuther.ai/"),j(R,"target","_blank"),j(R,"rel","noreferrer"),j(K,"class","font-mono bg-slate-50 p-3")},m(e,m){$(e,n,m),g(n,o),$(e,r,m),$(e,l,m),$(e,s,m),$(e,i,m),_(t,i,null),g(i,a),$(e,h,m),_(P,e,m),$(e,O,m),$(e,W,m),g(W,M),$(e,D,m),_(S,e,m),$(e,p,m),$(e,w,m),g(w,Ae),g(w,re),g(re,Ee),g(w,Ie),g(w,ae),g(ae,We),g(w,Be),g(w,oe),g(oe,Se),g(w,ze),g(w,ie),g(ie,Oe),g(w,De),$(e,me,m),_(H,e,m),$(e,ue,m),$(e,Q,m),g(Q,Me),$(e,de,m),_(N,e,m),$(e,pe,m),$(e,V,m),g(V,Fe),$(e,ce,m),$(e,B,m),g(B,je),_(U,B,null),g(B,Le),_(X,B,null),g(B,Ce),_(Y,B,null),g(B,He),g(B,R),g(R,Ne),g(B,Re),$(e,$e,m),$(e,Z,m),g(Z,qe),$(e,ge,m),_(J,e,m),$(e,we,m),$(e,q,m),g(q,Ue),g(q,le),g(le,Xe),g(q,Ye),$(e,ye,m),$(e,K,m),g(K,Je),$(e,ve,m),$(e,ee,m),g(ee,Ke),_e=!0},p(e,m){const te={};m&1024&&(te.$$scope={dirty:m,ctx:e}),t.$set(te);const se={};m&1024&&(se.$$scope={dirty:m,ctx:e}),P.$set(se);const z={};m&1024&&(z.$$scope={dirty:m,ctx:e}),S.$set(z);const fe={};m&1024&&(fe.$$scope={dirty:m,ctx:e}),H.$set(fe);const he={};m&1024&&(he.$$scope={dirty:m,ctx:e}),N.$set(he)},i(e){_e||(d(t.$$.fragment,e),d(P.$$.fragment,e),d(S.$$.fragment,e),d(H.$$.fragment,e),d(N.$$.fragment,e),d(U.$$.fragment,e),d(X.$$.fragment,e),d(Y.$$.fragment,e),d(J.$$.fragment,e),_e=!0)},o(e){c(t.$$.fragment,e),c(P.$$.fragment,e),c(S.$$.fragment,e),c(H.$$.fragment,e),c(N.$$.fragment,e),c(U.$$.fragment,e),c(X.$$.fragment,e),c(Y.$$.fragment,e),c(J.$$.fragment,e),_e=!1},d(e){e&&f(n),e&&f(r),e&&f(l),e&&f(s),e&&f(i),k(t),e&&f(h),k(P,e),e&&f(O),e&&f(W),e&&f(D),k(S,e),e&&f(p),e&&f(w),e&&f(me),k(H,e),e&&f(ue),e&&f(Q),e&&f(de),k(N,e),e&&f(pe),e&&f(V),e&&f(ce),e&&f(B),k(U),k(X),k(Y),e&&f($e),e&&f(Z),e&&f(ge),k(J,e),e&&f(we),e&&f(q),e&&f(ye),e&&f(K),e&&f(ve),e&&f(ee)}}}function Bt(u){let n,o,r,l,s,i;return r=new gt({props:{$$slots:{default:[Wt]},$$scope:{ctx:u}}}),s=new wt({props:{references:u[0]}}),{c(){n=G("meta"),o=E(),y(r.$$.fragment),l=E(),y(s.$$.fragment),this.h()},l(t){const a=$t("svelte-1pzf1sl",document.head);n=A(a,"META",{name:!0,content:!0}),a.forEach(f),o=I(t),v(r.$$.fragment,t),l=I(t),v(s.$$.fragment,t),this.h()},h(){document.title="GPT - World4AI",j(n,"name","description"),j(n,"content","GPT, short for generative pre-training, is a family of decoder based transformer models developed by OpenAI. The models are trained based on next token predictions and can perform zero-shot, one-shot or few-shots learning.")},m(t,a){g(document.head,n),$(t,o,a),_(r,t,a),$(t,l,a),_(s,t,a),i=!0},p(t,[a]){const h={};a&1024&&(h.$$scope={dirty:a,ctx:t}),r.$set(h)},i(t){i||(d(r.$$.fragment,t),d(s.$$.fragment,t),i=!0)},o(t){c(r.$$.fragment,t),c(s.$$.fragment,t),i=!1},d(t){f(n),t&&f(o),k(r,t),t&&f(l),k(s,t)}}}function St(u){return[[{author:"Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever",title:"Improving Language Understanding by Generative Pre-Training",journal:"",year:"2018",pages:"",volume:"",issue:""},{author:"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",title:"Language Models are Unsupervised Multitask Learners",journal:"",year:"2019",pages:"",volume:"",issue:""},{author:"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et. al",title:"Language Models are Few-Shot Learners",journal:"",year:"2020",pages:"",volume:"",issue:""}],["what","is","your","name"]]}class Ht extends dt{constructor(n){super(),pt(this,n,St,Bt,ct,{})}}export{Ht as default};
