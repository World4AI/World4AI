import{S as ze,i as De,s as Ve,k as b,a as p,q as u,y as c,W as Be,l as k,h as n,c as m,m as T,r as f,z as d,n as ke,N as $,b as l,A as g,g as w,d as _,B as v}from"../chunks/index.4d92b023.js";import{C as Me}from"../chunks/Container.b0705c7b.js";import{H as re}from"../chunks/Highlight.b7c1de53.js";import{A as Ge}from"../chunks/Alert.25a852b3.js";import{N as ie}from"../chunks/NeuralNetwork.9b1e2957.js";import{L as be}from"../chunks/Latex.e0b308c0.js";function Je(o){let a;return{c(){a=u("what is a neural network?")},l(t){a=f(t,"what is a neural network?")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function Ke(o){let a;return{c(){a=u(`A neural network is an object, that structures individual neurons in a
    hierarchy of layers and combines them into a single model, by feeding the
    outputs of a layer as inputs into the next layer.`)},l(t){a=f(t,`A neural network is an object, that structures individual neurons in a
    hierarchy of layers and combines them into a single model, by feeding the
    outputs of a layer as inputs into the next layer.`)},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function Qe(o){let a;return{c(){a=u("x_1 - x_4")},l(t){a=f(t,"x_1 - x_4")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function Re(o){let a;return{c(){a=u("w_1 - w_4")},l(t){a=f(t,"w_1 - w_4")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function Ue(o){let a;return{c(){a=u("b")},l(t){a=f(t,"b")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function Xe(o){let a;return{c(){a=u("\\sigma")},l(t){a=f(t,"\\sigma")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function Ye(o){let a;return{c(){a=u("layer")},l(t){a=f(t,"layer")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function Ze(o){let a;return{c(){a=u("hidden features")},l(t){a=f(t,"hidden features")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function et(o){let a;return{c(){a=u("forward pass")},l(t){a=f(t,"forward pass")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function tt(o){let a;return{c(){a=u("backward pass")},l(t){a=f(t,"backward pass")},m(t,i){l(t,a,i)},d(t){t&&n(a)}}}function at(o){let a,t,i,P,x,I,L,E,A,r,h,y,xe,H,Ne,W,Te,q,Ee,oe,B,ue,C,Ie,S,Le,fe,M,$e,F,Pe,O,Ae,pe,G,me,Y,He,he,Z,We,ce,j,qe,z,Ce,de,J,ge,D,Se,V,Fe,we,K,_e,ee,Oe,ve,te,ye;return i=new re({props:{$$slots:{default:[Je]},$$scope:{ctx:o}}}),x=new Ge({props:{type:"info",$$slots:{default:[Ke]},$$scope:{ctx:o}}}),y=new be({props:{$$slots:{default:[Qe]},$$scope:{ctx:o}}}),H=new be({props:{$$slots:{default:[Re]},$$scope:{ctx:o}}}),W=new be({props:{$$slots:{default:[Ue]},$$scope:{ctx:o}}}),q=new be({props:{$$slots:{default:[Xe]},$$scope:{ctx:o}}}),B=new ie({props:{layers:o[0],height:130,padding:{left:0,right:50}}}),S=new re({props:{$$slots:{default:[Ye]},$$scope:{ctx:o}}}),M=new ie({props:{layers:o[1],height:130,padding:{left:0,right:50}}}),O=new re({props:{$$slots:{default:[Ze]},$$scope:{ctx:o}}}),G=new ie({props:{layers:o[2],height:130,padding:{left:0,right:10}}}),z=new re({props:{$$slots:{default:[et]},$$scope:{ctx:o}}}),J=new ie({props:{layers:o[3],height:150}}),V=new re({props:{$$slots:{default:[tt]},$$scope:{ctx:o}}}),K=new ie({props:{layers:o[3],height:150,speed:.5,connectionStyle:"stroke-red-500"}}),{c(){a=b("p"),t=u("Let's start this chapter with the obvious question: "),c(i.$$.fragment),P=p(),c(x.$$.fragment),I=p(),L=b("p"),E=u(`If the above definition does not make any sense to you, below is a more
    intuitive explanation.`),A=p(),r=b("p"),h=u(`There are many different activation functions out there, but for now we will
    assume that we are dealing with the sigmoid activation function. That means,
    that a neuron is essentially a separate logistic regression unit with
    individual weights and a bias. The neuron below for example takes features `),c(y.$$.fragment),xe=u(" as inputs, multiplies those with individual weights "),c(H.$$.fragment),Ne=u(", adds the bias "),c(W.$$.fragment),Te=u(` and applies the sigmoid activation function
    `),c(q.$$.fragment),Ee=u("."),oe=p(),c(B.$$.fragment),ue=p(),C=b("p"),Ie=u(`In a neural network the same are used to produce several different neurons.
    Those neurons utilize different weights and biases and produce therefore
    different outputs. Such a collection of neurons is called a `),c(S.$$.fragment),Le=u("."),fe=p(),c(M.$$.fragment),$e=p(),F=b("p"),Pe=u(`We can stack several layers after each other to produce a neural network.
    The outputs of the previous layer are used as inputs instead of the input
    features. Often the input neurons are also called `),c(O.$$.fragment),Ae=u("."),pe=p(),c(G.$$.fragment),me=p(),Y=b("p"),He=u(`The output neuron(s) is (are) used as an input into the loss function, for
    example the cross-entropy loss if we are dealing with a classificatio
    problem.`),he=p(),Z=b("p"),We=u(`We can train neural networks that can classify images, generate text or play
    computer games. No matter what task we are trying to accomplish and how the
    neural network is structured, the training process of neural networks is
    always done using the same steps that we used in linear and logistic
    regression.`),ce=p(),j=b("p"),qe=u("In the "),c(z.$$.fragment),Ce=u(`
    the features are processed layer by layer and neuron by neuron to finally determine
    the loss of the neural network and to construct a computational graph.`),de=p(),c(J.$$.fragment),ge=p(),D=b("p"),Se=u("In the "),c(V.$$.fragment),Fe=u(` we use the backpropagation algorithm
    to calculate the gradients for all weights and biases.`),we=p(),c(K.$$.fragment),_e=p(),ee=b("p"),Oe=u(`Conceptually the whole learning process is not much different from what we
    saw in the previous chapters. The computational graph is larger and broader,
    but the ideas are the same.`),ve=p(),te=b("div"),this.h()},l(e){a=k(e,"P",{});var s=T(a);t=f(s,"Let's start this chapter with the obvious question: "),d(i.$$.fragment,s),s.forEach(n),P=m(e),d(x.$$.fragment,e),I=m(e),L=k(e,"P",{});var ae=T(L);E=f(ae,`If the above definition does not make any sense to you, below is a more
    intuitive explanation.`),ae.forEach(n),A=m(e),r=k(e,"P",{});var N=T(r);h=f(N,`There are many different activation functions out there, but for now we will
    assume that we are dealing with the sigmoid activation function. That means,
    that a neuron is essentially a separate logistic regression unit with
    individual weights and a bias. The neuron below for example takes features `),d(y.$$.fragment,N),xe=f(N," as inputs, multiplies those with individual weights "),d(H.$$.fragment,N),Ne=f(N,", adds the bias "),d(W.$$.fragment,N),Te=f(N,` and applies the sigmoid activation function
    `),d(q.$$.fragment,N),Ee=f(N,"."),N.forEach(n),oe=m(e),d(B.$$.fragment,e),ue=m(e),C=k(e,"P",{});var Q=T(C);Ie=f(Q,`In a neural network the same are used to produce several different neurons.
    Those neurons utilize different weights and biases and produce therefore
    different outputs. Such a collection of neurons is called a `),d(S.$$.fragment,Q),Le=f(Q,"."),Q.forEach(n),fe=m(e),d(M.$$.fragment,e),$e=m(e),F=k(e,"P",{});var R=T(F);Pe=f(R,`We can stack several layers after each other to produce a neural network.
    The outputs of the previous layer are used as inputs instead of the input
    features. Often the input neurons are also called `),d(O.$$.fragment,R),Ae=f(R,"."),R.forEach(n),pe=m(e),d(G.$$.fragment,e),me=m(e),Y=k(e,"P",{});var se=T(Y);He=f(se,`The output neuron(s) is (are) used as an input into the loss function, for
    example the cross-entropy loss if we are dealing with a classificatio
    problem.`),se.forEach(n),he=m(e),Z=k(e,"P",{});var ne=T(Z);We=f(ne,`We can train neural networks that can classify images, generate text or play
    computer games. No matter what task we are trying to accomplish and how the
    neural network is structured, the training process of neural networks is
    always done using the same steps that we used in linear and logistic
    regression.`),ne.forEach(n),ce=m(e),j=k(e,"P",{});var U=T(j);qe=f(U,"In the "),d(z.$$.fragment,U),Ce=f(U,`
    the features are processed layer by layer and neuron by neuron to finally determine
    the loss of the neural network and to construct a computational graph.`),U.forEach(n),de=m(e),d(J.$$.fragment,e),ge=m(e),D=k(e,"P",{});var X=T(D);Se=f(X,"In the "),d(V.$$.fragment,X),Fe=f(X,` we use the backpropagation algorithm
    to calculate the gradients for all weights and biases.`),X.forEach(n),we=m(e),d(K.$$.fragment,e),_e=m(e),ee=k(e,"P",{});var le=T(ee);Oe=f(le,`Conceptually the whole learning process is not much different from what we
    saw in the previous chapters. The computational graph is larger and broader,
    but the ideas are the same.`),le.forEach(n),ve=m(e),te=k(e,"DIV",{class:!0}),T(te).forEach(n),this.h()},h(){ke(te,"class","separator")},m(e,s){l(e,a,s),$(a,t),g(i,a,null),l(e,P,s),g(x,e,s),l(e,I,s),l(e,L,s),$(L,E),l(e,A,s),l(e,r,s),$(r,h),g(y,r,null),$(r,xe),g(H,r,null),$(r,Ne),g(W,r,null),$(r,Te),g(q,r,null),$(r,Ee),l(e,oe,s),g(B,e,s),l(e,ue,s),l(e,C,s),$(C,Ie),g(S,C,null),$(C,Le),l(e,fe,s),g(M,e,s),l(e,$e,s),l(e,F,s),$(F,Pe),g(O,F,null),$(F,Ae),l(e,pe,s),g(G,e,s),l(e,me,s),l(e,Y,s),$(Y,He),l(e,he,s),l(e,Z,s),$(Z,We),l(e,ce,s),l(e,j,s),$(j,qe),g(z,j,null),$(j,Ce),l(e,de,s),g(J,e,s),l(e,ge,s),l(e,D,s),$(D,Se),g(V,D,null),$(D,Fe),l(e,we,s),g(K,e,s),l(e,_e,s),l(e,ee,s),$(ee,Oe),l(e,ve,s),l(e,te,s),ye=!0},p(e,s){const ae={};s&16&&(ae.$$scope={dirty:s,ctx:e}),i.$set(ae);const N={};s&16&&(N.$$scope={dirty:s,ctx:e}),x.$set(N);const Q={};s&16&&(Q.$$scope={dirty:s,ctx:e}),y.$set(Q);const R={};s&16&&(R.$$scope={dirty:s,ctx:e}),H.$set(R);const se={};s&16&&(se.$$scope={dirty:s,ctx:e}),W.$set(se);const ne={};s&16&&(ne.$$scope={dirty:s,ctx:e}),q.$set(ne);const U={};s&16&&(U.$$scope={dirty:s,ctx:e}),S.$set(U);const X={};s&16&&(X.$$scope={dirty:s,ctx:e}),O.$set(X);const le={};s&16&&(le.$$scope={dirty:s,ctx:e}),z.$set(le);const je={};s&16&&(je.$$scope={dirty:s,ctx:e}),V.$set(je)},i(e){ye||(w(i.$$.fragment,e),w(x.$$.fragment,e),w(y.$$.fragment,e),w(H.$$.fragment,e),w(W.$$.fragment,e),w(q.$$.fragment,e),w(B.$$.fragment,e),w(S.$$.fragment,e),w(M.$$.fragment,e),w(O.$$.fragment,e),w(G.$$.fragment,e),w(z.$$.fragment,e),w(J.$$.fragment,e),w(V.$$.fragment,e),w(K.$$.fragment,e),ye=!0)},o(e){_(i.$$.fragment,e),_(x.$$.fragment,e),_(y.$$.fragment,e),_(H.$$.fragment,e),_(W.$$.fragment,e),_(q.$$.fragment,e),_(B.$$.fragment,e),_(S.$$.fragment,e),_(M.$$.fragment,e),_(O.$$.fragment,e),_(G.$$.fragment,e),_(z.$$.fragment,e),_(J.$$.fragment,e),_(V.$$.fragment,e),_(K.$$.fragment,e),ye=!1},d(e){e&&n(a),v(i),e&&n(P),v(x,e),e&&n(I),e&&n(L),e&&n(A),e&&n(r),v(y),v(H),v(W),v(q),e&&n(oe),v(B,e),e&&n(ue),e&&n(C),v(S),e&&n(fe),v(M,e),e&&n($e),e&&n(F),v(O),e&&n(pe),v(G,e),e&&n(me),e&&n(Y),e&&n(he),e&&n(Z),e&&n(ce),e&&n(j),v(z),e&&n(de),v(J,e),e&&n(ge),e&&n(D),v(V),e&&n(we),v(K,e),e&&n(_e),e&&n(ee),e&&n(ve),e&&n(te)}}}function st(o){let a,t,i,P,x,I,L,E,A;return E=new Me({props:{$$slots:{default:[at]},$$scope:{ctx:o}}}),{c(){a=b("meta"),t=p(),i=b("h1"),P=u("Neural Network"),x=p(),I=b("div"),L=p(),c(E.$$.fragment),this.h()},l(r){const h=Be("svelte-9gq92f",document.head);a=k(h,"META",{name:!0,content:!0}),h.forEach(n),t=m(r),i=k(r,"H1",{});var y=T(i);P=f(y,"Neural Network"),y.forEach(n),x=m(r),I=k(r,"DIV",{class:!0}),T(I).forEach(n),L=m(r),d(E.$$.fragment,r),this.h()},h(){document.title="Neural Network Introduction - World4AI",ke(a,"name","description"),ke(a,"content","A neural network is an object, that combines layers of neurons into a single model. The training process of a neural network consists of a forward and a backward pass. In the forward pass the features are used to calculate the loss of the function. In the backward pass the loss is distributed among individual neurons."),ke(I,"class","separator")},m(r,h){$(document.head,a),l(r,t,h),l(r,i,h),$(i,P),l(r,x,h),l(r,I,h),l(r,L,h),g(E,r,h),A=!0},p(r,[h]){const y={};h&16&&(y.$$scope={dirty:h,ctx:r}),E.$set(y)},i(r){A||(w(E.$$.fragment,r),A=!0)},o(r){_(E.$$.fragment,r),A=!1},d(r){n(a),r&&n(t),r&&n(i),r&&n(x),r&&n(I),r&&n(L),v(E,r)}}}function nt(o){return[[{title:"Features",nodes:[{value:"x_1",class:"fill-gray-200"},{value:"x_2",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"}]},{title:"Neuron",nodes:[{value:"a",class:"fill-blue-400"}]}],[{title:"Features",nodes:[{value:"x_1",class:"fill-gray-200"},{value:"x_2",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"}]},{title:"Neurons Layer",nodes:[{value:"a_1",class:"fill-blue-400"},{value:"a_2",class:"fill-blue-400"},{value:"a_3",class:"fill-blue-400"}]}],[{title:"Features",nodes:[{value:"x_1",class:"fill-gray-200"},{value:"x_2",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"}]},{title:"Hidden Layer 1",nodes:[{value:"a_1^1",class:"fill-blue-400"},{value:"a_2^1",class:"fill-blue-400"},{value:"a_3^1",class:"fill-blue-400"}]},{title:"Hidden Layer 2",nodes:[{value:"a_1^2",class:"fill-blue-400"},{value:"a_2^2",class:"fill-blue-400"}]},{title:"Output Layer",nodes:[{value:"o_1",class:"fill-yellow-400"}]}],[{title:"Features",nodes:[{value:"x_1",class:"fill-gray-200"},{value:"x_2",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"},{value:"x_3",class:"fill-gray-200"}]},{title:"Hidden Layer",nodes:[{value:"a_1",class:"fill-blue-400"},{value:"a_2",class:"fill-blue-400"},{value:"a_3",class:"fill-blue-400"}]},{title:"Output Layer",nodes:[{value:"o_1",class:"fill-yellow-400"}]},{title:"Loss",nodes:[{value:"L",class:"fill-red-500"}]}]]}class $t extends ze{constructor(a){super(),De(this,a,nt,st,Ve,{})}}export{$t as default};
