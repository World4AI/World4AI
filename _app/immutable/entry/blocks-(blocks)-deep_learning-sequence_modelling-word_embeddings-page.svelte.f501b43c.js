import{S as ms,i as _s,s as vs,k as _,a as x,q as $,y as K,W as ws,l as v,h as s,c as E,m as b,r as y,z as Q,n as I,N as h,b as f,A as U,g as G,d as J,B as R,P as A,C as ee,u as be,e as _l}from"../chunks/index.4d92b023.js";import{C as bs}from"../chunks/Container.b0705c7b.js";import{H as Xl}from"../chunks/Highlight.b7c1de53.js";import{L as Tn}from"../chunks/Latex.e0b308c0.js";import{A as gs}from"../chunks/Alert.25a852b3.js";import{P as Fe}from"../chunks/PythonCode.212ba7a6.js";function Nn(a,t,n){const l=a.slice();return l[4]=t[n],l[6]=n,l}function qn(a,t,n){const l=a.slice();return l[7]=t[n],l[9]=n,l}function Wn(a,t,n){const l=a.slice();return l[4]=t[n],l[6]=n,l}function Cn(a,t,n){const l=a.slice();return l[7]=t[n],l[9]=n,l}function Fn(a,t,n){const l=a.slice();return l[4]=t[n],l[6]=n,l}function On(a,t,n){const l=a.slice();return l[7]=t[n],l[9]=n,l}function Ln(a,t,n){const l=a.slice();return l[7]=t[n],l[6]=n,l}function Yn(a,t,n){const l=a.slice();return l[4]=t[n],l}function Hn(a,t,n){const l=a.slice();return l[4]=t[n],l}function Bn(a,t,n){const l=a.slice();return l[4]=t[n],l[6]=n,l}function $s(a,t,n){const l=a.slice();return l[7]=t[n],l[6]=n,l}function Mn(a,t,n){const l=a.slice();return l[4]=t[n],l}function Kn(a,t,n){const l=a.slice();return l[4]=t[n],l}function Qn(a,t,n){const l=a.slice();return l[4]=t[n],l}function Un(a,t,n){const l=a.slice();return l[4]=t[n],l}function Gn(a,t,n){const l=a.slice();return l[4]=t[n],l}function Jn(a,t,n){const l=a.slice();return l[4]=t[n],l}function Rn(a,t,n){const l=a.slice();return l[4]=t[n],l}function ys(a){let t;return{c(){t=$("text needs to be vectorized")},l(n){t=y(n,"text needs to be vectorized")},m(n,l){f(n,t,l)},d(n){n&&s(t)}}}function ks(a){let t,n,l,r;return n=new Xl({props:{$$slots:{default:[ys]},$$scope:{ctx:a}}}),{c(){t=$(`A neural network takes only numerical values as input, while text is
    represented as a sequence of characters or words. In order to make text
    compatible with nearal networks, it needs to be transformed into a numerical
    representation. In other words: `),K(n.$$.fragment),l=$(".")},l(i){t=y(i,`A neural network takes only numerical values as input, while text is
    represented as a sequence of characters or words. In order to make text
    compatible with nearal networks, it needs to be transformed into a numerical
    representation. In other words: `),Q(n.$$.fragment,i),l=y(i,".")},m(i,g){f(i,t,g),U(n,i,g),f(i,l,g),r=!0},p(i,g){const P={};g[1]&16&&(P.$$scope={dirty:g,ctx:i}),n.$set(P)},i(i){r||(G(n.$$.fragment,i),r=!0)},o(i){J(n.$$.fragment,i),r=!1},d(i){i&&s(t),R(n,i),i&&s(l)}}}function Xn(a){let t,n=a[4]+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-violet-100 text-center p-1 m-1 w-32 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function Zn(a){let t,n=a[4]+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-red-100 text-center p-1 m-1 w-32 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function es(a){let t,n=a[4].toLowerCase().replace(/[.]/g,"")+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-violet-100 text-center p-1 m-1 w-32 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function ts(a){let t,n=a[4].toLowerCase().replace(/[.]/g,"")+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-red-100 text-center p-1 m-1 w-32 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function ls(a){let t,n=a[4]+"",l,r;return{c(){t=_("div"),l=$(n),r=x(),this.h()},l(i){t=v(i,"DIV",{class:!0});var g=b(t);l=y(g,n),r=E(g),g.forEach(s),this.h()},h(){I(t,"class","bg-slate-300 w-32 text-center border border-gray-400 mb-1")},m(i,g){f(i,t,g),h(t,l),h(t,r)},p(i,g){g[0]&1&&n!==(n=i[4]+"")&&be(l,n)},d(i){i&&s(t)}}}function ns(a){let t,n=a[4].toLowerCase().replace(/[.]/g,"").replace(/^iii$/g,"<unk>")+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-violet-100 text-center p-1 m-1 w-32 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function ss(a){let t,n=a[4].toLowerCase().replace(/[.]/g,"").replace(/^ii$/g,"<unk>")+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-red-100 text-center p-1 m-1 w-32 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function xs(a){let t,n="<pad>",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-red-100 text-center p-1 m-1 w-32 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function os(a){let t,n,l=a[4]+"",r,i,g,P,w;return{c(){t=_("div"),n=_("div"),r=$(l),i=x(),g=_("div"),P=$(a[6]),w=x(),this.h()},l(d){t=v(d,"DIV",{class:!0});var m=b(t);n=v(m,"DIV",{class:!0});var u=b(n);r=y(u,l),u.forEach(s),i=E(m),g=v(m,"DIV",{class:!0});var j=b(g);P=y(j,a[6]),j.forEach(s),w=E(m),m.forEach(s),this.h()},h(){I(n,"class","bg-slate-300 w-28 py-1 "),I(g,"class","bg-yellow-200 w-10 flex justify-center items-center"),I(t,"class","flex text-center mb-1 border border-gray-300")},m(d,m){f(d,t,m),h(t,n),h(n,r),h(t,i),h(t,g),h(g,P),h(t,w)},p(d,m){m[0]&1&&l!==(l=d[4]+"")&&be(r,l)},d(d){d&&s(t)}}}function rs(a){let t,n=a[1][a[4]]+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-violet-100 text-center p-1 m-1 w-14 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p(r,i){i[0]&2&&n!==(n=r[1][r[4]]+"")&&be(l,n)},d(r){r&&s(t)}}}function is(a){let t,n=a[1][a[4]]+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-red-100 text-center p-1 m-1 w-14 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p(r,i){i[0]&2&&n!==(n=r[1][r[4]]+"")&&be(l,n)},d(r){r&&s(t)}}}function as(a){let t,n=a[1]["<pad>"]+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-red-100 text-center p-1 m-1 w-14 flex justify-center border border-gray-300")},m(r,i){f(r,t,i),h(t,l)},p(r,i){i[0]&2&&n!==(n=r[1]["<pad>"]+"")&&be(l,n)},d(r){r&&s(t)}}}function Es(a){let t;return{c(){t=$("one-hot vectors")},l(n){t=y(n,"one-hot vectors")},m(n,l){f(n,t,l)},d(n){n&&s(t)}}}function Is(a){let t,n;return{c(){t=_("span"),n=$("0"),this.h()},l(l){t=v(l,"SPAN",{class:!0});var r=b(t);n=y(r,"0"),r.forEach(s),this.h()},h(){I(t,"class","bg-blue-100 w-4 flex justify-center items-center")},m(l,r){f(l,t,r),h(t,n)},d(l){l&&s(t)}}}function zs(a){let t,n;return{c(){t=_("span"),n=$("1"),this.h()},l(l){t=v(l,"SPAN",{class:!0});var r=b(t);n=y(r,"1"),r.forEach(s),this.h()},h(){I(t,"class","bg-red-100 w-4 flex justify-center items-center")},m(l,r){f(l,t,r),h(t,n)},d(l){l&&s(t)}}}function cs(a){let t;function n(i,g){return i[9]===i[1][i[4]]?zs:Is}let l=n(a),r=l(a);return{c(){r.c(),t=_l()},l(i){r.l(i),t=_l()},m(i,g){r.m(i,g),f(i,t,g)},p(i,g){l!==(l=n(i))&&(r.d(1),r=l(i),r&&(r.c(),r.m(t.parentNode,t)))},d(i){r.d(i),i&&s(t)}}}function fs(a){let t,n,l=a[1][a[4]]+"",r,i,g,P=Array(a[0].length),w=[];for(let d=0;d<P.length;d+=1)w[d]=cs(On(a,P,d));return{c(){t=_("div"),n=_("span"),r=$(l),i=x();for(let d=0;d<w.length;d+=1)w[d].c();g=x(),this.h()},l(d){t=v(d,"DIV",{class:!0});var m=b(t);n=v(m,"SPAN",{class:!0});var u=b(n);r=y(u,l),u.forEach(s),i=E(m);for(let j=0;j<w.length;j+=1)w[j].l(m);g=E(m),m.forEach(s),this.h()},h(){I(n,"class","bg-slate-100 w-6 flex justify-center items-center"),I(t,"class","flex justify-center items-center")},m(d,m){f(d,t,m),h(t,n),h(n,r),h(t,i);for(let u=0;u<w.length;u+=1)w[u]&&w[u].m(t,null);h(t,g)},p(d,m){if(m[0]&3&&l!==(l=d[1][d[4]]+"")&&be(r,l),m[0]&3){P=Array(d[0].length);let u;for(u=0;u<P.length;u+=1){const j=On(d,P,u);w[u]?w[u].p(j,m):(w[u]=cs(j),w[u].c(),w[u].m(t,g))}for(;u<w.length;u+=1)w[u].d(1);w.length=P.length}},d(d){d&&s(t),A(w,d)}}}function Ps(a){let t,n;return{c(){t=_("span"),n=$("0"),this.h()},l(l){t=v(l,"SPAN",{class:!0});var r=b(t);n=y(r,"0"),r.forEach(s),this.h()},h(){I(t,"class","bg-blue-100 w-4 flex justify-center items-center")},m(l,r){f(l,t,r),h(t,n)},d(l){l&&s(t)}}}function js(a){let t,n;return{c(){t=_("span"),n=$("1"),this.h()},l(l){t=v(l,"SPAN",{class:!0});var r=b(t);n=y(r,"1"),r.forEach(s),this.h()},h(){I(t,"class","bg-red-100 w-4 flex justify-center items-center")},m(l,r){f(l,t,r),h(t,n)},d(l){l&&s(t)}}}function hs(a){let t;function n(i,g){return i[9]===i[1][i[4]]?js:Ps}let l=n(a),r=l(a);return{c(){r.c(),t=_l()},l(i){r.l(i),t=_l()},m(i,g){r.m(i,g),f(i,t,g)},p(i,g){l!==(l=n(i))&&(r.d(1),r=l(i),r&&(r.c(),r.m(t.parentNode,t)))},d(i){r.d(i),i&&s(t)}}}function us(a){let t,n,l=a[1][a[4]]+"",r,i,g,P=Array(a[0].length),w=[];for(let d=0;d<P.length;d+=1)w[d]=hs(Cn(a,P,d));return{c(){t=_("div"),n=_("span"),r=$(l),i=x();for(let d=0;d<w.length;d+=1)w[d].c();g=x(),this.h()},l(d){t=v(d,"DIV",{class:!0});var m=b(t);n=v(m,"SPAN",{class:!0});var u=b(n);r=y(u,l),u.forEach(s),i=E(m);for(let j=0;j<w.length;j+=1)w[j].l(m);g=E(m),m.forEach(s),this.h()},h(){I(n,"class","bg-slate-100 w-6 flex justify-center items-center"),I(t,"class","flex justify-center items-center")},m(d,m){f(d,t,m),h(t,n),h(n,r),h(t,i);for(let u=0;u<w.length;u+=1)w[u]&&w[u].m(t,null);h(t,g)},p(d,m){if(m[0]&2&&l!==(l=d[1][d[4]]+"")&&be(r,l),m[0]&7){P=Array(d[0].length);let u;for(u=0;u<P.length;u+=1){const j=Cn(d,P,u);w[u]?w[u].p(j,m):(w[u]=hs(j),w[u].c(),w[u].m(t,g))}for(;u<w.length;u+=1)w[u].d(1);w.length=P.length}},d(d){d&&s(t),A(w,d)}}}function As(a){let t;return{c(){t=$("embeddings")},l(n){t=y(n,"embeddings")},m(n,l){f(n,t,l)},d(n){n&&s(t)}}}function ds(a){let t,n=Math.random().toFixed(2)+"",l;return{c(){t=_("span"),l=$(n),this.h()},l(r){t=v(r,"SPAN",{class:!0});var i=b(t);l=y(i,n),i.forEach(s),this.h()},h(){I(t,"class","bg-red-100 w-10 flex justify-center items-center border border-b-gray-300")},m(r,i){f(r,t,i),h(t,l)},p:ee,d(r){r&&s(t)}}}function ps(a){let t,n,l=a[1][a[4]]+"",r,i,g,P=Array(4),w=[];for(let d=0;d<P.length;d+=1)w[d]=ds(qn(a,P,d));return{c(){t=_("div"),n=_("span"),r=$(l),i=x();for(let d=0;d<w.length;d+=1)w[d].c();g=x(),this.h()},l(d){t=v(d,"DIV",{class:!0});var m=b(t);n=v(m,"SPAN",{class:!0});var u=b(n);r=y(u,l),u.forEach(s),i=E(m);for(let j=0;j<w.length;j+=1)w[j].l(m);g=E(m),m.forEach(s),this.h()},h(){I(n,"class","bg-slate-100 w-6 flex justify-center items-center"),I(t,"class","flex justify-center items-center mb-1")},m(d,m){f(d,t,m),h(t,n),h(n,r),h(t,i);for(let u=0;u<w.length;u+=1)w[u]&&w[u].m(t,null);h(t,g)},p(d,m){if(m[0]&3&&l!==(l=d[1][d[4]]+"")&&be(r,l),m&0){P=Array(4);let u;for(u=0;u<P.length;u+=1){const j=qn(d,P,u);w[u]?w[u].p(j,m):(w[u]=ds(),w[u].c(),w[u].m(t,g))}for(;u<w.length;u+=1)w[u].d(1);w.length=P.length}},d(d){d&&s(t),A(w,d)}}}function Ds(a){let t=String.raw`
    \begin{bmatrix}
      w_1 & w_2 \\ 
      w_3 & w_4 \\ 
      w_5 & w_6 \\ 
      w_7 & w_8 \\ 
      w_9 & w_{10} \\ 
    \end{bmatrix}
  `+"",n;return{c(){n=$(t)},l(l){n=y(l,t)},m(l,r){f(l,n,r)},p:ee,d(l){l&&s(n)}}}function Vs(a){let t=String.raw`
    \begin{bmatrix}
      1 & 0 & 0 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
      w_1 & w_2 \\ 
      w_3 & w_4 \\ 
      w_5 & w_6 \\ 
      w_7 & w_8 \\ 
      w_9 & w_{10} \\ 
    \end{bmatrix}
  `+"",n;return{c(){n=$(t)},l(l){n=y(l,t)},m(l,r){f(l,n,r)},p:ee,d(l){l&&s(n)}}}function Ss(a){let t,n,l,r,i,g,P,w,d,m,u,j,vl,yt,Oe,wl,kt,te,ge,bl,$e,xt,Le,gl,Et,le,ye,$l,ke,It,Ye,yl,zt,ce,Pt,M,kl,Zl="<pad>",xl,El,en="<unk>",Il,zl,tn="<pad>",Pl,jl,ln="<unk>",Al,Dl,jt,ne,xe,Vl,se,At,Dt,He,Sl,Vt,fe,St,Be,Tl,Tt,oe,Ee,Nl,X,Nt,qt,he,ql,ue,Wl,Wt,Ie,Me,Ct,Ke,Cl,Ft,ze,Qe,Ot,Ue,Fl,Lt,de,Ol,pe,Ll,Yt,Ge,Yl,Ht,Pe,Je,Bt,Re,Hl,Mt,je,me,Kt,Xe,Bl,Qt,Ae,_e,Ut,Ze,Ml,Gt,et,Kl,Jt,ve,Ql,we,wt,Ul,Gl,Rt,De,Xt,tt,Jl,Zt,Ve,el,Se,tl,lt,Rl,ll,Te,nl,Ne,sl,qe,ol,We,rl,nt,il;r=new gs({props:{type:"info",$$slots:{default:[ks]},$$scope:{ctx:a}}});let st=ie.split(" "),D=[];for(let e=0;e<st.length;e+=1)D[e]=Xn(Rn(a,st,e));let ot=ae.split(" "),V=[];for(let e=0;e<ot.length;e+=1)V[e]=Zn(Jn(a,ot,e));let rt=ie.split(" "),S=[];for(let e=0;e<rt.length;e+=1)S[e]=es(Gn(a,rt,e));let it=ae.split(" "),T=[];for(let e=0;e<it.length;e+=1)T[e]=ts(Un(a,it,e));let at=a[0],N=[];for(let e=0;e<at.length;e+=1)N[e]=ls(Qn(a,at,e));let ct=ie.split(" "),q=[];for(let e=0;e<ct.length;e+=1)q[e]=ns(Kn(a,ct,e));let ft=ae.split(" "),W=[];for(let e=0;e<ft.length;e+=1)W[e]=ss(Mn(a,ft,e));let nn=Array(2),re=[];for(let e=0;e<nn.length;e+=1)re[e]=xs($s(a,nn,e));let ht=a[0],C=[];for(let e=0;e<ht.length;e+=1)C[e]=os(Bn(a,ht,e));let ut=a[2].split(" "),F=[];for(let e=0;e<ut.length;e+=1)F[e]=rs(Hn(a,ut,e));let dt=a[3].split(" "),O=[];for(let e=0;e<dt.length;e+=1)O[e]=is(Yn(a,dt,e));let pt=Array(2),L=[];for(let e=0;e<pt.length;e+=1)L[e]=as(Ln(a,pt,e));ue=new Xl({props:{$$slots:{default:[Es]},$$scope:{ctx:a}}});let mt=a[0],Y=[];for(let e=0;e<mt.length;e+=1)Y[e]=fs(Fn(a,mt,e));let _t=a[2].split(" "),H=[];for(let e=0;e<_t.length;e+=1)H[e]=us(Wn(a,_t,e));pe=new Xl({props:{$$slots:{default:[As]},$$scope:{ctx:a}}});let vt=a[0],B=[];for(let e=0;e<vt.length;e+=1)B[e]=ps(Nn(a,vt,e));return me=new Tn({props:{$$slots:{default:[Ds]},$$scope:{ctx:a}}}),_e=new Tn({props:{$$slots:{default:[Vs]},$$scope:{ctx:a}}}),De=new Fe({props:{code:`vocabulary_size = 10
embedding_dim = 4
batch_size=5
seq_len=3`}}),Ve=new Fe({props:{code:`sequence = torch.randint(low=0, 
                         high=vocabulary_size, 
                         size=(batch_size, seq_len))
print(sequence.shape)`}}),Se=new Fe({props:{code:"torch.Size([5, 3])",isOutput:!0}}),Te=new Fe({props:{code:`embedding = nn.Embedding(num_embeddings=vocabulary_size, 
                         embedding_dim=embedding_dim)
print(embedding.weight.shape)`}}),Ne=new Fe({props:{code:"torch.Size([10, 4])",isOutput:!0}}),qe=new Fe({props:{code:`embeddings = embedding(sequence)
print(embeddings.shape)`}}),We=new Fe({props:{code:"torch.Size([5, 3, 4])",isOutput:!0}}),{c(){t=_("p"),n=$(`In the previous sections we have learned how we can use recurrent neural
    networks to deal with sequences. Yet we still face a problem that we need to
    solve, before we can train those models on textual data.`),l=x(),K(r.$$.fragment),i=x(),g=_("p"),P=$(`In order to get a good intuition for the vectorization process let's work
    through a dummy example. The whole time we will assume that our dataset
    consists of these two sentences.`),w=x(),d=_("p"),m=$(ie),u=x(),j=_("p"),vl=$(ae),yt=x(),Oe=_("p"),wl=$(`In the first step of the transformation process we need to tokenize our
    sentences. During the tokenization process we divide the sentence into its
    atomic parts, so called tokens. While theoretically we can divide a sentence
    into a set of characters (like letters), usually a sentence is divided into
    individual words (or subwords). Tokenization can be a daunting task, so we
    will stick to the basics here.`),kt=x(),te=_("div"),ge=_("div");for(let e=0;e<D.length;e+=1)D[e].c();bl=x(),$e=_("div");for(let e=0;e<V.length;e+=1)V[e].c();xt=x(),Le=_("p"),gl=$(`During tokenization, the words are often also standardized, by stripping
    punctuation and turning letters into their lower case counterparts.`),Et=x(),le=_("div"),ye=_("div");for(let e=0;e<S.length;e+=1)S[e].c();$l=x(),ke=_("div");for(let e=0;e<T.length;e+=1)T[e].c();It=x(),Ye=_("p"),yl=$(`Once we have tokenized all words, we can create a vocabulary. A vocabulary
    is the set of all available tokens. For the sentences above we will end up
    with the following vocabulary.`),zt=x(),ce=_("div");for(let e=0;e<N.length;e+=1)N[e].c();Pt=x(),M=_("p"),kl=$(`You have probably noticed that additionally to the tokens we have derived
    from our dataset we have also introduced `),xl=$(Zl),El=$(" and "),Il=$(en),zl=$(`. For the
    most part the size of the sentences is going to be of different size, but if
    we want to use batches of samples, we need to standardize the length of the
    sequence. For that purpose we use padding, which means that we fill the
    shorter sentences with `),Pl=$(tn),jl=$(" tokens. The token for unknown words "),Al=$(ln),Dl=$(`
    is used for words that are outside of the vocabulary. This happens for example
    if the vocabulary that is built using the training dataset does not contain some
    words from the testing dataset. Additionally we often limit the size of the vocabulary
    in order to save computational power. In our example we assume that roman numerals
    are extremely rare and replace them by the special tokens.`),jt=x(),ne=_("div"),xe=_("div");for(let e=0;e<q.length;e+=1)q[e].c();Vl=x(),se=_("div");for(let e=0;e<W.length;e+=1)W[e].c();At=x();for(let e=0;e<re.length;e+=1)re[e].c();Dt=x(),He=_("p"),Sl=$("In the next step each token in the vocabulary gets assigned an index."),Vt=x(),fe=_("div");for(let e=0;e<C.length;e+=1)C[e].c();St=x(),Be=_("p"),Tl=$("Next we replace all tokens in the sentence by the corresponding index."),Tt=x(),oe=_("div"),Ee=_("div");for(let e=0;e<F.length;e+=1)F[e].c();Nl=x(),X=_("div");for(let e=0;e<O.length;e+=1)O[e].c();Nt=x();for(let e=0;e<L.length;e+=1)L[e].c();qt=x(),he=_("p"),ql=$(`Theoretically we have already accomplished the task of turning words into
    numerical values, but using indices as input inot the neural netowrk is
    problematic, because those indices imply that there is a ranking in the
    words. So the word with the index 2 is somehow higher than the word with the
    index 1. Instead we create so called `),K(ue.$$.fragment),Wl=$(`.
    These vectors have as many dimensions, as there are tokens in the
    vocabulary. For the most part the vector consists of zeros, but at the index
    that corresponds to the word in the vocabulary the value is 1. For our
    vocabulary of size 15, so we have access to 15 one-hot vectors.`),Wt=x(),Ie=_("div"),Me=_("div");for(let e=0;e<Y.length;e+=1)Y[e].c();Ct=x(),Ke=_("p"),Cl=$(`Our first sentence for example would correspond to a sequence of the
    following one-hot vectors.`),Ft=x(),ze=_("div"),Qe=_("div");for(let e=0;e<H.length;e+=1)H[e].c();Ot=x(),Ue=_("p"),Fl=$(`While we have managed to turn our sentences into vectors, this is not the
    final step. One-hot vectors are problematic, because the dimensionaly of
    vectors growth with the size of the vocabulary. We might deal with a
    vocabulary of 30,000 words, which will produce vectors of size 30,000. If we
    input those vectors directly into a recurrent neural network, the
    computation will become intractable.`),Lt=x(),de=_("p"),Ol=$(`Instead we first turn the one-hot representation into a dense representation
    of lower dimensionality, those vectors are called `),K(pe.$$.fragment),Ll=$("."),Yt=x(),Ge=_("p"),Yl=$(`For example those embeddings might look like the following vecors. We turn
    15-dimensional sparse vectors into 4-dimensional dense vectors.`),Ht=x(),Pe=_("div"),Je=_("div");for(let e=0;e<B.length;e+=1)B[e].c();Bt=x(),Re=_("p"),Hl=$(`Theoretically such a word embedding matrix can be trained using a fully
    connected layer. Assuming that we have a 5-dimensional one-hot vector and
    that we want to turn it into a 2-dimensional word embedding, we define the
    word embedding matrix as trainable weights of the corresponding size.`),Mt=x(),je=_("div"),K(me.$$.fragment),Kt=x(),Xe=_("p"),Bl=$(`When we want to obtain the embedding for a corresponding one-hot vector, we
    multiply the two.`),Qt=x(),Ae=_("div"),K(_e.$$.fragment),Ut=x(),Ze=_("p"),Ml=$(`The multiplication will select the correct row and result in a 2-dimensional
    vector, that can be used as an input into our sequence-language model. This
    operation will be tracked by the autograd package and those weights will
    update over the time, optimizing the embedding representation.`),Gt=x(),et=_("p"),Kl=$(`In practice all major frameworks have a dedicated embedding layer, that does
    this operation via a lookup. Instead of actually using matrix
    multiplication, this layer takes the value from the embedding, that
    corresponds to the index of the word. This is just a more efficient
    approach, but the results of the computation should be the same.`),Jt=x(),ve=_("p"),Ql=$("The "),we=_("a"),wt=_("code"),Ul=$("nn.Embedding"),Gl=$(` layer from PyTorch has two positional arguments: the first corresponds to the
    size of the vocabulary and the second corresponds to the dimension of the embedding
    vector.`),Rt=x(),K(De.$$.fragment),Xt=x(),tt=_("p"),Jl=$("We assume that we have 5 sentences, each consisting of 3 words."),Zt=x(),K(Ve.$$.fragment),el=x(),K(Se.$$.fragment),tl=x(),lt=_("p"),Rl=$(`The embedding maps directly from one of ten indices to the 4 dimensional
    embedding and there is no need to create one-hot encodings in PyTorch.`),ll=x(),K(Te.$$.fragment),nl=x(),K(Ne.$$.fragment),sl=x(),K(qe.$$.fragment),ol=x(),K(We.$$.fragment),rl=x(),nt=_("div"),this.h()},l(e){t=v(e,"P",{});var c=b(t);n=y(c,`In the previous sections we have learned how we can use recurrent neural
    networks to deal with sequences. Yet we still face a problem that we need to
    solve, before we can train those models on textual data.`),c.forEach(s),l=E(e),Q(r.$$.fragment,e),i=E(e),g=v(e,"P",{});var p=b(g);P=y(p,`In order to get a good intuition for the vectorization process let's work
    through a dummy example. The whole time we will assume that our dataset
    consists of these two sentences.`),p.forEach(s),w=E(e),d=v(e,"P",{class:!0});var bt=b(d);m=y(bt,ie),bt.forEach(s),u=E(e),j=v(e,"P",{class:!0});var gt=b(j);vl=y(gt,ae),gt.forEach(s),yt=E(e),Oe=v(e,"P",{});var $t=b(Oe);wl=y($t,`In the first step of the transformation process we need to tokenize our
    sentences. During the tokenization process we divide the sentence into its
    atomic parts, so called tokens. While theoretically we can divide a sentence
    into a set of characters (like letters), usually a sentence is divided into
    individual words (or subwords). Tokenization can be a daunting task, so we
    will stick to the basics here.`),$t.forEach(s),kt=E(e),te=v(e,"DIV",{class:!0});var Ce=b(te);ge=v(Ce,"DIV",{class:!0});var o=b(ge);for(let k=0;k<D.length;k+=1)D[k].l(o);o.forEach(s),bl=E(Ce),$e=v(Ce,"DIV",{class:!0});var z=b($e);for(let k=0;k<V.length;k+=1)V[k].l(z);z.forEach(s),Ce.forEach(s),xt=E(e),Le=v(e,"P",{});var sn=b(Le);gl=y(sn,`During tokenization, the words are often also standardized, by stripping
    punctuation and turning letters into their lower case counterparts.`),sn.forEach(s),Et=E(e),le=v(e,"DIV",{class:!0});var al=b(le);ye=v(al,"DIV",{class:!0});var on=b(ye);for(let k=0;k<S.length;k+=1)S[k].l(on);on.forEach(s),$l=E(al),ke=v(al,"DIV",{class:!0});var rn=b(ke);for(let k=0;k<T.length;k+=1)T[k].l(rn);rn.forEach(s),al.forEach(s),It=E(e),Ye=v(e,"P",{});var an=b(Ye);yl=y(an,`Once we have tokenized all words, we can create a vocabulary. A vocabulary
    is the set of all available tokens. For the sentences above we will end up
    with the following vocabulary.`),an.forEach(s),zt=E(e),ce=v(e,"DIV",{class:!0});var cn=b(ce);for(let k=0;k<N.length;k+=1)N[k].l(cn);cn.forEach(s),Pt=E(e),M=v(e,"P",{});var Z=b(M);kl=y(Z,`You have probably noticed that additionally to the tokens we have derived
    from our dataset we have also introduced `),xl=y(Z,Zl),El=y(Z," and "),Il=y(Z,en),zl=y(Z,`. For the
    most part the size of the sentences is going to be of different size, but if
    we want to use batches of samples, we need to standardize the length of the
    sequence. For that purpose we use padding, which means that we fill the
    shorter sentences with `),Pl=y(Z,tn),jl=y(Z," tokens. The token for unknown words "),Al=y(Z,ln),Dl=y(Z,`
    is used for words that are outside of the vocabulary. This happens for example
    if the vocabulary that is built using the training dataset does not contain some
    words from the testing dataset. Additionally we often limit the size of the vocabulary
    in order to save computational power. In our example we assume that roman numerals
    are extremely rare and replace them by the special tokens.`),Z.forEach(s),jt=E(e),ne=v(e,"DIV",{class:!0});var cl=b(ne);xe=v(cl,"DIV",{class:!0});var fn=b(xe);for(let k=0;k<q.length;k+=1)q[k].l(fn);fn.forEach(s),Vl=E(cl),se=v(cl,"DIV",{class:!0});var fl=b(se);for(let k=0;k<W.length;k+=1)W[k].l(fl);At=E(fl);for(let k=0;k<re.length;k+=1)re[k].l(fl);fl.forEach(s),cl.forEach(s),Dt=E(e),He=v(e,"P",{});var hn=b(He);Sl=y(hn,"In the next step each token in the vocabulary gets assigned an index."),hn.forEach(s),Vt=E(e),fe=v(e,"DIV",{class:!0});var un=b(fe);for(let k=0;k<C.length;k+=1)C[k].l(un);un.forEach(s),St=E(e),Be=v(e,"P",{});var dn=b(Be);Tl=y(dn,"Next we replace all tokens in the sentence by the corresponding index."),dn.forEach(s),Tt=E(e),oe=v(e,"DIV",{class:!0});var hl=b(oe);Ee=v(hl,"DIV",{class:!0});var pn=b(Ee);for(let k=0;k<F.length;k+=1)F[k].l(pn);pn.forEach(s),Nl=E(hl),X=v(hl,"DIV",{class:!0});var ul=b(X);for(let k=0;k<O.length;k+=1)O[k].l(ul);Nt=E(ul);for(let k=0;k<L.length;k+=1)L[k].l(ul);ul.forEach(s),hl.forEach(s),qt=E(e),he=v(e,"P",{});var dl=b(he);ql=y(dl,`Theoretically we have already accomplished the task of turning words into
    numerical values, but using indices as input inot the neural netowrk is
    problematic, because those indices imply that there is a ranking in the
    words. So the word with the index 2 is somehow higher than the word with the
    index 1. Instead we create so called `),Q(ue.$$.fragment,dl),Wl=y(dl,`.
    These vectors have as many dimensions, as there are tokens in the
    vocabulary. For the most part the vector consists of zeros, but at the index
    that corresponds to the word in the vocabulary the value is 1. For our
    vocabulary of size 15, so we have access to 15 one-hot vectors.`),dl.forEach(s),Wt=E(e),Ie=v(e,"DIV",{class:!0});var mn=b(Ie);Me=v(mn,"DIV",{});var _n=b(Me);for(let k=0;k<Y.length;k+=1)Y[k].l(_n);_n.forEach(s),mn.forEach(s),Ct=E(e),Ke=v(e,"P",{});var vn=b(Ke);Cl=y(vn,`Our first sentence for example would correspond to a sequence of the
    following one-hot vectors.`),vn.forEach(s),Ft=E(e),ze=v(e,"DIV",{class:!0});var wn=b(ze);Qe=v(wn,"DIV",{});var bn=b(Qe);for(let k=0;k<H.length;k+=1)H[k].l(bn);bn.forEach(s),wn.forEach(s),Ot=E(e),Ue=v(e,"P",{});var gn=b(Ue);Fl=y(gn,`While we have managed to turn our sentences into vectors, this is not the
    final step. One-hot vectors are problematic, because the dimensionaly of
    vectors growth with the size of the vocabulary. We might deal with a
    vocabulary of 30,000 words, which will produce vectors of size 30,000. If we
    input those vectors directly into a recurrent neural network, the
    computation will become intractable.`),gn.forEach(s),Lt=E(e),de=v(e,"P",{});var pl=b(de);Ol=y(pl,`Instead we first turn the one-hot representation into a dense representation
    of lower dimensionality, those vectors are called `),Q(pe.$$.fragment,pl),Ll=y(pl,"."),pl.forEach(s),Yt=E(e),Ge=v(e,"P",{});var $n=b(Ge);Yl=y($n,`For example those embeddings might look like the following vecors. We turn
    15-dimensional sparse vectors into 4-dimensional dense vectors.`),$n.forEach(s),Ht=E(e),Pe=v(e,"DIV",{class:!0});var yn=b(Pe);Je=v(yn,"DIV",{});var kn=b(Je);for(let k=0;k<B.length;k+=1)B[k].l(kn);kn.forEach(s),yn.forEach(s),Bt=E(e),Re=v(e,"P",{});var xn=b(Re);Hl=y(xn,`Theoretically such a word embedding matrix can be trained using a fully
    connected layer. Assuming that we have a 5-dimensional one-hot vector and
    that we want to turn it into a 2-dimensional word embedding, we define the
    word embedding matrix as trainable weights of the corresponding size.`),xn.forEach(s),Mt=E(e),je=v(e,"DIV",{class:!0});var En=b(je);Q(me.$$.fragment,En),En.forEach(s),Kt=E(e),Xe=v(e,"P",{});var In=b(Xe);Bl=y(In,`When we want to obtain the embedding for a corresponding one-hot vector, we
    multiply the two.`),In.forEach(s),Qt=E(e),Ae=v(e,"DIV",{class:!0});var zn=b(Ae);Q(_e.$$.fragment,zn),zn.forEach(s),Ut=E(e),Ze=v(e,"P",{});var Pn=b(Ze);Ml=y(Pn,`The multiplication will select the correct row and result in a 2-dimensional
    vector, that can be used as an input into our sequence-language model. This
    operation will be tracked by the autograd package and those weights will
    update over the time, optimizing the embedding representation.`),Pn.forEach(s),Gt=E(e),et=v(e,"P",{});var jn=b(et);Kl=y(jn,`In practice all major frameworks have a dedicated embedding layer, that does
    this operation via a lookup. Instead of actually using matrix
    multiplication, this layer takes the value from the embedding, that
    corresponds to the index of the word. This is just a more efficient
    approach, but the results of the computation should be the same.`),jn.forEach(s),Jt=E(e),ve=v(e,"P",{});var ml=b(ve);Ql=y(ml,"The "),we=v(ml,"A",{href:!0,target:!0,rel:!0});var An=b(we);wt=v(An,"CODE",{});var Dn=b(wt);Ul=y(Dn,"nn.Embedding"),Dn.forEach(s),An.forEach(s),Gl=y(ml,` layer from PyTorch has two positional arguments: the first corresponds to the
    size of the vocabulary and the second corresponds to the dimension of the embedding
    vector.`),ml.forEach(s),Rt=E(e),Q(De.$$.fragment,e),Xt=E(e),tt=v(e,"P",{});var Vn=b(tt);Jl=y(Vn,"We assume that we have 5 sentences, each consisting of 3 words."),Vn.forEach(s),Zt=E(e),Q(Ve.$$.fragment,e),el=E(e),Q(Se.$$.fragment,e),tl=E(e),lt=v(e,"P",{});var Sn=b(lt);Rl=y(Sn,`The embedding maps directly from one of ten indices to the 4 dimensional
    embedding and there is no need to create one-hot encodings in PyTorch.`),Sn.forEach(s),ll=E(e),Q(Te.$$.fragment,e),nl=E(e),Q(Ne.$$.fragment,e),sl=E(e),Q(qe.$$.fragment,e),ol=E(e),Q(We.$$.fragment,e),rl=E(e),nt=v(e,"DIV",{class:!0}),b(nt).forEach(s),this.h()},h(){I(d,"class","bg-violet-100 text-center p-1 rounded-xl"),I(j,"class","bg-red-100 text-center p-1 rounded-xl"),I(ge,"class","flex justify-center items-center flex-col "),I($e,"class","flex justify-center items-center flex-col"),I(te,"class","flex justify-center items-start mb-2"),I(ye,"class","flex justify-center items-center flex-col mb-2"),I(ke,"class","flex justify-center items-center flex-col"),I(le,"class","flex justify-center items-start mb-2"),I(ce,"class","flex justify-center items-center flex-col"),I(xe,"class","flex justify-center items-center flex-col mb-2"),I(se,"class","flex justify-center items-center flex-col mb-2"),I(ne,"class","flex justify-center items-start mb-2"),I(fe,"class","flex justify-center items-center flex-col"),I(Ee,"class","flex justify-center items-center flex-col mb-2"),I(X,"class","flex justify-center items-center flex-col mb-2"),I(oe,"class","flex justify-center items-start mb-2"),I(Ie,"class","flex justify-center items-center"),I(ze,"class","flex justify-center items-center"),I(Pe,"class","flex justify-center items-center"),I(je,"class","flex justify-center"),I(Ae,"class","flex justify-center"),I(we,"href","https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"),I(we,"target","_blank"),I(we,"rel","noreferrer"),I(nt,"class","separator")},m(e,c){f(e,t,c),h(t,n),f(e,l,c),U(r,e,c),f(e,i,c),f(e,g,c),h(g,P),f(e,w,c),f(e,d,c),h(d,m),f(e,u,c),f(e,j,c),h(j,vl),f(e,yt,c),f(e,Oe,c),h(Oe,wl),f(e,kt,c),f(e,te,c),h(te,ge);for(let p=0;p<D.length;p+=1)D[p]&&D[p].m(ge,null);h(te,bl),h(te,$e);for(let p=0;p<V.length;p+=1)V[p]&&V[p].m($e,null);f(e,xt,c),f(e,Le,c),h(Le,gl),f(e,Et,c),f(e,le,c),h(le,ye);for(let p=0;p<S.length;p+=1)S[p]&&S[p].m(ye,null);h(le,$l),h(le,ke);for(let p=0;p<T.length;p+=1)T[p]&&T[p].m(ke,null);f(e,It,c),f(e,Ye,c),h(Ye,yl),f(e,zt,c),f(e,ce,c);for(let p=0;p<N.length;p+=1)N[p]&&N[p].m(ce,null);f(e,Pt,c),f(e,M,c),h(M,kl),h(M,xl),h(M,El),h(M,Il),h(M,zl),h(M,Pl),h(M,jl),h(M,Al),h(M,Dl),f(e,jt,c),f(e,ne,c),h(ne,xe);for(let p=0;p<q.length;p+=1)q[p]&&q[p].m(xe,null);h(ne,Vl),h(ne,se);for(let p=0;p<W.length;p+=1)W[p]&&W[p].m(se,null);h(se,At);for(let p=0;p<re.length;p+=1)re[p]&&re[p].m(se,null);f(e,Dt,c),f(e,He,c),h(He,Sl),f(e,Vt,c),f(e,fe,c);for(let p=0;p<C.length;p+=1)C[p]&&C[p].m(fe,null);f(e,St,c),f(e,Be,c),h(Be,Tl),f(e,Tt,c),f(e,oe,c),h(oe,Ee);for(let p=0;p<F.length;p+=1)F[p]&&F[p].m(Ee,null);h(oe,Nl),h(oe,X);for(let p=0;p<O.length;p+=1)O[p]&&O[p].m(X,null);h(X,Nt);for(let p=0;p<L.length;p+=1)L[p]&&L[p].m(X,null);f(e,qt,c),f(e,he,c),h(he,ql),U(ue,he,null),h(he,Wl),f(e,Wt,c),f(e,Ie,c),h(Ie,Me);for(let p=0;p<Y.length;p+=1)Y[p]&&Y[p].m(Me,null);f(e,Ct,c),f(e,Ke,c),h(Ke,Cl),f(e,Ft,c),f(e,ze,c),h(ze,Qe);for(let p=0;p<H.length;p+=1)H[p]&&H[p].m(Qe,null);f(e,Ot,c),f(e,Ue,c),h(Ue,Fl),f(e,Lt,c),f(e,de,c),h(de,Ol),U(pe,de,null),h(de,Ll),f(e,Yt,c),f(e,Ge,c),h(Ge,Yl),f(e,Ht,c),f(e,Pe,c),h(Pe,Je);for(let p=0;p<B.length;p+=1)B[p]&&B[p].m(Je,null);f(e,Bt,c),f(e,Re,c),h(Re,Hl),f(e,Mt,c),f(e,je,c),U(me,je,null),f(e,Kt,c),f(e,Xe,c),h(Xe,Bl),f(e,Qt,c),f(e,Ae,c),U(_e,Ae,null),f(e,Ut,c),f(e,Ze,c),h(Ze,Ml),f(e,Gt,c),f(e,et,c),h(et,Kl),f(e,Jt,c),f(e,ve,c),h(ve,Ql),h(ve,we),h(we,wt),h(wt,Ul),h(ve,Gl),f(e,Rt,c),U(De,e,c),f(e,Xt,c),f(e,tt,c),h(tt,Jl),f(e,Zt,c),U(Ve,e,c),f(e,el,c),U(Se,e,c),f(e,tl,c),f(e,lt,c),h(lt,Rl),f(e,ll,c),U(Te,e,c),f(e,nl,c),U(Ne,e,c),f(e,sl,c),U(qe,e,c),f(e,ol,c),U(We,e,c),f(e,rl,c),f(e,nt,c),il=!0},p(e,c){const p={};if(c[1]&16&&(p.$$scope={dirty:c,ctx:e}),r.$set(p),c&0){st=ie.split(" ");let o;for(o=0;o<st.length;o+=1){const z=Rn(e,st,o);D[o]?D[o].p(z,c):(D[o]=Xn(z),D[o].c(),D[o].m(ge,null))}for(;o<D.length;o+=1)D[o].d(1);D.length=st.length}if(c&0){ot=ae.split(" ");let o;for(o=0;o<ot.length;o+=1){const z=Jn(e,ot,o);V[o]?V[o].p(z,c):(V[o]=Zn(z),V[o].c(),V[o].m($e,null))}for(;o<V.length;o+=1)V[o].d(1);V.length=ot.length}if(c&0){rt=ie.split(" ");let o;for(o=0;o<rt.length;o+=1){const z=Gn(e,rt,o);S[o]?S[o].p(z,c):(S[o]=es(z),S[o].c(),S[o].m(ye,null))}for(;o<S.length;o+=1)S[o].d(1);S.length=rt.length}if(c&0){it=ae.split(" ");let o;for(o=0;o<it.length;o+=1){const z=Un(e,it,o);T[o]?T[o].p(z,c):(T[o]=ts(z),T[o].c(),T[o].m(ke,null))}for(;o<T.length;o+=1)T[o].d(1);T.length=it.length}if(c[0]&1){at=e[0];let o;for(o=0;o<at.length;o+=1){const z=Qn(e,at,o);N[o]?N[o].p(z,c):(N[o]=ls(z),N[o].c(),N[o].m(ce,null))}for(;o<N.length;o+=1)N[o].d(1);N.length=at.length}if(c&0){ct=ie.split(" ");let o;for(o=0;o<ct.length;o+=1){const z=Kn(e,ct,o);q[o]?q[o].p(z,c):(q[o]=ns(z),q[o].c(),q[o].m(xe,null))}for(;o<q.length;o+=1)q[o].d(1);q.length=ct.length}if(c&0){ft=ae.split(" ");let o;for(o=0;o<ft.length;o+=1){const z=Mn(e,ft,o);W[o]?W[o].p(z,c):(W[o]=ss(z),W[o].c(),W[o].m(se,At))}for(;o<W.length;o+=1)W[o].d(1);W.length=ft.length}if(c[0]&1){ht=e[0];let o;for(o=0;o<ht.length;o+=1){const z=Bn(e,ht,o);C[o]?C[o].p(z,c):(C[o]=os(z),C[o].c(),C[o].m(fe,null))}for(;o<C.length;o+=1)C[o].d(1);C.length=ht.length}if(c[0]&6){ut=e[2].split(" ");let o;for(o=0;o<ut.length;o+=1){const z=Hn(e,ut,o);F[o]?F[o].p(z,c):(F[o]=rs(z),F[o].c(),F[o].m(Ee,null))}for(;o<F.length;o+=1)F[o].d(1);F.length=ut.length}if(c[0]&10){dt=e[3].split(" ");let o;for(o=0;o<dt.length;o+=1){const z=Yn(e,dt,o);O[o]?O[o].p(z,c):(O[o]=is(z),O[o].c(),O[o].m(X,Nt))}for(;o<O.length;o+=1)O[o].d(1);O.length=dt.length}if(c[0]&2){pt=Array(2);let o;for(o=0;o<pt.length;o+=1){const z=Ln(e,pt,o);L[o]?L[o].p(z,c):(L[o]=as(z),L[o].c(),L[o].m(X,null))}for(;o<L.length;o+=1)L[o].d(1);L.length=pt.length}const bt={};if(c[1]&16&&(bt.$$scope={dirty:c,ctx:e}),ue.$set(bt),c[0]&3){mt=e[0];let o;for(o=0;o<mt.length;o+=1){const z=Fn(e,mt,o);Y[o]?Y[o].p(z,c):(Y[o]=fs(z),Y[o].c(),Y[o].m(Me,null))}for(;o<Y.length;o+=1)Y[o].d(1);Y.length=mt.length}if(c[0]&7){_t=e[2].split(" ");let o;for(o=0;o<_t.length;o+=1){const z=Wn(e,_t,o);H[o]?H[o].p(z,c):(H[o]=us(z),H[o].c(),H[o].m(Qe,null))}for(;o<H.length;o+=1)H[o].d(1);H.length=_t.length}const gt={};if(c[1]&16&&(gt.$$scope={dirty:c,ctx:e}),pe.$set(gt),c[0]&3){vt=e[0];let o;for(o=0;o<vt.length;o+=1){const z=Nn(e,vt,o);B[o]?B[o].p(z,c):(B[o]=ps(z),B[o].c(),B[o].m(Je,null))}for(;o<B.length;o+=1)B[o].d(1);B.length=vt.length}const $t={};c[1]&16&&($t.$$scope={dirty:c,ctx:e}),me.$set($t);const Ce={};c[1]&16&&(Ce.$$scope={dirty:c,ctx:e}),_e.$set(Ce)},i(e){il||(G(r.$$.fragment,e),G(ue.$$.fragment,e),G(pe.$$.fragment,e),G(me.$$.fragment,e),G(_e.$$.fragment,e),G(De.$$.fragment,e),G(Ve.$$.fragment,e),G(Se.$$.fragment,e),G(Te.$$.fragment,e),G(Ne.$$.fragment,e),G(qe.$$.fragment,e),G(We.$$.fragment,e),il=!0)},o(e){J(r.$$.fragment,e),J(ue.$$.fragment,e),J(pe.$$.fragment,e),J(me.$$.fragment,e),J(_e.$$.fragment,e),J(De.$$.fragment,e),J(Ve.$$.fragment,e),J(Se.$$.fragment,e),J(Te.$$.fragment,e),J(Ne.$$.fragment,e),J(qe.$$.fragment,e),J(We.$$.fragment,e),il=!1},d(e){e&&s(t),e&&s(l),R(r,e),e&&s(i),e&&s(g),e&&s(w),e&&s(d),e&&s(u),e&&s(j),e&&s(yt),e&&s(Oe),e&&s(kt),e&&s(te),A(D,e),A(V,e),e&&s(xt),e&&s(Le),e&&s(Et),e&&s(le),A(S,e),A(T,e),e&&s(It),e&&s(Ye),e&&s(zt),e&&s(ce),A(N,e),e&&s(Pt),e&&s(M),e&&s(jt),e&&s(ne),A(q,e),A(W,e),A(re,e),e&&s(Dt),e&&s(He),e&&s(Vt),e&&s(fe),A(C,e),e&&s(St),e&&s(Be),e&&s(Tt),e&&s(oe),A(F,e),A(O,e),A(L,e),e&&s(qt),e&&s(he),R(ue),e&&s(Wt),e&&s(Ie),A(Y,e),e&&s(Ct),e&&s(Ke),e&&s(Ft),e&&s(ze),A(H,e),e&&s(Ot),e&&s(Ue),e&&s(Lt),e&&s(de),R(pe),e&&s(Yt),e&&s(Ge),e&&s(Ht),e&&s(Pe),A(B,e),e&&s(Bt),e&&s(Re),e&&s(Mt),e&&s(je),R(me),e&&s(Kt),e&&s(Xe),e&&s(Qt),e&&s(Ae),R(_e),e&&s(Ut),e&&s(Ze),e&&s(Gt),e&&s(et),e&&s(Jt),e&&s(ve),e&&s(Rt),R(De,e),e&&s(Xt),e&&s(tt),e&&s(Zt),R(Ve,e),e&&s(el),R(Se,e),e&&s(tl),e&&s(lt),e&&s(ll),R(Te,e),e&&s(nl),R(Ne,e),e&&s(sl),R(qe,e),e&&s(ol),R(We,e),e&&s(rl),e&&s(nt)}}}function Ts(a){let t,n,l,r,i,g,P,w,d;return w=new bs({props:{$$slots:{default:[Ss]},$$scope:{ctx:a}}}),{c(){t=_("meta"),n=x(),l=_("h1"),r=$("Word Embeddings"),i=x(),g=_("div"),P=x(),K(w.$$.fragment),this.h()},l(m){const u=ws("svelte-19zhfy3",document.head);t=v(u,"META",{name:!0,content:!0}),u.forEach(s),n=E(m),l=v(m,"H1",{});var j=b(l);r=y(j,"Word Embeddings"),j.forEach(s),i=E(m),g=v(m,"DIV",{class:!0}),b(g).forEach(s),P=E(m),Q(w.$$.fragment,m),this.h()},h(){document.title="Word Embeddings - World4AI",I(t,"name","description"),I(t,"content","It is not possible to train a neural network directly on text, as a neural network requires numerical input. Before we can train language models, we have to turn text into word embeddings, dense vectors, that represent a word in vector space."),I(g,"class","separator")},m(m,u){h(document.head,t),f(m,n,u),f(m,l,u),h(l,r),f(m,i,u),f(m,g,u),f(m,P,u),U(w,m,u),d=!0},p(m,u){const j={};u[0]&3|u[1]&16&&(j.$$scope={dirty:u,ctx:m}),w.$set(j)},i(m){d||(G(w.$$.fragment,m),d=!0)},o(m){J(w.$$.fragment,m),d=!1},d(m){s(t),m&&s(n),m&&s(l),m&&s(i),m&&s(g),m&&s(P),R(w,m)}}}const ie="Charles III is the king of the United Kingdom.",ae="Queen Elizabeth II ruled for 70 years.";function Ns(a,t,n){let l=ie.toLowerCase().replace(/[.]/g,"").replace(/iii/g,"<unk>"),r=ae.toLowerCase().replace(/[.]/g,"").replace(/ii/g,"<unk>"),i=[].concat(l.split(" "),r.split(" "));i=new Set(i),i=["<pad>",...i],i.sort((P,w)=>{if(P==="<unk>")return-1});let g={};return i.forEach((P,w)=>{n(1,g[P]=w,g)}),[i,g,l,r]}class Ys extends ms{constructor(t){super(),_s(this,t,Ns,Ts,vs,{},null,[-1,-1])}}export{Ys as default};
