import{S as va,i as _a,s as ba,k as M,a as g,q as l,y as m,W as ka,l as y,h as o,c as v,m as T,r as f,z as p,n as fn,N as h,b as r,A as c,g as u,d as w,B as d,C as Ce}from"../chunks/index.4d92b023.js";import{C as xa}from"../chunks/Container.b0705c7b.js";import{H}from"../chunks/Highlight.b7c1de53.js";import{L as A}from"../chunks/Latex.e0b308c0.js";import{A as vt}from"../chunks/Alert.25a852b3.js";import{M as Re,S as ln}from"../chunks/Sequence.ddbdf7ae.js";function Ma(i){let n;return{c(){n=l("S_0")},l(t){n=f(t,"S_0")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function ya(i){let n;return{c(){n=l("A_0")},l(t){n=f(t,"A_0")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ta(i){let n;return{c(){n=l("S_1")},l(t){n=f(t,"S_1")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Aa(i){let n;return{c(){n=l("R_1")},l(t){n=f(t,"R_1")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Pa(i){let n;return{c(){n=l("A_1")},l(t){n=f(t,"A_1")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ia(i){let n;return{c(){n=l("Markov decision process")},l(t){n=f(t,"Markov decision process")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ba(i){let n;return{c(){n=l("stochastic process")},l(t){n=f(t,"stochastic process")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ea(i){let n;return{c(){n=l("Markov")},l(t){n=f(t,"Markov")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Sa(i){let n;return{c(){n=l("decisions")},l(t){n=f(t,"decisions")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function qa(i){let n,t,s,_,b,x,I,k;return t=new H({props:{$$slots:{default:[Ba]},$$scope:{ctx:i}}}),_=new H({props:{$$slots:{default:[Ea]},$$scope:{ctx:i}}}),x=new H({props:{$$slots:{default:[Sa]},$$scope:{ctx:i}}}),{c(){n=l(`A Markov decision process is a
    `),m(t.$$.fragment),s=l(" with a "),m(_.$$.fragment),b=l(`
    property, that provides a mechanism for the agent to make
    `),m(x.$$.fragment),I=l(" and receive rewards.")},l($){n=f($,`A Markov decision process is a
    `),p(t.$$.fragment,$),s=f($," with a "),p(_.$$.fragment,$),b=f($,`
    property, that provides a mechanism for the agent to make
    `),p(x.$$.fragment,$),I=f($," and receive rewards.")},m($,P){r($,n,P),c(t,$,P),r($,s,P),c(_,$,P),r($,b,P),c(x,$,P),r($,I,P),k=!0},p($,P){const j={};P&1&&(j.$$scope={dirty:P,ctx:$}),t.$set(j);const Fe={};P&1&&(Fe.$$scope={dirty:P,ctx:$}),_.$set(Fe);const R={};P&1&&(R.$$scope={dirty:P,ctx:$}),x.$set(R)},i($){k||(u(t.$$.fragment,$),u(_.$$.fragment,$),u(x.$$.fragment,$),k=!0)},o($){w(t.$$.fragment,$),w(_.$$.fragment,$),w(x.$$.fragment,$),k=!1},d($){$&&o(n),d(t,$),$&&o(s),d(_,$),$&&o(b),d(x,$),$&&o(I)}}}function Da(i){let n;return{c(){n=l(`A stochastic process is a sequence of random variables that develops over
    time.`)},l(t){n=f(t,`A stochastic process is a sequence of random variables that develops over
    time.`)},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function La(i){let n;return{c(){n=l("X")},l(t){n=f(t,"X")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function ja(i){let n;return{c(){n=l("p")},l(t){n=f(t,"p")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ra(i){let n;return{c(){n=l("1-p")},l(t){n=f(t,"1-p")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ca(i){let n;return{c(){n=l("p")},l(t){n=f(t,"p")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Fa(i){let n;return{c(){n=l("p")},l(t){n=f(t,"p")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Wa(i){let n;return{c(){n=l("Markov chain")},l(t){n=f(t,"Markov chain")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Na(i){let n;return{c(){n=l("A Markov chain is a stochastic process that has the Markov property.")},l(t){n=f(t,"A Markov chain is a stochastic process that has the Markov property.")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function za(i){let n;return{c(){n=l("Markov reward process")},l(t){n=f(t,"Markov reward process")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ha(i){let n;return{c(){n=l(`A Markov decision process is a Markov chain, such that the agent can
    partially influence the outcomes of the chain.`)},l(t){n=f(t,`A Markov decision process is a Markov chain, such that the agent can
    partially influence the outcomes of the chain.`)},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Xa(i){let n=String.raw`(\mathcal{S, A}, P, R)`+"",t;return{c(){t=l(n)},l(s){t=f(s,n)},m(s,_){r(s,t,_)},p:Ce,d(s){s&&o(t)}}}function Ua(i){let n,t,s,_;return t=new A({props:{$$slots:{default:[Xa]},$$scope:{ctx:i}}}),{c(){n=l("A Markov decision process can be defined as a tuple with four components: "),m(t.$$.fragment),s=l(".")},l(b){n=f(b,"A Markov decision process can be defined as a tuple with four components: "),p(t.$$.fragment,b),s=f(b,".")},m(b,x){r(b,n,x),c(t,b,x),r(b,s,x),_=!0},p(b,x){const I={};x&1&&(I.$$scope={dirty:x,ctx:b}),t.$set(I)},i(b){_||(u(t.$$.fragment,b),_=!0)},o(b){w(t.$$.fragment,b),_=!1},d(b){b&&o(n),d(t,b),b&&o(s)}}}function Va(i){let n=String.raw`\mathcal{S}`+"",t;return{c(){t=l(n)},l(s){t=f(s,n)},m(s,_){r(s,t,_)},p:Ce,d(s){s&&o(t)}}}function Ya(i){let n;return{c(){n=l("state space")},l(t){n=f(t,"state space")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Ga(i){let n=String.raw`\mathcal{S}=[0, 1]`+"",t;return{c(){t=l(n)},l(s){t=f(s,n)},m(s,_){r(s,t,_)},p:Ce,d(s){s&&o(t)}}}function Ja(i){let n=String.raw`\mathcal{A}`+"",t;return{c(){t=l(n)},l(s){t=f(s,n)},m(s,_){r(s,t,_)},p:Ce,d(s){s&&o(t)}}}function Ka(i){let n;return{c(){n=l("action space")},l(t){n=f(t,"action space")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Oa(i){let n=String.raw`\mathcal{A}=[A, B]`+"",t;return{c(){t=l(n)},l(s){t=f(s,n)},m(s,_){r(s,t,_)},p:Ce,d(s){s&&o(t)}}}function Qa(i){let n;return{c(){n=l("P")},l(t){n=f(t,"P")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function Za(i){let n;return{c(){n=l("transition probability function")},l(t){n=f(t,"transition probability function")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function es(i){let n;return{c(){n=l("s'")},l(t){n=f(t,"s'")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function ts(i){let n;return{c(){n=l("s")},l(t){n=f(t,"s")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function ns(i){let n;return{c(){n=l("a")},l(t){n=f(t,"a")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function as(i){let n=String.raw`P(s' \mid s, a) \doteq Pr[S_{t+1}=s' \mid S_t=s, A_t=a], \forall s, s' \in \mathcal{S}, a \in \mathcal{A}`+"",t;return{c(){t=l(n)},l(s){t=f(s,n)},m(s,_){r(s,t,_)},p:Ce,d(s){s&&o(t)}}}function ss(i){let n;return{c(){n=l("P(1 \\mid 0, B) = 0.8")},l(t){n=f(t,"P(1 \\mid 0, B) = 0.8")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function os(i){let n;return{c(){n=l("R")},l(t){n=f(t,"R")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function rs(i){let n;return{c(){n=l("reward function")},l(t){n=f(t,"reward function")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function is(i){let n;return{c(){n=l("s")},l(t){n=f(t,"s")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function ls(i){let n;return{c(){n=l("a")},l(t){n=f(t,"a")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function fs(i){let n;return{c(){n=l("s'")},l(t){n=f(t,"s'")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function $s(i){let n=String.raw`R(s,a, s')`+"",t;return{c(){t=l(n)},l(s){t=f(s,n)},m(s,_){r(s,t,_)},p:Ce,d(s){s&&o(t)}}}function hs(i){let n;return{c(){n=l("R(1, A, 1) = 5")},l(t){n=f(t,"R(1, A, 1) = 5")},m(t,s){r(t,n,s)},d(t){t&&o(n)}}}function ms(i){let n,t,s,_,b,x,I,k,$,P,j,Fe,R,$n,U,hn,_t,V,bt,We,mn,kt,Y,xt,D,pn,G,cn,J,un,K,wn,O,dn,Mt,ye,yt,Ne,gn,Tt,Te,At,W,vn,Q,_n,Z,bn,Pt,ee,It,ze,kn,Bt,He,xn,Et,Xe,Mn,St,Ae,qt,Ue,yn,Dt,Pe,Lt,Ve,Tn,jt,Ie,Rt,Ye,An,Ct,te,Pn,ne,In,Ft,Ge,Bn,Wt,Be,Nt,Ee,zt,Je,En,Ht,Se,Xt,Ke,Sn,Ut,ae,Vt,Oe,qn,Yt,qe,Gt,De,Jt,Qe,Dn,Kt,Le,Ot,Ze,Ln,Qt,et,jn,Zt,se,en,C,oe,Rn,re,Cn,ie,Fn,tn,F,le,Wn,fe,Nn,$e,zn,nn,B,he,Hn,me,Xn,pe,Un,ce,Vn,ue,Yn,we,Gn,de,Jn,an,E,ge,Kn,ve,On,_e,Qn,be,Zn,ke,ea,xe,ta,Me,na,sn,tt,aa,on,nt,rn;return b=new A({props:{$$slots:{default:[Ma]},$$scope:{ctx:i}}}),I=new A({props:{$$slots:{default:[ya]},$$scope:{ctx:i}}}),$=new A({props:{$$slots:{default:[Ta]},$$scope:{ctx:i}}}),j=new A({props:{$$slots:{default:[Aa]},$$scope:{ctx:i}}}),R=new A({props:{$$slots:{default:[Pa]},$$scope:{ctx:i}}}),U=new H({props:{$$slots:{default:[Ia]},$$scope:{ctx:i}}}),V=new vt({props:{type:"info",$$slots:{default:[qa]},$$scope:{ctx:i}}}),Y=new vt({props:{type:"info",$$slots:{default:[Da]},$$scope:{ctx:i}}}),G=new A({props:{$$slots:{default:[La]},$$scope:{ctx:i}}}),J=new A({props:{$$slots:{default:[ja]},$$scope:{ctx:i}}}),K=new A({props:{$$slots:{default:[Ra]},$$scope:{ctx:i}}}),O=new A({props:{$$slots:{default:[Ca]},$$scope:{ctx:i}}}),ye=new Re({props:{config:{root:"X",states:["0","1"],p:[.5,.5],type:"chain"}}}),Te=new ln({props:{f:cs}}),Q=new A({props:{$$slots:{default:[Fa]},$$scope:{ctx:i}}}),Z=new H({props:{$$slots:{default:[Wa]},$$scope:{ctx:i}}}),ee=new vt({props:{type:"info",$$slots:{default:[Na]},$$scope:{ctx:i}}}),Ae=new Re({props:{config:{root:"0",states:["0","1"],p:[.8,.2],type:"chain"}}}),Pe=new Re({props:{config:{root:"0",states:["0","1"],p:[.2,.8],type:"chain"}}}),Ie=new ln({props:{f:us()}}),ne=new H({props:{$$slots:{default:[za]},$$scope:{ctx:i}}}),Be=new Re({props:{config:{root:"0",states:["0","1"],rewards:["-1","5"],p:[.8,.2],type:"reward"}}}),Ee=new Re({props:{config:{root:"1",states:["0","1"],rewards:["-1","5"],p:[.2,.8],type:"reward"}}}),Se=new ln({props:{f:ws()}}),ae=new vt({props:{type:"info",$$slots:{default:[Ha]},$$scope:{ctx:i}}}),qe=new Re({props:{config:{root:"0",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[.5,.5],p:[.7,.3,.2,.8],type:"decision"}}}),De=new Re({props:{config:{root:"1",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[.5,.5],p:[.9,.1,.3,.7],type:"decision"}}}),Le=new ln({props:{f:ds()}}),se=new vt({props:{type:"info",$$slots:{default:[Ua]},$$scope:{ctx:i}}}),oe=new A({props:{$$slots:{default:[Va]},$$scope:{ctx:i}}}),re=new H({props:{$$slots:{default:[Ya]},$$scope:{ctx:i}}}),ie=new A({props:{$$slots:{default:[Ga]},$$scope:{ctx:i}}}),le=new A({props:{$$slots:{default:[Ja]},$$scope:{ctx:i}}}),fe=new H({props:{$$slots:{default:[Ka]},$$scope:{ctx:i}}}),$e=new A({props:{$$slots:{default:[Oa]},$$scope:{ctx:i}}}),he=new A({props:{$$slots:{default:[Qa]},$$scope:{ctx:i}}}),me=new H({props:{$$slots:{default:[Za]},$$scope:{ctx:i}}}),pe=new A({props:{$$slots:{default:[es]},$$scope:{ctx:i}}}),ce=new A({props:{$$slots:{default:[ts]},$$scope:{ctx:i}}}),ue=new A({props:{$$slots:{default:[ns]},$$scope:{ctx:i}}}),we=new A({props:{$$slots:{default:[as]},$$scope:{ctx:i}}}),de=new A({props:{$$slots:{default:[ss]},$$scope:{ctx:i}}}),ge=new A({props:{$$slots:{default:[os]},$$scope:{ctx:i}}}),ve=new H({props:{$$slots:{default:[rs]},$$scope:{ctx:i}}}),_e=new A({props:{$$slots:{default:[is]},$$scope:{ctx:i}}}),be=new A({props:{$$slots:{default:[ls]},$$scope:{ctx:i}}}),ke=new A({props:{$$slots:{default:[fs]},$$scope:{ctx:i}}}),xe=new A({props:{$$slots:{default:[$s]},$$scope:{ctx:i}}}),Me=new A({props:{$$slots:{default:[hs]},$$scope:{ctx:i}}}),{c(){n=M("div"),t=g(),s=M("p"),_=l(`In reinforcement learning the agent and the environment interract with each
    other in discrete timesteps. At the beginnig of the interraction the agent
    receives the initial state of the environment
    `),m(b.$$.fragment),x=l(" and produces the action "),m(I.$$.fragment),k=l(`
    . Based on that action the environment transitions into a new state `),m($.$$.fragment),P=l(" and generates the corresponding reward for the agent "),m(j.$$.fragment),Fe=l(`.
    The agent in turn reacts with the action `),m(R.$$.fragment),$n=l(`
    and the interaction continues. In order to be able to model this interaction,
    each environment has a so called `),m(U.$$.fragment),hn=l(` under the hood. These three words allow us to fully describe how the interaction
    works.`),_t=g(),m(V.$$.fragment),bt=g(),We=M("p"),mn=l("Intuitively we can define a stochastic process in the following way."),kt=g(),m(Y.$$.fragment),xt=g(),D=M("p"),pn=l("Let's have a look at the Bernoulli distribution "),m(G.$$.fragment),cn=l(` for example.
    This random variable produces the value 1 with the probability `),m(J.$$.fragment),un=l(" and the value 0 with the probability "),m(K.$$.fragment),wn=l(". Let's assume that "),m(O.$$.fragment),dn=l(" eqauls to 0.5."),Mt=g(),m(ye.$$.fragment),yt=g(),Ne=M("p"),gn=l(`At each timestep we draw a value from the distribution and generate a
    sequence of random variables.`),Tt=g(),m(Te.$$.fragment),At=g(),W=M("p"),vn=l(`In essence we can regard the Bernoulli process as an environment with just
    two states: state 0 and state 1. The environment transitions from one state
    to the next based on the probability `),m(Q.$$.fragment),_n=l(`. In this type of
    environment no actions or rewards are present. If we used the Bernoulli
    process in reinforcement learning our agent would drift from one state into
    the next state without any agency and any rewards to guide its actions. The
    stochastic process that is used for reinforcement learning is called the
    `),m(Z.$$.fragment),bn=l("."),Pt=g(),m(ee.$$.fragment),It=g(),ze=M("p"),kn=l(`The Bernoulli process generates a sequence of independent random variables.
    No matter what state came before the current time step, the probability for
    the next state stays unchanged. The Markov chain on the other hand exhibits
    a so called Markov property. In simple words that means that the probability
    of transitioning into the next state depends on the current state, and the
    current state only. The whole history of past states is irrelevant.`),Bt=g(),He=M("p"),xn=l("Let's have a look at an example of a Markov chain with two possible states."),Et=g(),Xe=M("p"),Mn=l(`If the current state of the environment is 0 the environment transitions
    into the state 1 with 20% probability and remains with 80% probability in
    state 0.`),St=g(),m(Ae.$$.fragment),qt=g(),Ue=M("p"),yn=l(`If the current state of the environment is 1 on the other hand the
    environment transitions into the state 0 with 20% probability and remains
    with 80% probability in the state 0.`),Dt=g(),m(Pe.$$.fragment),Lt=g(),Ve=M("p"),Tn=l(`In other words in this example the environment tends to stay in the same
    state.`),jt=g(),m(Ie.$$.fragment),Rt=g(),Ye=M("p"),An=l(`But why does it make sense to model the agent-environment interaction with a
    Markov chain? Why can't we just use for example the Bernoulli process? Think
    about a chess board for example. The state of the board does not change
    randomly from move to move. We move one piece at a time, while other pieces
    remain fairly constant. The board configuration always depends on the
    configuration from the previous time step, but only on the directly
    preceding state configuration. You do not need to know how the whole game
    developed in order to make the next move. The current positions of the chess
    pieces and your actions are sufficient to create the next state of the
    board.`),Ct=g(),te=M("p"),Pn=l("The "),m(ne.$$.fragment),In=l(" adds a reward for to each state."),Ft=g(),Ge=M("p"),Bn=l(`In the below example whenever the agent lands in the state 0, it gets a
    negative reward of -1 and whenever the agent lands in the state 1 it gets a
    positive reward of 5. The reward can theoretically also be calculated
    stochastically, but let's keep it simple and assume a deterministic reward.`),Wt=g(),m(Be.$$.fragment),Nt=g(),m(Ee.$$.fragment),zt=g(),Je=M("p"),En=l(`In a Markov reward process the sequence of random variables not only
    contains states but also rewards. The process drifts randomly from one state
    into the next and gives out rewards to the agent, but the agent has no
    influence on the process whatsoever, even though the agent would prefer to
    gravitate towards the state 1 as its goal is to maximize the expected sum of
    rewards.`),Ht=g(),m(Se.$$.fragment),Xt=g(),Ke=M("p"),Sn=l(`A Markov decision process adds, as the name suggests, decisions to the
    Markov chain. In other words the agent gets some agency.`),Ut=g(),m(ae.$$.fragment),Vt=g(),Oe=M("p"),qn=l(`Additionally to the two states we add the option for the agent to take one
    of the actions: A or B. The action A increases the likelihood to move
    towards the 0's state, while the action B increases the probability to move
    towards the state 1. For the time being let's assume that the agent takes
    random actions.`),Yt=g(),m(qe.$$.fragment),Gt=g(),m(De.$$.fragment),Jt=g(),Qe=M("p"),Dn=l(`This time around the random process does not only contain states and
    rewards, but also actions taken by the agent.`),Kt=g(),m(Le.$$.fragment),Ot=g(),Ze=M("p"),Ln=l(`In the example above the environment has still two states, state 0 and state
    1. Landing in state 1 is more advantagous as the agent receives a reward of
    5, while landing in the 0's state generates a reward of -1. In both states
    the agent can choose between two actions. The action A is the action toward
    the state 0 and the action B is the action towards the state 1. For example
    if the agent is in the state 0 it should choose the action B to move with a
    probability of 70% to the state B and receive a reward of 5. The whole
    process is the succession of states, actions and rewards. This is the same
    mechanism, that we introduced previously as the interaction between the
    agent and the environment. The interaction is nothing more than a Markov
    chain that is made out of two elements. The first element, the environment
    is static and can not be changed in any form. The second component is the
    agent. The agent can change the probabilities of the whole chain, by
    tweaking the probabilities of actions. In the example above, we expect the
    agent to improve by increasing the probabilities to move to state 1. Ideally
    that would mean that the agent will only choose action B.`),Qt=g(),et=M("p"),jn=l(`Now we are ready to look at the formal definition of a Markov decision
    process. This definition deals with the four components that a MDP has to
    contain in order to be valid.`),Zt=g(),m(se.$$.fragment),en=g(),C=M("p"),m(oe.$$.fragment),Rn=l(" is the "),m(re.$$.fragment),Cn=l(`, that contains all possible states of the environment. In the examples
    above we were dealing with just two states, therefore our state space would
    correspond to: `),m(ie.$$.fragment),Fn=l("."),tn=g(),F=M("p"),m(le.$$.fragment),Wn=l(" is the "),m(fe.$$.fragment),Nn=l(`, that contains all possible actions of the environment. Above we were
    dealing with the environment with two actions:
    `),m($e.$$.fragment),zn=l("."),nn=g(),B=M("p"),m(he.$$.fragment),Hn=l(` is
    `),m(me.$$.fragment),Xn=l(`
    that provides a probability for the next state `),m(pe.$$.fragment),Un=l(` given the current
    state `),m(ce.$$.fragment),Vn=l(" and the action "),m(ue.$$.fragment),Yn=l(`. Mathematically we
    can express that idea as:
    `),m(we.$$.fragment),Gn=l(`. For example the probability to transition into the state 1 from state 0,
    given that the agent chose the action B is 80%: `),m(de.$$.fragment),Jn=l("."),an=g(),E=M("p"),m(ge.$$.fragment),Kn=l(" is "),m(ve.$$.fragment),On=l(`. This function
    calculates the reward given the state `),m(_e.$$.fragment),Qn=l(`
    , the action `),m(be.$$.fragment),Zn=l(" and the next state "),m(ke.$$.fragment),ea=g(),m(xe.$$.fragment),ta=l(`. For example our agent would receive
    a reward of 5, whenever it landed in state 1, `),m(Me.$$.fragment),na=l("."),sn=g(),tt=M("p"),aa=l(`It does not matter if we are talking about a grid world, a game of chess,
    StarCraft or the real world. We always expect the environment to conain a
    MDP with the four components under the hood. Usually we do not have access
    to those components and can not examine them explicitly, but just knowing
    that they are there allows the agent to interract with the environement and
    to learn from those interractions.`),on=g(),nt=M("div"),this.h()},l(e){n=y(e,"DIV",{class:!0}),T(n).forEach(o),t=v(e),s=y(e,"P",{});var a=T(s);_=f(a,`In reinforcement learning the agent and the environment interract with each
    other in discrete timesteps. At the beginnig of the interraction the agent
    receives the initial state of the environment
    `),p(b.$$.fragment,a),x=f(a," and produces the action "),p(I.$$.fragment,a),k=f(a,`
    . Based on that action the environment transitions into a new state `),p($.$$.fragment,a),P=f(a," and generates the corresponding reward for the agent "),p(j.$$.fragment,a),Fe=f(a,`.
    The agent in turn reacts with the action `),p(R.$$.fragment,a),$n=f(a,`
    and the interaction continues. In order to be able to model this interaction,
    each environment has a so called `),p(U.$$.fragment,a),hn=f(a,` under the hood. These three words allow us to fully describe how the interaction
    works.`),a.forEach(o),_t=v(e),p(V.$$.fragment,e),bt=v(e),We=y(e,"P",{});var at=T(We);mn=f(at,"Intuitively we can define a stochastic process in the following way."),at.forEach(o),kt=v(e),p(Y.$$.fragment,e),xt=v(e),D=y(e,"P",{});var L=T(D);pn=f(L,"Let's have a look at the Bernoulli distribution "),p(G.$$.fragment,L),cn=f(L,` for example.
    This random variable produces the value 1 with the probability `),p(J.$$.fragment,L),un=f(L," and the value 0 with the probability "),p(K.$$.fragment,L),wn=f(L,". Let's assume that "),p(O.$$.fragment,L),dn=f(L," eqauls to 0.5."),L.forEach(o),Mt=v(e),p(ye.$$.fragment,e),yt=v(e),Ne=y(e,"P",{});var st=T(Ne);gn=f(st,`At each timestep we draw a value from the distribution and generate a
    sequence of random variables.`),st.forEach(o),Tt=v(e),p(Te.$$.fragment,e),At=v(e),W=y(e,"P",{});var X=T(W);vn=f(X,`In essence we can regard the Bernoulli process as an environment with just
    two states: state 0 and state 1. The environment transitions from one state
    to the next based on the probability `),p(Q.$$.fragment,X),_n=f(X,`. In this type of
    environment no actions or rewards are present. If we used the Bernoulli
    process in reinforcement learning our agent would drift from one state into
    the next state without any agency and any rewards to guide its actions. The
    stochastic process that is used for reinforcement learning is called the
    `),p(Z.$$.fragment,X),bn=f(X,"."),X.forEach(o),Pt=v(e),p(ee.$$.fragment,e),It=v(e),ze=y(e,"P",{});var ot=T(ze);kn=f(ot,`The Bernoulli process generates a sequence of independent random variables.
    No matter what state came before the current time step, the probability for
    the next state stays unchanged. The Markov chain on the other hand exhibits
    a so called Markov property. In simple words that means that the probability
    of transitioning into the next state depends on the current state, and the
    current state only. The whole history of past states is irrelevant.`),ot.forEach(o),Bt=v(e),He=y(e,"P",{});var rt=T(He);xn=f(rt,"Let's have a look at an example of a Markov chain with two possible states."),rt.forEach(o),Et=v(e),Xe=y(e,"P",{});var it=T(Xe);Mn=f(it,`If the current state of the environment is 0 the environment transitions
    into the state 1 with 20% probability and remains with 80% probability in
    state 0.`),it.forEach(o),St=v(e),p(Ae.$$.fragment,e),qt=v(e),Ue=y(e,"P",{});var lt=T(Ue);yn=f(lt,`If the current state of the environment is 1 on the other hand the
    environment transitions into the state 0 with 20% probability and remains
    with 80% probability in the state 0.`),lt.forEach(o),Dt=v(e),p(Pe.$$.fragment,e),Lt=v(e),Ve=y(e,"P",{});var ft=T(Ve);Tn=f(ft,`In other words in this example the environment tends to stay in the same
    state.`),ft.forEach(o),jt=v(e),p(Ie.$$.fragment,e),Rt=v(e),Ye=y(e,"P",{});var $t=T(Ye);An=f($t,`But why does it make sense to model the agent-environment interaction with a
    Markov chain? Why can't we just use for example the Bernoulli process? Think
    about a chess board for example. The state of the board does not change
    randomly from move to move. We move one piece at a time, while other pieces
    remain fairly constant. The board configuration always depends on the
    configuration from the previous time step, but only on the directly
    preceding state configuration. You do not need to know how the whole game
    developed in order to make the next move. The current positions of the chess
    pieces and your actions are sufficient to create the next state of the
    board.`),$t.forEach(o),Ct=v(e),te=y(e,"P",{});var je=T(te);Pn=f(je,"The "),p(ne.$$.fragment,je),In=f(je," adds a reward for to each state."),je.forEach(o),Ft=v(e),Ge=y(e,"P",{});var ht=T(Ge);Bn=f(ht,`In the below example whenever the agent lands in the state 0, it gets a
    negative reward of -1 and whenever the agent lands in the state 1 it gets a
    positive reward of 5. The reward can theoretically also be calculated
    stochastically, but let's keep it simple and assume a deterministic reward.`),ht.forEach(o),Wt=v(e),p(Be.$$.fragment,e),Nt=v(e),p(Ee.$$.fragment,e),zt=v(e),Je=y(e,"P",{});var mt=T(Je);En=f(mt,`In a Markov reward process the sequence of random variables not only
    contains states but also rewards. The process drifts randomly from one state
    into the next and gives out rewards to the agent, but the agent has no
    influence on the process whatsoever, even though the agent would prefer to
    gravitate towards the state 1 as its goal is to maximize the expected sum of
    rewards.`),mt.forEach(o),Ht=v(e),p(Se.$$.fragment,e),Xt=v(e),Ke=y(e,"P",{});var pt=T(Ke);Sn=f(pt,`A Markov decision process adds, as the name suggests, decisions to the
    Markov chain. In other words the agent gets some agency.`),pt.forEach(o),Ut=v(e),p(ae.$$.fragment,e),Vt=v(e),Oe=y(e,"P",{});var ct=T(Oe);qn=f(ct,`Additionally to the two states we add the option for the agent to take one
    of the actions: A or B. The action A increases the likelihood to move
    towards the 0's state, while the action B increases the probability to move
    towards the state 1. For the time being let's assume that the agent takes
    random actions.`),ct.forEach(o),Yt=v(e),p(qe.$$.fragment,e),Gt=v(e),p(De.$$.fragment,e),Jt=v(e),Qe=y(e,"P",{});var ut=T(Qe);Dn=f(ut,`This time around the random process does not only contain states and
    rewards, but also actions taken by the agent.`),ut.forEach(o),Kt=v(e),p(Le.$$.fragment,e),Ot=v(e),Ze=y(e,"P",{});var wt=T(Ze);Ln=f(wt,`In the example above the environment has still two states, state 0 and state
    1. Landing in state 1 is more advantagous as the agent receives a reward of
    5, while landing in the 0's state generates a reward of -1. In both states
    the agent can choose between two actions. The action A is the action toward
    the state 0 and the action B is the action towards the state 1. For example
    if the agent is in the state 0 it should choose the action B to move with a
    probability of 70% to the state B and receive a reward of 5. The whole
    process is the succession of states, actions and rewards. This is the same
    mechanism, that we introduced previously as the interaction between the
    agent and the environment. The interaction is nothing more than a Markov
    chain that is made out of two elements. The first element, the environment
    is static and can not be changed in any form. The second component is the
    agent. The agent can change the probabilities of the whole chain, by
    tweaking the probabilities of actions. In the example above, we expect the
    agent to improve by increasing the probabilities to move to state 1. Ideally
    that would mean that the agent will only choose action B.`),wt.forEach(o),Qt=v(e),et=y(e,"P",{});var dt=T(et);jn=f(dt,`Now we are ready to look at the formal definition of a Markov decision
    process. This definition deals with the four components that a MDP has to
    contain in order to be valid.`),dt.forEach(o),Zt=v(e),p(se.$$.fragment,e),en=v(e),C=y(e,"P",{});var N=T(C);p(oe.$$.fragment,N),Rn=f(N," is the "),p(re.$$.fragment,N),Cn=f(N,`, that contains all possible states of the environment. In the examples
    above we were dealing with just two states, therefore our state space would
    correspond to: `),p(ie.$$.fragment,N),Fn=f(N,"."),N.forEach(o),tn=v(e),F=y(e,"P",{});var z=T(F);p(le.$$.fragment,z),Wn=f(z," is the "),p(fe.$$.fragment,z),Nn=f(z,`, that contains all possible actions of the environment. Above we were
    dealing with the environment with two actions:
    `),p($e.$$.fragment,z),zn=f(z,"."),z.forEach(o),nn=v(e),B=y(e,"P",{});var S=T(B);p(he.$$.fragment,S),Hn=f(S,` is
    `),p(me.$$.fragment,S),Xn=f(S,`
    that provides a probability for the next state `),p(pe.$$.fragment,S),Un=f(S,` given the current
    state `),p(ce.$$.fragment,S),Vn=f(S," and the action "),p(ue.$$.fragment,S),Yn=f(S,`. Mathematically we
    can express that idea as:
    `),p(we.$$.fragment,S),Gn=f(S,`. For example the probability to transition into the state 1 from state 0,
    given that the agent chose the action B is 80%: `),p(de.$$.fragment,S),Jn=f(S,"."),S.forEach(o),an=v(e),E=y(e,"P",{});var q=T(E);p(ge.$$.fragment,q),Kn=f(q," is "),p(ve.$$.fragment,q),On=f(q,`. This function
    calculates the reward given the state `),p(_e.$$.fragment,q),Qn=f(q,`
    , the action `),p(be.$$.fragment,q),Zn=f(q," and the next state "),p(ke.$$.fragment,q),ea=v(q),p(xe.$$.fragment,q),ta=f(q,`. For example our agent would receive
    a reward of 5, whenever it landed in state 1, `),p(Me.$$.fragment,q),na=f(q,"."),q.forEach(o),sn=v(e),tt=y(e,"P",{});var gt=T(tt);aa=f(gt,`It does not matter if we are talking about a grid world, a game of chess,
    StarCraft or the real world. We always expect the environment to conain a
    MDP with the four components under the hood. Usually we do not have access
    to those components and can not examine them explicitly, but just knowing
    that they are there allows the agent to interract with the environement and
    to learn from those interractions.`),gt.forEach(o),on=v(e),nt=y(e,"DIV",{class:!0}),T(nt).forEach(o),this.h()},h(){fn(n,"class","separator"),fn(nt,"class","separator")},m(e,a){r(e,n,a),r(e,t,a),r(e,s,a),h(s,_),c(b,s,null),h(s,x),c(I,s,null),h(s,k),c($,s,null),h(s,P),c(j,s,null),h(s,Fe),c(R,s,null),h(s,$n),c(U,s,null),h(s,hn),r(e,_t,a),c(V,e,a),r(e,bt,a),r(e,We,a),h(We,mn),r(e,kt,a),c(Y,e,a),r(e,xt,a),r(e,D,a),h(D,pn),c(G,D,null),h(D,cn),c(J,D,null),h(D,un),c(K,D,null),h(D,wn),c(O,D,null),h(D,dn),r(e,Mt,a),c(ye,e,a),r(e,yt,a),r(e,Ne,a),h(Ne,gn),r(e,Tt,a),c(Te,e,a),r(e,At,a),r(e,W,a),h(W,vn),c(Q,W,null),h(W,_n),c(Z,W,null),h(W,bn),r(e,Pt,a),c(ee,e,a),r(e,It,a),r(e,ze,a),h(ze,kn),r(e,Bt,a),r(e,He,a),h(He,xn),r(e,Et,a),r(e,Xe,a),h(Xe,Mn),r(e,St,a),c(Ae,e,a),r(e,qt,a),r(e,Ue,a),h(Ue,yn),r(e,Dt,a),c(Pe,e,a),r(e,Lt,a),r(e,Ve,a),h(Ve,Tn),r(e,jt,a),c(Ie,e,a),r(e,Rt,a),r(e,Ye,a),h(Ye,An),r(e,Ct,a),r(e,te,a),h(te,Pn),c(ne,te,null),h(te,In),r(e,Ft,a),r(e,Ge,a),h(Ge,Bn),r(e,Wt,a),c(Be,e,a),r(e,Nt,a),c(Ee,e,a),r(e,zt,a),r(e,Je,a),h(Je,En),r(e,Ht,a),c(Se,e,a),r(e,Xt,a),r(e,Ke,a),h(Ke,Sn),r(e,Ut,a),c(ae,e,a),r(e,Vt,a),r(e,Oe,a),h(Oe,qn),r(e,Yt,a),c(qe,e,a),r(e,Gt,a),c(De,e,a),r(e,Jt,a),r(e,Qe,a),h(Qe,Dn),r(e,Kt,a),c(Le,e,a),r(e,Ot,a),r(e,Ze,a),h(Ze,Ln),r(e,Qt,a),r(e,et,a),h(et,jn),r(e,Zt,a),c(se,e,a),r(e,en,a),r(e,C,a),c(oe,C,null),h(C,Rn),c(re,C,null),h(C,Cn),c(ie,C,null),h(C,Fn),r(e,tn,a),r(e,F,a),c(le,F,null),h(F,Wn),c(fe,F,null),h(F,Nn),c($e,F,null),h(F,zn),r(e,nn,a),r(e,B,a),c(he,B,null),h(B,Hn),c(me,B,null),h(B,Xn),c(pe,B,null),h(B,Un),c(ce,B,null),h(B,Vn),c(ue,B,null),h(B,Yn),c(we,B,null),h(B,Gn),c(de,B,null),h(B,Jn),r(e,an,a),r(e,E,a),c(ge,E,null),h(E,Kn),c(ve,E,null),h(E,On),c(_e,E,null),h(E,Qn),c(be,E,null),h(E,Zn),c(ke,E,null),h(E,ea),c(xe,E,null),h(E,ta),c(Me,E,null),h(E,na),r(e,sn,a),r(e,tt,a),h(tt,aa),r(e,on,a),r(e,nt,a),rn=!0},p(e,a){const at={};a&1&&(at.$$scope={dirty:a,ctx:e}),b.$set(at);const L={};a&1&&(L.$$scope={dirty:a,ctx:e}),I.$set(L);const st={};a&1&&(st.$$scope={dirty:a,ctx:e}),$.$set(st);const X={};a&1&&(X.$$scope={dirty:a,ctx:e}),j.$set(X);const ot={};a&1&&(ot.$$scope={dirty:a,ctx:e}),R.$set(ot);const rt={};a&1&&(rt.$$scope={dirty:a,ctx:e}),U.$set(rt);const it={};a&1&&(it.$$scope={dirty:a,ctx:e}),V.$set(it);const lt={};a&1&&(lt.$$scope={dirty:a,ctx:e}),Y.$set(lt);const ft={};a&1&&(ft.$$scope={dirty:a,ctx:e}),G.$set(ft);const $t={};a&1&&($t.$$scope={dirty:a,ctx:e}),J.$set($t);const je={};a&1&&(je.$$scope={dirty:a,ctx:e}),K.$set(je);const ht={};a&1&&(ht.$$scope={dirty:a,ctx:e}),O.$set(ht);const mt={};a&1&&(mt.$$scope={dirty:a,ctx:e}),Q.$set(mt);const pt={};a&1&&(pt.$$scope={dirty:a,ctx:e}),Z.$set(pt);const ct={};a&1&&(ct.$$scope={dirty:a,ctx:e}),ee.$set(ct);const ut={};a&1&&(ut.$$scope={dirty:a,ctx:e}),ne.$set(ut);const wt={};a&1&&(wt.$$scope={dirty:a,ctx:e}),ae.$set(wt);const dt={};a&1&&(dt.$$scope={dirty:a,ctx:e}),se.$set(dt);const N={};a&1&&(N.$$scope={dirty:a,ctx:e}),oe.$set(N);const z={};a&1&&(z.$$scope={dirty:a,ctx:e}),re.$set(z);const S={};a&1&&(S.$$scope={dirty:a,ctx:e}),ie.$set(S);const q={};a&1&&(q.$$scope={dirty:a,ctx:e}),le.$set(q);const gt={};a&1&&(gt.$$scope={dirty:a,ctx:e}),fe.$set(gt);const sa={};a&1&&(sa.$$scope={dirty:a,ctx:e}),$e.$set(sa);const oa={};a&1&&(oa.$$scope={dirty:a,ctx:e}),he.$set(oa);const ra={};a&1&&(ra.$$scope={dirty:a,ctx:e}),me.$set(ra);const ia={};a&1&&(ia.$$scope={dirty:a,ctx:e}),pe.$set(ia);const la={};a&1&&(la.$$scope={dirty:a,ctx:e}),ce.$set(la);const fa={};a&1&&(fa.$$scope={dirty:a,ctx:e}),ue.$set(fa);const $a={};a&1&&($a.$$scope={dirty:a,ctx:e}),we.$set($a);const ha={};a&1&&(ha.$$scope={dirty:a,ctx:e}),de.$set(ha);const ma={};a&1&&(ma.$$scope={dirty:a,ctx:e}),ge.$set(ma);const pa={};a&1&&(pa.$$scope={dirty:a,ctx:e}),ve.$set(pa);const ca={};a&1&&(ca.$$scope={dirty:a,ctx:e}),_e.$set(ca);const ua={};a&1&&(ua.$$scope={dirty:a,ctx:e}),be.$set(ua);const wa={};a&1&&(wa.$$scope={dirty:a,ctx:e}),ke.$set(wa);const da={};a&1&&(da.$$scope={dirty:a,ctx:e}),xe.$set(da);const ga={};a&1&&(ga.$$scope={dirty:a,ctx:e}),Me.$set(ga)},i(e){rn||(u(b.$$.fragment,e),u(I.$$.fragment,e),u($.$$.fragment,e),u(j.$$.fragment,e),u(R.$$.fragment,e),u(U.$$.fragment,e),u(V.$$.fragment,e),u(Y.$$.fragment,e),u(G.$$.fragment,e),u(J.$$.fragment,e),u(K.$$.fragment,e),u(O.$$.fragment,e),u(ye.$$.fragment,e),u(Te.$$.fragment,e),u(Q.$$.fragment,e),u(Z.$$.fragment,e),u(ee.$$.fragment,e),u(Ae.$$.fragment,e),u(Pe.$$.fragment,e),u(Ie.$$.fragment,e),u(ne.$$.fragment,e),u(Be.$$.fragment,e),u(Ee.$$.fragment,e),u(Se.$$.fragment,e),u(ae.$$.fragment,e),u(qe.$$.fragment,e),u(De.$$.fragment,e),u(Le.$$.fragment,e),u(se.$$.fragment,e),u(oe.$$.fragment,e),u(re.$$.fragment,e),u(ie.$$.fragment,e),u(le.$$.fragment,e),u(fe.$$.fragment,e),u($e.$$.fragment,e),u(he.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(ce.$$.fragment,e),u(ue.$$.fragment,e),u(we.$$.fragment,e),u(de.$$.fragment,e),u(ge.$$.fragment,e),u(ve.$$.fragment,e),u(_e.$$.fragment,e),u(be.$$.fragment,e),u(ke.$$.fragment,e),u(xe.$$.fragment,e),u(Me.$$.fragment,e),rn=!0)},o(e){w(b.$$.fragment,e),w(I.$$.fragment,e),w($.$$.fragment,e),w(j.$$.fragment,e),w(R.$$.fragment,e),w(U.$$.fragment,e),w(V.$$.fragment,e),w(Y.$$.fragment,e),w(G.$$.fragment,e),w(J.$$.fragment,e),w(K.$$.fragment,e),w(O.$$.fragment,e),w(ye.$$.fragment,e),w(Te.$$.fragment,e),w(Q.$$.fragment,e),w(Z.$$.fragment,e),w(ee.$$.fragment,e),w(Ae.$$.fragment,e),w(Pe.$$.fragment,e),w(Ie.$$.fragment,e),w(ne.$$.fragment,e),w(Be.$$.fragment,e),w(Ee.$$.fragment,e),w(Se.$$.fragment,e),w(ae.$$.fragment,e),w(qe.$$.fragment,e),w(De.$$.fragment,e),w(Le.$$.fragment,e),w(se.$$.fragment,e),w(oe.$$.fragment,e),w(re.$$.fragment,e),w(ie.$$.fragment,e),w(le.$$.fragment,e),w(fe.$$.fragment,e),w($e.$$.fragment,e),w(he.$$.fragment,e),w(me.$$.fragment,e),w(pe.$$.fragment,e),w(ce.$$.fragment,e),w(ue.$$.fragment,e),w(we.$$.fragment,e),w(de.$$.fragment,e),w(ge.$$.fragment,e),w(ve.$$.fragment,e),w(_e.$$.fragment,e),w(be.$$.fragment,e),w(ke.$$.fragment,e),w(xe.$$.fragment,e),w(Me.$$.fragment,e),rn=!1},d(e){e&&o(n),e&&o(t),e&&o(s),d(b),d(I),d($),d(j),d(R),d(U),e&&o(_t),d(V,e),e&&o(bt),e&&o(We),e&&o(kt),d(Y,e),e&&o(xt),e&&o(D),d(G),d(J),d(K),d(O),e&&o(Mt),d(ye,e),e&&o(yt),e&&o(Ne),e&&o(Tt),d(Te,e),e&&o(At),e&&o(W),d(Q),d(Z),e&&o(Pt),d(ee,e),e&&o(It),e&&o(ze),e&&o(Bt),e&&o(He),e&&o(Et),e&&o(Xe),e&&o(St),d(Ae,e),e&&o(qt),e&&o(Ue),e&&o(Dt),d(Pe,e),e&&o(Lt),e&&o(Ve),e&&o(jt),d(Ie,e),e&&o(Rt),e&&o(Ye),e&&o(Ct),e&&o(te),d(ne),e&&o(Ft),e&&o(Ge),e&&o(Wt),d(Be,e),e&&o(Nt),d(Ee,e),e&&o(zt),e&&o(Je),e&&o(Ht),d(Se,e),e&&o(Xt),e&&o(Ke),e&&o(Ut),d(ae,e),e&&o(Vt),e&&o(Oe),e&&o(Yt),d(qe,e),e&&o(Gt),d(De,e),e&&o(Jt),e&&o(Qe),e&&o(Kt),d(Le,e),e&&o(Ot),e&&o(Ze),e&&o(Qt),e&&o(et),e&&o(Zt),d(se,e),e&&o(en),e&&o(C),d(oe),d(re),d(ie),e&&o(tn),e&&o(F),d(le),d(fe),d($e),e&&o(nn),e&&o(B),d(he),d(me),d(pe),d(ce),d(ue),d(we),d(de),e&&o(an),e&&o(E),d(ge),d(ve),d(_e),d(be),d(ke),d(xe),d(Me),e&&o(sn),e&&o(tt),e&&o(on),e&&o(nt)}}}function ps(i){let n,t,s,_,b,x,I;return x=new xa({props:{$$slots:{default:[ms]},$$scope:{ctx:i}}}),{c(){n=M("meta"),t=g(),s=M("h1"),_=l("Definition of a Markov Decision Process"),b=g(),m(x.$$.fragment),this.h()},l(k){const $=ka("svelte-oxmc4j",document.head);n=y($,"META",{name:!0,content:!0}),$.forEach(o),t=v(k),s=y(k,"H1",{});var P=T(s);_=f(P,"Definition of a Markov Decision Process"),P.forEach(o),b=v(k),p(x.$$.fragment,k),this.h()},h(){document.title="Markov Decision Process Definition - World4AI",fn(n,"name","description"),fn(n,"content","The Markov decision process (MDP) is a stochastic process with the markov property that contains an interface for an agent to take actions.")},m(k,$){h(document.head,n),r(k,t,$),r(k,s,$),h(s,_),r(k,b,$),c(x,k,$),I=!0},p(k,[$]){const P={};$&1&&(P.$$scope={dirty:$,ctx:k}),x.$set(P)},i(k){I||(u(x.$$.fragment,k),I=!0)},o(k){w(x.$$.fragment,k),I=!1},d(k){o(n),k&&o(t),k&&o(s),k&&o(b),d(x,k)}}}function cs(){return{type:"state",value:Math.random()>.5?1:0}}function us(){let i=0;return()=>{let t=i,s=Math.random()>.8;return i===0?s&&(t=1):i===1&&s&&(t=0),i=t,{type:"state",value:t}}}function ws(){let i=0,n="reward";return()=>{if(n==="reward"){let s=i,_=Math.random()>.8;return i===0?_&&(s=1):i===1&&_&&(s=0),i=s,n="state",{type:"state",value:s}}if(n==="state"){n="reward";let s;return i===0?s=-1:i===1&&(s=5),{type:"reward",value:s}}}}function ds(){let i=0,n="A",t="action";return()=>{if(t==="action")return i===0&&(n==="A"?i=Math.random()<.7?0:1:n==="B"&&(i=Math.random()<.2?0:1)),i===1&&(n==="A"?i=Math.random()<.9?0:1:n==="B"&&(i=Math.random()<.3?0:1)),t="state",{type:"state",value:i};if(t==="reward")return t="action",n=Math.random()<.5?"A":"B",{type:"action",value:n};if(t==="state"){t="reward";let _;return i===0?_=-1:i===1&&(_=5),{type:"reward",value:_}}}}class Ms extends va{constructor(n){super(),_a(this,n,null,ps,ba,{})}}export{Ms as default};
