import{S as Ht,i as jt,s as Yt,k as D,a as E,q as B,y as $,W as Ft,l as M,h as n,c as R,m as L,r as A,z as f,n as q,N,b as h,A as l,g as m,d as u,B as p,L as Ot,Q as We,R as ze,C as U}from"../chunks/index.4d92b023.js";import{C as Vt}from"../chunks/Container.b0705c7b.js";import{F as Gt,I as Jt}from"../chunks/InternalLink.7deb899c.js";import{L as oe}from"../chunks/Latex.e0b308c0.js";import{H as Ut}from"../chunks/Highlight.b7c1de53.js";import{P as qt}from"../chunks/PythonCode.212ba7a6.js";import{S as Le}from"../chunks/SvgContainer.f70b5745.js";import{B as z}from"../chunks/Block.059eddcd.js";import{A as W}from"../chunks/Arrow.ae91874c.js";import{P as ht}from"../chunks/Plus.fc904b16.js";const Kt=""+new URL("../assets/skip_metrics.b44532e4.webp",import.meta.url).href;function Qt(P){let t;return{c(){t=B("skip connections")},l(i){t=A(i,"skip connections")},m(i,a){h(i,t,a)},d(i){i&&n(t)}}}function Xt(P){let t;return{c(){t=B("degradation problem")},l(i){t=A(i,"degradation problem")},m(i,a){h(i,t,a)},d(i){i&&n(t)}}}function Zt(P){let t,i,a,d,k,y,_,x,g,w,I;return i=new z({props:{x:50,y:15,width:55,height:13,text:"Activation"}}),a=new W({props:{data:[{x:50,y:25},{x:50,y:35}],dashed:!0,moving:!0}}),d=new z({props:{x:50,y:45,width:55,height:13,text:"Net Input"}}),k=new W({props:{data:[{x:50,y:55},{x:50,y:65}],dashed:!0,moving:!0}}),y=new z({props:{x:50,y:75,width:55,height:13,text:"Activation"}}),_=new W({props:{data:[{x:50,y:85},{x:50,y:95}],dashed:!0,moving:!0}}),x=new z({props:{x:50,y:105,width:55,height:13,text:"Net Input"}}),g=new W({props:{data:[{x:50,y:115},{x:50,y:125}],dashed:!0,moving:!0}}),w=new z({props:{x:50,y:135,width:55,height:13,text:"Activation"}}),{c(){t=We("svg"),$(i.$$.fragment),$(a.$$.fragment),$(d.$$.fragment),$(k.$$.fragment),$(y.$$.fragment),$(_.$$.fragment),$(x.$$.fragment),$(g.$$.fragment),$(w.$$.fragment),this.h()},l(o){t=ze(o,"svg",{viewBox:!0});var c=L(t);f(i.$$.fragment,c),f(a.$$.fragment,c),f(d.$$.fragment,c),f(k.$$.fragment,c),f(y.$$.fragment,c),f(_.$$.fragment,c),f(x.$$.fragment,c),f(g.$$.fragment,c),f(w.$$.fragment,c),c.forEach(n),this.h()},h(){q(t,"viewBox","0 0 100 150")},m(o,c){h(o,t,c),l(i,t,null),l(a,t,null),l(d,t,null),l(k,t,null),l(y,t,null),l(_,t,null),l(x,t,null),l(g,t,null),l(w,t,null),I=!0},p:U,i(o){I||(m(i.$$.fragment,o),m(a.$$.fragment,o),m(d.$$.fragment,o),m(k.$$.fragment,o),m(y.$$.fragment,o),m(_.$$.fragment,o),m(x.$$.fragment,o),m(g.$$.fragment,o),m(w.$$.fragment,o),I=!0)},o(o){u(i.$$.fragment,o),u(a.$$.fragment,o),u(d.$$.fragment,o),u(k.$$.fragment,o),u(y.$$.fragment,o),u(_.$$.fragment,o),u(x.$$.fragment,o),u(g.$$.fragment,o),u(w.$$.fragment,o),I=!1},d(o){o&&n(t),p(i),p(a),p(d),p(k),p(y),p(_),p(x),p(g),p(w)}}}function en(P){let t,i,a,d,k,y,_,x,g,w,I,o,c,T,C,v,S;return i=new z({props:{x:50,y:10,width:55,height:13,text:"Activation"}}),a=new W({props:{data:[{x:50,y:20},{x:50,y:30}],dashed:!0,moving:!0}}),d=new W({props:{data:[{x:50,y:25},{x:10,y:25},{x:10,y:60},{x:42,y:60}],dashed:!0,moving:!0}}),k=new z({props:{x:50,y:40,width:55,height:13,text:"Net Input"}}),y=new W({props:{data:[{x:50,y:48},{x:50,y:53}],dashed:!0,moving:!0}}),_=new ht({props:{x:50,y:60}}),x=new W({props:{data:[{x:50,y:65},{x:50,y:70}],dashed:!0,moving:!0}}),g=new z({props:{x:50,y:80,width:55,height:13,text:"Activation"}}),w=new W({props:{data:[{x:50,y:90},{x:50,y:100}],dashed:!0,moving:!0}}),I=new W({props:{data:[{x:50,y:95},{x:10,y:95},{x:10,y:130},{x:42,y:130}],dashed:!0,moving:!0}}),o=new z({props:{x:50,y:110,width:55,height:13,text:"Net Input"}}),c=new W({props:{data:[{x:50,y:118},{x:50,y:123}],dashed:!0,moving:!0}}),T=new ht({props:{x:50,y:130}}),C=new W({props:{data:[{x:50,y:135},{x:50,y:140}],dashed:!0,moving:!0}}),v=new z({props:{x:50,y:150,width:55,height:13,text:"Activation"}}),{c(){t=We("svg"),$(i.$$.fragment),$(a.$$.fragment),$(d.$$.fragment),$(k.$$.fragment),$(y.$$.fragment),$(_.$$.fragment),$(x.$$.fragment),$(g.$$.fragment),$(w.$$.fragment),$(I.$$.fragment),$(o.$$.fragment),$(c.$$.fragment),$(T.$$.fragment),$(C.$$.fragment),$(v.$$.fragment),this.h()},l(s){t=ze(s,"svg",{viewBox:!0});var b=L(t);f(i.$$.fragment,b),f(a.$$.fragment,b),f(d.$$.fragment,b),f(k.$$.fragment,b),f(y.$$.fragment,b),f(_.$$.fragment,b),f(x.$$.fragment,b),f(g.$$.fragment,b),f(w.$$.fragment,b),f(I.$$.fragment,b),f(o.$$.fragment,b),f(c.$$.fragment,b),f(T.$$.fragment,b),f(C.$$.fragment,b),f(v.$$.fragment,b),b.forEach(n),this.h()},h(){q(t,"viewBox","0 0 100 160")},m(s,b){h(s,t,b),l(i,t,null),l(a,t,null),l(d,t,null),l(k,t,null),l(y,t,null),l(_,t,null),l(x,t,null),l(g,t,null),l(w,t,null),l(I,t,null),l(o,t,null),l(c,t,null),l(T,t,null),l(C,t,null),l(v,t,null),S=!0},p:U,i(s){S||(m(i.$$.fragment,s),m(a.$$.fragment,s),m(d.$$.fragment,s),m(k.$$.fragment,s),m(y.$$.fragment,s),m(_.$$.fragment,s),m(x.$$.fragment,s),m(g.$$.fragment,s),m(w.$$.fragment,s),m(I.$$.fragment,s),m(o.$$.fragment,s),m(c.$$.fragment,s),m(T.$$.fragment,s),m(C.$$.fragment,s),m(v.$$.fragment,s),S=!0)},o(s){u(i.$$.fragment,s),u(a.$$.fragment,s),u(d.$$.fragment,s),u(k.$$.fragment,s),u(y.$$.fragment,s),u(_.$$.fragment,s),u(x.$$.fragment,s),u(g.$$.fragment,s),u(w.$$.fragment,s),u(I.$$.fragment,s),u(o.$$.fragment,s),u(c.$$.fragment,s),u(T.$$.fragment,s),u(C.$$.fragment,s),u(v.$$.fragment,s),S=!1},d(s){s&&n(t),p(i),p(a),p(d),p(k),p(y),p(_),p(x),p(g),p(w),p(I),p(o),p(c),p(T),p(C),p(v)}}}function tn(P){let t;return{c(){t=B("f")},l(i){t=A(i,"f")},m(i,a){h(i,t,a)},d(i){i&&n(t)}}}function nn(P){let t=String.raw`\mathbf{a}^{<l>} = f(\mathbf{z}^{<l>}_{vanilla})`+"",i;return{c(){i=B(t)},l(a){i=A(a,t)},m(a,d){h(a,i,d)},p:U,d(a){a&&n(i)}}}function rn(P){let t=String.raw`\mathbf{a}^{<l>} = f(\mathbf{a}^{<l-1>} + \mathbf{z}^{<l>}_{skip} )`+"",i;return{c(){i=B(t)},l(a){i=A(a,t)},m(a,d){h(a,i,d)},p:U,d(a){a&&n(i)}}}function an(P){let t=String.raw`
    \mathbf{z}^{<l>}_{skip} =  \mathbf{z}^{<l>}_{vanilla} - \mathbf{a}^{<l-1>}
      `+"",i;return{c(){i=B(t)},l(a){i=A(a,t)},m(a,d){h(a,i,d)},p:U,d(a){a&&n(i)}}}function on(P){let t,i,a,d,k,y,_,x;return i=new z({props:{x:50,y:15,width:55,height:13,text:"Activation",color:"var(--main-color-1)"}}),a=new z({props:{x:50,y:45,width:55,height:13,text:"Net Input",color:"var(--main-color-1)"}}),d=new z({props:{x:50,y:75,width:55,height:13,text:"Activation",color:"var(--main-color-1)"}}),k=new z({props:{x:50,y:105,width:55,height:13,text:"Net Input",color:"var(--main-color-1)"}}),y=new W({props:{data:[{x:50,y:125},{x:50,y:115}],dashed:!0,moving:!0}}),_=new z({props:{x:50,y:135,width:55,height:13,text:"Activation"}}),{c(){t=We("svg"),$(i.$$.fragment),$(a.$$.fragment),$(d.$$.fragment),$(k.$$.fragment),$(y.$$.fragment),$(_.$$.fragment),this.h()},l(g){t=ze(g,"svg",{viewBox:!0});var w=L(t);f(i.$$.fragment,w),f(a.$$.fragment,w),f(d.$$.fragment,w),f(k.$$.fragment,w),f(y.$$.fragment,w),f(_.$$.fragment,w),w.forEach(n),this.h()},h(){q(t,"viewBox","0 0 100 150")},m(g,w){h(g,t,w),l(i,t,null),l(a,t,null),l(d,t,null),l(k,t,null),l(y,t,null),l(_,t,null),x=!0},p:U,i(g){x||(m(i.$$.fragment,g),m(a.$$.fragment,g),m(d.$$.fragment,g),m(k.$$.fragment,g),m(y.$$.fragment,g),m(_.$$.fragment,g),x=!0)},o(g){u(i.$$.fragment,g),u(a.$$.fragment,g),u(d.$$.fragment,g),u(k.$$.fragment,g),u(y.$$.fragment,g),u(_.$$.fragment,g),x=!1},d(g){g&&n(t),p(i),p(a),p(d),p(k),p(y),p(_)}}}function sn(P){let t,i,a,d,k,y,_,x,g,w,I,o,c,T,C,v,S;return i=new z({props:{x:50,y:10,width:55,height:13,text:"Activation"}}),a=new W({props:{data:[{x:50,y:32},{x:50,y:22}],dashed:!0,moving:!0}}),d=new W({props:{data:[{x:45,y:60},{x:10,y:60},{x:10,y:25},{x:45,y:25}],dashed:!0,moving:!0}}),k=new z({props:{x:50,y:40,width:55,height:13,text:"Net Input"}}),y=new W({props:{data:[{x:50,y:55},{x:50,y:50}],dashed:!0,moving:!0}}),_=new ht({props:{x:50,y:60}}),x=new W({props:{data:[{x:50,y:73},{x:50,y:67}],dashed:!0,moving:!0}}),g=new z({props:{x:50,y:80,width:55,height:13,text:"Activation"}}),w=new W({props:{data:[{x:50,y:103},{x:50,y:90}],dashed:!0,moving:!0}}),I=new W({props:{data:[{x:45,y:130},{x:10,y:130},{x:10,y:95},{x:45,y:95}],dashed:!0,moving:!0}}),o=new z({props:{x:50,y:110,width:55,height:13,text:"Net Input",color:"var(--main-color-1)"}}),c=new W({props:{data:[{x:50,y:125},{x:50,y:120}],dashed:!0,moving:!0}}),T=new ht({props:{x:50,y:130}}),C=new W({props:{data:[{x:50,y:142},{x:50,y:137}],dashed:!0,moving:!0}}),v=new z({props:{x:50,y:150,width:55,height:13,text:"Activation"}}),{c(){t=We("svg"),$(i.$$.fragment),$(a.$$.fragment),$(d.$$.fragment),$(k.$$.fragment),$(y.$$.fragment),$(_.$$.fragment),$(x.$$.fragment),$(g.$$.fragment),$(w.$$.fragment),$(I.$$.fragment),$(o.$$.fragment),$(c.$$.fragment),$(T.$$.fragment),$(C.$$.fragment),$(v.$$.fragment),this.h()},l(s){t=ze(s,"svg",{viewBox:!0});var b=L(t);f(i.$$.fragment,b),f(a.$$.fragment,b),f(d.$$.fragment,b),f(k.$$.fragment,b),f(y.$$.fragment,b),f(_.$$.fragment,b),f(x.$$.fragment,b),f(g.$$.fragment,b),f(w.$$.fragment,b),f(I.$$.fragment,b),f(o.$$.fragment,b),f(c.$$.fragment,b),f(T.$$.fragment,b),f(C.$$.fragment,b),f(v.$$.fragment,b),b.forEach(n),this.h()},h(){q(t,"viewBox","0 0 100 160")},m(s,b){h(s,t,b),l(i,t,null),l(a,t,null),l(d,t,null),l(k,t,null),l(y,t,null),l(_,t,null),l(x,t,null),l(g,t,null),l(w,t,null),l(I,t,null),l(o,t,null),l(c,t,null),l(T,t,null),l(C,t,null),l(v,t,null),S=!0},p:U,i(s){S||(m(i.$$.fragment,s),m(a.$$.fragment,s),m(d.$$.fragment,s),m(k.$$.fragment,s),m(y.$$.fragment,s),m(_.$$.fragment,s),m(x.$$.fragment,s),m(g.$$.fragment,s),m(w.$$.fragment,s),m(I.$$.fragment,s),m(o.$$.fragment,s),m(c.$$.fragment,s),m(T.$$.fragment,s),m(C.$$.fragment,s),m(v.$$.fragment,s),S=!0)},o(s){u(i.$$.fragment,s),u(a.$$.fragment,s),u(d.$$.fragment,s),u(k.$$.fragment,s),u(y.$$.fragment,s),u(_.$$.fragment,s),u(x.$$.fragment,s),u(g.$$.fragment,s),u(w.$$.fragment,s),u(I.$$.fragment,s),u(o.$$.fragment,s),u(c.$$.fragment,s),u(T.$$.fragment,s),u(C.$$.fragment,s),u(v.$$.fragment,s),S=!1},d(s){s&&n(t),p(i),p(a),p(d),p(k),p(y),p(_),p(x),p(g),p(w),p(I),p(o),p(c),p(T),p(C),p(v)}}}function $n(P){let t,i,a,d,k,y,_,x,g,w,I,o,c,T,C;return i=new z({props:{x:50,y:15,width:55,height:13,text:"Activation",color:"var(--main-color-3)"}}),a=new W({props:{data:[{x:50,y:25},{x:50,y:35}],dashed:!0,moving:!0}}),d=new z({props:{x:50,y:45,width:55,height:13,text:"Net Input",color:"var(--main-color-3)"}}),k=new W({props:{data:[{x:50,y:55},{x:50,y:65}],dashed:!0,moving:!0}}),y=new z({props:{x:50,y:75,width:55,height:13,text:"Activation",color:"var(--main-color-3)"}}),_=new W({props:{data:[{x:50,y:85},{x:50,y:95}],dashed:!0,moving:!0}}),x=new z({props:{x:50,y:105,width:55,height:13,text:"Net Input",color:"var(--main-color-3)"}}),g=new W({props:{data:[{x:50,y:115},{x:50,y:125}],dashed:!0,moving:!0}}),w=new z({props:{x:50,y:135,width:55,height:13,text:"Activation",color:"var(--main-color-3)"}}),I=new W({props:{data:[{x:50,y:145},{x:50,y:155}],dashed:!0,moving:!0}}),o=new z({props:{x:50,y:165,width:55,height:13,text:"Net Input",color:"var(--main-color-2)"}}),c=new W({props:{data:[{x:50,y:175},{x:50,y:185}],dashed:!0,moving:!0}}),T=new z({props:{x:50,y:195,width:55,height:13,text:"Activation",color:"var(--main-color-2)"}}),{c(){t=We("svg"),$(i.$$.fragment),$(a.$$.fragment),$(d.$$.fragment),$(k.$$.fragment),$(y.$$.fragment),$(_.$$.fragment),$(x.$$.fragment),$(g.$$.fragment),$(w.$$.fragment),$(I.$$.fragment),$(o.$$.fragment),$(c.$$.fragment),$(T.$$.fragment),this.h()},l(v){t=ze(v,"svg",{viewBox:!0});var S=L(t);f(i.$$.fragment,S),f(a.$$.fragment,S),f(d.$$.fragment,S),f(k.$$.fragment,S),f(y.$$.fragment,S),f(_.$$.fragment,S),f(x.$$.fragment,S),f(g.$$.fragment,S),f(w.$$.fragment,S),f(I.$$.fragment,S),f(o.$$.fragment,S),f(c.$$.fragment,S),f(T.$$.fragment,S),S.forEach(n),this.h()},h(){q(t,"viewBox","0 0 100 210")},m(v,S){h(v,t,S),l(i,t,null),l(a,t,null),l(d,t,null),l(k,t,null),l(y,t,null),l(_,t,null),l(x,t,null),l(g,t,null),l(w,t,null),l(I,t,null),l(o,t,null),l(c,t,null),l(T,t,null),C=!0},p:U,i(v){C||(m(i.$$.fragment,v),m(a.$$.fragment,v),m(d.$$.fragment,v),m(k.$$.fragment,v),m(y.$$.fragment,v),m(_.$$.fragment,v),m(x.$$.fragment,v),m(g.$$.fragment,v),m(w.$$.fragment,v),m(I.$$.fragment,v),m(o.$$.fragment,v),m(c.$$.fragment,v),m(T.$$.fragment,v),C=!0)},o(v){u(i.$$.fragment,v),u(a.$$.fragment,v),u(d.$$.fragment,v),u(k.$$.fragment,v),u(y.$$.fragment,v),u(_.$$.fragment,v),u(x.$$.fragment,v),u(g.$$.fragment,v),u(w.$$.fragment,v),u(I.$$.fragment,v),u(o.$$.fragment,v),u(c.$$.fragment,v),u(T.$$.fragment,v),C=!1},d(v){v&&n(t),p(i),p(a),p(d),p(k),p(y),p(_),p(x),p(g),p(w),p(I),p(o),p(c),p(T)}}}function fn(P){let t=String.raw`\mathbf{a}^{<l-1>} = \mathbf{a}^{<l>}`+"",i;return{c(){i=B(t)},l(a){i=A(a,t)},m(a,d){h(a,i,d)},p:U,d(a){a&&n(i)}}}function ln(P){let t=String.raw`\mathbf{a}^{<l>} = f(\mathbf{z}^{<l>}_{vanilla}) = \mathbf{a}^{<l-1>} `+"",i;return{c(){i=B(t)},l(a){i=A(a,t)},m(a,d){h(a,i,d)},p:U,d(a){a&&n(i)}}}function mn(P){let t=String.raw`\mathbf{a}^{<l>} = f(\mathbf{a}^{<l-1>} + \xcancel{\mathbf{z}^{<l>}_{skip}})`+"",i;return{c(){i=B(t)},l(a){i=A(a,t)},m(a,d){h(a,i,d)},p:U,d(a){a&&n(i)}}}function un(P){let t,i,a,d,k,y,_,x,g,w,I,o,c,T,C,v,S,s,b,Ce,j,De,Y,gt,F,dt,Me,O,Ue,se,wt,qe,V,He,$e,ct,je,G,Ye,fe,vt,Fe,le,yt,Oe,J,Ve,me,xt,Ge,K,Je,ue,kt,Ke,pe,_t,Qe,Q,Xe,he,bt,Ze,X,et,ge,It,tt,de,Et,nt,Z,rt,we,Rt,at,ee,it,ce,Bt,ot,te,At,xe,Pt,Tt,st,ne,$t,ve,St,ft,re,lt,ye,Nt,mt,ae,Lt,ut;return a=new Ut({props:{$$slots:{default:[Qt]},$$scope:{ctx:P}}}),x=new Jt({props:{type:"reference",id:"1"}}),w=new Ut({props:{$$slots:{default:[Xt]},$$scope:{ctx:P}}}),v=new Le({props:{maxWidth:"300px",$$slots:{default:[Zt]},$$scope:{ctx:P}}}),j=new Le({props:{maxWidth:"300px",$$slots:{default:[en]},$$scope:{ctx:P}}}),F=new oe({props:{$$slots:{default:[tn]},$$scope:{ctx:P}}}),O=new oe({props:{$$slots:{default:[nn]},$$scope:{ctx:P}}}),V=new oe({props:{$$slots:{default:[rn]},$$scope:{ctx:P}}}),G=new oe({props:{$$slots:{default:[an]},$$scope:{ctx:P}}}),J=new Le({props:{maxWidth:"300px",$$slots:{default:[on]},$$scope:{ctx:P}}}),K=new Le({props:{maxWidth:"300px",$$slots:{default:[sn]},$$scope:{ctx:P}}}),Q=new Le({props:{maxWidth:"300px",$$slots:{default:[$n]},$$scope:{ctx:P}}}),X=new oe({props:{$$slots:{default:[fn]},$$scope:{ctx:P}}}),Z=new oe({props:{$$slots:{default:[ln]},$$scope:{ctx:P}}}),ee=new oe({props:{$$slots:{default:[mn]},$$scope:{ctx:P}}}),ne=new qt({props:{code:`class ResBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(HIDDEN, HIDDEN)
        
    def forward(self, features):
        output = F.relu(self.linear(features))
        return features + output
`}}),re=new qt({props:{code:`class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
                nn.Flatten(),
                nn.Linear(NUM_FEATURES, HIDDEN),
                nn.ReLU(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                ResBlock(),
                nn.Linear(HIDDEN, NUM_LABELS),
            )
    
    def forward(self, features):
        return self.layers(features)
`}}),{c(){t=D("p"),i=B(`If we had to pick just one invention in deep learning, that had allowed us
    to train truely deep nearal networks, it would most likely be `),$(a.$$.fragment),d=B(`. This technique is the bread and butter of many modern-day AI researchers
    and practicioners. If you removed skip connections from state of the art
    deep learning architectures, most of them would fall apart. So let's have a
    look at them.`),k=E(),y=D("p"),_=B(`Usually we expect deep neural networks to perform better than their shallow
    counterparts. Deeper architecures have more parameters and should be able to
    model more complex relationships. Yet when we increase the number of layers,
    training becomes impractical and performance deteriorates. While the usual
    suspect is the vanishing gradient problem, He et al.`),$(x.$$.fragment),g=B(" were primarily motivated by the so called "),$(w.$$.fragment),I=B(`, when they developed the ResNet (residual network) architecure. In this
    section we are going to cover both possibilities: we will discuss how skip
    connections might reduce the risk of vanishing gradients and we will discuss
    the degradation problem. As with many other techniques in deep learning, we
    know that a certain architecture works empirically, but often we do not know
    exactly why.`),o=E(),c=D("p"),T=B(`In the arcitectures that we covered so far, data flows from one calculation
    block into the next, from net inputs to activations and vice versa.`),C=E(),$(v.$$.fragment),S=E(),s=D("p"),b=B(`When we add skip connections, we add an additional path for the data to
    flow. Additionally to flowing into the next net input layer directly, the
    output of an activation is routed directly into one of the future activation
    layers. The streams from the net input and a previous activation are joined
    through a simple summation and the sum is used as input into the following
    activation function.`),Ce=E(),$(j.$$.fragment),De=E(),Y=D("p"),gt=B(`Usually when we calculate the output of a neuron, we just pass the net input
    through the activation function `),$(F.$$.fragment),dt=B("."),Me=E(),$(O.$$.fragment),Ue=E(),se=D("p"),wt=B(`With skip connections what we actually calculate are the so called residual
    values.`),qe=E(),$(V.$$.fragment),He=E(),$e=D("p"),ct=B(`The residuals are basically the differences between the actual net inputs
    and the outputs from the previous layer.`),je=E(),$(G.$$.fragment),Ye=E(),fe=D("p"),vt=B(`Theoretically skip connections should produce the same results, because we
    are not changing our task completely, we are just reformulating it. Yet the
    reality is different, because training deep neaural networks with skip
    connections is easier.`),Fe=E(),le=D("p"),yt=B(`Let's imagine we face the usual problem of vanishing gradients. In a certain
    layer the information flow stops, because the gradient gets close to zero.
    Once that happens, all the preceding layers don't get their gradients
    updated due to the chain rule and training essentially stops.`),Oe=E(),$(J.$$.fragment),Ve=E(),me=D("p"),xt=B(`If we have skip connections on the other hand, information can flow through
    the additional connection. That way we can circumvent the dead nodes in the
    neural network and the gradients can keep flowing.`),Ge=E(),$(K.$$.fragment),Je=E(),ue=D("p"),kt=B(`The authors of the ResNet paper argued, that the vanishing gradient problem
    has been solved by modern activation functions, weight inintialization
    schemes and batch normalization. The degradation problem therefore had to
    have a different origin.`),Ke=E(),pe=D("p"),_t=B(`Let's discuss the example below to try to understand the problem. If we
    start with the yellow network and add an additional (blue) layer, we would
    expect the performance to be at least as good as that of the smaller
    (yellow) one.`),Qe=E(),$(Q.$$.fragment),Xe=E(),he=D("p"),bt=B(`If the yellow network has already achieved the best performance, the
    addional layer should learn the identity function.`),Ze=E(),$(X.$$.fragment),et=E(),ge=D("p"),It=B(`That statement should apply, no matter how many additional layer we add.
    Performance should not deteriorate, because the last layers can always learn
    to output the input of the previous layer without change. Yet we know that
    shallow neural networks often outperform their deep counterparts.`),tt=E(),de=D("p"),Et=B(`Maybe it is not as easy to learn the identity function as we imagine. The
    neural network has to find the weights that exactly reproduce the input and
    this is not always a trivial task.`),nt=E(),$(Z.$$.fragment),rt=E(),we=D("p"),Rt=B(`Skip connections on the other hand make it easy for the neural network to
    create an idenity functoin. All the network has to do is to set the weights
    and biases to 0.`),at=E(),$(ee.$$.fragment),it=E(),ce=D("p"),Bt=B(`If we use the ReLU activation function, the equality above will hold,
    because two ReLUs in a row do not change the outcome.`),ot=E(),te=D("p"),At=B(`Impelementing skip connectins in PyTorch is a piece of cake. Below we create
    a new module called `),xe=D("code"),Pt=B("ResBlock"),Tt=B(`. The block implements a skip
    connection by adding the input of the module to the output of the activation
    function.`),st=E(),$(ne.$$.fragment),$t=E(),ve=D("p"),St=B(`We implement our model by stacking 20 of residual blocks and we train the
    model on the MNIST dataset.`),ft=E(),$(re.$$.fragment),lt=E(),ye=D("p"),Nt=B(`While we can observe some overfitting, we do not have any trouble training
    such a deep neaural network.`),mt=E(),ae=D("img"),this.h()},l(e){t=M(e,"P",{});var r=L(t);i=A(r,`If we had to pick just one invention in deep learning, that had allowed us
    to train truely deep nearal networks, it would most likely be `),f(a.$$.fragment,r),d=A(r,`. This technique is the bread and butter of many modern-day AI researchers
    and practicioners. If you removed skip connections from state of the art
    deep learning architectures, most of them would fall apart. So let's have a
    look at them.`),r.forEach(n),k=R(e),y=M(e,"P",{});var H=L(y);_=A(H,`Usually we expect deep neural networks to perform better than their shallow
    counterparts. Deeper architecures have more parameters and should be able to
    model more complex relationships. Yet when we increase the number of layers,
    training becomes impractical and performance deteriorates. While the usual
    suspect is the vanishing gradient problem, He et al.`),f(x.$$.fragment,H),g=A(H," were primarily motivated by the so called "),f(w.$$.fragment,H),I=A(H,`, when they developed the ResNet (residual network) architecure. In this
    section we are going to cover both possibilities: we will discuss how skip
    connections might reduce the risk of vanishing gradients and we will discuss
    the degradation problem. As with many other techniques in deep learning, we
    know that a certain architecture works empirically, but often we do not know
    exactly why.`),H.forEach(n),o=R(e),c=M(e,"P",{});var ke=L(c);T=A(ke,`In the arcitectures that we covered so far, data flows from one calculation
    block into the next, from net inputs to activations and vice versa.`),ke.forEach(n),C=R(e),f(v.$$.fragment,e),S=R(e),s=M(e,"P",{});var _e=L(s);b=A(_e,`When we add skip connections, we add an additional path for the data to
    flow. Additionally to flowing into the next net input layer directly, the
    output of an activation is routed directly into one of the future activation
    layers. The streams from the net input and a previous activation are joined
    through a simple summation and the sum is used as input into the following
    activation function.`),_e.forEach(n),Ce=R(e),f(j.$$.fragment,e),De=R(e),Y=M(e,"P",{});var ie=L(Y);gt=A(ie,`Usually when we calculate the output of a neuron, we just pass the net input
    through the activation function `),f(F.$$.fragment,ie),dt=A(ie,"."),ie.forEach(n),Me=R(e),f(O.$$.fragment,e),Ue=R(e),se=M(e,"P",{});var be=L(se);wt=A(be,`With skip connections what we actually calculate are the so called residual
    values.`),be.forEach(n),qe=R(e),f(V.$$.fragment,e),He=R(e),$e=M(e,"P",{});var Ie=L($e);ct=A(Ie,`The residuals are basically the differences between the actual net inputs
    and the outputs from the previous layer.`),Ie.forEach(n),je=R(e),f(G.$$.fragment,e),Ye=R(e),fe=M(e,"P",{});var Ee=L(fe);vt=A(Ee,`Theoretically skip connections should produce the same results, because we
    are not changing our task completely, we are just reformulating it. Yet the
    reality is different, because training deep neaural networks with skip
    connections is easier.`),Ee.forEach(n),Fe=R(e),le=M(e,"P",{});var Re=L(le);yt=A(Re,`Let's imagine we face the usual problem of vanishing gradients. In a certain
    layer the information flow stops, because the gradient gets close to zero.
    Once that happens, all the preceding layers don't get their gradients
    updated due to the chain rule and training essentially stops.`),Re.forEach(n),Oe=R(e),f(J.$$.fragment,e),Ve=R(e),me=M(e,"P",{});var Be=L(me);xt=A(Be,`If we have skip connections on the other hand, information can flow through
    the additional connection. That way we can circumvent the dead nodes in the
    neural network and the gradients can keep flowing.`),Be.forEach(n),Ge=R(e),f(K.$$.fragment,e),Je=R(e),ue=M(e,"P",{});var Ae=L(ue);kt=A(Ae,`The authors of the ResNet paper argued, that the vanishing gradient problem
    has been solved by modern activation functions, weight inintialization
    schemes and batch normalization. The degradation problem therefore had to
    have a different origin.`),Ae.forEach(n),Ke=R(e),pe=M(e,"P",{});var Pe=L(pe);_t=A(Pe,`Let's discuss the example below to try to understand the problem. If we
    start with the yellow network and add an additional (blue) layer, we would
    expect the performance to be at least as good as that of the smaller
    (yellow) one.`),Pe.forEach(n),Qe=R(e),f(Q.$$.fragment,e),Xe=R(e),he=M(e,"P",{});var Te=L(he);bt=A(Te,`If the yellow network has already achieved the best performance, the
    addional layer should learn the identity function.`),Te.forEach(n),Ze=R(e),f(X.$$.fragment,e),et=R(e),ge=M(e,"P",{});var Se=L(ge);It=A(Se,`That statement should apply, no matter how many additional layer we add.
    Performance should not deteriorate, because the last layers can always learn
    to output the input of the previous layer without change. Yet we know that
    shallow neural networks often outperform their deep counterparts.`),Se.forEach(n),tt=R(e),de=M(e,"P",{});var Ne=L(de);Et=A(Ne,`Maybe it is not as easy to learn the identity function as we imagine. The
    neural network has to find the weights that exactly reproduce the input and
    this is not always a trivial task.`),Ne.forEach(n),nt=R(e),f(Z.$$.fragment,e),rt=R(e),we=M(e,"P",{});var Wt=L(we);Rt=A(Wt,`Skip connections on the other hand make it easy for the neural network to
    create an idenity functoin. All the network has to do is to set the weights
    and biases to 0.`),Wt.forEach(n),at=R(e),f(ee.$$.fragment,e),it=R(e),ce=M(e,"P",{});var zt=L(ce);Bt=A(zt,`If we use the ReLU activation function, the equality above will hold,
    because two ReLUs in a row do not change the outcome.`),zt.forEach(n),ot=R(e),te=M(e,"P",{});var pt=L(te);At=A(pt,`Impelementing skip connectins in PyTorch is a piece of cake. Below we create
    a new module called `),xe=M(pt,"CODE",{});var Ct=L(xe);Pt=A(Ct,"ResBlock"),Ct.forEach(n),Tt=A(pt,`. The block implements a skip
    connection by adding the input of the module to the output of the activation
    function.`),pt.forEach(n),st=R(e),f(ne.$$.fragment,e),$t=R(e),ve=M(e,"P",{});var Dt=L(ve);St=A(Dt,`We implement our model by stacking 20 of residual blocks and we train the
    model on the MNIST dataset.`),Dt.forEach(n),ft=R(e),f(re.$$.fragment,e),lt=R(e),ye=M(e,"P",{});var Mt=L(ye);Nt=A(Mt,`While we can observe some overfitting, we do not have any trouble training
    such a deep neaural network.`),Mt.forEach(n),mt=R(e),ae=M(e,"IMG",{src:!0,alt:!0}),this.h()},h(){Ot(ae.src,Lt=Kt)||q(ae,"src",Lt),q(ae,"alt","Metrics of a deep neural network with skip connections")},m(e,r){h(e,t,r),N(t,i),l(a,t,null),N(t,d),h(e,k,r),h(e,y,r),N(y,_),l(x,y,null),N(y,g),l(w,y,null),N(y,I),h(e,o,r),h(e,c,r),N(c,T),h(e,C,r),l(v,e,r),h(e,S,r),h(e,s,r),N(s,b),h(e,Ce,r),l(j,e,r),h(e,De,r),h(e,Y,r),N(Y,gt),l(F,Y,null),N(Y,dt),h(e,Me,r),l(O,e,r),h(e,Ue,r),h(e,se,r),N(se,wt),h(e,qe,r),l(V,e,r),h(e,He,r),h(e,$e,r),N($e,ct),h(e,je,r),l(G,e,r),h(e,Ye,r),h(e,fe,r),N(fe,vt),h(e,Fe,r),h(e,le,r),N(le,yt),h(e,Oe,r),l(J,e,r),h(e,Ve,r),h(e,me,r),N(me,xt),h(e,Ge,r),l(K,e,r),h(e,Je,r),h(e,ue,r),N(ue,kt),h(e,Ke,r),h(e,pe,r),N(pe,_t),h(e,Qe,r),l(Q,e,r),h(e,Xe,r),h(e,he,r),N(he,bt),h(e,Ze,r),l(X,e,r),h(e,et,r),h(e,ge,r),N(ge,It),h(e,tt,r),h(e,de,r),N(de,Et),h(e,nt,r),l(Z,e,r),h(e,rt,r),h(e,we,r),N(we,Rt),h(e,at,r),l(ee,e,r),h(e,it,r),h(e,ce,r),N(ce,Bt),h(e,ot,r),h(e,te,r),N(te,At),N(te,xe),N(xe,Pt),N(te,Tt),h(e,st,r),l(ne,e,r),h(e,$t,r),h(e,ve,r),N(ve,St),h(e,ft,r),l(re,e,r),h(e,lt,r),h(e,ye,r),N(ye,Nt),h(e,mt,r),h(e,ae,r),ut=!0},p(e,r){const H={};r&2&&(H.$$scope={dirty:r,ctx:e}),a.$set(H);const ke={};r&2&&(ke.$$scope={dirty:r,ctx:e}),w.$set(ke);const _e={};r&2&&(_e.$$scope={dirty:r,ctx:e}),v.$set(_e);const ie={};r&2&&(ie.$$scope={dirty:r,ctx:e}),j.$set(ie);const be={};r&2&&(be.$$scope={dirty:r,ctx:e}),F.$set(be);const Ie={};r&2&&(Ie.$$scope={dirty:r,ctx:e}),O.$set(Ie);const Ee={};r&2&&(Ee.$$scope={dirty:r,ctx:e}),V.$set(Ee);const Re={};r&2&&(Re.$$scope={dirty:r,ctx:e}),G.$set(Re);const Be={};r&2&&(Be.$$scope={dirty:r,ctx:e}),J.$set(Be);const Ae={};r&2&&(Ae.$$scope={dirty:r,ctx:e}),K.$set(Ae);const Pe={};r&2&&(Pe.$$scope={dirty:r,ctx:e}),Q.$set(Pe);const Te={};r&2&&(Te.$$scope={dirty:r,ctx:e}),X.$set(Te);const Se={};r&2&&(Se.$$scope={dirty:r,ctx:e}),Z.$set(Se);const Ne={};r&2&&(Ne.$$scope={dirty:r,ctx:e}),ee.$set(Ne)},i(e){ut||(m(a.$$.fragment,e),m(x.$$.fragment,e),m(w.$$.fragment,e),m(v.$$.fragment,e),m(j.$$.fragment,e),m(F.$$.fragment,e),m(O.$$.fragment,e),m(V.$$.fragment,e),m(G.$$.fragment,e),m(J.$$.fragment,e),m(K.$$.fragment,e),m(Q.$$.fragment,e),m(X.$$.fragment,e),m(Z.$$.fragment,e),m(ee.$$.fragment,e),m(ne.$$.fragment,e),m(re.$$.fragment,e),ut=!0)},o(e){u(a.$$.fragment,e),u(x.$$.fragment,e),u(w.$$.fragment,e),u(v.$$.fragment,e),u(j.$$.fragment,e),u(F.$$.fragment,e),u(O.$$.fragment,e),u(V.$$.fragment,e),u(G.$$.fragment,e),u(J.$$.fragment,e),u(K.$$.fragment,e),u(Q.$$.fragment,e),u(X.$$.fragment,e),u(Z.$$.fragment,e),u(ee.$$.fragment,e),u(ne.$$.fragment,e),u(re.$$.fragment,e),ut=!1},d(e){e&&n(t),p(a),e&&n(k),e&&n(y),p(x),p(w),e&&n(o),e&&n(c),e&&n(C),p(v,e),e&&n(S),e&&n(s),e&&n(Ce),p(j,e),e&&n(De),e&&n(Y),p(F),e&&n(Me),p(O,e),e&&n(Ue),e&&n(se),e&&n(qe),p(V,e),e&&n(He),e&&n($e),e&&n(je),p(G,e),e&&n(Ye),e&&n(fe),e&&n(Fe),e&&n(le),e&&n(Oe),p(J,e),e&&n(Ve),e&&n(me),e&&n(Ge),p(K,e),e&&n(Je),e&&n(ue),e&&n(Ke),e&&n(pe),e&&n(Qe),p(Q,e),e&&n(Xe),e&&n(he),e&&n(Ze),p(X,e),e&&n(et),e&&n(ge),e&&n(tt),e&&n(de),e&&n(nt),p(Z,e),e&&n(rt),e&&n(we),e&&n(at),p(ee,e),e&&n(it),e&&n(ce),e&&n(ot),e&&n(te),e&&n(st),p(ne,e),e&&n($t),e&&n(ve),e&&n(ft),p(re,e),e&&n(lt),e&&n(ye),e&&n(mt),e&&n(ae)}}}function pn(P){let t,i,a,d,k,y,_,x,g,w,I;return x=new Vt({props:{$$slots:{default:[un]},$$scope:{ctx:P}}}),w=new Gt({props:{references:P[0]}}),{c(){t=D("meta"),i=E(),a=D("h1"),d=B("Skip Connections"),k=E(),y=D("div"),_=E(),$(x.$$.fragment),g=E(),$(w.$$.fragment),this.h()},l(o){const c=Ft("svelte-21muhf",document.head);t=M(c,"META",{name:!0,content:!0}),c.forEach(n),i=R(o),a=M(o,"H1",{});var T=L(a);d=A(T,"Skip Connections"),T.forEach(n),k=R(o),y=M(o,"DIV",{class:!0}),L(y).forEach(n),_=R(o),f(x.$$.fragment,o),g=R(o),f(w.$$.fragment,o),this.h()},h(){document.title="Skip Connections - World4AI",q(t,"name","description"),q(t,"content","Skip connections allow us to train very deep neural networks. This architecture alleviates the vanishing gradient problem and deals with the degradation problem at the same time."),q(y,"class","separator")},m(o,c){N(document.head,t),h(o,i,c),h(o,a,c),N(a,d),h(o,k,c),h(o,y,c),h(o,_,c),l(x,o,c),h(o,g,c),l(w,o,c),I=!0},p(o,[c]){const T={};c&2&&(T.$$scope={dirty:c,ctx:o}),x.$set(T)},i(o){I||(m(x.$$.fragment,o),m(w.$$.fragment,o),I=!0)},o(o){u(x.$$.fragment,o),u(w.$$.fragment,o),I=!1},d(o){n(t),o&&n(i),o&&n(a),o&&n(k),o&&n(y),o&&n(_),p(x,o),o&&n(g),p(w,o)}}}function hn(P){return[[{author:"K. He, X. Zhang, S. Ren and J. Sun",title:"Deep Residual Learning for Image Recognition",journal:"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",year:"2016",pages:"770-778",volume:"",issue:""}]]}class In extends Ht{constructor(t){super(),jt(this,t,hn,pn,Yt,{})}}export{In as default};
