import{S as J,i as K,s as L,k as C,a as S,q,y as N,W as O,l as A,h as r,c as B,m as c,r as V,z as Q,n as e,N as p,b as $,A as R,g as j,d as F,B as G,Q as f,R as m,C as U}from"../chunks/index.4d92b023.js";import{C as X}from"../chunks/Container.b0705c7b.js";import{S as Y}from"../chunks/SvgContainer.f70b5745.js";import{H as Z}from"../chunks/Highlight.b7c1de53.js";function ee(D){let o,i,a,x,d,v,g,s,l,u,y,t,n,h,_,E,w,k,b,M;return{c(){o=f("svg"),i=f("g"),a=f("g"),x=f("path"),d=f("path"),v=f("path"),g=f("path"),s=f("path"),l=f("path"),u=f("g"),y=f("ellipse"),t=f("ellipse"),n=f("ellipse"),h=f("g"),_=f("ellipse"),E=f("ellipse"),w=f("ellipse"),k=f("ellipse"),b=f("ellipse"),M=f("ellipse"),this.h()},l(z){o=m(z,"svg",{version:!0,viewBox:!0,xmlns:!0});var H=c(o);i=m(H,"g",{stroke:!0});var T=c(i);a=m(T,"g",{id:!0,class:!0,"stroke-width":!0});var P=c(a);x=m(P,"path",{d:!0}),c(x).forEach(r),d=m(P,"path",{d:!0}),c(d).forEach(r),v=m(P,"path",{d:!0}),c(v).forEach(r),g=m(P,"path",{d:!0}),c(g).forEach(r),s=m(P,"path",{d:!0}),c(s).forEach(r),l=m(P,"path",{d:!0}),c(l).forEach(r),P.forEach(r),u=m(T,"g",{id:!0,class:!0,"stroke-dasharray":!0,"stroke-linecap":!0});var W=c(u);y=m(W,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0,opacity:!0}),c(y).forEach(r),t=m(W,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0}),c(t).forEach(r),n=m(W,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0}),c(n).forEach(r),W.forEach(r),h=m(T,"g",{id:!0,class:!0,"stroke-linecap":!0,stroke:!0});var I=c(h);_=m(I,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0,opacity:!0}),c(_).forEach(r),E=m(I,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0}),c(E).forEach(r),w=m(I,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0}),c(w).forEach(r),k=m(I,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0,opacity:!0}),c(k).forEach(r),b=m(I,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0}),c(b).forEach(r),M=m(I,"ellipse",{id:!0,cx:!0,cy:!0,rx:!0,ry:!0}),c(M).forEach(r),I.forEach(r),T.forEach(r),H.forEach(r),this.h()},h(){e(x,"d","m250 34 100 130"),e(d,"d","m250 34-100 120"),e(v,"d","m150 154-110 160"),e(g,"d","m150 164 100 150"),e(s,"d","m350 164 100 150"),e(l,"d","m350 164-100 150"),e(a,"id","connections"),e(a,"class","stroke-black fill-none"),e(a,"stroke-width","1px"),e(y,"id","left-action-focus"),e(y,"cx","144.72"),e(y,"cy","157.52"),e(y,"rx","25"),e(y,"ry","25"),e(y,"opacity","0.7"),e(t,"id","left-action"),e(t,"cx","144.72"),e(t,"cy","157.52"),e(t,"rx","16.504"),e(t,"ry","16.504"),e(n,"id","right-action"),e(n,"cx","349.98"),e(n,"cy","160.66"),e(n,"rx","16.504"),e(n,"ry","16.504"),e(u,"id","actions"),e(u,"class","fill-blue-200 stroke-black"),e(u,"stroke-dasharray","1, 1"),e(u,"stroke-linecap","round"),e(_,"id","top-state-focus"),e(_,"cx","249.5"),e(_,"cy","36.616"),e(_,"rx","35"),e(_,"ry","35"),e(_,"opacity","0.2"),e(E,"id","top-state"),e(E,"cx","249.5"),e(E,"cy","36.616"),e(E,"rx","24.5"),e(E,"ry","24.5"),e(w,"id","left-state"),e(w,"cx","44.173"),e(w,"cy","313.73"),e(w,"rx","24.5"),e(w,"ry","24.5"),e(k,"id","mid-state-focus"),e(k,"cx","248.06"),e(k,"cy","313.73"),e(k,"rx","35"),e(k,"ry","35"),e(k,"opacity","0.2"),e(b,"id","mid-state"),e(b,"cx","248.06"),e(b,"cy","313.73"),e(b,"rx","24.5"),e(b,"ry","24.5"),e(M,"id","right-state"),e(M,"cx","448.45"),e(M,"cy","313.73"),e(M,"rx","24.5"),e(M,"ry","24.5"),e(h,"id","states"),e(h,"class","fill-slate-800"),e(h,"stroke-linecap","round"),e(h,"stroke","none"),e(i,"stroke","#000"),e(o,"version","1.1"),e(o,"viewBox","0 0 500 350"),e(o,"xmlns","http://www.w3.org/2000/svg")},m(z,H){$(z,o,H),p(o,i),p(i,a),p(a,x),p(a,d),p(a,v),p(a,g),p(a,s),p(a,l),p(i,u),p(u,y),p(u,t),p(u,n),p(i,h),p(h,_),p(h,E),p(h,w),p(h,k),p(h,b),p(h,M)},p:U,d(z){z&&r(o)}}}function te(D){let o;return{c(){o=q("Markov Decision Process")},l(i){o=V(i,"Markov Decision Process")},m(i,a){$(i,o,a)},d(i){i&&r(o)}}}function re(D){let o,i,a,x,d,v,g;return o=new Y({props:{maxWidth:"300px",$$slots:{default:[ee]},$$scope:{ctx:D}}}),d=new Z({props:{$$slots:{default:[te]},$$scope:{ctx:D}}}),{c(){N(o.$$.fragment),i=S(),a=C("p"),x=q(`In order to find an optimal solution to a reinforcement learning problem it
    is essential to formalize the problem in a mathematical framework. This
    allows researchers to study the properties of the problem and to develop
    algorithms to solve the problem. In reinforcement learning the tool that is
    used for this purpos is the `),N(d.$$.fragment),v=q(`,
    often abbrevieated as MDP. Many of the components of the Markov decision
    process were already covered in the previous chapter, but while the focus of
    the previous chapter was the intuition, this chapter is going to develop the
    necessary mathematical foundation.`)},l(s){Q(o.$$.fragment,s),i=B(s),a=A(s,"P",{});var l=c(a);x=V(l,`In order to find an optimal solution to a reinforcement learning problem it
    is essential to formalize the problem in a mathematical framework. This
    allows researchers to study the properties of the problem and to develop
    algorithms to solve the problem. In reinforcement learning the tool that is
    used for this purpos is the `),Q(d.$$.fragment,l),v=V(l,`,
    often abbrevieated as MDP. Many of the components of the Markov decision
    process were already covered in the previous chapter, but while the focus of
    the previous chapter was the intuition, this chapter is going to develop the
    necessary mathematical foundation.`),l.forEach(r)},m(s,l){R(o,s,l),$(s,i,l),$(s,a,l),p(a,x),R(d,a,null),p(a,v),g=!0},p(s,l){const u={};l&1&&(u.$$scope={dirty:l,ctx:s}),o.$set(u);const y={};l&1&&(y.$$scope={dirty:l,ctx:s}),d.$set(y)},i(s){g||(j(o.$$.fragment,s),j(d.$$.fragment,s),g=!0)},o(s){F(o.$$.fragment,s),F(d.$$.fragment,s),g=!1},d(s){G(o,s),s&&r(i),s&&r(a),G(d)}}}function se(D){let o,i,a,x,d,v,g,s,l,u,y;return s=new X({props:{$$slots:{default:[re]},$$scope:{ctx:D}}}),{c(){o=C("meta"),i=S(),a=C("h1"),x=q("Markov Decision Process"),d=S(),v=C("div"),g=S(),N(s.$$.fragment),l=S(),u=C("div"),this.h()},l(t){const n=O("svelte-9o2zma",document.head);o=A(n,"META",{name:!0,content:!0}),n.forEach(r),i=B(t),a=A(t,"H1",{});var h=c(a);x=V(h,"Markov Decision Process"),h.forEach(r),d=B(t),v=A(t,"DIV",{class:!0}),c(v).forEach(r),g=B(t),Q(s.$$.fragment,t),l=B(t),u=A(t,"DIV",{class:!0}),c(u).forEach(r),this.h()},h(){document.title="Markov Decision Process - World4AI",e(o,"name","description"),e(o,"content","In reinforcement learning the Markov decison process is the mathematical formalization of the agent environment interaction."),e(v,"class","separator"),e(u,"class","separator")},m(t,n){p(document.head,o),$(t,i,n),$(t,a,n),p(a,x),$(t,d,n),$(t,v,n),$(t,g,n),R(s,t,n),$(t,l,n),$(t,u,n),y=!0},p(t,[n]){const h={};n&1&&(h.$$scope={dirty:n,ctx:t}),s.$set(h)},i(t){y||(j(s.$$.fragment,t),y=!0)},o(t){F(s.$$.fragment,t),y=!1},d(t){r(o),t&&r(i),t&&r(a),t&&r(d),t&&r(v),t&&r(g),G(s,t),t&&r(l),t&&r(u)}}}class ne extends J{constructor(o){super(),K(this,o,null,se,L,{})}}export{ne as default};
