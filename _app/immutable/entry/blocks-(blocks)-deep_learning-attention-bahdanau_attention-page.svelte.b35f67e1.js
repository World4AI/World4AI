import{S as mt,i as dt,s as pt,k as H,a as q,q as y,y as m,W as _t,l as V,h as $,c as C,m as W,r as k,z as d,n as L,N as S,b as w,A as p,g as c,d as g,B as _,Q as he,R as ue,C as T,P as ze,e as we}from"../chunks/index.4d92b023.js";import{C as wt}from"../chunks/Container.b0705c7b.js";import{F as vt,I as xt}from"../chunks/InternalLink.7deb899c.js";import{L as z}from"../chunks/Latex.e0b308c0.js";import{H as gt}from"../chunks/Highlight.b7c1de53.js";import{S as We}from"../chunks/SvgContainer.f70b5745.js";import{B as F}from"../chunks/Block.059eddcd.js";import{A as N}from"../chunks/Arrow.ae91874c.js";function yt(l,e,t){const n=l.slice();return n[1]=e[t],n[3]=t,n}function kt(l,e,t){const n=l.slice();return n[1]=e[t],n[3]=t,n}function bt(l,e,t){const n=l.slice();return n[1]=e[t],n[3]=t,n}function At(l,e,t){const n=l.slice();return n[1]=e[t],n[3]=t,n}function Bt(l){let e;return{c(){e=y("h_4")},l(t){e=k(t,"h_4")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function St(l){let e,t,n,a,s,o,i;return t=new N({props:{strokeWidth:"2",data:[{x:31,y:45},{x:76,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),n=new N({props:{strokeWidth:"2",data:[{x:100,y:62},{x:100,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),a=new F({props:{x:"100",y:"45",width:"30",height:"30",class:"fill-slate-500"}}),s=new F({props:{text:"x_"+(l[3]+1),type:"latex",fontSize:12,x:"15",y:"45",width:"25",height:"25",class:"fill-blue-100"}}),o=new F({props:{type:"latex",text:"h_"+(l[3]+1),fontSize:12,x:"100",y:"100",width:"25",height:"25",class:"fill-yellow-100"}}),{c(){e=he("g"),m(t.$$.fragment),m(n.$$.fragment),m(a.$$.fragment),m(s.$$.fragment),m(o.$$.fragment),this.h()},l(u){e=ue(u,"g",{transform:!0});var h=W(e);d(t.$$.fragment,h),d(n.$$.fragment,h),d(a.$$.fragment,h),d(s.$$.fragment,h),d(o.$$.fragment,h),h.forEach($),this.h()},h(){L(e,"transform","translate(0, "+(l[3]*120-20)+")")},m(u,h){w(u,e,h),p(t,e,null),p(n,e,null),p(a,e,null),p(s,e,null),p(o,e,null),i=!0},p:T,i(u){i||(c(t.$$.fragment,u),c(n.$$.fragment,u),c(a.$$.fragment,u),c(s.$$.fragment,u),c(o.$$.fragment,u),i=!0)},o(u){g(t.$$.fragment,u),g(n.$$.fragment,u),g(a.$$.fragment,u),g(s.$$.fragment,u),g(o.$$.fragment,u),i=!1},d(u){u&&$(e),_(t),_(n),_(a),_(s),_(o)}}}function jt(l){let e,t,n=Array(4),a=[];for(let s=0;s<n.length;s+=1)a[s]=St(At(l,n,s));return{c(){e=he("svg");for(let s=0;s<a.length;s+=1)a[s].c();this.h()},l(s){e=ue(s,"svg",{viewBox:!0});var o=W(e);for(let i=0;i<a.length;i+=1)a[i].l(o);o.forEach($),this.h()},h(){L(e,"viewBox","0 0 200 470")},m(s,o){w(s,e,o);for(let i=0;i<a.length;i+=1)a[i]&&a[i].m(e,null);t=!0},p:T,i(s){if(!t){for(let o=0;o<n.length;o+=1)c(a[o]);t=!0}},o(s){a=a.filter(Boolean);for(let o=0;o<a.length;o+=1)g(a[o]);t=!1},d(s){s&&$(e),ze(a,s)}}}function Et(l){let e,t;return e=new N({props:{strokeWidth:"2",data:[{x:100,y:62},{x:100,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),{c(){m(e.$$.fragment)},l(n){d(e.$$.fragment,n)},m(n,a){p(e,n,a),t=!0},i(n){t||(c(e.$$.fragment,n),t=!0)},o(n){g(e.$$.fragment,n),t=!1},d(n){_(e,n)}}}function It(l){let e,t;return e=new N({props:{strokeWidth:"2",data:[{x:100,y:0},{x:100,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),{c(){m(e.$$.fragment)},l(n){d(e.$$.fragment,n)},m(n,a){p(e,n,a),t=!0},i(n){t||(c(e.$$.fragment,n),t=!0)},o(n){g(e.$$.fragment,n),t=!1},d(n){_(e,n)}}}function Dt(l){let e,t,n,a,s,o,i,u,h,b,I;t=new N({props:{strokeWidth:"2",data:[{x:120,y:45},{x:164,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),n=new N({props:{strokeWidth:"2",data:[{x:31,y:45},{x:76,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}});const x=[It,Et],E=[];function D(j,v){return j[3]===0?0:1}return a=D(l),s=E[a]=x[a](l),i=new F({props:{x:"100",y:"45",width:"30",height:"30",class:"fill-slate-500"}}),u=new F({props:{type:"latex",text:"y_"+(l[3]+1),fontSize:12,x:"185",y:"45",width:"25",height:"25",class:"fill-lime-200"}}),h=new F({props:{text:"y_"+l[3],type:"latex",fontSize:12,x:"15",y:"45",width:"25",height:"25",class:"fill-lime-200"}}),b=new F({props:{type:"latex",text:"s_"+(l[3]+1),fontSize:12,x:"100",y:"100",width:"25",height:"25",class:"fill-orange-200"}}),{c(){e=he("g"),m(t.$$.fragment),m(n.$$.fragment),s.c(),o=we(),m(i.$$.fragment),m(u.$$.fragment),m(h.$$.fragment),m(b.$$.fragment),this.h()},l(j){e=ue(j,"g",{transform:!0});var v=W(e);d(t.$$.fragment,v),d(n.$$.fragment,v),s.l(v),o=we(),d(i.$$.fragment,v),d(u.$$.fragment,v),d(h.$$.fragment,v),d(b.$$.fragment,v),v.forEach($),this.h()},h(){L(e,"transform","translate(0, "+(l[3]*120-20)+")")},m(j,v){w(j,e,v),p(t,e,null),p(n,e,null),E[a].m(e,null),S(e,o),p(i,e,null),p(u,e,null),p(h,e,null),p(b,e,null),I=!0},p:T,i(j){I||(c(t.$$.fragment,j),c(n.$$.fragment,j),c(s),c(i.$$.fragment,j),c(u.$$.fragment,j),c(h.$$.fragment,j),c(b.$$.fragment,j),I=!0)},o(j){g(t.$$.fragment,j),g(n.$$.fragment,j),g(s),g(i.$$.fragment,j),g(u.$$.fragment,j),g(h.$$.fragment,j),g(b.$$.fragment,j),I=!1},d(j){j&&$(e),_(t),_(n),E[a].d(),_(i),_(u),_(h),_(b)}}}function Wt(l){let e,t,n=Array(2),a=[];for(let s=0;s<n.length;s+=1)a[s]=Dt(bt(l,n,s));return{c(){e=he("svg");for(let s=0;s<a.length;s+=1)a[s].c();this.h()},l(s){e=ue(s,"svg",{viewBox:!0});var o=W(e);for(let i=0;i<a.length;i+=1)a[i].l(o);o.forEach($),this.h()},h(){L(e,"viewBox","0 0 200 250")},m(s,o){w(s,e,o);for(let i=0;i<a.length;i+=1)a[i]&&a[i].m(e,null);t=!0},p:T,i(s){if(!t){for(let o=0;o<n.length;o+=1)c(a[o]);t=!0}},o(s){a=a.filter(Boolean);for(let o=0;o<a.length;o+=1)g(a[o]);t=!1},d(s){s&&$(e),ze(a,s)}}}function zt(l){let e;return{c(){e=y("Bahdanau Attention")},l(t){e=k(t,"Bahdanau Attention")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function Tt(l){let e;return{c(){e=y("c_i")},l(t){e=k(t,"c_i")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function Pt(l){let e,t,n,a,s,o,i,u;return t=new N({props:{strokeWidth:"2",data:[{x:31,y:45},{x:76,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),n=new N({props:{strokeWidth:"2",data:[{x:100,y:62},{x:100,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),a=new N({props:{strokeWidth:"2",data:[{x:100,y:100},{x:180,y:250-l[3]*115}],dashed:!0,moving:!0,showMarker:!1,strokeDashArray:"4 4"}}),s=new F({props:{x:"100",y:"45",width:"30",height:"30",class:"fill-slate-500"}}),o=new F({props:{text:"x_"+(l[3]+1),type:"latex",fontSize:12,x:"15",y:"45",width:"25",height:"25",class:"fill-blue-100"}}),i=new F({props:{type:"latex",text:"h_"+(l[3]+1),fontSize:12,x:"100",y:"100",width:"25",height:"25",class:"fill-yellow-100"}}),{c(){e=he("g"),m(t.$$.fragment),m(n.$$.fragment),m(a.$$.fragment),m(s.$$.fragment),m(o.$$.fragment),m(i.$$.fragment),this.h()},l(h){e=ue(h,"g",{transform:!0});var b=W(e);d(t.$$.fragment,b),d(n.$$.fragment,b),d(a.$$.fragment,b),d(s.$$.fragment,b),d(o.$$.fragment,b),d(i.$$.fragment,b),b.forEach($),this.h()},h(){L(e,"transform","translate(0, "+(l[3]*120-20)+")")},m(h,b){w(h,e,b),p(t,e,null),p(n,e,null),p(a,e,null),p(s,e,null),p(o,e,null),p(i,e,null),u=!0},p:T,i(h){u||(c(t.$$.fragment,h),c(n.$$.fragment,h),c(a.$$.fragment,h),c(s.$$.fragment,h),c(o.$$.fragment,h),c(i.$$.fragment,h),u=!0)},o(h){g(t.$$.fragment,h),g(n.$$.fragment,h),g(a.$$.fragment,h),g(s.$$.fragment,h),g(o.$$.fragment,h),g(i.$$.fragment,h),u=!1},d(h){h&&$(e),_(t),_(n),_(a),_(s),_(o),_(i)}}}function qt(l){let e,t,n,a,s=Array(4),o=[];for(let i=0;i<s.length;i+=1)o[i]=Pt(kt(l,s,i));return n=new F({props:{x:"180",y:"240",width:"30",height:"30",class:"fill-yellow-200",type:"latex",text:"c_i",fontSize:15}}),{c(){e=he("svg");for(let i=0;i<o.length;i+=1)o[i].c();t=we(),m(n.$$.fragment),this.h()},l(i){e=ue(i,"svg",{viewBox:!0});var u=W(e);for(let h=0;h<o.length;h+=1)o[h].l(u);t=we(),d(n.$$.fragment,u),u.forEach($),this.h()},h(){L(e,"viewBox","0 0 200 470")},m(i,u){w(i,e,u);for(let h=0;h<o.length;h+=1)o[h]&&o[h].m(e,null);S(e,t),p(n,e,null),a=!0},p:T,i(i){if(!a){for(let u=0;u<s.length;u+=1)c(o[u]);c(n.$$.fragment,i),a=!0}},o(i){o=o.filter(Boolean);for(let u=0;u<o.length;u+=1)g(o[u]);g(n.$$.fragment,i),a=!1},d(i){i&&$(e),ze(o,i),_(n)}}}function Ct(l){let e,t;return e=new N({props:{strokeWidth:"2",data:[{x:100,y:62},{x:100,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),{c(){m(e.$$.fragment)},l(n){d(e.$$.fragment,n)},m(n,a){p(e,n,a),t=!0},i(n){t||(c(e.$$.fragment,n),t=!0)},o(n){g(e.$$.fragment,n),t=!1},d(n){_(e,n)}}}function Ft(l){let e,t;return e=new N({props:{strokeWidth:"2",data:[{x:100,y:0},{x:100,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),{c(){m(e.$$.fragment)},l(n){d(e.$$.fragment,n)},m(n,a){p(e,n,a),t=!0},i(n){t||(c(e.$$.fragment,n),t=!0)},o(n){g(e.$$.fragment,n),t=!1},d(n){_(e,n)}}}function Lt(l){let e,t,n,a,s,o,i,u,h,b,I,x;t=new N({props:{strokeWidth:"2",data:[{x:120,y:45},{x:164,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),n=new N({props:{strokeWidth:"2",data:[{x:31,y:45},{x:76,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}});const E=[Ft,Ct],D=[];function j(v,P){return v[3]===0?0:1}return a=j(l),s=D[a]=E[a](l),i=new F({props:{x:"100",y:"45",width:"30",height:"30",class:"fill-slate-500"}}),u=new F({props:{type:"latex",text:"y_"+(l[3]+1),fontSize:12,x:"185",y:"45",width:"25",height:"25",class:"fill-lime-200"}}),h=new F({props:{text:"y_"+l[3],type:"latex",fontSize:12,x:"15",y:"45",width:"25",height:"25",class:"fill-lime-200"}}),b=new F({props:{text:"c_"+(l[3]+1),type:"latex",fontSize:12,x:"15",y:"75",width:"25",height:"25",class:"fill-yellow-200"}}),I=new F({props:{type:"latex",text:"s_"+(l[3]+1),fontSize:12,x:"100",y:"100",width:"25",height:"25",class:"fill-orange-200"}}),{c(){e=he("g"),m(t.$$.fragment),m(n.$$.fragment),s.c(),o=we(),m(i.$$.fragment),m(u.$$.fragment),m(h.$$.fragment),m(b.$$.fragment),m(I.$$.fragment),this.h()},l(v){e=ue(v,"g",{transform:!0});var P=W(e);d(t.$$.fragment,P),d(n.$$.fragment,P),s.l(P),o=we(),d(i.$$.fragment,P),d(u.$$.fragment,P),d(h.$$.fragment,P),d(b.$$.fragment,P),d(I.$$.fragment,P),P.forEach($),this.h()},h(){L(e,"transform","translate(0, "+(l[3]*120-20)+")")},m(v,P){w(v,e,P),p(t,e,null),p(n,e,null),D[a].m(e,null),S(e,o),p(i,e,null),p(u,e,null),p(h,e,null),p(b,e,null),p(I,e,null),x=!0},p:T,i(v){x||(c(t.$$.fragment,v),c(n.$$.fragment,v),c(s),c(i.$$.fragment,v),c(u.$$.fragment,v),c(h.$$.fragment,v),c(b.$$.fragment,v),c(I.$$.fragment,v),x=!0)},o(v){g(t.$$.fragment,v),g(n.$$.fragment,v),g(s),g(i.$$.fragment,v),g(u.$$.fragment,v),g(h.$$.fragment,v),g(b.$$.fragment,v),g(I.$$.fragment,v),x=!1},d(v){v&&$(e),_(t),_(n),D[a].d(),_(i),_(u),_(h),_(b),_(I)}}}function Nt(l){let e,t,n=Array(2),a=[];for(let s=0;s<n.length;s+=1)a[s]=Lt(yt(l,n,s));return{c(){e=he("svg");for(let s=0;s<a.length;s+=1)a[s].c();this.h()},l(s){e=ue(s,"svg",{viewBox:!0});var o=W(e);for(let i=0;i<a.length;i+=1)a[i].l(o);o.forEach($),this.h()},h(){L(e,"viewBox","0 0 200 250")},m(s,o){w(s,e,o);for(let i=0;i<a.length;i+=1)a[i]&&a[i].m(e,null);t=!0},p:T,i(s){if(!t){for(let o=0;o<n.length;o+=1)c(a[o]);t=!0}},o(s){a=a.filter(Boolean);for(let o=0;o<a.length;o+=1)g(a[o]);t=!1},d(s){s&&$(e),ze(a,s)}}}function Ht(l){let e=String.raw`e_{ij} = a(s_{i-1}, h_j)`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function Vt(l){let e=String.raw`s_{i-1}`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function Gt(l){let e;return{c(){e=y("h_j")},l(t){e=k(t,"h_j")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function Mt(l){let e=String.raw`s_{i-1}`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function Ot(l){let e;return{c(){e=y("s_2")},l(t){e=k(t,"s_2")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function Rt(l){let e=String.raw`e_{21} = a(s_{1}, h_1)`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function Kt(l){let e=String.raw`e_{22} = a(s_{1}, h_2)`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function Qt(l){let e=String.raw`e_{23} = a(s_{1}, h_3)`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function Yt(l){let e=String.raw`e_{23} = a(s_{1}, h_4)`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function Jt(l){let e;return{c(){e=y("a")},l(t){e=k(t,"a")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function Ut(l){let e;return{c(){e=y("\\alpha")},l(t){e=k(t,"\\alpha")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function Xt(l){let e;return{c(){e=y("i")},l(t){e=k(t,"i")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function Zt(l){let e;return{c(){e=y("j")},l(t){e=k(t,"j")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function en(l){let e=String.raw`\alpha_{ij} = \dfrac{\exp(e_{ij})}{\sum_j \exp(e_{ij})}`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function tn(l){let e;return{c(){e=y("context vector")},l(t){e=k(t,"context vector")},m(t,n){w(t,e,n)},d(t){t&&$(e)}}}function nn(l){let e=String.raw`c_i = \sum_j \alpha_{ij} h_j`+"",t;return{c(){t=y(e)},l(n){t=k(n,e)},m(n,a){w(n,t,a)},p:T,d(n){n&&$(t)}}}function rn(l){let e,t,n,a,s,o,i,u,h,b,I,x,E,D,j,v,P,M,ce,Te,O,Pe,be,R,Ae,_e,qe,Be,K,Se,A,Ce,Q,Fe,Y,Le,J,Ne,U,He,X,Ve,Z,Ge,ee,Me,te,Oe,ne,Re,re,Ke,ae,Qe,se,Ye,oe,Je,je,ge,le,Ee,ie,Ue,fe,Xe,Ie,me,$e,De;return o=new z({props:{$$slots:{default:[Bt]},$$scope:{ctx:l}}}),h=new We({props:{maxWidth:"250px",$$slots:{default:[jt]},$$scope:{ctx:l}}}),D=new We({props:{maxWidth:"250px",$$slots:{default:[Wt]},$$scope:{ctx:l}}}),M=new gt({props:{$$slots:{default:[zt]},$$scope:{ctx:l}}}),ce=new xt({props:{id:1,type:"reference"}}),O=new z({props:{$$slots:{default:[Tt]},$$scope:{ctx:l}}}),R=new We({props:{maxWidth:"250px",$$slots:{default:[qt]},$$scope:{ctx:l}}}),K=new We({props:{maxWidth:"250px",$$slots:{default:[Nt]},$$scope:{ctx:l}}}),Q=new z({props:{$$slots:{default:[Ht]},$$scope:{ctx:l}}}),Y=new z({props:{$$slots:{default:[Vt]},$$scope:{ctx:l}}}),J=new z({props:{$$slots:{default:[Gt]},$$scope:{ctx:l}}}),U=new z({props:{$$slots:{default:[Mt]},$$scope:{ctx:l}}}),X=new z({props:{$$slots:{default:[Ot]},$$scope:{ctx:l}}}),Z=new z({props:{$$slots:{default:[Rt]},$$scope:{ctx:l}}}),ee=new z({props:{$$slots:{default:[Kt]},$$scope:{ctx:l}}}),te=new z({props:{$$slots:{default:[Qt]},$$scope:{ctx:l}}}),ne=new z({props:{$$slots:{default:[Yt]},$$scope:{ctx:l}}}),re=new z({props:{$$slots:{default:[Jt]},$$scope:{ctx:l}}}),ae=new z({props:{$$slots:{default:[Ut]},$$scope:{ctx:l}}}),se=new z({props:{$$slots:{default:[Xt]},$$scope:{ctx:l}}}),oe=new z({props:{$$slots:{default:[Zt]},$$scope:{ctx:l}}}),le=new z({props:{$$slots:{default:[en]},$$scope:{ctx:l}}}),fe=new gt({props:{$$slots:{default:[tn]},$$scope:{ctx:l}}}),$e=new z({props:{$$slots:{default:[nn]},$$scope:{ctx:l}}}),{c(){e=H("p"),t=y(`In order to understand modern attention architectures, it makes sense to
    study the historical context in which these architectures were developed and
    the problems that the new systems tried to solve. For that purpose let's
    remember the encoder-decoder architecture from the last chapter and try to
    figure out in what regard this design might be problematic.`),n=q(),a=H("p"),s=y(`Let's imagine we are trying to solve a translation task with and
    enocder-decoder architecture. The encoder takes the sequence in the original
    language as input and returns a single vector, marked as `),m(o.$$.fragment),i=y(`.
    In other words the whole meaning of the original language is compressed into
    a single vector.`),u=q(),m(h.$$.fragment),b=q(),I=H("p"),x=y(`The decoder uses this hidden vector and previously generated words as input
    and generates a translation one word at a time. As the hidden vector moves
    through the decoder it gets modified and the original meaning of the
    sentence gets more and more diluted. By the time this vector arrives at the
    end of the decoder, hardly anything is left from the input language and the
    translation quality suffers.`),E=q(),m(D.$$.fragment),j=q(),v=H("p"),P=y(`In order to tackle the above problem Dzmitry Bahdanau and his colleagues
    developed the so called `),m(M.$$.fragment),m(ce.$$.fragment),Te=y(`. The authors had a simple yet
    powerful idea to take all outputs from the encoder as inputs into the
    decoder at each step of the decoding process and thus reduced the
    information bottleneck that results from relying on solely one vector.
    Obviously not all encoder outputs are relevant equally for each part of the
    decoder section. So at each step of the decoding (translation) process the
    decoder pays attention to certain parts of the encoder outputs and weighs
    each accordingly. The weighted sum is eventually used as the input into the
    decoder network. In this section we will refer to this variable as the
    context `),m(O.$$.fragment),Pe=y("."),be=q(),m(R.$$.fragment),Ae=q(),_e=H("p"),qe=y(`There might be different strategies to use the context as an input to the
    decoder. In our implementation we simply concatenate the previous decoder
    output with the context and use that as the input.`),Be=q(),m(K.$$.fragment),Se=q(),A=H("p"),Ce=y(`In order to calculate the context we have to take a series of steps. In the
    very first step we calculate the so called energy, `),m(Q.$$.fragment),Fe=y(` . At each decoding/translation step we measure the engergy between the previously
    generated hidden state of the decoder `),m(Y.$$.fragment),Le=y(` and
    each of the enocder outputs. The energy measures the the strength of the connection
    between a decoder output `),m(J.$$.fragment),Ne=y(" and the previous decoder output "),m(U.$$.fragment),He=y(`. Given that we have 4 encoder outputs if we want to generate the energies
    needed for the
    `),m(X.$$.fragment),Ve=y(" state, we calculte the following energies: "),m(Z.$$.fragment),Ge=y(", "),m(ee.$$.fragment),Me=y(", "),m(te.$$.fragment),Oe=y(" and "),m(ne.$$.fragment),Re=y(` . The higher the energy,
    the higher the attention to that particular encoder output is going to be. The
    function `),m(re.$$.fragment),Ke=y(` is implemented as a neural network, that is trained
    jointly with other parts of the whole encoder-decoder architecture. Calculating
    the actual attention weights `),m(ae.$$.fragment),Qe=y(" for the decoder input "),m(se.$$.fragment),Ye=y(" towards the encoder output "),m(oe.$$.fragment),Je=y(` is just a matter of using the engergies
    as an input into the softmax function.`),je=q(),ge=H("div"),m(le.$$.fragment),Ee=q(),ie=H("p"),Ue=y(`Finally we use the attention weights to calculate the weighted sum of
    encoder outputs, the `),m(fe.$$.fragment),Xe=y(`, which is used as
    the input into the decoder together with the previously generated word
    token.`),Ie=q(),me=H("div"),m($e.$$.fragment),this.h()},l(r){e=V(r,"P",{});var f=W(e);t=k(f,`In order to understand modern attention architectures, it makes sense to
    study the historical context in which these architectures were developed and
    the problems that the new systems tried to solve. For that purpose let's
    remember the encoder-decoder architecture from the last chapter and try to
    figure out in what regard this design might be problematic.`),f.forEach($),n=C(r),a=V(r,"P",{});var de=W(a);s=k(de,`Let's imagine we are trying to solve a translation task with and
    enocder-decoder architecture. The encoder takes the sequence in the original
    language as input and returns a single vector, marked as `),d(o.$$.fragment,de),i=k(de,`.
    In other words the whole meaning of the original language is compressed into
    a single vector.`),de.forEach($),u=C(r),d(h.$$.fragment,r),b=C(r),I=V(r,"P",{});var ve=W(I);x=k(ve,`The decoder uses this hidden vector and previously generated words as input
    and generates a translation one word at a time. As the hidden vector moves
    through the decoder it gets modified and the original meaning of the
    sentence gets more and more diluted. By the time this vector arrives at the
    end of the decoder, hardly anything is left from the input language and the
    translation quality suffers.`),ve.forEach($),E=C(r),d(D.$$.fragment,r),j=C(r),v=V(r,"P",{});var G=W(v);P=k(G,`In order to tackle the above problem Dzmitry Bahdanau and his colleagues
    developed the so called `),d(M.$$.fragment,G),d(ce.$$.fragment,G),Te=k(G,`. The authors had a simple yet
    powerful idea to take all outputs from the encoder as inputs into the
    decoder at each step of the decoding process and thus reduced the
    information bottleneck that results from relying on solely one vector.
    Obviously not all encoder outputs are relevant equally for each part of the
    decoder section. So at each step of the decoding (translation) process the
    decoder pays attention to certain parts of the encoder outputs and weighs
    each accordingly. The weighted sum is eventually used as the input into the
    decoder network. In this section we will refer to this variable as the
    context `),d(O.$$.fragment,G),Pe=k(G,"."),G.forEach($),be=C(r),d(R.$$.fragment,r),Ae=C(r),_e=V(r,"P",{});var xe=W(_e);qe=k(xe,`There might be different strategies to use the context as an input to the
    decoder. In our implementation we simply concatenate the previous decoder
    output with the context and use that as the input.`),xe.forEach($),Be=C(r),d(K.$$.fragment,r),Se=C(r),A=V(r,"P",{});var B=W(A);Ce=k(B,`In order to calculate the context we have to take a series of steps. In the
    very first step we calculate the so called energy, `),d(Q.$$.fragment,B),Fe=k(B,` . At each decoding/translation step we measure the engergy between the previously
    generated hidden state of the decoder `),d(Y.$$.fragment,B),Le=k(B,` and
    each of the enocder outputs. The energy measures the the strength of the connection
    between a decoder output `),d(J.$$.fragment,B),Ne=k(B," and the previous decoder output "),d(U.$$.fragment,B),He=k(B,`. Given that we have 4 encoder outputs if we want to generate the energies
    needed for the
    `),d(X.$$.fragment,B),Ve=k(B," state, we calculte the following energies: "),d(Z.$$.fragment,B),Ge=k(B,", "),d(ee.$$.fragment,B),Me=k(B,", "),d(te.$$.fragment,B),Oe=k(B," and "),d(ne.$$.fragment,B),Re=k(B,` . The higher the energy,
    the higher the attention to that particular encoder output is going to be. The
    function `),d(re.$$.fragment,B),Ke=k(B,` is implemented as a neural network, that is trained
    jointly with other parts of the whole encoder-decoder architecture. Calculating
    the actual attention weights `),d(ae.$$.fragment,B),Qe=k(B," for the decoder input "),d(se.$$.fragment,B),Ye=k(B," towards the encoder output "),d(oe.$$.fragment,B),Je=k(B,` is just a matter of using the engergies
    as an input into the softmax function.`),B.forEach($),je=C(r),ge=V(r,"DIV",{class:!0});var ye=W(ge);d(le.$$.fragment,ye),ye.forEach($),Ee=C(r),ie=V(r,"P",{});var pe=W(ie);Ue=k(pe,`Finally we use the attention weights to calculate the weighted sum of
    encoder outputs, the `),d(fe.$$.fragment,pe),Xe=k(pe,`, which is used as
    the input into the decoder together with the previously generated word
    token.`),pe.forEach($),Ie=C(r),me=V(r,"DIV",{class:!0});var ke=W(me);d($e.$$.fragment,ke),ke.forEach($),this.h()},h(){L(ge,"class","flex justify-center"),L(me,"class","flex justify-center")},m(r,f){w(r,e,f),S(e,t),w(r,n,f),w(r,a,f),S(a,s),p(o,a,null),S(a,i),w(r,u,f),p(h,r,f),w(r,b,f),w(r,I,f),S(I,x),w(r,E,f),p(D,r,f),w(r,j,f),w(r,v,f),S(v,P),p(M,v,null),p(ce,v,null),S(v,Te),p(O,v,null),S(v,Pe),w(r,be,f),p(R,r,f),w(r,Ae,f),w(r,_e,f),S(_e,qe),w(r,Be,f),p(K,r,f),w(r,Se,f),w(r,A,f),S(A,Ce),p(Q,A,null),S(A,Fe),p(Y,A,null),S(A,Le),p(J,A,null),S(A,Ne),p(U,A,null),S(A,He),p(X,A,null),S(A,Ve),p(Z,A,null),S(A,Ge),p(ee,A,null),S(A,Me),p(te,A,null),S(A,Oe),p(ne,A,null),S(A,Re),p(re,A,null),S(A,Ke),p(ae,A,null),S(A,Qe),p(se,A,null),S(A,Ye),p(oe,A,null),S(A,Je),w(r,je,f),w(r,ge,f),p(le,ge,null),w(r,Ee,f),w(r,ie,f),S(ie,Ue),p(fe,ie,null),S(ie,Xe),w(r,Ie,f),w(r,me,f),p($e,me,null),De=!0},p(r,f){const de={};f&128&&(de.$$scope={dirty:f,ctx:r}),o.$set(de);const ve={};f&128&&(ve.$$scope={dirty:f,ctx:r}),h.$set(ve);const G={};f&128&&(G.$$scope={dirty:f,ctx:r}),D.$set(G);const xe={};f&128&&(xe.$$scope={dirty:f,ctx:r}),M.$set(xe);const B={};f&128&&(B.$$scope={dirty:f,ctx:r}),O.$set(B);const ye={};f&128&&(ye.$$scope={dirty:f,ctx:r}),R.$set(ye);const pe={};f&128&&(pe.$$scope={dirty:f,ctx:r}),K.$set(pe);const ke={};f&128&&(ke.$$scope={dirty:f,ctx:r}),Q.$set(ke);const Ze={};f&128&&(Ze.$$scope={dirty:f,ctx:r}),Y.$set(Ze);const et={};f&128&&(et.$$scope={dirty:f,ctx:r}),J.$set(et);const tt={};f&128&&(tt.$$scope={dirty:f,ctx:r}),U.$set(tt);const nt={};f&128&&(nt.$$scope={dirty:f,ctx:r}),X.$set(nt);const rt={};f&128&&(rt.$$scope={dirty:f,ctx:r}),Z.$set(rt);const at={};f&128&&(at.$$scope={dirty:f,ctx:r}),ee.$set(at);const st={};f&128&&(st.$$scope={dirty:f,ctx:r}),te.$set(st);const ot={};f&128&&(ot.$$scope={dirty:f,ctx:r}),ne.$set(ot);const lt={};f&128&&(lt.$$scope={dirty:f,ctx:r}),re.$set(lt);const it={};f&128&&(it.$$scope={dirty:f,ctx:r}),ae.$set(it);const ft={};f&128&&(ft.$$scope={dirty:f,ctx:r}),se.$set(ft);const $t={};f&128&&($t.$$scope={dirty:f,ctx:r}),oe.$set($t);const ht={};f&128&&(ht.$$scope={dirty:f,ctx:r}),le.$set(ht);const ut={};f&128&&(ut.$$scope={dirty:f,ctx:r}),fe.$set(ut);const ct={};f&128&&(ct.$$scope={dirty:f,ctx:r}),$e.$set(ct)},i(r){De||(c(o.$$.fragment,r),c(h.$$.fragment,r),c(D.$$.fragment,r),c(M.$$.fragment,r),c(ce.$$.fragment,r),c(O.$$.fragment,r),c(R.$$.fragment,r),c(K.$$.fragment,r),c(Q.$$.fragment,r),c(Y.$$.fragment,r),c(J.$$.fragment,r),c(U.$$.fragment,r),c(X.$$.fragment,r),c(Z.$$.fragment,r),c(ee.$$.fragment,r),c(te.$$.fragment,r),c(ne.$$.fragment,r),c(re.$$.fragment,r),c(ae.$$.fragment,r),c(se.$$.fragment,r),c(oe.$$.fragment,r),c(le.$$.fragment,r),c(fe.$$.fragment,r),c($e.$$.fragment,r),De=!0)},o(r){g(o.$$.fragment,r),g(h.$$.fragment,r),g(D.$$.fragment,r),g(M.$$.fragment,r),g(ce.$$.fragment,r),g(O.$$.fragment,r),g(R.$$.fragment,r),g(K.$$.fragment,r),g(Q.$$.fragment,r),g(Y.$$.fragment,r),g(J.$$.fragment,r),g(U.$$.fragment,r),g(X.$$.fragment,r),g(Z.$$.fragment,r),g(ee.$$.fragment,r),g(te.$$.fragment,r),g(ne.$$.fragment,r),g(re.$$.fragment,r),g(ae.$$.fragment,r),g(se.$$.fragment,r),g(oe.$$.fragment,r),g(le.$$.fragment,r),g(fe.$$.fragment,r),g($e.$$.fragment,r),De=!1},d(r){r&&$(e),r&&$(n),r&&$(a),_(o),r&&$(u),_(h,r),r&&$(b),r&&$(I),r&&$(E),_(D,r),r&&$(j),r&&$(v),_(M),_(ce),_(O),r&&$(be),_(R,r),r&&$(Ae),r&&$(_e),r&&$(Be),_(K,r),r&&$(Se),r&&$(A),_(Q),_(Y),_(J),_(U),_(X),_(Z),_(ee),_(te),_(ne),_(re),_(ae),_(se),_(oe),r&&$(je),r&&$(ge),_(le),r&&$(Ee),r&&$(ie),_(fe),r&&$(Ie),r&&$(me),_($e)}}}function an(l){let e,t,n,a,s,o,i,u,h,b,I;return u=new wt({props:{$$slots:{default:[rn]},$$scope:{ctx:l}}}),b=new vt({props:{references:l[0]}}),{c(){e=H("meta"),t=q(),n=H("h1"),a=y("Bahdanau Attention"),s=q(),o=H("div"),i=q(),m(u.$$.fragment),h=q(),m(b.$$.fragment),this.h()},l(x){const E=_t("svelte-1t26v2z",document.head);e=V(E,"META",{name:!0,content:!0}),E.forEach($),t=C(x),n=V(x,"H1",{});var D=W(n);a=k(D,"Bahdanau Attention"),D.forEach($),s=C(x),o=V(x,"DIV",{class:!0}),W(o).forEach($),i=C(x),d(u.$$.fragment,x),h=C(x),d(b.$$.fragment,x),this.h()},h(){document.title="Bahdanau Attention - World4AI",L(e,"name","description"),L(e,"content","Bahdanau attention builds on the classical encoder-decoder RNN architecture, but instead of taking a single hidden unit input, the decoder calculates attention scores and weights all output of the decoder, thus increasing the performance of the neural network."),L(o,"class","separator")},m(x,E){S(document.head,e),w(x,t,E),w(x,n,E),S(n,a),w(x,s,E),w(x,o,E),w(x,i,E),p(u,x,E),w(x,h,E),p(b,x,E),I=!0},p(x,[E]){const D={};E&128&&(D.$$scope={dirty:E,ctx:x}),u.$set(D)},i(x){I||(c(u.$$.fragment,x),c(b.$$.fragment,x),I=!0)},o(x){g(u.$$.fragment,x),g(b.$$.fragment,x),I=!1},d(x){$(e),x&&$(t),x&&$(n),x&&$(s),x&&$(o),x&&$(i),_(u,x),x&&$(h),_(b,x)}}}function sn(l){return[[{author:"Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio",title:"Neural machine translation by jointly learning to align and translate",journal:"2014"}]]}class mn extends mt{constructor(e){super(),dt(this,e,sn,an,pt,{})}}export{mn as default};
