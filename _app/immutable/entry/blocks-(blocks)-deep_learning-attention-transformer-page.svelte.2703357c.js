import{S as bo,i as vo,s as ko,k as P,a as z,y as g,W as xo,l as q,h as a,c as W,z as c,n as H,N as T,b as f,A as w,g as h,d as u,B as _,q as x,m as S,r as A,Q as ne,R as re,C as Q,e as G,v as at,f as st,P as ee}from"../chunks/index.4d92b023.js";import{C as Ao}from"../chunks/Container.b0705c7b.js";import{F as Eo,I as ao}from"../chunks/InternalLink.7deb899c.js";import{H as ue}from"../chunks/Highlight.b7c1de53.js";import{A as To}from"../chunks/Alert.25a852b3.js";import{L as O}from"../chunks/Latex.e0b308c0.js";import{B as zo}from"../chunks/ButtonContainer.e9aac418.js";import{P as Wo}from"../chunks/PlayButton.85103c5a.js";import{P as hn}from"../chunks/PythonCode.212ba7a6.js";import{S as le}from"../chunks/SvgContainer.f70b5745.js";import{B as N}from"../chunks/Block.059eddcd.js";import{A as M}from"../chunks/Arrow.ae91874c.js";function so(m,t,n){const r=m.slice();return r[6]=t[n],r[8]=n,r}function So(m,t,n){const r=m.slice();return r[6]=t[n],r[10]=n,r}function oo(m,t,n){const r=m.slice();return r[6]=t[n],r[12]=n,r}function Po(m,t,n){const r=m.slice();return r[6]=t[n],r[14]=n,r}function io(m,t,n){const r=m.slice();return r[6]=t[n],r[12]=n,r}function qo(m,t,n){const r=m.slice();return r[6]=t[n],r[14]=n,r}function Do(m,t,n){const r=m.slice();return r[6]=t[n],r[18]=n,r}function Io(m,t,n){const r=m.slice();return r[6]=t[n],r[18]=n,r}function lo(m,t,n){const r=m.slice();return r[20]=t[n],r[8]=n,r}function fo(m,t,n){const r=m.slice();return r[6]=t[n],r[23]=n,r}function No(m,t,n){const r=m.slice();return r[6]=t[n],r[8]=n,r}function $o(m,t,n){const r=m.slice();return r[6]=t[n],r[26]=n,r}function Bo(m,t,n){const r=m.slice();return r[6]=t[n],r[10]=n,r}function mo(m,t,n){const r=m.slice();return r[20]=t[n],r[8]=n,r}function Lo(m){let t;return{c(){t=x("Transformer")},l(n){t=A(n,"Transformer")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function Vo(m){let t,n,r,s,i,d,l,y,v,b,E,$,k,p,L,I,F,C,Y;return n=new N({props:{x:330,y:30,width:100,height:25,text:"Softmax",fontSize:15,color:"none"}}),r=new N({props:{x:330,y:100,width:100,height:25,text:"Linear",fontSize:15,color:"none"}}),s=new N({props:{x:330,y:295,width:100,height:250,text:"Decoder",fontSize:20,class:"fill-blue-200"}}),i=new N({props:{x:70,y:330,width:100,height:180,text:"Encoder",fontSize:20,class:"fill-violet-200"}}),d=new N({props:{x:330,y:485,width:100,height:25,text:"Embedding",fontSize:15,color:"none",class:"fill-slate-300"}}),l=new N({props:{x:70,y:485,width:100,height:25,text:"Embedding",fontSize:15,color:"none",class:"fill-slate-300"}}),y=new N({props:{x:330,y:580,width:100,height:25,text:"Target Text",fontSize:15,color:"none",class:"fill-yellow-100"}}),v=new N({props:{x:70,y:580,width:100,height:25,text:"Source Text",fontSize:15,color:"none",class:"fill-yellow-100"}}),b=new N({props:{x:250,y:295,width:30,height:30,text:"Nx",fontSize:20,class:"fill-blue-200"}}),E=new N({props:{x:150,y:330,width:30,height:30,text:"Nx",fontSize:20,class:"fill-violet-200"}}),$=new M({props:{data:[{x:70,y:565},{x:70,y:505}],strokeWidth:"2",dashed:!0,strokeDashArray:"6 4",moving:!0}}),k=new M({props:{data:[{x:330,y:565},{x:330,y:505}],strokeWidth:"2",dashed:!0,strokeDashArray:"6 4",moving:!0}}),p=new M({props:{data:[{x:70,y:470},{x:70,y:430}],strokeWidth:"2",dashed:!0,strokeDashArray:"6 4",moving:!0}}),L=new M({props:{data:[{x:330,y:470},{x:330,y:430}],strokeWidth:"2",dashed:!0,strokeDashArray:"6 4",moving:!0}}),I=new M({props:{data:[{x:330,y:165},{x:330,y:120}],strokeWidth:"2",dashed:!0,strokeDashArray:"6 4",moving:!0}}),F=new M({props:{data:[{x:330,y:80},{x:330,y:50}],strokeWidth:"2",dashed:!0,strokeDashArray:"6 4",moving:!0}}),C=new M({props:{data:[{x:70,y:230},{x:70,y:200},{x:270,y:200}],strokeWidth:"2",dashed:!0,strokeDashArray:"6 4",moving:!0}}),{c(){t=ne("svg"),g(n.$$.fragment),g(r.$$.fragment),g(s.$$.fragment),g(i.$$.fragment),g(d.$$.fragment),g(l.$$.fragment),g(y.$$.fragment),g(v.$$.fragment),g(b.$$.fragment),g(E.$$.fragment),g($.$$.fragment),g(k.$$.fragment),g(p.$$.fragment),g(L.$$.fragment),g(I.$$.fragment),g(F.$$.fragment),g(C.$$.fragment),this.h()},l(D){t=re(D,"svg",{viewBox:!0});var j=S(t);c(n.$$.fragment,j),c(r.$$.fragment,j),c(s.$$.fragment,j),c(i.$$.fragment,j),c(d.$$.fragment,j),c(l.$$.fragment,j),c(y.$$.fragment,j),c(v.$$.fragment,j),c(b.$$.fragment,j),c(E.$$.fragment,j),c($.$$.fragment,j),c(k.$$.fragment,j),c(p.$$.fragment,j),c(L.$$.fragment,j),c(I.$$.fragment,j),c(F.$$.fragment,j),c(C.$$.fragment,j),j.forEach(a),this.h()},h(){H(t,"viewBox","0 0 400 600")},m(D,j){f(D,t,j),w(n,t,null),w(r,t,null),w(s,t,null),w(i,t,null),w(d,t,null),w(l,t,null),w(y,t,null),w(v,t,null),w(b,t,null),w(E,t,null),w($,t,null),w(k,t,null),w(p,t,null),w(L,t,null),w(I,t,null),w(F,t,null),w(C,t,null),Y=!0},p:Q,i(D){Y||(h(n.$$.fragment,D),h(r.$$.fragment,D),h(s.$$.fragment,D),h(i.$$.fragment,D),h(d.$$.fragment,D),h(l.$$.fragment,D),h(y.$$.fragment,D),h(v.$$.fragment,D),h(b.$$.fragment,D),h(E.$$.fragment,D),h($.$$.fragment,D),h(k.$$.fragment,D),h(p.$$.fragment,D),h(L.$$.fragment,D),h(I.$$.fragment,D),h(F.$$.fragment,D),h(C.$$.fragment,D),Y=!0)},o(D){u(n.$$.fragment,D),u(r.$$.fragment,D),u(s.$$.fragment,D),u(i.$$.fragment,D),u(d.$$.fragment,D),u(l.$$.fragment,D),u(y.$$.fragment,D),u(v.$$.fragment,D),u(b.$$.fragment,D),u(E.$$.fragment,D),u($.$$.fragment,D),u(k.$$.fragment,D),u(p.$$.fragment,D),u(L.$$.fragment,D),u(I.$$.fragment,D),u(F.$$.fragment,D),u(C.$$.fragment,D),Y=!1},d(D){D&&a(t),_(n),_(r),_(s),_(i),_(d),_(l),_(y),_(v),_(b),_(E),_($),_(k),_(p),_(L),_(I),_(F),_(C)}}}function Fo(m){let t;return{c(){t=x("self-attention")},l(n){t=A(n,"self-attention")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function ho(m){let t,n,r;return t=new N({props:{x:50,y:25+m[8]*40,width:90,height:30,fontSize:20,text:m[20],class:"fill-purple-200"}}),n=new N({props:{x:450,y:25+m[8]*40,width:90,height:30,fontSize:20,text:m[20],class:"fill-green-100"}}),{c(){g(t.$$.fragment),g(n.$$.fragment)},l(s){c(t.$$.fragment,s),c(n.$$.fragment,s)},m(s,i){w(t,s,i),w(n,s,i),r=!0},p:Q,i(s){r||(h(t.$$.fragment,s),h(n.$$.fragment,s),r=!0)},o(s){u(t.$$.fragment,s),u(n.$$.fragment,s),r=!1},d(s){_(t,s),_(n,s)}}}function Ho(m){let t,n;return t=new M({props:{data:[{x:100,y:25+m[26]*40},{x:400,y:25+m[10]*40}],strokeWidth:2,moving:!0,speed:50,dashed:!0,showMarker:!1,strokeDashArray:"4, 4"}}),{c(){g(t.$$.fragment)},l(r){c(t.$$.fragment,r)},m(r,s){w(t,r,s),n=!0},p:Q,i(r){n||(h(t.$$.fragment,r),n=!0)},o(r){u(t.$$.fragment,r),n=!1},d(r){_(t,r)}}}function uo(m){let t,n,r=m[3],s=[];for(let i=0;i<r.length;i+=1)s[i]=Ho(Bo(m,r,i));return{c(){for(let i=0;i<s.length;i+=1)s[i].c();t=G()},l(i){for(let d=0;d<s.length;d+=1)s[d].l(i);t=G()},m(i,d){for(let l=0;l<s.length;l+=1)s[l]&&s[l].m(i,d);f(i,t,d),n=!0},p:Q,i(i){if(!n){for(let d=0;d<r.length;d+=1)h(s[d]);n=!0}},o(i){s=s.filter(Boolean);for(let d=0;d<s.length;d+=1)u(s[d]);n=!1},d(i){ee(s,i),i&&a(t)}}}function Mo(m){let t,n,r,s=m[3],i=[];for(let b=0;b<s.length;b+=1)i[b]=ho(mo(m,s,b));const d=b=>u(i[b],1,1,()=>{i[b]=null});let l=m[3],y=[];for(let b=0;b<l.length;b+=1)y[b]=uo($o(m,l,b));const v=b=>u(y[b],1,1,()=>{y[b]=null});return{c(){t=ne("svg");for(let b=0;b<i.length;b+=1)i[b].c();n=G();for(let b=0;b<y.length;b+=1)y[b].c();this.h()},l(b){t=re(b,"svg",{viewBox:!0});var E=S(t);for(let $=0;$<i.length;$+=1)i[$].l(E);n=G();for(let $=0;$<y.length;$+=1)y[$].l(E);E.forEach(a),this.h()},h(){H(t,"viewBox","0 0 500 300")},m(b,E){f(b,t,E);for(let $=0;$<i.length;$+=1)i[$]&&i[$].m(t,null);T(t,n);for(let $=0;$<y.length;$+=1)y[$]&&y[$].m(t,null);r=!0},p(b,E){if(E&8){s=b[3];let $;for($=0;$<s.length;$+=1){const k=mo(b,s,$);i[$]?(i[$].p(k,E),h(i[$],1)):(i[$]=ho(k),i[$].c(),h(i[$],1),i[$].m(t,n))}for(at(),$=s.length;$<i.length;$+=1)d($);st()}if(E&8){l=b[3];let $;for($=0;$<l.length;$+=1){const k=$o(b,l,$);y[$]?(y[$].p(k,E),h(y[$],1)):(y[$]=uo(k),y[$].c(),h(y[$],1),y[$].m(t,null))}for(at(),$=l.length;$<y.length;$+=1)v($);st()}},i(b){if(!r){for(let E=0;E<s.length;E+=1)h(i[E]);for(let E=0;E<l.length;E+=1)h(y[E]);r=!0}},o(b){i=i.filter(Boolean);for(let E=0;E<i.length;E+=1)u(i[E]);y=y.filter(Boolean);for(let E=0;E<y.length;E+=1)u(y[E]);r=!1},d(b){b&&a(t),ee(i,b),ee(y,b)}}}function jo(m){let t;return{c(){t=x("The purpose of self attention is to produce context-aware embeddings.")},l(n){t=A(n,"The purpose of self attention is to produce context-aware embeddings.")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function Uo(m){let t;return{c(){t=x("query")},l(n){t=A(n,"query")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function Ko(m){let t;return{c(){t=x("key")},l(n){t=A(n,"key")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function Co(m){let t;return{c(){t=x("value")},l(n){t=A(n,"value")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function Qo(m){let t,n,r;return t=new N({props:{x:0,y:m[8]*34,width:60,height:30,text:"key "+(m[8]+1),fontSize:20}}),n=new N({props:{x:80,y:m[8]*34,width:90,height:30,text:"value "+(m[8]+1),fontSize:20}}),{c(){g(t.$$.fragment),g(n.$$.fragment)},l(s){c(t.$$.fragment,s),c(n.$$.fragment,s)},m(s,i){w(t,s,i),w(n,s,i),r=!0},p:Q,i(s){r||(h(t.$$.fragment,s),h(n.$$.fragment,s),r=!0)},o(s){u(t.$$.fragment,s),u(n.$$.fragment,s),r=!1},d(s){_(t,s),_(n,s)}}}function Yo(m){let t,n,r,s,i,d,l;r=new N({props:{x:0,y:-40,width:60,height:30,text:"key",fontSize:20,class:"fill-indigo-200"}}),s=new N({props:{x:80,y:-40,width:90,height:30,text:"value",fontSize:20,class:"fill-indigo-200"}});let y=Array(5),v=[];for(let b=0;b<y.length;b+=1)v[b]=Qo(No(m,y,b));return i=new N({props:{x:250,y:270,width:400,height:40,text:"SELECT value WHERE key='key 1'",fontSize:20,class:"fill-gray-200"}}),d=new M({props:{data:[{x:50,y:270},{x:5,y:270},{x:5,y:60},{x:150,y:60}],strokeWidth:2}}),{c(){t=ne("svg"),n=ne("g"),g(r.$$.fragment),g(s.$$.fragment);for(let b=0;b<v.length;b+=1)v[b].c();g(i.$$.fragment),g(d.$$.fragment),this.h()},l(b){t=re(b,"svg",{viewBox:!0});var E=S(t);n=re(E,"g",{transform:!0});var $=S(n);c(r.$$.fragment,$),c(s.$$.fragment,$);for(let k=0;k<v.length;k+=1)v[k].l($);$.forEach(a),c(i.$$.fragment,E),c(d.$$.fragment,E),E.forEach(a),this.h()},h(){H(n,"transform","translate(200, 60)"),H(t,"viewBox","0 0 500 300")},m(b,E){f(b,t,E),T(t,n),w(r,n,null),w(s,n,null);for(let $=0;$<v.length;$+=1)v[$]&&v[$].m(n,null);w(i,t,null),w(d,t,null),l=!0},p:Q,i(b){if(!l){h(r.$$.fragment,b),h(s.$$.fragment,b);for(let E=0;E<y.length;E+=1)h(v[E]);h(i.$$.fragment,b),h(d.$$.fragment,b),l=!0}},o(b){u(r.$$.fragment,b),u(s.$$.fragment,b),v=v.filter(Boolean);for(let E=0;E<v.length;E+=1)u(v[E]);u(i.$$.fragment,b),u(d.$$.fragment,b),l=!1},d(b){b&&a(t),_(r),_(s),ee(v,b),_(i),_(d)}}}function Go(m){let t=String.raw`
  k_1 = 
  \begin{bmatrix}
    1  \\
    0  \\
    1  \\
    0  \\
  \end{bmatrix},
  k_2 = 
  \begin{bmatrix}
     0 \\
     1 \\
     1 \\
     0 \\
  \end{bmatrix}
 `+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function Ro(m){let t=String.raw`
 q = 
  \begin{bmatrix}
    1  \\
    0  \\
    1  \\
    0  \\
  \end{bmatrix}
 `+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function Jo(m){let t=String.raw`s_1 = q \cdot k_1 = 2`+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function Oo(m){let t=String.raw`s_2 = q \cdot k_2 = 1`+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function Xo(m){let t=String.raw`\large w_j = \dfrac{e^{s_j}}{\sum_i e^{s_i}}`+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function Zo(m){let t=String.raw`a = \sum_j w_{j}v_j`+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function ei(m){let t;return{c(){t=x("E")},l(n){t=A(n,"E")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function ti(m){let t;return{c(){t=x("Q")},l(n){t=A(n,"Q")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function ni(m){let t;return{c(){t=x("K")},l(n){t=A(n,"K")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function ri(m){let t;return{c(){t=x("V")},l(n){t=A(n,"V")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function ai(m){let t;return{c(){t=x("A")},l(n){t=A(n,"A")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function si(m){let t,n,r,s,i,d,l,y,v,b,E,$,k;return n=new N({props:{x:20,y:100,width:30,height:30,text:"E",fontSize:20,class:"fill-slate-200"}}),r=new N({props:{x:150,y:16,width:30,height:30,text:"Q",fontSize:20,class:"fill-red-400"}}),s=new N({props:{x:150,y:100,width:30,height:30,text:"K",fontSize:20,class:"fill-green-400"}}),i=new N({props:{x:150,y:184,width:30,height:30,text:"V",fontSize:20,class:"fill-blue-400"}}),d=new N({props:{x:300-16,y:100,width:30,height:30,text:"A",fontSize:20,class:"fill-yellow-300"}}),l=new M({props:{data:[{x:45,y:100},{x:125,y:16}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),y=new M({props:{data:[{x:45,y:100},{x:125,y:100}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),v=new M({props:{data:[{x:45,y:100},{x:125,y:184}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),b=new M({props:{data:[{x:170,y:16},{x:260,y:90}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),E=new M({props:{data:[{x:170,y:100},{x:260,y:100}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),$=new M({props:{data:[{x:170,y:184},{x:260,y:110}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),{c(){t=ne("svg"),g(n.$$.fragment),g(r.$$.fragment),g(s.$$.fragment),g(i.$$.fragment),g(d.$$.fragment),g(l.$$.fragment),g(y.$$.fragment),g(v.$$.fragment),g(b.$$.fragment),g(E.$$.fragment),g($.$$.fragment),this.h()},l(p){t=re(p,"svg",{viewBox:!0});var L=S(t);c(n.$$.fragment,L),c(r.$$.fragment,L),c(s.$$.fragment,L),c(i.$$.fragment,L),c(d.$$.fragment,L),c(l.$$.fragment,L),c(y.$$.fragment,L),c(v.$$.fragment,L),c(b.$$.fragment,L),c(E.$$.fragment,L),c($.$$.fragment,L),L.forEach(a),this.h()},h(){H(t,"viewBox","0 0 300 200")},m(p,L){f(p,t,L),w(n,t,null),w(r,t,null),w(s,t,null),w(i,t,null),w(d,t,null),w(l,t,null),w(y,t,null),w(v,t,null),w(b,t,null),w(E,t,null),w($,t,null),k=!0},p:Q,i(p){k||(h(n.$$.fragment,p),h(r.$$.fragment,p),h(s.$$.fragment,p),h(i.$$.fragment,p),h(d.$$.fragment,p),h(l.$$.fragment,p),h(y.$$.fragment,p),h(v.$$.fragment,p),h(b.$$.fragment,p),h(E.$$.fragment,p),h($.$$.fragment,p),k=!0)},o(p){u(n.$$.fragment,p),u(r.$$.fragment,p),u(s.$$.fragment,p),u(i.$$.fragment,p),u(d.$$.fragment,p),u(l.$$.fragment,p),u(y.$$.fragment,p),u(v.$$.fragment,p),u(b.$$.fragment,p),u(E.$$.fragment,p),u($.$$.fragment,p),k=!1},d(p){p&&a(t),_(n),_(r),_(s),_(i),_(d),_(l),_(y),_(v),_(b),_(E),_($)}}}function oi(m){let t=String.raw`A = \text{softmax}(\dfrac{QK^T}{\sqrt{d}})V`+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function ii(m){let t=String.raw`d`+"",n;return{c(){n=x(t)},l(r){n=A(r,t)},m(r,s){f(r,n,s)},p:Q,d(r){r&&a(n)}}}function li(m){let t;return{c(){t=x("scaled dot-product attention")},l(n){t=A(n,"scaled dot-product attention")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function fi(m){let t;return{c(){t=x("A")},l(n){t=A(n,"A")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function $i(m){let t;return{c(){t=x("multihead attention")},l(n){t=A(n,"multihead attention")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function mi(m){let t;return{c(){t=x("attention head")},l(n){t=A(n,"attention head")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function hi(m){let t;return{c(){t=x("Q")},l(n){t=A(n,"Q")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function ui(m){let t;return{c(){t=x("K")},l(n){t=A(n,"K")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function di(m){let t;return{c(){t=x("V")},l(n){t=A(n,"V")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function pi(m){let t,n,r,s,i,d,l,y,v,b,E,$,k,p,L,I,F,C,Y,D,j;return n=new N({props:{x:20,y:100,width:30,height:30,text:"E",fontSize:20,class:"fill-slate-200"}}),r=new N({props:{x:150,y:16,width:20,height:20,text:"Q",fontSize:20,class:"fill-red-400"}}),s=new N({props:{x:160,y:26,width:20,height:20,text:"Q",fontSize:20,class:"fill-red-400"}}),i=new N({props:{x:170,y:36,width:20,height:20,text:"Q",fontSize:20,class:"fill-red-400"}}),d=new N({props:{x:150,y:100,width:20,height:20,text:"K",fontSize:20,class:"fill-green-400"}}),l=new N({props:{x:160,y:110,width:20,height:20,text:"K",fontSize:20,class:"fill-green-400"}}),y=new N({props:{x:170,y:120,width:20,height:20,text:"K",fontSize:20,class:"fill-green-400"}}),v=new N({props:{x:150,y:184,width:20,height:20,text:"V",fontSize:20,class:"fill-blue-400"}}),b=new N({props:{x:160,y:174,width:20,height:20,text:"V",fontSize:20,class:"fill-blue-400"}}),E=new N({props:{x:170,y:164,width:20,height:20,text:"V",fontSize:20,class:"fill-blue-400"}}),$=new N({props:{x:300-16,y:100,width:20,height:20,text:"A",fontSize:20,class:"fill-yellow-300"}}),k=new N({props:{x:300-16-10,y:110,width:20,height:20,text:"A",fontSize:20,class:"fill-yellow-300"}}),p=new N({props:{x:300-16-20,y:120,width:20,height:20,text:"A",fontSize:20,class:"fill-yellow-300"}}),L=new M({props:{data:[{x:45,y:100},{x:125,y:16}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),I=new M({props:{data:[{x:45,y:100},{x:125,y:100}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),F=new M({props:{data:[{x:45,y:100},{x:125,y:184}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),C=new M({props:{data:[{x:170,y:16},{x:260,y:90}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),Y=new M({props:{data:[{x:170,y:100},{x:260,y:100}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),D=new M({props:{data:[{x:170,y:184},{x:260,y:110}],strokeWidth:"2",moving:!0,speed:50,dashed:!0,strokeDashArray:"4 4"}}),{c(){t=ne("svg"),g(n.$$.fragment),g(r.$$.fragment),g(s.$$.fragment),g(i.$$.fragment),g(d.$$.fragment),g(l.$$.fragment),g(y.$$.fragment),g(v.$$.fragment),g(b.$$.fragment),g(E.$$.fragment),g($.$$.fragment),g(k.$$.fragment),g(p.$$.fragment),g(L.$$.fragment),g(I.$$.fragment),g(F.$$.fragment),g(C.$$.fragment),g(Y.$$.fragment),g(D.$$.fragment),this.h()},l(B){t=re(B,"svg",{viewBox:!0});var U=S(t);c(n.$$.fragment,U),c(r.$$.fragment,U),c(s.$$.fragment,U),c(i.$$.fragment,U),c(d.$$.fragment,U),c(l.$$.fragment,U),c(y.$$.fragment,U),c(v.$$.fragment,U),c(b.$$.fragment,U),c(E.$$.fragment,U),c($.$$.fragment,U),c(k.$$.fragment,U),c(p.$$.fragment,U),c(L.$$.fragment,U),c(I.$$.fragment,U),c(F.$$.fragment,U),c(C.$$.fragment,U),c(Y.$$.fragment,U),c(D.$$.fragment,U),U.forEach(a),this.h()},h(){H(t,"viewBox","0 0 300 200")},m(B,U){f(B,t,U),w(n,t,null),w(r,t,null),w(s,t,null),w(i,t,null),w(d,t,null),w(l,t,null),w(y,t,null),w(v,t,null),w(b,t,null),w(E,t,null),w($,t,null),w(k,t,null),w(p,t,null),w(L,t,null),w(I,t,null),w(F,t,null),w(C,t,null),w(Y,t,null),w(D,t,null),j=!0},p:Q,i(B){j||(h(n.$$.fragment,B),h(r.$$.fragment,B),h(s.$$.fragment,B),h(i.$$.fragment,B),h(d.$$.fragment,B),h(l.$$.fragment,B),h(y.$$.fragment,B),h(v.$$.fragment,B),h(b.$$.fragment,B),h(E.$$.fragment,B),h($.$$.fragment,B),h(k.$$.fragment,B),h(p.$$.fragment,B),h(L.$$.fragment,B),h(I.$$.fragment,B),h(F.$$.fragment,B),h(C.$$.fragment,B),h(Y.$$.fragment,B),h(D.$$.fragment,B),j=!0)},o(B){u(n.$$.fragment,B),u(r.$$.fragment,B),u(s.$$.fragment,B),u(i.$$.fragment,B),u(d.$$.fragment,B),u(l.$$.fragment,B),u(y.$$.fragment,B),u(v.$$.fragment,B),u(b.$$.fragment,B),u(E.$$.fragment,B),u($.$$.fragment,B),u(k.$$.fragment,B),u(p.$$.fragment,B),u(L.$$.fragment,B),u(I.$$.fragment,B),u(F.$$.fragment,B),u(C.$$.fragment,B),u(Y.$$.fragment,B),u(D.$$.fragment,B),j=!1},d(B){B&&a(t),_(n),_(r),_(s),_(i),_(d),_(l),_(y),_(v),_(b),_(E),_($),_(k),_(p),_(L),_(I),_(F),_(C),_(Y),_(D)}}}function gi(m){let t,n;return t=new Wo({props:{f:m[5]}}),{c(){g(t.$$.fragment)},l(r){c(t.$$.fragment,r)},m(r,s){w(t,r,s),n=!0},p:Q,i(r){n||(h(t.$$.fragment,r),n=!0)},o(r){u(t.$$.fragment,r),n=!1},d(r){_(t,r)}}}function po(m){let t,n;return t=new M({props:{data:[{x:40+120*m[8]+m[23]*15,y:195},{x:220+m[23]*15,y:130}],dashed:!0,strokeDashArray:"2 2"}}),{c(){g(t.$$.fragment)},l(r){c(t.$$.fragment,r)},m(r,s){w(t,r,s),n=!0},i(r){n||(h(t.$$.fragment,r),n=!0)},o(r){u(t.$$.fragment,r),n=!1},d(r){_(t,r)}}}function go(m){let t,n,r,s=m[8]===m[0]&&po(m);return n=new N({props:{x:40+120*m[8]+m[23]*15,y:200,width:10,height:10,fontSize:20,text:"",class:"fill-blue-100"}}),{c(){s&&s.c(),t=G(),g(n.$$.fragment)},l(i){s&&s.l(i),t=G(),c(n.$$.fragment,i)},m(i,d){s&&s.m(i,d),f(i,t,d),w(n,i,d),r=!0},p(i,d){i[8]===i[0]?s?d&1&&h(s,1):(s=po(i),s.c(),h(s,1),s.m(t.parentNode,t)):s&&(at(),u(s,1,1,()=>{s=null}),st())},i(i){r||(h(s),h(n.$$.fragment,i),r=!0)},o(i){u(s),u(n.$$.fragment,i),r=!1},d(i){s&&s.d(i),i&&a(t),_(n,i)}}}function co(m){let t,n,r;t=new N({props:{x:70+120*m[8],y:230,width:90,height:30,fontSize:20,text:m[20],class:"fill-blue-100"}});let s=Array(5),i=[];for(let l=0;l<s.length;l+=1)i[l]=go(fo(m,s,l));const d=l=>u(i[l],1,1,()=>{i[l]=null});return{c(){g(t.$$.fragment);for(let l=0;l<i.length;l+=1)i[l].c();n=G()},l(l){c(t.$$.fragment,l);for(let y=0;y<i.length;y+=1)i[y].l(l);n=G()},m(l,y){w(t,l,y);for(let v=0;v<i.length;v+=1)i[v]&&i[v].m(l,y);f(l,n,y),r=!0},p(l,y){if(y&1){s=Array(5);let v;for(v=0;v<s.length;v+=1){const b=fo(l,s,v);i[v]?(i[v].p(b,y),h(i[v],1)):(i[v]=go(b),i[v].c(),h(i[v],1),i[v].m(n.parentNode,n))}for(at(),v=s.length;v<i.length;v+=1)d(v);st()}},i(l){if(!r){h(t.$$.fragment,l);for(let y=0;y<s.length;y+=1)h(i[y]);r=!0}},o(l){u(t.$$.fragment,l),i=i.filter(Boolean);for(let y=0;y<i.length;y+=1)u(i[y]);r=!1},d(l){_(t,l),ee(i,l),l&&a(n)}}}function ci(m){let t,n,r;return t=new N({props:{x:220+m[18]*15,y:125,width:10,height:10,fontSize:20,text:"",class:"fill-red-400"}}),n=new N({props:{x:220+m[18]*15,y:15,width:10,height:10,fontSize:20,text:"",class:"fill-red-400"}}),{c(){g(t.$$.fragment),g(n.$$.fragment)},l(s){c(t.$$.fragment,s),c(n.$$.fragment,s)},m(s,i){w(t,s,i),w(n,s,i),r=!0},p:Q,i(s){r||(h(t.$$.fragment,s),h(n.$$.fragment,s),r=!0)},o(s){u(t.$$.fragment,s),u(n.$$.fragment,s),r=!1},d(s){_(t,s),_(n,s)}}}function wi(m){let t,n;return t=new N({props:{x:195+m[18]*15,y:70,width:10,height:10,fontSize:20,text:"",class:"fill-red-400"}}),{c(){g(t.$$.fragment)},l(r){c(t.$$.fragment,r)},m(r,s){w(t,r,s),n=!0},p:Q,i(r){n||(h(t.$$.fragment,r),n=!0)},o(r){u(t.$$.fragment,r),n=!1},d(r){_(t,r)}}}function _i(m){let t,n,r,s,i=m[2],d=[];for(let $=0;$<i.length;$+=1)d[$]=co(lo(m,i,$));const l=$=>u(d[$],1,1,()=>{d[$]=null});let y=Array(5),v=[];for(let $=0;$<y.length;$+=1)v[$]=ci(Io(m,y,$));let b=Array(8),E=[];for(let $=0;$<b.length;$+=1)E[$]=wi(Do(m,b,$));return{c(){t=ne("svg");for(let $=0;$<d.length;$+=1)d[$].c();n=G();for(let $=0;$<v.length;$+=1)v[$].c();r=G();for(let $=0;$<E.length;$+=1)E[$].c();this.h()},l($){t=re($,"svg",{viewBox:!0});var k=S(t);for(let p=0;p<d.length;p+=1)d[p].l(k);n=G();for(let p=0;p<v.length;p+=1)v[p].l(k);r=G();for(let p=0;p<E.length;p+=1)E[p].l(k);k.forEach(a),this.h()},h(){H(t,"viewBox","0 0 500 250")},m($,k){f($,t,k);for(let p=0;p<d.length;p+=1)d[p]&&d[p].m(t,null);T(t,n);for(let p=0;p<v.length;p+=1)v[p]&&v[p].m(t,null);T(t,r);for(let p=0;p<E.length;p+=1)E[p]&&E[p].m(t,null);s=!0},p($,k){if(k&5){i=$[2];let p;for(p=0;p<i.length;p+=1){const L=lo($,i,p);d[p]?(d[p].p(L,k),h(d[p],1)):(d[p]=co(L),d[p].c(),h(d[p],1),d[p].m(t,n))}for(at(),p=i.length;p<d.length;p+=1)l(p);st()}},i($){if(!s){for(let k=0;k<i.length;k+=1)h(d[k]);for(let k=0;k<y.length;k+=1)h(v[k]);for(let k=0;k<b.length;k+=1)h(E[k]);s=!0}},o($){d=d.filter(Boolean);for(let k=0;k<d.length;k+=1)u(d[k]);v=v.filter(Boolean);for(let k=0;k<v.length;k+=1)u(v[k]);E=E.filter(Boolean);for(let k=0;k<E.length;k+=1)u(E[k]);s=!1},d($){$&&a(t),ee(d,$),ee(v,$),ee(E,$)}}}function yi(m){let t,n,r,s,i,d,l,y,v,b,E,$,k,p,L;return n=new N({props:{x:200,y:200,width:200,height:350,text:"",fontSize:20}}),r=new N({props:{x:350,y:130,width:30,height:30,text:"Nx",fontSize:20}}),s=new M({props:{data:[{x:200,y:400},{x:200,y:325}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),i=new M({props:{data:[{x:200,y:400},{x:200,y:340},{x:140,y:340},{x:140,y:325}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),d=new M({props:{data:[{x:200,y:400},{x:200,y:340},{x:260,y:340},{x:260,y:325}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),l=new M({props:{data:[{x:200,y:400},{x:200,y:360},{x:50,y:360},{x:50,y:250},{x:115,y:250}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),y=new M({props:{data:[{x:200,y:300},{x:200,y:175}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),v=new M({props:{data:[{x:200,y:200},{x:50,y:200},{x:50,y:100},{x:115,y:100}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),b=new M({props:{data:[{x:200,y:150},{x:200,y:10}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),E=new N({props:{x:200,y:100,width:150,height:30,text:"Add & Norm",fontSize:15,class:"fill-red-400"}}),$=new N({props:{x:200,y:150,width:150,height:30,text:"P.w. Feed Forward",fontSize:15,class:"fill-red-400"}}),k=new N({props:{x:200,y:250,width:150,height:30,text:"Add & Norm",fontSize:15,class:"fill-red-400"}}),p=new N({props:{x:200,y:300,width:150,height:30,text:"Multihead Atention",fontSize:15,class:"fill-red-400"}}),{c(){t=ne("svg"),g(n.$$.fragment),g(r.$$.fragment),g(s.$$.fragment),g(i.$$.fragment),g(d.$$.fragment),g(l.$$.fragment),g(y.$$.fragment),g(v.$$.fragment),g(b.$$.fragment),g(E.$$.fragment),g($.$$.fragment),g(k.$$.fragment),g(p.$$.fragment),this.h()},l(I){t=re(I,"svg",{viewBox:!0});var F=S(t);c(n.$$.fragment,F),c(r.$$.fragment,F),c(s.$$.fragment,F),c(i.$$.fragment,F),c(d.$$.fragment,F),c(l.$$.fragment,F),c(y.$$.fragment,F),c(v.$$.fragment,F),c(b.$$.fragment,F),c(E.$$.fragment,F),c($.$$.fragment,F),c(k.$$.fragment,F),c(p.$$.fragment,F),F.forEach(a),this.h()},h(){H(t,"viewBox","0 0 400 400")},m(I,F){f(I,t,F),w(n,t,null),w(r,t,null),w(s,t,null),w(i,t,null),w(d,t,null),w(l,t,null),w(y,t,null),w(v,t,null),w(b,t,null),w(E,t,null),w($,t,null),w(k,t,null),w(p,t,null),L=!0},p:Q,i(I){L||(h(n.$$.fragment,I),h(r.$$.fragment,I),h(s.$$.fragment,I),h(i.$$.fragment,I),h(d.$$.fragment,I),h(l.$$.fragment,I),h(y.$$.fragment,I),h(v.$$.fragment,I),h(b.$$.fragment,I),h(E.$$.fragment,I),h($.$$.fragment,I),h(k.$$.fragment,I),h(p.$$.fragment,I),L=!0)},o(I){u(n.$$.fragment,I),u(r.$$.fragment,I),u(s.$$.fragment,I),u(i.$$.fragment,I),u(d.$$.fragment,I),u(l.$$.fragment,I),u(y.$$.fragment,I),u(v.$$.fragment,I),u(b.$$.fragment,I),u(E.$$.fragment,I),u($.$$.fragment,I),u(k.$$.fragment,I),u(p.$$.fragment,I),L=!1},d(I){I&&a(t),_(n),_(r),_(s),_(i),_(d),_(l),_(y),_(v),_(b),_(E),_($),_(k),_(p)}}}function bi(m){let t,n;return t=new N({props:{x:10+12*m[14],y:40+12*m[12],width:10,height:10,class:m[14]===0?"fill-red-400":"none"}}),{c(){g(t.$$.fragment)},l(r){c(t.$$.fragment,r)},m(r,s){w(t,r,s),n=!0},p:Q,i(r){n||(h(t.$$.fragment,r),n=!0)},o(r){u(t.$$.fragment,r),n=!1},d(r){_(t,r)}}}function wo(m){let t,n,r=Array(10),s=[];for(let i=0;i<r.length;i+=1)s[i]=bi(qo(m,r,i));return{c(){for(let i=0;i<s.length;i+=1)s[i].c();t=G()},l(i){for(let d=0;d<s.length;d+=1)s[d].l(i);t=G()},m(i,d){for(let l=0;l<s.length;l+=1)s[l]&&s[l].m(i,d);f(i,t,d),n=!0},p:Q,i(i){if(!n){for(let d=0;d<r.length;d+=1)h(s[d]);n=!0}},o(i){s=s.filter(Boolean);for(let d=0;d<s.length;d+=1)u(s[d]);n=!1},d(i){ee(s,i),i&&a(t)}}}function vi(m){let t,n;return t=new N({props:{x:150+12*m[14],y:40+12*m[12],width:10,height:10,class:m[12]===0?"fill-red-400":"none"}}),{c(){g(t.$$.fragment)},l(r){c(t.$$.fragment,r)},m(r,s){w(t,r,s),n=!0},p:Q,i(r){n||(h(t.$$.fragment,r),n=!0)},o(r){u(t.$$.fragment,r),n=!1},d(r){_(t,r)}}}function _o(m){let t,n,r=Array(10),s=[];for(let i=0;i<r.length;i+=1)s[i]=vi(Po(m,r,i));return{c(){for(let i=0;i<s.length;i+=1)s[i].c();t=G()},l(i){for(let d=0;d<s.length;d+=1)s[d].l(i);t=G()},m(i,d){for(let l=0;l<s.length;l+=1)s[l]&&s[l].m(i,d);f(i,t,d),n=!0},p:Q,i(i){if(!n){for(let d=0;d<r.length;d+=1)h(s[d]);n=!0}},o(i){s=s.filter(Boolean);for(let d=0;d<s.length;d+=1)u(s[d]);n=!1},d(i){ee(s,i),i&&a(t)}}}function ki(m){let t,n,r,s,i;n=new N({props:{x:35,y:10,width:60,height:15,text:"Batch Norm",fontSize:9}});let d=Array(5),l=[];for(let $=0;$<d.length;$+=1)l[$]=wo(io(m,d,$));const y=$=>u(l[$],1,1,()=>{l[$]=null});s=new N({props:{x:175,y:10,width:60,height:15,text:"Layer Norm",fontSize:9}});let v=Array(5),b=[];for(let $=0;$<v.length;$+=1)b[$]=_o(oo(m,v,$));const E=$=>u(b[$],1,1,()=>{b[$]=null});return{c(){t=ne("svg"),g(n.$$.fragment);for(let $=0;$<l.length;$+=1)l[$].c();r=G(),g(s.$$.fragment);for(let $=0;$<b.length;$+=1)b[$].c();this.h()},l($){t=re($,"svg",{viewBox:!0});var k=S(t);c(n.$$.fragment,k);for(let p=0;p<l.length;p+=1)l[p].l(k);r=G(),c(s.$$.fragment,k);for(let p=0;p<b.length;p+=1)b[p].l(k);k.forEach(a),this.h()},h(){H(t,"viewBox","0 0 270 100")},m($,k){f($,t,k),w(n,t,null);for(let p=0;p<l.length;p+=1)l[p]&&l[p].m(t,null);T(t,r),w(s,t,null);for(let p=0;p<b.length;p+=1)b[p]&&b[p].m(t,null);i=!0},p($,k){if(k&0){d=Array(5);let p;for(p=0;p<d.length;p+=1){const L=io($,d,p);l[p]?(l[p].p(L,k),h(l[p],1)):(l[p]=wo(L),l[p].c(),h(l[p],1),l[p].m(t,r))}for(at(),p=d.length;p<l.length;p+=1)y(p);st()}if(k&0){v=Array(5);let p;for(p=0;p<v.length;p+=1){const L=oo($,v,p);b[p]?(b[p].p(L,k),h(b[p],1)):(b[p]=_o(L),b[p].c(),h(b[p],1),b[p].m(t,null))}for(at(),p=v.length;p<b.length;p+=1)E(p);st()}},i($){if(!i){h(n.$$.fragment,$);for(let k=0;k<d.length;k+=1)h(l[k]);h(s.$$.fragment,$);for(let k=0;k<v.length;k+=1)h(b[k]);i=!0}},o($){u(n.$$.fragment,$),l=l.filter(Boolean);for(let k=0;k<l.length;k+=1)u(l[k]);u(s.$$.fragment,$),b=b.filter(Boolean);for(let k=0;k<b.length;k+=1)u(b[k]);i=!1},d($){$&&a(t),_(n),ee(l,$),_(s),ee(b,$)}}}function xi(m){let t,n,r,s,i,d,l,y,v,b,E,$,k,p,L,I,F,C,Y,D,j,B,U;return n=new N({props:{x:200,y:250,width:200,height:450,text:"",fontSize:20}}),r=new N({props:{x:380,y:200,width:30,height:30,text:"Nx",fontSize:20}}),s=new M({props:{data:[{x:200,y:500},{x:200,y:425}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),i=new M({props:{data:[{x:200,y:500},{x:200,y:440},{x:140,y:440},{x:140,y:425}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),d=new M({props:{data:[{x:200,y:500},{x:200,y:440},{x:260,y:440},{x:260,y:425}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),l=new M({props:{data:[{x:200,y:500},{x:200,y:460},{x:50,y:460},{x:50,y:350},{x:115,y:350}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),y=new M({props:{data:[{x:200,y:400},{x:200,y:300},{x:140,y:300},{x:140,y:275}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),v=new M({props:{data:[{x:350,y:295},{x:200,y:295},{x:200,y:275}],strokeWidth:2,dashed:!0,strokeDashArray:"4, 4"}}),b=new M({props:{data:[{x:350,y:290},{x:260,y:290},{x:260,y:275}],strokeWidth:2,dashed:!0,strokeDashArray:"4, 4"}}),E=new M({props:{data:[{x:200,y:320},{x:50,y:320},{x:50,y:200},{x:115,y:200}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),$=new M({props:{data:[{x:200,y:250},{x:200,y:125}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),k=new M({props:{data:[{x:200,y:170},{x:50,y:170},{x:50,y:50},{x:115,y:50}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),p=new M({props:{data:[{x:200,y:100},{x:200,y:10}],strokeWidth:2,dashed:!0,strokeDashArray:"4 4"}}),L=new N({props:{x:200,y:50,width:150,height:30,text:"Add & Norm",fontSize:15,class:"fill-red-400"}}),I=new N({props:{x:200,y:100,width:150,height:30,text:"P.w. Feed Forward",fontSize:15,class:"fill-red-400"}}),F=new N({props:{x:200,y:200,width:150,height:30,text:"Add & Norm",fontSize:15,class:"fill-red-400"}}),C=new N({props:{x:200,y:250,width:150,height:30,text:"Multihead Atention",fontSize:15,class:"fill-red-400"}}),Y=new N({props:{x:200,y:350,width:150,height:30,text:"Add & Norm",fontSize:15,class:"fill-red-400"}}),D=new N({props:{x:200,y:400,width:150,height:30,text:"Multihead Atention",fontSize:15,class:"fill-red-400"}}),j=new N({props:{x:90,y:400,width:60,height:30,text:"Masked",fontSize:15,class:"fill-red-400"}}),B=new N({props:{x:355,y:295,width:80,height:30,text:"Encoder",fontSize:15,class:"fill-slate-100"}}),{c(){t=ne("svg"),g(n.$$.fragment),g(r.$$.fragment),g(s.$$.fragment),g(i.$$.fragment),g(d.$$.fragment),g(l.$$.fragment),g(y.$$.fragment),g(v.$$.fragment),g(b.$$.fragment),g(E.$$.fragment),g($.$$.fragment),g(k.$$.fragment),g(p.$$.fragment),g(L.$$.fragment),g(I.$$.fragment),g(F.$$.fragment),g(C.$$.fragment),g(Y.$$.fragment),g(D.$$.fragment),g(j.$$.fragment),g(B.$$.fragment),this.h()},l(V){t=re(V,"svg",{viewBox:!0});var K=S(t);c(n.$$.fragment,K),c(r.$$.fragment,K),c(s.$$.fragment,K),c(i.$$.fragment,K),c(d.$$.fragment,K),c(l.$$.fragment,K),c(y.$$.fragment,K),c(v.$$.fragment,K),c(b.$$.fragment,K),c(E.$$.fragment,K),c($.$$.fragment,K),c(k.$$.fragment,K),c(p.$$.fragment,K),c(L.$$.fragment,K),c(I.$$.fragment,K),c(F.$$.fragment,K),c(C.$$.fragment,K),c(Y.$$.fragment,K),c(D.$$.fragment,K),c(j.$$.fragment,K),c(B.$$.fragment,K),K.forEach(a),this.h()},h(){H(t,"viewBox","0 0 400 500")},m(V,K){f(V,t,K),w(n,t,null),w(r,t,null),w(s,t,null),w(i,t,null),w(d,t,null),w(l,t,null),w(y,t,null),w(v,t,null),w(b,t,null),w(E,t,null),w($,t,null),w(k,t,null),w(p,t,null),w(L,t,null),w(I,t,null),w(F,t,null),w(C,t,null),w(Y,t,null),w(D,t,null),w(j,t,null),w(B,t,null),U=!0},p:Q,i(V){U||(h(n.$$.fragment,V),h(r.$$.fragment,V),h(s.$$.fragment,V),h(i.$$.fragment,V),h(d.$$.fragment,V),h(l.$$.fragment,V),h(y.$$.fragment,V),h(v.$$.fragment,V),h(b.$$.fragment,V),h(E.$$.fragment,V),h($.$$.fragment,V),h(k.$$.fragment,V),h(p.$$.fragment,V),h(L.$$.fragment,V),h(I.$$.fragment,V),h(F.$$.fragment,V),h(C.$$.fragment,V),h(Y.$$.fragment,V),h(D.$$.fragment,V),h(j.$$.fragment,V),h(B.$$.fragment,V),U=!0)},o(V){u(n.$$.fragment,V),u(r.$$.fragment,V),u(s.$$.fragment,V),u(i.$$.fragment,V),u(d.$$.fragment,V),u(l.$$.fragment,V),u(y.$$.fragment,V),u(v.$$.fragment,V),u(b.$$.fragment,V),u(E.$$.fragment,V),u($.$$.fragment,V),u(k.$$.fragment,V),u(p.$$.fragment,V),u(L.$$.fragment,V),u(I.$$.fragment,V),u(F.$$.fragment,V),u(C.$$.fragment,V),u(Y.$$.fragment,V),u(D.$$.fragment,V),u(j.$$.fragment,V),u(B.$$.fragment,V),U=!1},d(V){V&&a(t),_(n),_(r),_(s),_(i),_(d),_(l),_(y),_(v),_(b),_(E),_($),_(k),_(p),_(L),_(I),_(F),_(C),_(Y),_(D),_(j),_(B)}}}function Ai(m){let t,n;return t=new M({props:{data:[{x:25+m[8]*50,y:120},{x:25+m[10]*50+m[8]*4,y:50}],dashed:!0,moving:!0,speed:50}}),{c(){g(t.$$.fragment)},l(r){c(t.$$.fragment,r)},m(r,s){w(t,r,s),n=!0},i(r){n||(h(t.$$.fragment,r),n=!0)},o(r){u(t.$$.fragment,r),n=!1},d(r){_(t,r)}}}function Ei(m){let t,n,r=m[10]>=m[8]&&Ai(m);return{c(){r&&r.c(),t=G()},l(s){r&&r.l(s),t=G()},m(s,i){r&&r.m(s,i),f(s,t,i),n=!0},p:Q,i(s){n||(h(r),n=!0)},o(s){u(r),n=!1},d(s){r&&r.d(s),s&&a(t)}}}function yo(m){let t,n,r,s;t=new N({props:{x:25+m[8]*50,y:130,width:30,height:20,text:m[4][m[8]],class:"fill-lime-200",fontSize:9}}),n=new N({props:{x:25+m[8]*50,y:35,width:30,height:20,text:m[4][m[8]+1],class:"fill-yellow-200",fontSize:9}});let i=Array(4),d=[];for(let l=0;l<i.length;l+=1)d[l]=Ei(So(m,i,l));return{c(){g(t.$$.fragment),g(n.$$.fragment);for(let l=0;l<d.length;l+=1)d[l].c();r=G()},l(l){c(t.$$.fragment,l),c(n.$$.fragment,l);for(let y=0;y<d.length;y+=1)d[y].l(l);r=G()},m(l,y){w(t,l,y),w(n,l,y);for(let v=0;v<d.length;v+=1)d[v]&&d[v].m(l,y);f(l,r,y),s=!0},p:Q,i(l){if(!s){h(t.$$.fragment,l),h(n.$$.fragment,l);for(let y=0;y<i.length;y+=1)h(d[y]);s=!0}},o(l){u(t.$$.fragment,l),u(n.$$.fragment,l),d=d.filter(Boolean);for(let y=0;y<d.length;y+=1)u(d[y]);s=!1},d(l){_(t,l),_(n,l),ee(d,l),l&&a(r)}}}function Ti(m){let t,n,r,s=Array(4),i=[];for(let l=0;l<s.length;l+=1)i[l]=yo(so(m,s,l));const d=l=>u(i[l],1,1,()=>{i[l]=null});return{c(){t=ne("svg"),n=ne("g");for(let l=0;l<i.length;l+=1)i[l].c();this.h()},l(l){t=re(l,"svg",{viewBox:!0});var y=S(t);n=re(y,"g",{transform:!0});var v=S(n);for(let b=0;b<i.length;b+=1)i[b].l(v);v.forEach(a),y.forEach(a),this.h()},h(){H(n,"transform","translate(0 -20)"),H(t,"viewBox","0 0 200 130")},m(l,y){f(l,t,y),T(t,n);for(let v=0;v<i.length;v+=1)i[v]&&i[v].m(n,null);r=!0},p(l,y){if(y&16){s=Array(4);let v;for(v=0;v<s.length;v+=1){const b=so(l,s,v);i[v]?(i[v].p(b,y),h(i[v],1)):(i[v]=yo(b),i[v].c(),h(i[v],1),i[v].m(n,null))}for(at(),v=s.length;v<i.length;v+=1)d(v);st()}},i(l){if(!r){for(let y=0;y<s.length;y+=1)h(i[y]);r=!0}},o(l){i=i.filter(Boolean);for(let y=0;y<i.length;y+=1)u(i[y]);r=!1},d(l){l&&a(t),ee(i,l)}}}function zi(m){let t;return{c(){t=x("cross-attention")},l(n){t=A(n,"cross-attention")},m(n,r){f(n,t,r)},d(n){n&&a(t)}}}function Wi(m){let t,n,r,s,i,d,l,y,v,b,E,$,k,p,L,I,F,C,Y,D,j,B,U,V,K,jn,vt,ha,Un,kt,ua,Kn,xt,da,Cn,ot,Qn,At,Yn,Et,pa,Gn,de,ga,pe,ca,Rn,ge,Jn,Tt,wa,On,ce,Xn,zt,_a,Zn,ae,ya,Wt,ba,va,St,ka,xa,er,se,Aa,Pt,Ea,Ta,qt,za,Wa,tr,Dt,Sa,nr,It,Pa,rr,te,qa,we,Da,_e,Ia,ye,Na,ar,be,sr,Nt,Ba,or,Bt,La,ir,it,ve,lr,Lt,Va,fr,lt,ke,$r,Vt,Fa,mr,fe,xe,Ha,Ae,hr,Ft,Ma,ur,Ht,ja,dr,ft,Ee,pr,Mt,Ua,gr,$t,Te,cr,X,Ka,ze,Ca,We,Qa,Se,Ya,Pe,Ga,qe,Ra,wr,De,_r,jt,Ja,yr,mt,Ie,br,ie,Oa,Ne,Xa,Be,Za,vr,ht,kr,Ut,es,xr,R,ts,Le,ns,Ve,rs,Fe,as,He,ss,Me,os,je,is,Ar,Ue,Er,Kt,ls,Tr,ut,zr,Ct,Wr,Qt,fs,Sr,Yt,$s,Pr,Gt,ms,qr,Ke,Dr,Ce,Ir,Rt,hs,Nr,dt,Br,Jt,Lr,Ot,us,Vr,Xt,ds,Fr,Qe,Hr,Ye,ps,pt,gs,Mr,Zt,cs,jr,Ge,Ur,en,ws,Kr,gt,Cr,tn,Qr,nn,_s,Yr,rn,ys,Gr,Re,Rr,an,bs,Jr,Je,Or,sn,vs,Xr,Oe,ks,Xe,xs,Zr,on,As,ea,ct,ta,ln,na,fn,Es,ra,$n,Ts,aa,mn,zs,sa,wt,Ze,Ws,Ss,oa,_t,et,Ps,qs,ia,tt,Ds,nt,Is,Ns,la;return y=new ue({props:{$$slots:{default:[Lo]},$$scope:{ctx:m}}}),v=new ao({props:{type:"reference",id:1}}),D=new le({props:{maxWidth:"500px",$$slots:{default:[Vo]},$$scope:{ctx:m}}}),ot=new hn({props:{code:`class Embeddings(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.position_embedding = nn.Embedding(max_len, embedding_dim)
        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)

    def forward(self, x):
        seq_len = x.shape[1]
        token_embedding = self.token_embedding(x)
        position_embedding = self.position_embedding(
            torch.arange(0, seq_len, device=device).view(1, seq_len)
        )
        return token_embedding + position_embedding`}}),pe=new ue({props:{$$slots:{default:[Fo]},$$scope:{ctx:m}}}),ge=new le({props:{maxWidth:"500px",$$slots:{default:[Mo]},$$scope:{ctx:m}}}),ce=new To({props:{type:"info",$$slots:{default:[jo]},$$scope:{ctx:m}}}),we=new ue({props:{$$slots:{default:[Uo]},$$scope:{ctx:m}}}),_e=new ue({props:{$$slots:{default:[Ko]},$$scope:{ctx:m}}}),ye=new ue({props:{$$slots:{default:[Co]},$$scope:{ctx:m}}}),be=new le({props:{maxWidth:"500px",$$slots:{default:[Yo]},$$scope:{ctx:m}}}),ve=new O({props:{$$slots:{default:[Go]},$$scope:{ctx:m}}}),ke=new O({props:{$$slots:{default:[Ro]},$$scope:{ctx:m}}}),xe=new O({props:{$$slots:{default:[Jo]},$$scope:{ctx:m}}}),Ae=new O({props:{$$slots:{default:[Oo]},$$scope:{ctx:m}}}),Ee=new O({props:{$$slots:{default:[Xo]},$$scope:{ctx:m}}}),Te=new O({props:{$$slots:{default:[Zo]},$$scope:{ctx:m}}}),ze=new O({props:{$$slots:{default:[ei]},$$scope:{ctx:m}}}),We=new O({props:{$$slots:{default:[ti]},$$scope:{ctx:m}}}),Se=new O({props:{$$slots:{default:[ni]},$$scope:{ctx:m}}}),Pe=new O({props:{$$slots:{default:[ri]},$$scope:{ctx:m}}}),qe=new O({props:{$$slots:{default:[ai]},$$scope:{ctx:m}}}),De=new le({props:{maxWidth:"400px",$$slots:{default:[si]},$$scope:{ctx:m}}}),Ie=new O({props:{$$slots:{default:[oi]},$$scope:{ctx:m}}}),Ne=new O({props:{$$slots:{default:[ii]},$$scope:{ctx:m}}}),Be=new ue({props:{$$slots:{default:[li]},$$scope:{ctx:m}}}),ht=new hn({props:{code:`def attention(query, key, value, mask=None):
    scores = (query @ key.transpose(1, 2)) / torch.tensor(
        embedding_dim, device=device
    ).sqrt()
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    attn_weights = torch.softmax(scores, -1)
    attention = attn_weights @ value
    return attention
`}}),Le=new O({props:{$$slots:{default:[fi]},$$scope:{ctx:m}}}),Ve=new ue({props:{$$slots:{default:[$i]},$$scope:{ctx:m}}}),Fe=new ue({props:{$$slots:{default:[mi]},$$scope:{ctx:m}}}),He=new O({props:{$$slots:{default:[hi]},$$scope:{ctx:m}}}),Me=new O({props:{$$slots:{default:[ui]},$$scope:{ctx:m}}}),je=new O({props:{$$slots:{default:[di]},$$scope:{ctx:m}}}),Ue=new le({props:{maxWidth:"400px",$$slots:{default:[pi]},$$scope:{ctx:m}}}),ut=new hn({props:{code:`class AttentionHead(nn.Module):
    def __init__(self):
        super().__init__()
        self.query = nn.Linear(embedding_dim, head_dim)
        self.key = nn.Linear(embedding_dim, head_dim)
        self.value = nn.Linear(embedding_dim, head_dim)

    def forward(self, query, key, value, mask=None):
        query = self.query(query)
        key = self.key(key)
        value = self.value(value)
        return attention(query, key, value, mask)

class MultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.heads = nn.ModuleList([AttentionHead() for _ in range(num_heads)])
        self.output = nn.Linear(embedding_dim, embedding_dim)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        x = [head(query, key, value, mask) for head in self.heads]
        x = torch.cat(x, dim=-1)
        x = self.dropout(self.output(x))
        return x`}}),Ke=new zo({props:{$$slots:{default:[gi]},$$scope:{ctx:m}}}),Ce=new le({props:{maxWidth:"600px",$$slots:{default:[_i]},$$scope:{ctx:m}}}),dt=new hn({props:{code:`class PWFeedForward(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(embedding_dim, fc_dim),
            nn.ReLU(),
            nn.Linear(fc_dim, embedding_dim),
            nn.Dropout(p=dropout),
        )

    def forward(self, x):
        return self.layers(x)`}}),Qe=new le({props:{maxWidth:"500px",$$slots:{default:[yi]},$$scope:{ctx:m}}}),pt=new ao({props:{type:"reference",id:2}}),Ge=new le({props:{maxWidth:"500px",$$slots:{default:[ki]},$$scope:{ctx:m}}}),gt=new hn({props:{code:`class EncoderLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm1 = nn.LayerNorm(embedding_dim)
        self.norm2 = nn.LayerNorm(embedding_dim)
        self.self_attention = MultiHeadAttention()
        self.feed_forward = PWFeedForward()

    def forward(self, src, mask=None):
        normalized = self.norm1(src)
        src = src + self.self_attention(normalized, normalized, normalized, mask)
        normalized = self.norm2(src)
        src = src + self.feed_forward(normalized)
        return src


class Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([EncoderLayer() for _ in range(num_layers)])

    def forward(self, src, mask=None):
        for encoder in self.layers:
            src = encoder(src, mask)
        return src`}}),Re=new le({props:{maxWidth:"500px",$$slots:{default:[xi]},$$scope:{ctx:m}}}),Je=new le({props:{maxWidth:"400px",$$slots:{default:[Ti]},$$scope:{ctx:m}}}),Xe=new ue({props:{$$slots:{default:[zi]},$$scope:{ctx:m}}}),ct=new hn({props:{code:`class DecoderLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm1 = nn.LayerNorm(embedding_dim)
        self.norm2 = nn.LayerNorm(embedding_dim)
        self.norm3 = nn.LayerNorm(embedding_dim)
        self.self_attention = MultiHeadAttention()
        self.cross_attention = MultiHeadAttention()
        self.feed_forward = PWFeedForward()

    def forward(self, src, trg, src_mask, trg_mask):
        normalized = self.norm1(trg)
        trg = trg + self.self_attention(normalized, normalized, normalized, trg_mask)
        normalized = self.norm2(trg)
        trg = trg + self.cross_attention(trg, src, src, src_mask)
        normalized = self.norm3(trg)
        trg = trg + self.feed_forward(normalized)
        return trg


class Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([DecoderLayer() for _ in range(num_layers)])

    def forward(self, src, trg, src_mask=None, trg_mask=None):
        for decoder in self.layers:
            trg = decoder(src, trg, src_mask, trg_mask)
        return trg
`}}),{c(){t=P("h1"),n=x("Transformer"),r=z(),s=P("div"),i=z(),d=P("p"),l=x(`In the year 2017 researchers from Google introduced the so called
    `),g(y.$$.fragment),g(v.$$.fragment),b=x(`.
    Transformers have taken the world by storm after their initial release,
    starting with NLP first and slowly but surely spilling into computer vision,
    reinforcement learning and other domains. Nowadays transformers dominate
    most deep learning research and are an integral part of most state of the
    art models.`),E=z(),$=P("div"),k=z(),p=P("h2"),L=x("Encoder and Decoder"),I=z(),F=P("p"),C=x(`The original paper introduced the transformer as a language translation
    tool. Similar to recurrent seq-to-seq models the transformer is structured
    as an encoder-decoder architecture. The encoder takes the original sentence,
    processes each word in a series of layers and passes the results to the
    decoder, which in turn produces a translated version of the input sentence.`),Y=z(),g(D.$$.fragment),j=z(),B=P("p"),U=x(`The source text and the output text are embedded by their individual
    embedding layers, before they are transferred to the encoder and decoder
    respectively. We depict the encoder slightly smaller, due to a somewhat more
    complex nature of the decoder, but the components of the encoder and the
    decoder are actually almost identical. The Nx to the right of the encoder
    and to the left of the decoder indicate that both blocks are actually made
    up of several stacked layers. In the original paper 6 encoder and 6 decoder
    layers were utilized.`),V=z(),K=P("div"),jn=z(),vt=P("h2"),ha=x("Embeddings"),Un=z(),kt=P("p"),ua=x(`When we use a recurrent net, the relative position of the word in a sentence
    is implicitly conveyed to the network, because the words are processed in an
    ordered fashion. A transformer on the other hand processes all words in a
    sentence at the same time, without caring for the relative position of the
    word. Yet the order in which a word appears in a sentence does matter for
    the meaning of that sentece. We need to somehow inject addtioinal positional
    information into the embeddings.`),Kn=z(),xt=P("p"),da=x(`For that purpose we will use an additional embedding layer. We define an
    embedding layer which has as many embeddings, as the maximal sentence
    lengths requires. If you expect the longest sentence to consist of 100
    tokens, you will need to encode 100 values. The first token in the sentence
    will get an embedding that corresponds to index 0, the second word the
    embedding that corresponds to index 1 and so on. The output of the token
    embedding and the positional embedding is a 512 dimensional vector. We add
    both values to get our final embedding.`),Cn=z(),g(ot.$$.fragment),Qn=z(),At=P("div"),Yn=z(),Et=P("h2"),pa=x("Attention"),Gn=z(),de=P("p"),ga=x("The type of attention that the transformer uses is called "),g(pe.$$.fragment),ca=x(`. Given a sequence of tokens, each token focuses on all parts of the
    sequence at the same time (including itself), but with different levels of
    attention, called attention weights.`),Rn=z(),g(ge.$$.fragment),Jn=z(),Tt=P("p"),wa=x(`We miltiply attention weights with each of the token embeddings and add up
    the results, thereby creating a new embedding, that is more aware of the
    surrounding context of the word.`),On=z(),g(ce.$$.fragment),Xn=z(),zt=P("p"),_a=x(`The easiest way to explain what that means is to look at so called homonyms.
    Words that are written the same, but have a different meaning. Let's for
    example look the meaning of the word date.`),Zn=z(),ae=P("p"),ya=x("What is your "),Wt=P("span"),ba=x("date"),va=x(` of
    `),St=P("span"),ka=x("birthday"),xa=x("?"),er=z(),se=P("p"),Aa=x("The "),Pt=P("span"),Ea=x("date"),Ta=x(` is my favourite
    `),qt=P("span"),za=x("fruit"),Wa=x("."),tr=z(),Dt=P("p"),Sa=x(`In the first sentence the word date will pay attention to itself, but also
    to birthday and will incorporate the word date and the information that
    relates to time into a single vector. In the second sentence, the word date
    will pay attention to itself and the word fruit, incorporating the
    "fruitiness" aspect into the vector of the word date.`),nr=z(),It=P("p"),Pa=x(`Without the self-attention mechanism we would not be able to differentiate
    between the two words, because word embeddings produce the same vector for
    the same word, without incorporating the context that surrounds the word.
    But attention is obviously also useful for words other than homonyms,
    because it allows to create an embedding for each word, that is specific to
    the exact context that the word is surrounded by.`),rr=z(),te=P("p"),qa=x(`In practice the self-attention mechanism in transformers is inspired by
    information retrieval systems like database queries or search engines.
    Theses systems are based on notions of a `),g(we.$$.fragment),Da=x(", a "),g(_e.$$.fragment),Ia=x(`
    and a `),g(ye.$$.fragment),Na=x("."),ar=z(),g(be.$$.fragment),sr=z(),Nt=P("p"),Ba=x(`In a classical database, like the one above, it is relatively clear what
    values you will get back from your query. The value is returned, if the
    query alligns with the key. If for example we use the query "SELECT value
    WHERE key='key 1'", we should get value 1 in return.`),or=z(),Bt=P("p"),La=x(`When we deal with transformers we can think about a more "fuzzy" database,
    where we don't get a single value for a query, but a weighted sum of all
    values in the database. Let's for simplicity assume, that we have only two
    entries in the database with the following vector based keys.`),ir=z(),it=P("div"),g(ve.$$.fragment),lr=z(),Lt=P("p"),Va=x("We use the following vector based query."),fr=z(),lt=P("div"),g(ke.$$.fragment),$r=z(),Vt=P("p"),Fa=x(`We can determine the similarity between the query and each of the keys by
    calculating the dot product and we end up with the following results.`),mr=z(),fe=P("div"),g(xe.$$.fragment),Ha=z(),g(Ae.$$.fragment),hr=z(),Ft=P("p"),Ma=x(`The similarity between the query and the first key is larger than with the
    second key, because the query and the first key are identical. The query and
    the second key are also somewhat related, because they have identical values
    in some of the vector spots.`),ur=z(),Ht=P("p"),ja=x(`We can use these similarity scores to calculate the attention weights, by
    using them as input into the softmax function.`),dr=z(),ft=P("div"),g(Ee.$$.fragment),pr=z(),Mt=P("p"),Ua=x(`Finally we use attention weights to calculate the weighted sum of the values
    from the database. This is the value that you retrieve from the database. A`),gr=z(),$t=P("div"),g(Te.$$.fragment),cr=z(),X=P("p"),Ka=x(`The transformer is loosely based on this idea. In order to calculate the
    attention the transformer takes embeddings `),g(ze.$$.fragment),Ca=x(` as an input. These
    can be original embeddings from the embedding layer, or outputs from a previous
    encoder/decoder layer. These embeddings are used as inputs into three different
    linear layers (without any activations), producing queries
    `),g(We.$$.fragment),Qa=x(", keys "),g(Se.$$.fragment),Ya=x(" and values "),g(Pe.$$.fragment),Ga=x(` respectively.
    Those three are used to calculate the attention `),g(qe.$$.fragment),Ra=x(`. As the
    queries, keys and values are all based on the same inputs we are still
    dealing with self attention, but the linear layers introduce weights, that
    make the attention mechanism more powerful.`),wr=z(),g(De.$$.fragment),_r=z(),jt=P("p"),Ja=x(`The dimensions of the three matrices are identical: (batch size, sequence
    length, embedding dimension). This allows us to calculate the attention for
    all tokens and all batches in parallel.`),yr=z(),mt=P("div"),g(Ie.$$.fragment),br=z(),ie=P("p"),Oa=x("The only variable that is unknown to us is "),g(Ne.$$.fragment),Xa=x(`,
    the dimension of the key. If we are dealing with a 64 dimensional vector
    embedding for example, we have to divide the similarity by the root of 64.
    According to the authors this is done, because if the similarity between two
    vectors is too strong, the softmax might get into a region with very low
    gradients. The scaling helps to alleviate that problem. The whole expression
    above is called `),g(Be.$$.fragment),Za=x("."),vr=z(),g(ht.$$.fragment),kr=z(),Ut=P("p"),es=x(`In the code snippet above we additionally use a so called attention mask.
    The mask is used when we want the transformer to ignore a certain part of
    the sentence. If the values of the mask amount to 0, we replace the scores
    by a value of minus infinity, which essentially amounts to attention weights
    of 0 due to the softmax.`),xr=z(),R=P("p"),ts=x(`There is still one caveat we need to discuss. Instead of calculating a
    single attention `),g(Le.$$.fragment),ns=x(", we calculate a so called "),g(Ve.$$.fragment),rs=x(". A single "),g(Fe.$$.fragment),as=x(" calculates a separate "),g(He.$$.fragment),ss=x(", "),g(Me.$$.fragment),os=x(" and "),g(je.$$.fragment),is=x(`, but with a reduced embedding
    dimensionality. Instead of full 512 dimensional embeddings, each head uses
    only 64 dimensional vectors. Alltogether the transformer uses 8 heads, wich
    are concatenated in the final step.`),Ar=z(),g(Ue.$$.fragment),Er=z(),Kt=P("p"),ls=x(`This procedure might be useful, because each head can learn to focus on a
    separate context, thereby improving the performance of the transformer.`),Tr=z(),g(ut.$$.fragment),zr=z(),Ct=P("div"),Wr=z(),Qt=P("h2"),fs=x("Position-wise Feed-Forward Networks"),Sr=z(),Yt=P("p"),$s=x(`The encoder and decoder apply a so called position-wise feed-forward neural
    network. In essence that means that the same network, with the same weights
    is applied to each position of the sentence individually. Each embedded word
    in the sequence is passed though the network without interacting with any
    other word.`),Pr=z(),Gt=P("p"),ms=x(`The position-wise network is a two-layer neural network, that takes an
    embedding of size 512, increases the dimensionality to 2048 in the first
    linear layer, applies a ReLU activation function, followed again by a linear
    layer that transforms the embeddings back to lengths 512.`),qr=z(),g(Ke.$$.fragment),Dr=z(),g(Ce.$$.fragment),Ir=z(),Rt=P("p"),hs=x(`PyTorch does this procedure automatically. Each dimension of a tensor,
    except for the last one is treated similar to a batch dimension. Only the
    last dimension, the embedding dimension, is processed through the neural
    network. The batch dimensions are regarded as additional samples, which can
    be are processed simultaneouly on the GPU.`),Nr=z(),g(dt.$$.fragment),Br=z(),Jt=P("div"),Lr=z(),Ot=P("h2"),us=x("Encoder Layer"),Vr=z(),Xt=P("p"),ds=x(`The encoder layer is a combination of two sublayers: a multihead attention
    and a position-wise feed-forward neural network. Both sublayers make up an
    encoder layer, that is stacked N times.`),Fr=z(),g(Qe.$$.fragment),Hr=z(),Ye=P("p"),ps=x(`After both sublayers we use an "Add & Norm" block. The "Add" component
    indicates that we are using skip connections in order to mitigate vanishing
    gradients and stabilize training. The "Norm" part indicates, that we
    normalize the values, before we send the results to the next layer or
    sub-layer. In the original paper the authors used a so called layer
    normalization`),g(pt.$$.fragment),gs=x(`. When we use layer
    norm we do not calculate the mean and the standard deviation for the same
    features over the different batches, but over the different features within
    the same batch.`),Mr=z(),Zt=P("p"),cs=x(`Assuming we use a batch size of 5 and 10 features, the two approaches would
    differ in the following way.`),jr=z(),g(Ge.$$.fragment),Ur=z(),en=P("p"),ws=x(`You will notice in practice, that many modern implementation deviate from
    the original by normalizing the values first, before they are used as inputs
    into the sublayers. This is found to work better empirically and we do the
    same in the code snippets below.`),Kr=z(),g(gt.$$.fragment),Cr=z(),tn=P("div"),Qr=z(),nn=P("h2"),_s=x("Decoder Layer"),Yr=z(),rn=P("p"),ys=x(`The decoder layer is also stacks multihead-attention and position-wise
    feed-forward networks, but the implementation details are slightly
    different.`),Gr=z(),g(Re.$$.fragment),Rr=z(),an=P("p"),bs=x(`The embeddings from the target text are masked. This means that when we use
    multihead attention, the attention mechanism is only allowed to pay
    attention to words that were already generated. If that wouldn't be the
    case, the transformer would be allowed to cheat, by looking at the words it
    is expected to produce.`),Jr=z(),g(Je.$$.fragment),Or=z(),sn=P("p"),vs=x(`When we are about to produce the first word, the transformer is only allowed
    to see the start of sequence token. If it is about to produce the word "is",
    it is only allowed to additionally see the word "what". The transformer can
    pay attention to the words that came before, but never future words. To
    accomplish that practically we create a mask, which contains zeros at future
    positions.`),Xr=z(),Oe=P("p"),ks=x(`You have already probably noticed, that the decoder has an additional
    attention layer. The second multi-head attention layer combines the encoder
    with the decoder. This time the queries, values and keys do not come from
    the same embeddings. The query is based on the decoder embeddings, while the
    key and the value are based on the output of the last encoder layer. This
    attention mechanism is called `),g(Xe.$$.fragment),xs=x("."),Zr=z(),on=P("p"),As=x("The rest of the implementation is similar to the encoder."),ea=z(),g(ct.$$.fragment),ta=z(),ln=P("div"),na=z(),fn=P("h2"),Es=x("Further Sources"),ra=z(),$n=P("p"),Ts=x(`Understanding the transformer with all the details is not an easy task. It
    is unlikely that the section above is sufficient to completely cover this
    architecture. You should therefore study as many sources as possible. Up to
    this day the transformer is the most performant architecture in deep
    learning and it is essential to have a solid understanding of the basic
    principles of this architecture.`),aa=z(),mn=P("p"),zs=x(`You have to read the original paper by Vasvani et. al. We had to omit some
    of the implementation details, so if you want to implement the transformer
    on your own, reading this paper is a must.`),sa=z(),wt=P("p"),Ze=P("a"),Ws=x('"The Illustrated Transformer"'),Ss=x(` by Jay Alamar is a great resource if you need additional intuitive illustrations
    and explanations.`),oa=z(),_t=P("p"),et=P("a"),Ps=x('"The Annotated Transformer"'),qs=x(` from the Harvard University is a great choice if you need an in depths PyTorch
    implementation.`),ia=z(),tt=P("p"),Ds=x("The book "),nt=P("a"),Is=x('"Natural Language Processing with Transformers"'),Ns=x(` covers theory and applications of different transformer models in a very approachable
    manner.`),this.h()},l(e){t=q(e,"H1",{});var o=S(t);n=A(o,"Transformer"),o.forEach(a),r=W(e),s=q(e,"DIV",{class:!0}),S(s).forEach(a),i=W(e),d=q(e,"P",{});var rt=S(d);l=A(rt,`In the year 2017 researchers from Google introduced the so called
    `),c(y.$$.fragment,rt),c(v.$$.fragment,rt),b=A(rt,`.
    Transformers have taken the world by storm after their initial release,
    starting with NLP first and slowly but surely spilling into computer vision,
    reinforcement learning and other domains. Nowadays transformers dominate
    most deep learning research and are an integral part of most state of the
    art models.`),rt.forEach(a),E=W(e),$=q(e,"DIV",{class:!0}),S($).forEach(a),k=W(e),p=q(e,"H2",{});var un=S(p);L=A(un,"Encoder and Decoder"),un.forEach(a),I=W(e),F=q(e,"P",{});var dn=S(F);C=A(dn,`The original paper introduced the transformer as a language translation
    tool. Similar to recurrent seq-to-seq models the transformer is structured
    as an encoder-decoder architecture. The encoder takes the original sentence,
    processes each word in a series of layers and passes the results to the
    decoder, which in turn produces a translated version of the input sentence.`),dn.forEach(a),Y=W(e),c(D.$$.fragment,e),j=W(e),B=q(e,"P",{});var pn=S(B);U=A(pn,`The source text and the output text are embedded by their individual
    embedding layers, before they are transferred to the encoder and decoder
    respectively. We depict the encoder slightly smaller, due to a somewhat more
    complex nature of the decoder, but the components of the encoder and the
    decoder are actually almost identical. The Nx to the right of the encoder
    and to the left of the decoder indicate that both blocks are actually made
    up of several stacked layers. In the original paper 6 encoder and 6 decoder
    layers were utilized.`),pn.forEach(a),V=W(e),K=q(e,"DIV",{class:!0}),S(K).forEach(a),jn=W(e),vt=q(e,"H2",{});var gn=S(vt);ha=A(gn,"Embeddings"),gn.forEach(a),Un=W(e),kt=q(e,"P",{});var cn=S(kt);ua=A(cn,`When we use a recurrent net, the relative position of the word in a sentence
    is implicitly conveyed to the network, because the words are processed in an
    ordered fashion. A transformer on the other hand processes all words in a
    sentence at the same time, without caring for the relative position of the
    word. Yet the order in which a word appears in a sentence does matter for
    the meaning of that sentece. We need to somehow inject addtioinal positional
    information into the embeddings.`),cn.forEach(a),Kn=W(e),xt=q(e,"P",{});var wn=S(xt);da=A(wn,`For that purpose we will use an additional embedding layer. We define an
    embedding layer which has as many embeddings, as the maximal sentence
    lengths requires. If you expect the longest sentence to consist of 100
    tokens, you will need to encode 100 values. The first token in the sentence
    will get an embedding that corresponds to index 0, the second word the
    embedding that corresponds to index 1 and so on. The output of the token
    embedding and the positional embedding is a 512 dimensional vector. We add
    both values to get our final embedding.`),wn.forEach(a),Cn=W(e),c(ot.$$.fragment,e),Qn=W(e),At=q(e,"DIV",{class:!0}),S(At).forEach(a),Yn=W(e),Et=q(e,"H2",{});var _n=S(Et);pa=A(_n,"Attention"),_n.forEach(a),Gn=W(e),de=q(e,"P",{});var yt=S(de);ga=A(yt,"The type of attention that the transformer uses is called "),c(pe.$$.fragment,yt),ca=A(yt,`. Given a sequence of tokens, each token focuses on all parts of the
    sequence at the same time (including itself), but with different levels of
    attention, called attention weights.`),yt.forEach(a),Rn=W(e),c(ge.$$.fragment,e),Jn=W(e),Tt=q(e,"P",{});var yn=S(Tt);wa=A(yn,`We miltiply attention weights with each of the token embeddings and add up
    the results, thereby creating a new embedding, that is more aware of the
    surrounding context of the word.`),yn.forEach(a),On=W(e),c(ce.$$.fragment,e),Xn=W(e),zt=q(e,"P",{});var bn=S(zt);_a=A(bn,`The easiest way to explain what that means is to look at so called homonyms.
    Words that are written the same, but have a different meaning. Let's for
    example look the meaning of the word date.`),bn.forEach(a),Zn=W(e),ae=q(e,"P",{class:!0});var $e=S(ae);ya=A($e,"What is your "),Wt=q($e,"SPAN",{class:!0});var vn=S(Wt);ba=A(vn,"date"),vn.forEach(a),va=A($e,` of
    `),St=q($e,"SPAN",{class:!0});var kn=S(St);ka=A(kn,"birthday"),kn.forEach(a),xa=A($e,"?"),$e.forEach(a),er=W(e),se=q(e,"P",{class:!0});var me=S(se);Aa=A(me,"The "),Pt=q(me,"SPAN",{class:!0});var xn=S(Pt);Ea=A(xn,"date"),xn.forEach(a),Ta=A(me,` is my favourite
    `),qt=q(me,"SPAN",{class:!0});var An=S(qt);za=A(An,"fruit"),An.forEach(a),Wa=A(me,"."),me.forEach(a),tr=W(e),Dt=q(e,"P",{});var En=S(Dt);Sa=A(En,`In the first sentence the word date will pay attention to itself, but also
    to birthday and will incorporate the word date and the information that
    relates to time into a single vector. In the second sentence, the word date
    will pay attention to itself and the word fruit, incorporating the
    "fruitiness" aspect into the vector of the word date.`),En.forEach(a),nr=W(e),It=q(e,"P",{});var Tn=S(It);Pa=A(Tn,`Without the self-attention mechanism we would not be able to differentiate
    between the two words, because word embeddings produce the same vector for
    the same word, without incorporating the context that surrounds the word.
    But attention is obviously also useful for words other than homonyms,
    because it allows to create an embedding for each word, that is specific to
    the exact context that the word is surrounded by.`),Tn.forEach(a),rr=W(e),te=q(e,"P",{});var oe=S(te);qa=A(oe,`In practice the self-attention mechanism in transformers is inspired by
    information retrieval systems like database queries or search engines.
    Theses systems are based on notions of a `),c(we.$$.fragment,oe),Da=A(oe,", a "),c(_e.$$.fragment,oe),Ia=A(oe,`
    and a `),c(ye.$$.fragment,oe),Na=A(oe,"."),oe.forEach(a),ar=W(e),c(be.$$.fragment,e),sr=W(e),Nt=q(e,"P",{});var zn=S(Nt);Ba=A(zn,`In a classical database, like the one above, it is relatively clear what
    values you will get back from your query. The value is returned, if the
    query alligns with the key. If for example we use the query "SELECT value
    WHERE key='key 1'", we should get value 1 in return.`),zn.forEach(a),or=W(e),Bt=q(e,"P",{});var Wn=S(Bt);La=A(Wn,`When we deal with transformers we can think about a more "fuzzy" database,
    where we don't get a single value for a query, but a weighted sum of all
    values in the database. Let's for simplicity assume, that we have only two
    entries in the database with the following vector based keys.`),Wn.forEach(a),ir=W(e),it=q(e,"DIV",{class:!0});var Sn=S(it);c(ve.$$.fragment,Sn),Sn.forEach(a),lr=W(e),Lt=q(e,"P",{});var Pn=S(Lt);Va=A(Pn,"We use the following vector based query."),Pn.forEach(a),fr=W(e),lt=q(e,"DIV",{class:!0});var qn=S(lt);c(ke.$$.fragment,qn),qn.forEach(a),$r=W(e),Vt=q(e,"P",{});var Dn=S(Vt);Fa=A(Dn,`We can determine the similarity between the query and each of the keys by
    calculating the dot product and we end up with the following results.`),Dn.forEach(a),mr=W(e),fe=q(e,"DIV",{class:!0});var bt=S(fe);c(xe.$$.fragment,bt),Ha=W(bt),c(Ae.$$.fragment,bt),bt.forEach(a),hr=W(e),Ft=q(e,"P",{});var In=S(Ft);Ma=A(In,`The similarity between the query and the first key is larger than with the
    second key, because the query and the first key are identical. The query and
    the second key are also somewhat related, because they have identical values
    in some of the vector spots.`),In.forEach(a),ur=W(e),Ht=q(e,"P",{});var Nn=S(Ht);ja=A(Nn,`We can use these similarity scores to calculate the attention weights, by
    using them as input into the softmax function.`),Nn.forEach(a),dr=W(e),ft=q(e,"DIV",{class:!0});var Bn=S(ft);c(Ee.$$.fragment,Bn),Bn.forEach(a),pr=W(e),Mt=q(e,"P",{});var Ln=S(Mt);Ua=A(Ln,`Finally we use attention weights to calculate the weighted sum of the values
    from the database. This is the value that you retrieve from the database. A`),Ln.forEach(a),gr=W(e),$t=q(e,"DIV",{class:!0});var Vn=S($t);c(Te.$$.fragment,Vn),Vn.forEach(a),cr=W(e),X=q(e,"P",{});var Z=S(X);Ka=A(Z,`The transformer is loosely based on this idea. In order to calculate the
    attention the transformer takes embeddings `),c(ze.$$.fragment,Z),Ca=A(Z,` as an input. These
    can be original embeddings from the embedding layer, or outputs from a previous
    encoder/decoder layer. These embeddings are used as inputs into three different
    linear layers (without any activations), producing queries
    `),c(We.$$.fragment,Z),Qa=A(Z,", keys "),c(Se.$$.fragment,Z),Ya=A(Z," and values "),c(Pe.$$.fragment,Z),Ga=A(Z,` respectively.
    Those three are used to calculate the attention `),c(qe.$$.fragment,Z),Ra=A(Z,`. As the
    queries, keys and values are all based on the same inputs we are still
    dealing with self attention, but the linear layers introduce weights, that
    make the attention mechanism more powerful.`),Z.forEach(a),wr=W(e),c(De.$$.fragment,e),_r=W(e),jt=q(e,"P",{});var Fn=S(jt);Ja=A(Fn,`The dimensions of the three matrices are identical: (batch size, sequence
    length, embedding dimension). This allows us to calculate the attention for
    all tokens and all batches in parallel.`),Fn.forEach(a),yr=W(e),mt=q(e,"DIV",{class:!0});var Hn=S(mt);c(Ie.$$.fragment,Hn),Hn.forEach(a),br=W(e),ie=q(e,"P",{});var he=S(ie);Oa=A(he,"The only variable that is unknown to us is "),c(Ne.$$.fragment,he),Xa=A(he,`,
    the dimension of the key. If we are dealing with a 64 dimensional vector
    embedding for example, we have to divide the similarity by the root of 64.
    According to the authors this is done, because if the similarity between two
    vectors is too strong, the softmax might get into a region with very low
    gradients. The scaling helps to alleviate that problem. The whole expression
    above is called `),c(Be.$$.fragment,he),Za=A(he,"."),he.forEach(a),vr=W(e),c(ht.$$.fragment,e),kr=W(e),Ut=q(e,"P",{});var Mn=S(Ut);es=A(Mn,`In the code snippet above we additionally use a so called attention mask.
    The mask is used when we want the transformer to ignore a certain part of
    the sentence. If the values of the mask amount to 0, we replace the scores
    by a value of minus infinity, which essentially amounts to attention weights
    of 0 due to the softmax.`),Mn.forEach(a),xr=W(e),R=q(e,"P",{});var J=S(R);ts=A(J,`There is still one caveat we need to discuss. Instead of calculating a
    single attention `),c(Le.$$.fragment,J),ns=A(J,", we calculate a so called "),c(Ve.$$.fragment,J),rs=A(J,". A single "),c(Fe.$$.fragment,J),as=A(J," calculates a separate "),c(He.$$.fragment,J),ss=A(J,", "),c(Me.$$.fragment,J),os=A(J," and "),c(je.$$.fragment,J),is=A(J,`, but with a reduced embedding
    dimensionality. Instead of full 512 dimensional embeddings, each head uses
    only 64 dimensional vectors. Alltogether the transformer uses 8 heads, wich
    are concatenated in the final step.`),J.forEach(a),Ar=W(e),c(Ue.$$.fragment,e),Er=W(e),Kt=q(e,"P",{});var Vs=S(Kt);ls=A(Vs,`This procedure might be useful, because each head can learn to focus on a
    separate context, thereby improving the performance of the transformer.`),Vs.forEach(a),Tr=W(e),c(ut.$$.fragment,e),zr=W(e),Ct=q(e,"DIV",{class:!0}),S(Ct).forEach(a),Wr=W(e),Qt=q(e,"H2",{});var Fs=S(Qt);fs=A(Fs,"Position-wise Feed-Forward Networks"),Fs.forEach(a),Sr=W(e),Yt=q(e,"P",{});var Hs=S(Yt);$s=A(Hs,`The encoder and decoder apply a so called position-wise feed-forward neural
    network. In essence that means that the same network, with the same weights
    is applied to each position of the sentence individually. Each embedded word
    in the sequence is passed though the network without interacting with any
    other word.`),Hs.forEach(a),Pr=W(e),Gt=q(e,"P",{});var Ms=S(Gt);ms=A(Ms,`The position-wise network is a two-layer neural network, that takes an
    embedding of size 512, increases the dimensionality to 2048 in the first
    linear layer, applies a ReLU activation function, followed again by a linear
    layer that transforms the embeddings back to lengths 512.`),Ms.forEach(a),qr=W(e),c(Ke.$$.fragment,e),Dr=W(e),c(Ce.$$.fragment,e),Ir=W(e),Rt=q(e,"P",{});var js=S(Rt);hs=A(js,`PyTorch does this procedure automatically. Each dimension of a tensor,
    except for the last one is treated similar to a batch dimension. Only the
    last dimension, the embedding dimension, is processed through the neural
    network. The batch dimensions are regarded as additional samples, which can
    be are processed simultaneouly on the GPU.`),js.forEach(a),Nr=W(e),c(dt.$$.fragment,e),Br=W(e),Jt=q(e,"DIV",{class:!0}),S(Jt).forEach(a),Lr=W(e),Ot=q(e,"H2",{});var Us=S(Ot);us=A(Us,"Encoder Layer"),Us.forEach(a),Vr=W(e),Xt=q(e,"P",{});var Ks=S(Xt);ds=A(Ks,`The encoder layer is a combination of two sublayers: a multihead attention
    and a position-wise feed-forward neural network. Both sublayers make up an
    encoder layer, that is stacked N times.`),Ks.forEach(a),Fr=W(e),c(Qe.$$.fragment,e),Hr=W(e),Ye=q(e,"P",{});var fa=S(Ye);ps=A(fa,`After both sublayers we use an "Add & Norm" block. The "Add" component
    indicates that we are using skip connections in order to mitigate vanishing
    gradients and stabilize training. The "Norm" part indicates, that we
    normalize the values, before we send the results to the next layer or
    sub-layer. In the original paper the authors used a so called layer
    normalization`),c(pt.$$.fragment,fa),gs=A(fa,`. When we use layer
    norm we do not calculate the mean and the standard deviation for the same
    features over the different batches, but over the different features within
    the same batch.`),fa.forEach(a),Mr=W(e),Zt=q(e,"P",{});var Cs=S(Zt);cs=A(Cs,`Assuming we use a batch size of 5 and 10 features, the two approaches would
    differ in the following way.`),Cs.forEach(a),jr=W(e),c(Ge.$$.fragment,e),Ur=W(e),en=q(e,"P",{});var Qs=S(en);ws=A(Qs,`You will notice in practice, that many modern implementation deviate from
    the original by normalizing the values first, before they are used as inputs
    into the sublayers. This is found to work better empirically and we do the
    same in the code snippets below.`),Qs.forEach(a),Kr=W(e),c(gt.$$.fragment,e),Cr=W(e),tn=q(e,"DIV",{class:!0}),S(tn).forEach(a),Qr=W(e),nn=q(e,"H2",{});var Ys=S(nn);_s=A(Ys,"Decoder Layer"),Ys.forEach(a),Yr=W(e),rn=q(e,"P",{});var Gs=S(rn);ys=A(Gs,`The decoder layer is also stacks multihead-attention and position-wise
    feed-forward networks, but the implementation details are slightly
    different.`),Gs.forEach(a),Gr=W(e),c(Re.$$.fragment,e),Rr=W(e),an=q(e,"P",{});var Rs=S(an);bs=A(Rs,`The embeddings from the target text are masked. This means that when we use
    multihead attention, the attention mechanism is only allowed to pay
    attention to words that were already generated. If that wouldn't be the
    case, the transformer would be allowed to cheat, by looking at the words it
    is expected to produce.`),Rs.forEach(a),Jr=W(e),c(Je.$$.fragment,e),Or=W(e),sn=q(e,"P",{});var Js=S(sn);vs=A(Js,`When we are about to produce the first word, the transformer is only allowed
    to see the start of sequence token. If it is about to produce the word "is",
    it is only allowed to additionally see the word "what". The transformer can
    pay attention to the words that came before, but never future words. To
    accomplish that practically we create a mask, which contains zeros at future
    positions.`),Js.forEach(a),Xr=W(e),Oe=q(e,"P",{});var $a=S(Oe);ks=A($a,`You have already probably noticed, that the decoder has an additional
    attention layer. The second multi-head attention layer combines the encoder
    with the decoder. This time the queries, values and keys do not come from
    the same embeddings. The query is based on the decoder embeddings, while the
    key and the value are based on the output of the last encoder layer. This
    attention mechanism is called `),c(Xe.$$.fragment,$a),xs=A($a,"."),$a.forEach(a),Zr=W(e),on=q(e,"P",{});var Os=S(on);As=A(Os,"The rest of the implementation is similar to the encoder."),Os.forEach(a),ea=W(e),c(ct.$$.fragment,e),ta=W(e),ln=q(e,"DIV",{class:!0}),S(ln).forEach(a),na=W(e),fn=q(e,"H2",{});var Xs=S(fn);Es=A(Xs,"Further Sources"),Xs.forEach(a),ra=W(e),$n=q(e,"P",{});var Zs=S($n);Ts=A(Zs,`Understanding the transformer with all the details is not an easy task. It
    is unlikely that the section above is sufficient to completely cover this
    architecture. You should therefore study as many sources as possible. Up to
    this day the transformer is the most performant architecture in deep
    learning and it is essential to have a solid understanding of the basic
    principles of this architecture.`),Zs.forEach(a),aa=W(e),mn=q(e,"P",{});var eo=S(mn);zs=A(eo,`You have to read the original paper by Vasvani et. al. We had to omit some
    of the implementation details, so if you want to implement the transformer
    on your own, reading this paper is a must.`),eo.forEach(a),sa=W(e),wt=q(e,"P",{});var Bs=S(wt);Ze=q(Bs,"A",{href:!0,target:!0,rel:!0});var to=S(Ze);Ws=A(to,'"The Illustrated Transformer"'),to.forEach(a),Ss=A(Bs,` by Jay Alamar is a great resource if you need additional intuitive illustrations
    and explanations.`),Bs.forEach(a),oa=W(e),_t=q(e,"P",{});var Ls=S(_t);et=q(Ls,"A",{href:!0,target:!0,rel:!0});var no=S(et);Ps=A(no,'"The Annotated Transformer"'),no.forEach(a),qs=A(Ls,` from the Harvard University is a great choice if you need an in depths PyTorch
    implementation.`),Ls.forEach(a),ia=W(e),tt=q(e,"P",{});var ma=S(tt);Ds=A(ma,"The book "),nt=q(ma,"A",{href:!0,target:!0,rel:!0});var ro=S(nt);Is=A(ro,'"Natural Language Processing with Transformers"'),ro.forEach(a),Ns=A(ma,` covers theory and applications of different transformer models in a very approachable
    manner.`),ma.forEach(a),this.h()},h(){H(s,"class","separator"),H($,"class","separator"),H(K,"class","separator"),H(At,"class","separator"),H(Wt,"class","bg-blue-100 inline-block p-1"),H(St,"class","bg-blue-100 inline-block p-1"),H(ae,"class","text-center"),H(Pt,"class","bg-red-100 inline-block p-1"),H(qt,"class","bg-red-100 inline-block p-1"),H(se,"class","text-center"),H(it,"class","flex justify-center"),H(lt,"class","flex justify-center"),H(fe,"class","flex flex-col justify-center items-center"),H(ft,"class","flex justify-center"),H($t,"class","flex justify-center"),H(mt,"class","flex justify-center"),H(Ct,"class","separator"),H(Jt,"class","separator"),H(tn,"class","separator"),H(ln,"class","separator"),H(Ze,"href","https://jalammar.github.io/illustrated-transformer/"),H(Ze,"target","_blank"),H(Ze,"rel","noreferrer"),H(et,"href","http://nlp.seas.harvard.edu/annotated-transformer/"),H(et,"target","_blank"),H(et,"rel","noreferrer"),H(nt,"href","https://transformersbook.com/"),H(nt,"target","_blank"),H(nt,"rel","noreferrer")},m(e,o){f(e,t,o),T(t,n),f(e,r,o),f(e,s,o),f(e,i,o),f(e,d,o),T(d,l),w(y,d,null),w(v,d,null),T(d,b),f(e,E,o),f(e,$,o),f(e,k,o),f(e,p,o),T(p,L),f(e,I,o),f(e,F,o),T(F,C),f(e,Y,o),w(D,e,o),f(e,j,o),f(e,B,o),T(B,U),f(e,V,o),f(e,K,o),f(e,jn,o),f(e,vt,o),T(vt,ha),f(e,Un,o),f(e,kt,o),T(kt,ua),f(e,Kn,o),f(e,xt,o),T(xt,da),f(e,Cn,o),w(ot,e,o),f(e,Qn,o),f(e,At,o),f(e,Yn,o),f(e,Et,o),T(Et,pa),f(e,Gn,o),f(e,de,o),T(de,ga),w(pe,de,null),T(de,ca),f(e,Rn,o),w(ge,e,o),f(e,Jn,o),f(e,Tt,o),T(Tt,wa),f(e,On,o),w(ce,e,o),f(e,Xn,o),f(e,zt,o),T(zt,_a),f(e,Zn,o),f(e,ae,o),T(ae,ya),T(ae,Wt),T(Wt,ba),T(ae,va),T(ae,St),T(St,ka),T(ae,xa),f(e,er,o),f(e,se,o),T(se,Aa),T(se,Pt),T(Pt,Ea),T(se,Ta),T(se,qt),T(qt,za),T(se,Wa),f(e,tr,o),f(e,Dt,o),T(Dt,Sa),f(e,nr,o),f(e,It,o),T(It,Pa),f(e,rr,o),f(e,te,o),T(te,qa),w(we,te,null),T(te,Da),w(_e,te,null),T(te,Ia),w(ye,te,null),T(te,Na),f(e,ar,o),w(be,e,o),f(e,sr,o),f(e,Nt,o),T(Nt,Ba),f(e,or,o),f(e,Bt,o),T(Bt,La),f(e,ir,o),f(e,it,o),w(ve,it,null),f(e,lr,o),f(e,Lt,o),T(Lt,Va),f(e,fr,o),f(e,lt,o),w(ke,lt,null),f(e,$r,o),f(e,Vt,o),T(Vt,Fa),f(e,mr,o),f(e,fe,o),w(xe,fe,null),T(fe,Ha),w(Ae,fe,null),f(e,hr,o),f(e,Ft,o),T(Ft,Ma),f(e,ur,o),f(e,Ht,o),T(Ht,ja),f(e,dr,o),f(e,ft,o),w(Ee,ft,null),f(e,pr,o),f(e,Mt,o),T(Mt,Ua),f(e,gr,o),f(e,$t,o),w(Te,$t,null),f(e,cr,o),f(e,X,o),T(X,Ka),w(ze,X,null),T(X,Ca),w(We,X,null),T(X,Qa),w(Se,X,null),T(X,Ya),w(Pe,X,null),T(X,Ga),w(qe,X,null),T(X,Ra),f(e,wr,o),w(De,e,o),f(e,_r,o),f(e,jt,o),T(jt,Ja),f(e,yr,o),f(e,mt,o),w(Ie,mt,null),f(e,br,o),f(e,ie,o),T(ie,Oa),w(Ne,ie,null),T(ie,Xa),w(Be,ie,null),T(ie,Za),f(e,vr,o),w(ht,e,o),f(e,kr,o),f(e,Ut,o),T(Ut,es),f(e,xr,o),f(e,R,o),T(R,ts),w(Le,R,null),T(R,ns),w(Ve,R,null),T(R,rs),w(Fe,R,null),T(R,as),w(He,R,null),T(R,ss),w(Me,R,null),T(R,os),w(je,R,null),T(R,is),f(e,Ar,o),w(Ue,e,o),f(e,Er,o),f(e,Kt,o),T(Kt,ls),f(e,Tr,o),w(ut,e,o),f(e,zr,o),f(e,Ct,o),f(e,Wr,o),f(e,Qt,o),T(Qt,fs),f(e,Sr,o),f(e,Yt,o),T(Yt,$s),f(e,Pr,o),f(e,Gt,o),T(Gt,ms),f(e,qr,o),w(Ke,e,o),f(e,Dr,o),w(Ce,e,o),f(e,Ir,o),f(e,Rt,o),T(Rt,hs),f(e,Nr,o),w(dt,e,o),f(e,Br,o),f(e,Jt,o),f(e,Lr,o),f(e,Ot,o),T(Ot,us),f(e,Vr,o),f(e,Xt,o),T(Xt,ds),f(e,Fr,o),w(Qe,e,o),f(e,Hr,o),f(e,Ye,o),T(Ye,ps),w(pt,Ye,null),T(Ye,gs),f(e,Mr,o),f(e,Zt,o),T(Zt,cs),f(e,jr,o),w(Ge,e,o),f(e,Ur,o),f(e,en,o),T(en,ws),f(e,Kr,o),w(gt,e,o),f(e,Cr,o),f(e,tn,o),f(e,Qr,o),f(e,nn,o),T(nn,_s),f(e,Yr,o),f(e,rn,o),T(rn,ys),f(e,Gr,o),w(Re,e,o),f(e,Rr,o),f(e,an,o),T(an,bs),f(e,Jr,o),w(Je,e,o),f(e,Or,o),f(e,sn,o),T(sn,vs),f(e,Xr,o),f(e,Oe,o),T(Oe,ks),w(Xe,Oe,null),T(Oe,xs),f(e,Zr,o),f(e,on,o),T(on,As),f(e,ea,o),w(ct,e,o),f(e,ta,o),f(e,ln,o),f(e,na,o),f(e,fn,o),T(fn,Es),f(e,ra,o),f(e,$n,o),T($n,Ts),f(e,aa,o),f(e,mn,o),T(mn,zs),f(e,sa,o),f(e,wt,o),T(wt,Ze),T(Ze,Ws),T(wt,Ss),f(e,oa,o),f(e,_t,o),T(_t,et),T(et,Ps),T(_t,qs),f(e,ia,o),f(e,tt,o),T(tt,Ds),T(tt,nt),T(nt,Is),T(tt,Ns),la=!0},p(e,o){const rt={};o&536870912&&(rt.$$scope={dirty:o,ctx:e}),y.$set(rt);const un={};o&536870912&&(un.$$scope={dirty:o,ctx:e}),D.$set(un);const dn={};o&536870912&&(dn.$$scope={dirty:o,ctx:e}),pe.$set(dn);const pn={};o&536870912&&(pn.$$scope={dirty:o,ctx:e}),ge.$set(pn);const gn={};o&536870912&&(gn.$$scope={dirty:o,ctx:e}),ce.$set(gn);const cn={};o&536870912&&(cn.$$scope={dirty:o,ctx:e}),we.$set(cn);const wn={};o&536870912&&(wn.$$scope={dirty:o,ctx:e}),_e.$set(wn);const _n={};o&536870912&&(_n.$$scope={dirty:o,ctx:e}),ye.$set(_n);const yt={};o&536870912&&(yt.$$scope={dirty:o,ctx:e}),be.$set(yt);const yn={};o&536870912&&(yn.$$scope={dirty:o,ctx:e}),ve.$set(yn);const bn={};o&536870912&&(bn.$$scope={dirty:o,ctx:e}),ke.$set(bn);const $e={};o&536870912&&($e.$$scope={dirty:o,ctx:e}),xe.$set($e);const vn={};o&536870912&&(vn.$$scope={dirty:o,ctx:e}),Ae.$set(vn);const kn={};o&536870912&&(kn.$$scope={dirty:o,ctx:e}),Ee.$set(kn);const me={};o&536870912&&(me.$$scope={dirty:o,ctx:e}),Te.$set(me);const xn={};o&536870912&&(xn.$$scope={dirty:o,ctx:e}),ze.$set(xn);const An={};o&536870912&&(An.$$scope={dirty:o,ctx:e}),We.$set(An);const En={};o&536870912&&(En.$$scope={dirty:o,ctx:e}),Se.$set(En);const Tn={};o&536870912&&(Tn.$$scope={dirty:o,ctx:e}),Pe.$set(Tn);const oe={};o&536870912&&(oe.$$scope={dirty:o,ctx:e}),qe.$set(oe);const zn={};o&536870912&&(zn.$$scope={dirty:o,ctx:e}),De.$set(zn);const Wn={};o&536870912&&(Wn.$$scope={dirty:o,ctx:e}),Ie.$set(Wn);const Sn={};o&536870912&&(Sn.$$scope={dirty:o,ctx:e}),Ne.$set(Sn);const Pn={};o&536870912&&(Pn.$$scope={dirty:o,ctx:e}),Be.$set(Pn);const qn={};o&536870912&&(qn.$$scope={dirty:o,ctx:e}),Le.$set(qn);const Dn={};o&536870912&&(Dn.$$scope={dirty:o,ctx:e}),Ve.$set(Dn);const bt={};o&536870912&&(bt.$$scope={dirty:o,ctx:e}),Fe.$set(bt);const In={};o&536870912&&(In.$$scope={dirty:o,ctx:e}),He.$set(In);const Nn={};o&536870912&&(Nn.$$scope={dirty:o,ctx:e}),Me.$set(Nn);const Bn={};o&536870912&&(Bn.$$scope={dirty:o,ctx:e}),je.$set(Bn);const Ln={};o&536870912&&(Ln.$$scope={dirty:o,ctx:e}),Ue.$set(Ln);const Vn={};o&536870912&&(Vn.$$scope={dirty:o,ctx:e}),Ke.$set(Vn);const Z={};o&536870913&&(Z.$$scope={dirty:o,ctx:e}),Ce.$set(Z);const Fn={};o&536870912&&(Fn.$$scope={dirty:o,ctx:e}),Qe.$set(Fn);const Hn={};o&536870912&&(Hn.$$scope={dirty:o,ctx:e}),Ge.$set(Hn);const he={};o&536870912&&(he.$$scope={dirty:o,ctx:e}),Re.$set(he);const Mn={};o&536870912&&(Mn.$$scope={dirty:o,ctx:e}),Je.$set(Mn);const J={};o&536870912&&(J.$$scope={dirty:o,ctx:e}),Xe.$set(J)},i(e){la||(h(y.$$.fragment,e),h(v.$$.fragment,e),h(D.$$.fragment,e),h(ot.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(ce.$$.fragment,e),h(we.$$.fragment,e),h(_e.$$.fragment,e),h(ye.$$.fragment,e),h(be.$$.fragment,e),h(ve.$$.fragment,e),h(ke.$$.fragment,e),h(xe.$$.fragment,e),h(Ae.$$.fragment,e),h(Ee.$$.fragment,e),h(Te.$$.fragment,e),h(ze.$$.fragment,e),h(We.$$.fragment,e),h(Se.$$.fragment,e),h(Pe.$$.fragment,e),h(qe.$$.fragment,e),h(De.$$.fragment,e),h(Ie.$$.fragment,e),h(Ne.$$.fragment,e),h(Be.$$.fragment,e),h(ht.$$.fragment,e),h(Le.$$.fragment,e),h(Ve.$$.fragment,e),h(Fe.$$.fragment,e),h(He.$$.fragment,e),h(Me.$$.fragment,e),h(je.$$.fragment,e),h(Ue.$$.fragment,e),h(ut.$$.fragment,e),h(Ke.$$.fragment,e),h(Ce.$$.fragment,e),h(dt.$$.fragment,e),h(Qe.$$.fragment,e),h(pt.$$.fragment,e),h(Ge.$$.fragment,e),h(gt.$$.fragment,e),h(Re.$$.fragment,e),h(Je.$$.fragment,e),h(Xe.$$.fragment,e),h(ct.$$.fragment,e),la=!0)},o(e){u(y.$$.fragment,e),u(v.$$.fragment,e),u(D.$$.fragment,e),u(ot.$$.fragment,e),u(pe.$$.fragment,e),u(ge.$$.fragment,e),u(ce.$$.fragment,e),u(we.$$.fragment,e),u(_e.$$.fragment,e),u(ye.$$.fragment,e),u(be.$$.fragment,e),u(ve.$$.fragment,e),u(ke.$$.fragment,e),u(xe.$$.fragment,e),u(Ae.$$.fragment,e),u(Ee.$$.fragment,e),u(Te.$$.fragment,e),u(ze.$$.fragment,e),u(We.$$.fragment,e),u(Se.$$.fragment,e),u(Pe.$$.fragment,e),u(qe.$$.fragment,e),u(De.$$.fragment,e),u(Ie.$$.fragment,e),u(Ne.$$.fragment,e),u(Be.$$.fragment,e),u(ht.$$.fragment,e),u(Le.$$.fragment,e),u(Ve.$$.fragment,e),u(Fe.$$.fragment,e),u(He.$$.fragment,e),u(Me.$$.fragment,e),u(je.$$.fragment,e),u(Ue.$$.fragment,e),u(ut.$$.fragment,e),u(Ke.$$.fragment,e),u(Ce.$$.fragment,e),u(dt.$$.fragment,e),u(Qe.$$.fragment,e),u(pt.$$.fragment,e),u(Ge.$$.fragment,e),u(gt.$$.fragment,e),u(Re.$$.fragment,e),u(Je.$$.fragment,e),u(Xe.$$.fragment,e),u(ct.$$.fragment,e),la=!1},d(e){e&&a(t),e&&a(r),e&&a(s),e&&a(i),e&&a(d),_(y),_(v),e&&a(E),e&&a($),e&&a(k),e&&a(p),e&&a(I),e&&a(F),e&&a(Y),_(D,e),e&&a(j),e&&a(B),e&&a(V),e&&a(K),e&&a(jn),e&&a(vt),e&&a(Un),e&&a(kt),e&&a(Kn),e&&a(xt),e&&a(Cn),_(ot,e),e&&a(Qn),e&&a(At),e&&a(Yn),e&&a(Et),e&&a(Gn),e&&a(de),_(pe),e&&a(Rn),_(ge,e),e&&a(Jn),e&&a(Tt),e&&a(On),_(ce,e),e&&a(Xn),e&&a(zt),e&&a(Zn),e&&a(ae),e&&a(er),e&&a(se),e&&a(tr),e&&a(Dt),e&&a(nr),e&&a(It),e&&a(rr),e&&a(te),_(we),_(_e),_(ye),e&&a(ar),_(be,e),e&&a(sr),e&&a(Nt),e&&a(or),e&&a(Bt),e&&a(ir),e&&a(it),_(ve),e&&a(lr),e&&a(Lt),e&&a(fr),e&&a(lt),_(ke),e&&a($r),e&&a(Vt),e&&a(mr),e&&a(fe),_(xe),_(Ae),e&&a(hr),e&&a(Ft),e&&a(ur),e&&a(Ht),e&&a(dr),e&&a(ft),_(Ee),e&&a(pr),e&&a(Mt),e&&a(gr),e&&a($t),_(Te),e&&a(cr),e&&a(X),_(ze),_(We),_(Se),_(Pe),_(qe),e&&a(wr),_(De,e),e&&a(_r),e&&a(jt),e&&a(yr),e&&a(mt),_(Ie),e&&a(br),e&&a(ie),_(Ne),_(Be),e&&a(vr),_(ht,e),e&&a(kr),e&&a(Ut),e&&a(xr),e&&a(R),_(Le),_(Ve),_(Fe),_(He),_(Me),_(je),e&&a(Ar),_(Ue,e),e&&a(Er),e&&a(Kt),e&&a(Tr),_(ut,e),e&&a(zr),e&&a(Ct),e&&a(Wr),e&&a(Qt),e&&a(Sr),e&&a(Yt),e&&a(Pr),e&&a(Gt),e&&a(qr),_(Ke,e),e&&a(Dr),_(Ce,e),e&&a(Ir),e&&a(Rt),e&&a(Nr),_(dt,e),e&&a(Br),e&&a(Jt),e&&a(Lr),e&&a(Ot),e&&a(Vr),e&&a(Xt),e&&a(Fr),_(Qe,e),e&&a(Hr),e&&a(Ye),_(pt),e&&a(Mr),e&&a(Zt),e&&a(jr),_(Ge,e),e&&a(Ur),e&&a(en),e&&a(Kr),_(gt,e),e&&a(Cr),e&&a(tn),e&&a(Qr),e&&a(nn),e&&a(Yr),e&&a(rn),e&&a(Gr),_(Re,e),e&&a(Rr),e&&a(an),e&&a(Jr),_(Je,e),e&&a(Or),e&&a(sn),e&&a(Xr),e&&a(Oe),_(Xe),e&&a(Zr),e&&a(on),e&&a(ea),_(ct,e),e&&a(ta),e&&a(ln),e&&a(na),e&&a(fn),e&&a(ra),e&&a($n),e&&a(aa),e&&a(mn),e&&a(sa),e&&a(wt),e&&a(oa),e&&a(_t),e&&a(ia),e&&a(tt)}}}function Si(m){let t,n,r,s,i,d;return r=new Ao({props:{$$slots:{default:[Wi]},$$scope:{ctx:m}}}),i=new Eo({props:{references:m[1]}}),{c(){t=P("meta"),n=z(),g(r.$$.fragment),s=z(),g(i.$$.fragment),this.h()},l(l){const y=xo("svelte-1w56g42",document.head);t=q(y,"META",{name:!0,content:!0}),y.forEach(a),n=W(l),c(r.$$.fragment,l),s=W(l),c(i.$$.fragment,l),this.h()},h(){document.title="Transformer - World4AI",H(t,"name","description"),H(t,"content","The transformer architecture has produced state of the art results in natural language processing, computer vision and much more. Unlike the recurrent neural net, the transformer does not rely on recurrence, instead it relies on a self-attention mechanism.")},m(l,y){T(document.head,t),f(l,n,y),w(r,l,y),f(l,s,y),w(i,l,y),d=!0},p(l,[y]){const v={};y&536870913&&(v.$$scope={dirty:y,ctx:l}),r.$set(v)},i(l){d||(h(r.$$.fragment,l),h(i.$$.fragment,l),d=!0)},o(l){u(r.$$.fragment,l),u(i.$$.fragment,l),d=!1},d(l){a(t),l&&a(n),_(r,l),l&&a(s),_(i,l)}}}function Pi(m,t,n){const r=[{author:"Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia",title:"Attention is All you Need",journal:"Advances in Neural Information Processing Systems",year:"2017",pages:"",volume:"30",issue:""},{author:"Ba, Jimmy and Kiros, Jamie and Hinton, Geoffrey",title:"Layer Normalization",year:"2016"}],s=["what","is","your","name"],i=["what","is","your","date","of","birth","?"],d=["<sos>","what","is","your","name"];let l=0;return[l,r,s,i,d,()=>{n(0,l=(l+1)%4)}]}class Ki extends bo{constructor(t){super(),vo(this,t,Pi,Si,ko,{})}}export{Ki as default};
