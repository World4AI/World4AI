import{S as Sn,i as Wn,s as Pn,a as p,y as k,c as h,z as T,b as s,A as E,g as c,v as wn,d as _,f as cn,h as r,B as D,V as In,e as vn,P as zn,k as I,q as v,W as qn,l as z,m as M,r as y,n as ot,N as S,u as Mn,C as Y}from"../chunks/index.4d92b023.js";import{C as Ln}from"../chunks/Container.b0705c7b.js";import{H as yn}from"../chunks/Highlight.b7c1de53.js";import{B as An}from"../chunks/ButtonContainer.e9aac418.js";import{S as Nn}from"../chunks/StepButton.2fb0289b.js";import{t as On}from"../chunks/index.4de27e87.js";import{P as hn,T as dn}from"../chunks/Ticks.45eca5c5.js";import{X as gn,Y as _n}from"../chunks/YLabel.182e66a3.js";import{C as Lt}from"../chunks/Circle.f281e92b.js";import{P as Fn}from"../chunks/Path.7e6df014.js";import{T as Cn}from"../chunks/Text.b1e2b624.js";import{L as R}from"../chunks/Latex.e0b308c0.js";import{A as Mt}from"../chunks/Alert.25a852b3.js";import{N as Bn}from"../chunks/NeuralNetwork.9b1e2957.js";import{P as Hn}from"../chunks/PlayButton.85103c5a.js";import{R as xn}from"../chunks/Rectangle.45d8140d.js";import{L as un,V as $n}from"../chunks/Network.03de8e4c.js";function kn(l,n,a){const t=l.slice();return t[16]=n[a],t}function Tn(l){let n,a;return n=new An({props:{$$slots:{default:[Vn]},$$scope:{ctx:l}}}),{c(){k(n.$$.fragment)},l(t){T(n.$$.fragment,t)},m(t,o){E(n,t,o),a=!0},p(t,o){const u={};o&524292&&(u.$$scope={dirty:o,ctx:t}),n.$set(u)},i(t){a||(c(n.$$.fragment,t),a=!0)},o(t){_(n.$$.fragment,t),a=!1},d(t){D(n,t)}}}function Vn(l){let n,a;return n=new Nn({props:{disabled:l[2]}}),n.$on("click",l[5]),{c(){k(n.$$.fragment)},l(t){T(n.$$.fragment,t)},m(t,o){E(n,t,o),a=!0},p(t,o){const u={};o&4&&(u.disabled=t[2]),n.$set(u)},i(t){a||(c(n.$$.fragment,t),a=!0)},o(t){_(n.$$.fragment,t),a=!1},d(t){D(n,t)}}}function En(l){let n,a,t=l[3],o=[];for(let w=0;w<t.length;w+=1)o[w]=Dn(kn(l,t,w));const u=w=>_(o[w],1,1,()=>{o[w]=null});return{c(){for(let w=0;w<o.length;w+=1)o[w].c();n=vn()},l(w){for(let x=0;x<o.length;x+=1)o[x].l(w);n=vn()},m(w,x){for(let d=0;d<o.length;d+=1)o[d]&&o[d].m(w,x);s(w,n,x),a=!0},p(w,x){if(x&8){t=w[3];let d;for(d=0;d<t.length;d+=1){const A=kn(w,t,d);o[d]?(o[d].p(A,x),c(o[d],1)):(o[d]=Dn(A),o[d].c(),c(o[d],1),o[d].m(n.parentNode,n))}for(wn(),d=t.length;d<o.length;d+=1)u(d);cn()}},i(w){if(!a){for(let x=0;x<t.length;x+=1)c(o[x]);a=!0}},o(w){o=o.filter(Boolean);for(let x=0;x<o.length;x+=1)_(o[x]);a=!1},d(w){zn(o,w),w&&r(n)}}}function Dn(l){let n,a;return n=new Cn({props:{x:l[16].x,y:l[16].y+.1,text:`x: ${l[16].x.toFixed(2)} | y: ${l[16].y.toFixed(2)}`}}),{c(){k(n.$$.fragment)},l(t){T(n.$$.fragment,t)},m(t,o){E(n,t,o),a=!0},p(t,o){const u={};o&8&&(u.x=t[16].x),o&8&&(u.y=t[16].y+.1),o&8&&(u.text=`x: ${t[16].x.toFixed(2)} | y: ${t[16].y.toFixed(2)}`),n.$set(u)},i(t){a||(c(n.$$.fragment,t),a=!0)},o(t){_(n.$$.fragment,t),a=!1},d(t){D(n,t)}}}function Yn(l){let n,a,t,o,u,w,x,d,A,g,f;n=new dn({props:{xTicks:[-3,-2,-1,0,1,2,3],yTicks:[-3,-2,-1,0,1,2,3],xOffset:-15,yOffset:15}}),t=new Fn({props:{data:l[3],isClosed:!0}}),u=new Lt({props:{data:l[3],color:"var(--main-color-1)",radius:"5"}});let b=l[1]&&En(l);return d=new gn({props:{text:"Feature 1",fontSize:15}}),g=new _n({props:{text:"Feature 2",fontSize:15}}),{c(){k(n.$$.fragment),a=p(),k(t.$$.fragment),o=p(),k(u.$$.fragment),w=p(),b&&b.c(),x=p(),k(d.$$.fragment),A=p(),k(g.$$.fragment)},l(m){T(n.$$.fragment,m),a=h(m),T(t.$$.fragment,m),o=h(m),T(u.$$.fragment,m),w=h(m),b&&b.l(m),x=h(m),T(d.$$.fragment,m),A=h(m),T(g.$$.fragment,m)},m(m,q){E(n,m,q),s(m,a,q),E(t,m,q),s(m,o,q),E(u,m,q),s(m,w,q),b&&b.m(m,q),s(m,x,q),E(d,m,q),s(m,A,q),E(g,m,q),f=!0},p(m,q){const $={};q&8&&($.data=m[3]),t.$set($);const P={};q&8&&(P.data=m[3]),u.$set(P),m[1]?b?(b.p(m,q),q&2&&c(b,1)):(b=En(m),b.c(),c(b,1),b.m(x.parentNode,x)):b&&(wn(),_(b,1,1,()=>{b=null}),cn())},i(m){f||(c(n.$$.fragment,m),c(t.$$.fragment,m),c(u.$$.fragment,m),c(b),c(d.$$.fragment,m),c(g.$$.fragment,m),f=!0)},o(m){_(n.$$.fragment,m),_(t.$$.fragment,m),_(u.$$.fragment,m),_(b),_(d.$$.fragment,m),_(g.$$.fragment,m),f=!1},d(m){D(n,m),m&&r(a),D(t,m),m&&r(o),D(u,m),m&&r(w),b&&b.d(m),m&&r(x),D(d,m),m&&r(A),D(g,m)}}}function Gn(l){let n,a,t,o=l[0]&&Tn(l);return a=new hn({props:{width:500,height:500,maxWidth:600,domain:[-3,3],range:[-3,3],$$slots:{default:[Yn]},$$scope:{ctx:l}}}),{c(){o&&o.c(),n=p(),k(a.$$.fragment)},l(u){o&&o.l(u),n=h(u),T(a.$$.fragment,u)},m(u,w){o&&o.m(u,w),s(u,n,w),E(a,u,w),t=!0},p(u,[w]){u[0]?o?(o.p(u,w),w&1&&c(o,1)):(o=Tn(u),o.c(),c(o,1),o.m(n.parentNode,n)):o&&(wn(),_(o,1,1,()=>{o=null}),cn());const x={};w&524298&&(x.$$scope={dirty:w,ctx:u}),a.$set(x)},i(u){t||(c(o),c(a.$$.fragment,u),t=!0)},o(u){_(o),_(a.$$.fragment,u),t=!1},d(u){o&&o.d(u),u&&r(n),D(a,u)}}}function Rn(l){return 1/(1+Math.exp(-l))}function Xn(l){return Math.max(0,l)}function Jn(l,n,a){let t,{animated:o=!0}=n,{inputData:u=[]}=n,{matrix:w=[]}=n,{vector:x=[]}=n,{activation:d="identity"}=n,{showText:A=!0}=n,g=JSON.parse(JSON.stringify(u)),f=!1,b=!1,m=On(u,{duration:1e3});In(l,m,W=>a(3,t=W));function q(){return g=JSON.parse(JSON.stringify(u)),m.set(u)}function $(){return b=[],g.forEach(W=>{let O=W.x*w[0][0]+W.y*w[1][0],C=W.x*w[0][1]+W.y*w[1][1];b.push({x:O,y:C})}),g=b,m.set(g)}function P(){return b=[],g.forEach(W=>{let O=W.x+x[0],C=W.y+x[1];b.push({x:O,y:C})}),g=b,m.set(g)}function N(){let W=[],O=d==="sigmoid"?Rn:Xn;return g.forEach(C=>{let K=O(C.x),L=O(C.y);W.push({x:K,y:L})}),g=W,m.set(W)}async function F(){a(2,f=!0),b?(await q(),b=!1):(w.length>0&&await $(),x.length>0&&await P(),d!="identity"&&await N(),b=!0),a(2,f=!1)}return l.$$set=W=>{"animated"in W&&a(0,o=W.animated),"inputData"in W&&a(6,u=W.inputData),"matrix"in W&&a(7,w=W.matrix),"vector"in W&&a(8,x=W.vector),"activation"in W&&a(9,d=W.activation),"showText"in W&&a(1,A=W.showText)},[o,A,f,t,m,F,u,w,x,d]}class X extends Sn{constructor(n){super(),Wn(this,n,Jn,Gn,Pn,{animated:0,inputData:6,matrix:7,vector:8,activation:9,showText:1})}}function Un(l){let n;return{c(){n=v("How does a solution to a nonlinear problem looks geometrically?")},l(a){n=y(a,"How does a solution to a nonlinear problem looks geometrically?")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function jn(l){let n;return{c(){n=v("A neural network is a composition of transformations.")},l(a){n=y(a,"A neural network is a composition of transformations.")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function Kn(l){let n=String.raw`
        \mathbf{X} =
        \begin{bmatrix}
      -1 & 1 \\ 
      -1 & -1 \\
      1 &-1 \\
      1 & 1 \\
        \end{bmatrix}
          `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function Qn(l){let n;return{c(){n=v("A matrix multiplication is a linear transformation.")},l(a){n=y(a,"A matrix multiplication is a linear transformation.")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function Zn(l){let n=String.raw`\mathbf{X}`+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function er(l){let n=String.raw`\mathbf{W}^T`+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function tr(l){let n=String.raw`\mathbf{W}^T`+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function ar(l){let n=String.raw`
    \mathbf{W}^T =
    \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
    \end{bmatrix}`+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function nr(l){let n=String.raw`
  \mathbf{W}^T =
  \begin{bmatrix}
  2 & 0 \\
  0 & 1 \\
  \end{bmatrix}
  `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function rr(l){let n=String.raw`
  \mathbf{W}^T =
  \begin{bmatrix}
  1 & 0 \\
  0 & 0.5 \\
  \end{bmatrix}
  `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function ir(l){let n=String.raw`
  \mathbf{W}^T =
  \begin{bmatrix}
  1 & 0 \\
  1 & 1 \\
  \end{bmatrix}
  `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function sr(l){let n=String.raw`
  \mathbf{W}^T =
  \begin{bmatrix}
  1 & 1 \\
  0 & 1 \\
  \end{bmatrix}
  `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function or(l){let n=String.raw`
      \mathbf{W}^T =
      \begin{bmatrix}
      \cos(1) & -\sin(1) \\
      \sin(1) & \cos(1) \\
      \end{bmatrix}
      `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function lr(l){let n;return{c(){n=v("Bias addition is a translation.")},l(a){n=y(a,"Bias addition is a translation.")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function fr(l){let n=String.raw`
      \mathbf{b} =
      \begin{bmatrix}
      1  \\
      0 \\
      \end{bmatrix}
      `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function mr(l){let n=String.raw`
      \mathbf{b} =
      \begin{bmatrix}
      0  \\
      1 \\
      \end{bmatrix}
      `+"",a;return{c(){a=v(n)},l(t){a=y(t,n)},m(t,o){s(t,a,o)},p:Y,d(t){t&&r(a)}}}function ur(l){let n;return{c(){n=v("affine transformation")},l(a){n=y(a,"affine transformation")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function $r(l){let n;return{c(){n=v("A nonlinear activation function squishes the data.")},l(a){n=y(a,"A nonlinear activation function squishes the data.")},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function pr(l){let n,a;return n=new Hn({props:{f:l[7],delta:0}}),{c(){k(n.$$.fragment)},l(t){T(n.$$.fragment,t)},m(t,o){E(n,t,o),a=!0},p:Y,i(t){a||(c(n.$$.fragment,t),a=!0)},o(t){_(n.$$.fragment,t),a=!1},d(t){D(n,t)}}}function hr(l){let n,a,t,o,u,w,x,d,A,g,f,b,m,q;return n=new dn({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),t=new xn({props:{data:l[2][0],size:9,color:"var(--main-color-3)"}}),u=new xn({props:{data:l[2][1],size:9,color:"var(--main-color-4)"}}),x=new Lt({props:{data:l[6][0]}}),A=new Lt({props:{data:l[6][1],color:"var(--main-color-2)"}}),f=new gn({props:{text:"Feature 1",fontSize:15}}),m=new _n({props:{text:"Feature 2",fontSize:15}}),{c(){k(n.$$.fragment),a=p(),k(t.$$.fragment),o=p(),k(u.$$.fragment),w=p(),k(x.$$.fragment),d=p(),k(A.$$.fragment),g=p(),k(f.$$.fragment),b=p(),k(m.$$.fragment)},l($){T(n.$$.fragment,$),a=h($),T(t.$$.fragment,$),o=h($),T(u.$$.fragment,$),w=h($),T(x.$$.fragment,$),d=h($),T(A.$$.fragment,$),g=h($),T(f.$$.fragment,$),b=h($),T(m.$$.fragment,$)},m($,P){E(n,$,P),s($,a,P),E(t,$,P),s($,o,P),E(u,$,P),s($,w,P),E(x,$,P),s($,d,P),E(A,$,P),s($,g,P),E(f,$,P),s($,b,P),E(m,$,P),q=!0},p($,P){const N={};P&4&&(N.data=$[2][0]),t.$set(N);const F={};P&4&&(F.data=$[2][1]),u.$set(F)},i($){q||(c(n.$$.fragment,$),c(t.$$.fragment,$),c(u.$$.fragment,$),c(x.$$.fragment,$),c(A.$$.fragment,$),c(f.$$.fragment,$),c(m.$$.fragment,$),q=!0)},o($){_(n.$$.fragment,$),_(t.$$.fragment,$),_(u.$$.fragment,$),_(x.$$.fragment,$),_(A.$$.fragment,$),_(f.$$.fragment,$),_(m.$$.fragment,$),q=!1},d($){D(n,$),$&&r(a),D(t,$),$&&r(o),D(u,$),$&&r(w),D(x,$),$&&r(d),D(A,$),$&&r(g),D(f,$),$&&r(b),D(m,$)}}}function wr(l){let n,a,t,o,u,w,x,d,A,g;return n=new dn({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),t=new Lt({props:{data:l[3][0]}}),u=new Lt({props:{data:l[3][1],color:"var(--main-color-2)"}}),x=new gn({props:{text:"Hidden Feature 1",fontSize:15}}),A=new _n({props:{text:"Hidden Feature 2",fontSize:15}}),{c(){k(n.$$.fragment),a=p(),k(t.$$.fragment),o=p(),k(u.$$.fragment),w=p(),k(x.$$.fragment),d=p(),k(A.$$.fragment)},l(f){T(n.$$.fragment,f),a=h(f),T(t.$$.fragment,f),o=h(f),T(u.$$.fragment,f),w=h(f),T(x.$$.fragment,f),d=h(f),T(A.$$.fragment,f)},m(f,b){E(n,f,b),s(f,a,b),E(t,f,b),s(f,o,b),E(u,f,b),s(f,w,b),E(x,f,b),s(f,d,b),E(A,f,b),g=!0},p(f,b){const m={};b&8&&(m.data=f[3][0]),t.$set(m);const q={};b&8&&(q.data=f[3][1]),u.$set(q)},i(f){g||(c(n.$$.fragment,f),c(t.$$.fragment,f),c(u.$$.fragment,f),c(x.$$.fragment,f),c(A.$$.fragment,f),g=!0)},o(f){_(n.$$.fragment,f),_(t.$$.fragment,f),_(u.$$.fragment,f),_(x.$$.fragment,f),_(A.$$.fragment,f),g=!1},d(f){D(n,f),f&&r(a),D(t,f),f&&r(o),D(u,f),f&&r(w),D(x,f),f&&r(d),D(A,f)}}}function cr(l){let n;return{c(){n=v(`Affine transformations move, scale, rotate the data and move it between
        different dimensions. Activation functions squish or restraint the data
        to deal with nonlinearity. The last layers contain the hidden features,
        that can be linearly separated to solve a particular problem.`)},l(a){n=y(a,`Affine transformations move, scale, rotate the data and move it between
        different dimensions. Activation functions squish or restraint the data
        to deal with nonlinearity. The last layers contain the hidden features,
        that can be linearly separated to solve a particular problem.`)},m(a,t){s(a,n,t)},d(a){a&&r(n)}}}function dr(l){let n,a,t,o,u,w,x,d,A,g,f,b,m,q,$,P,N,F,W,O,C,K,L,G,U,j,lt,Je,V,ya,Q,xa,Z,ka,ee,Ta,Nt,Ue,Ea,Ot,te,Ft,je,Da,Ct,ae,Sa,ne,Wa,Bt,re,Ht,ie,Pa,se,Aa,Vt,oe,Yt,le,Ia,fe,za,Gt,me,Rt,ue,qa,$e,Ma,Xt,pe,Jt,he,La,we,Na,Ut,ce,jt,Ke,Oa,Kt,de,Qt,ge,Fa,_e,Ca,Zt,be,ea,ve,Ba,ye,Ha,ta,xe,aa,Qe,Va,na,Me,ra,Ze,Ya,ia,Le,sa,ke,Ga,Te,Ra,oa,et,Xa,la,Ee,fa,tt,Ja,ma,De,ua,at,Ua,$a,Se,pa,nt,ja,ha,rt,Ka,wa,Ne,ca,it,Qa,da,st,Za,ga,We,_a,Pe,en,ft=l[4].toFixed(6)+"",ba,B,Ae,Ie,tn,ze,an,mt,nn,rn,ut,sn,on,qe,ln,$t,fn,mn,pt,ht;return t=new yn({props:{$$slots:{default:[Un]},$$scope:{ctx:l}}}),d=new Mt({props:{type:"info",$$slots:{default:[jn]},$$scope:{ctx:l}}}),q=new R({props:{$$slots:{default:[Kn]},$$scope:{ctx:l}}}),W=new X({props:{animated:!1,inputData:l[0]}}),G=new Mt({props:{type:"info",$$slots:{default:[Qn]},$$scope:{ctx:l}}}),Q=new R({props:{$$slots:{default:[Zn]},$$scope:{ctx:l}}}),Z=new R({props:{$$slots:{default:[er]},$$scope:{ctx:l}}}),ee=new R({props:{$$slots:{default:[tr]},$$scope:{ctx:l}}}),te=new R({props:{$$slots:{default:[ar]},$$scope:{ctx:l}}}),ne=new R({props:{$$slots:{default:[nr]},$$scope:{ctx:l}}}),re=new X({props:{inputData:l[0],matrix:[[2,0],[0,1]]}}),se=new R({props:{$$slots:{default:[rr]},$$scope:{ctx:l}}}),oe=new X({props:{inputData:l[0],matrix:[[1,0],[0,.5]]}}),fe=new R({props:{$$slots:{default:[ir]},$$scope:{ctx:l}}}),me=new X({props:{inputData:l[0],matrix:[[1,0],[1,1]]}}),$e=new R({props:{$$slots:{default:[sr]},$$scope:{ctx:l}}}),pe=new X({props:{inputData:l[0],matrix:[[1,1],[0,1]]}}),we=new R({props:{$$slots:{default:[or]},$$scope:{ctx:l}}}),ce=new X({props:{inputData:l[0],matrix:[[Math.cos(1),-Math.sin(1)],[Math.sin(1),Math.cos(1)]]}}),de=new Mt({props:{type:"info",$$slots:{default:[lr]},$$scope:{ctx:l}}}),_e=new R({props:{$$slots:{default:[fr]},$$scope:{ctx:l}}}),be=new X({props:{inputData:l[0],vector:[1,0]}}),ye=new R({props:{$$slots:{default:[mr]},$$scope:{ctx:l}}}),xe=new X({props:{inputData:l[0],vector:[0,1]}}),Me=new X({props:{inputData:[{x:0,y:0},{x:1,y:1},{x:-1,y:1}],matrix:[[Math.cos(1),-Math.sin(1)],[Math.sin(1),Math.cos(1)]]}}),Le=new X({props:{inputData:[{x:0,y:0},{x:1,y:1},{x:-1,y:1}],vector:[1,1]}}),Te=new yn({props:{$$slots:{default:[ur]},$$scope:{ctx:l}}}),Ee=new Mt({props:{type:"info",$$slots:{default:[$r]},$$scope:{ctx:l}}}),De=new X({props:{inputData:l[1],activation:"sigmoid",showText:!1}}),Se=new X({props:{inputData:l[1],activation:"relu",showText:!1}}),Ne=new Bn({props:{layers:l[5],height:140,padding:{right:10,left:0}}}),We=new An({props:{$$slots:{default:[pr]},$$scope:{ctx:l}}}),Ie=new hn({props:{width:500,height:500,maxWidth:600,domain:[0,1],range:[0,1],$$slots:{default:[hr]},$$scope:{ctx:l}}}),ze=new hn({props:{width:500,height:500,maxWidth:600,domain:[0,1],range:[0,1],$$slots:{default:[wr]},$$scope:{ctx:l}}}),qe=new Mt({props:{type:"info",$$slots:{default:[cr]},$$scope:{ctx:l}}}),{c(){n=I("p"),a=v(`So far we have discussed how a computational graph works and how we can use
    autodiff packages to solve nonlinear problems. Yet we are still missing a
    crucial component that will allow us to understand the workings of neural
    networks. We need to answer the following question. `),k(t.$$.fragment),o=p(),u=I("p"),w=v(`To get to that understanding, first we have to understand, that a neural
    network is a series of transormations. Multiplying the features with a
    matrix is a linear transformation, adding a bias is a translation and
    applying an activation function squishes the data. Each of these
    transformations accomplishes a different task and can be interpreted
    visually. Additionally we can stack several layers in a neural network, thus
    creating a transformation composition.`),x=p(),k(d.$$.fragment),A=p(),g=I("p"),f=v(`To demonstrate the visual interpretation of a transformation we will utilize
    the following matrix of features with 2 features and 4 samples.`),b=p(),m=I("div"),k(q.$$.fragment),$=p(),P=I("p"),N=v("The four samples build a square in a 2d coordinate system."),F=p(),k(W.$$.fragment),O=p(),C=I("p"),K=v("We will start with matrix multiplications."),L=p(),k(G.$$.fragment),U=p(),j=I("p"),lt=v(`While there is a formal definition of linear transformations, we could use a
    somewhat loose definition that you can use as a mental model. In a linear
    transformation parallel lines remain parallel and the origin does not move.
    So the four lines of the square above will remain parallel lines after the
    linear transformation.`),Je=p(),V=I("p"),ya=v("We can using linear transformations by multiplying the features matrix "),k(Q.$$.fragment),xa=v(` by the weight matrix
    `),k(Z.$$.fragment),ka=v(`, our transformation matrix.
    Depending on the contents of `),k(ee.$$.fragment),Ta=v(` different
    types of transformations are produced. The weight matrix is going to be a 2x2
    matrix for now. That way we are using 2 features per sample as input and generate
    2 transformed features per sample as output.`),Nt=p(),Ue=I("p"),Ea=v("The identity matrix is the easiest matrix to understand."),Ot=p(),k(te.$$.fragment),Ft=p(),je=I("p"),Da=v("Applying this transformation keeps the original matrix."),Ct=p(),ae=I("p"),Sa=v(`If we change the values of the identity matrix slightly, we scale the
    original square. The matrix
    `),k(ne.$$.fragment),Wa=v(" for example scales the input square in the x direction by a factor of 2."),Bt=p(),k(re.$$.fragment),Ht=p(),ie=I("p"),Pa=v(`The matrix
    `),k(se.$$.fragment),Aa=v(" on the other hand scales the matrix in the y direction by a factor of 0.5."),Vt=p(),k(oe.$$.fragment),Yt=p(),le=I("p"),Ia=v(`So far we have used only one diagonal of the matrix to scale the square. The
    other diagonal can be used for the so called sheer operation. When we use
    the below matrix
    `),k(fe.$$.fragment),za=v(" for example, the top and the bottom lines are moved right and left respectively."),Gt=p(),k(me.$$.fragment),Rt=p(),ue=I("p"),qa=v(`The matrix
    `),k($e.$$.fragment),Ma=v(" on the other hand move the left and the right lines to the bottom or top respectively."),Xt=p(),k(pe.$$.fragment),Jt=p(),he=I("p"),La=v(`We can combine scaling and sheering to achieve interesting transformations.
    The matrix
    `),k(we.$$.fragment),Na=v(" for example rotates the data."),Ut=p(),k(ce.$$.fragment),jt=p(),Ke=I("p"),Oa=v("Next let's look at the visual interpretation of the bias."),Kt=p(),k(de.$$.fragment),Qt=p(),ge=I("p"),Fa=v(`A bias allows us to translate the data. That means that each point is
    equally moved. The vector
    `),k(_e.$$.fragment),Ca=v(" would move all points in the x direction by 1."),Zt=p(),k(be.$$.fragment),ea=p(),ve=I("p"),Ba=v(`The vector
    `),k(ye.$$.fragment),Ha=v(" on the other hand, moves all points by 1 in the y direction."),ta=p(),k(xe.$$.fragment),aa=p(),Qe=I("p"),Va=v(`A translation is not a linear transformation. If we apply a linear
    transformation, that induces rotation, the zero point remains intact after
    the transformation.`),na=p(),k(Me.$$.fragment),ra=p(),Ze=I("p"),Ya=v("A translation on the other hand moves the origin."),ia=p(),k(Le.$$.fragment),sa=p(),ke=I("p"),Ga=v(`A neural network combines a liner transformation with a translation. In
    linear algebra this transformation combination is called `),k(Te.$$.fragment),Ra=v("."),oa=p(),et=I("p"),Xa=v("Let's finally move to activation functions."),la=p(),k(Ee.$$.fragment),fa=p(),tt=I("p"),Ja=v(`We can imagine the nonlinear transformations as some sort of "squishing",
    where the activation function limits the data to a certain range. The
    sigmoid that we have utilized so far pushes the vectors into a 1 by 1 box.`),ma=p(),k(De.$$.fragment),ua=p(),at=I("p"),Ua=v(`The ReLU activation function is even wilder. The function turns negative
    numbers into zeros and leaves positive numbers untouched. With a ReLU
    parallel lines do not necessarily stay parallel.`),$a=p(),k(Se.$$.fragment),pa=p(),nt=I("p"),ja=v(`There is a final remark we would like to make in regards with linear
    transformations. So far we have used a 2x2 weight matrix for our linear
    transformations. We made this in order to keep the number of dimensions
    constant. We took in two features and produced two neurons. That way we
    could visualize the results in a 2d plane. If on the other hand we used a
    2x3 matrix, the transformation would have pushed the features into 3d space.
    In deep learning we change the amount of dimensions all the time by changing
    the number of neurons from layer to layer. Sometimes the network can find a
    better solution in a different dimension.`),ha=p(),rt=I("p"),Ka=v(`So what exactly does a neural network try to achieve throught those
    transformations? We are going to use a slightly different architecture to
    solve our circular data problem. The architecture below was not picked
    randomly, but to show some magic that is hidden under the hood of a neural
    network.`),wa=p(),k(Ne.$$.fragment),ca=p(),it=I("p"),Qa=v(`Let us remember that logistic regression is able to deal with classification
    problems, but only if the data is linearly separable. The last layer of the
    above neural network looks exacly like logistic regression with two input
    features. That must mean, that the neural network is somehow able to extract
    features through linear and nonlinear transformations, that are linearly
    separable.`),da=p(),st=I("p"),Za=v(`The example below shows how the neural network learns those transformations.
    On one side you can see the original inputs with the learned decision
    boundary, on the other side are the two extracted features that are used as
    inputs into the output layer. When the neural network has learned to
    separate the two circles, that means that the two features from the last
    hidden layer are linearly separable. Start the example and observe the
    learning process. At the beginning the hidden features are clustered
    together, but after a while you will notice that you could separate the
    different colored circles by a single line.`),ga=p(),k(We.$$.fragment),_a=p(),Pe=I("span"),en=v("Cross-Entropy: "),ba=v(ft),B=I("span"),Ae=I("div"),k(Ie.$$.fragment),tn=p(),k(ze.$$.fragment),an=p(),mt=I("p"),nn=v(`It is not always clear how the neural network does those
        transformations, but we could use the example above to get some
        intuition for the process. If you look at the original circular data
        again you might notice something peculiar. Imagine the data is actually
        located in 3d space and you are looking at the data from above. Now
        imagine that the blue and the red dots are located on different heights
        (z-axis). Wouldn't that mean that you could construct a 2d plane in 3d
        space to linearly separate the data? Yes it would. The first hidden
        layer of our neural network transforms the 2d data into 4d data.
        Afterwards we move the processed features back into 2d space.`),rn=p(),ut=I("p"),sn=v(`Modern neural networks have hundreds or thousands of dimensions and
        hidden layers and we can not visualize the hidden features to get a
        better feel for what the neural network does. But generally speaking we
        can state the folllowing.`),on=p(),k(qe.$$.fragment),ln=p(),$t=I("p"),fn=v(`Try to keep this intuition in mind while you move forward with your
        studies. It is easy to forget.`),mn=p(),pt=I("div"),this.h()},l(e){n=z(e,"P",{});var i=M(n);a=y(i,`So far we have discussed how a computational graph works and how we can use
    autodiff packages to solve nonlinear problems. Yet we are still missing a
    crucial component that will allow us to understand the workings of neural
    networks. We need to answer the following question. `),T(t.$$.fragment,i),i.forEach(r),o=h(e),u=z(e,"P",{});var wt=M(u);w=y(wt,`To get to that understanding, first we have to understand, that a neural
    network is a series of transormations. Multiplying the features with a
    matrix is a linear transformation, adding a bias is a translation and
    applying an activation function squishes the data. Each of these
    transformations accomplishes a different task and can be interpreted
    visually. Additionally we can stack several layers in a neural network, thus
    creating a transformation composition.`),wt.forEach(r),x=h(e),T(d.$$.fragment,e),A=h(e),g=z(e,"P",{});var ct=M(g);f=y(ct,`To demonstrate the visual interpretation of a transformation we will utilize
    the following matrix of features with 2 features and 4 samples.`),ct.forEach(r),b=h(e),m=z(e,"DIV",{class:!0});var dt=M(m);T(q.$$.fragment,dt),dt.forEach(r),$=h(e),P=z(e,"P",{});var gt=M(P);N=y(gt,"The four samples build a square in a 2d coordinate system."),gt.forEach(r),F=h(e),T(W.$$.fragment,e),O=h(e),C=z(e,"P",{});var _t=M(C);K=y(_t,"We will start with matrix multiplications."),_t.forEach(r),L=h(e),T(G.$$.fragment,e),U=h(e),j=z(e,"P",{});var bt=M(j);lt=y(bt,`While there is a formal definition of linear transformations, we could use a
    somewhat loose definition that you can use as a mental model. In a linear
    transformation parallel lines remain parallel and the origin does not move.
    So the four lines of the square above will remain parallel lines after the
    linear transformation.`),bt.forEach(r),Je=h(e),V=z(e,"P",{});var J=M(V);ya=y(J,"We can using linear transformations by multiplying the features matrix "),T(Q.$$.fragment,J),xa=y(J,` by the weight matrix
    `),T(Z.$$.fragment,J),ka=y(J,`, our transformation matrix.
    Depending on the contents of `),T(ee.$$.fragment,J),Ta=y(J,` different
    types of transformations are produced. The weight matrix is going to be a 2x2
    matrix for now. That way we are using 2 features per sample as input and generate
    2 transformed features per sample as output.`),J.forEach(r),Nt=h(e),Ue=z(e,"P",{});var vt=M(Ue);Ea=y(vt,"The identity matrix is the easiest matrix to understand."),vt.forEach(r),Ot=h(e),T(te.$$.fragment,e),Ft=h(e),je=z(e,"P",{});var yt=M(je);Da=y(yt,"Applying this transformation keeps the original matrix."),yt.forEach(r),Ct=h(e),ae=z(e,"P",{});var Oe=M(ae);Sa=y(Oe,`If we change the values of the identity matrix slightly, we scale the
    original square. The matrix
    `),T(ne.$$.fragment,Oe),Wa=y(Oe," for example scales the input square in the x direction by a factor of 2."),Oe.forEach(r),Bt=h(e),T(re.$$.fragment,e),Ht=h(e),ie=z(e,"P",{});var Fe=M(ie);Pa=y(Fe,`The matrix
    `),T(se.$$.fragment,Fe),Aa=y(Fe," on the other hand scales the matrix in the y direction by a factor of 0.5."),Fe.forEach(r),Vt=h(e),T(oe.$$.fragment,e),Yt=h(e),le=z(e,"P",{});var Ce=M(le);Ia=y(Ce,`So far we have used only one diagonal of the matrix to scale the square. The
    other diagonal can be used for the so called sheer operation. When we use
    the below matrix
    `),T(fe.$$.fragment,Ce),za=y(Ce," for example, the top and the bottom lines are moved right and left respectively."),Ce.forEach(r),Gt=h(e),T(me.$$.fragment,e),Rt=h(e),ue=z(e,"P",{});var Be=M(ue);qa=y(Be,`The matrix
    `),T($e.$$.fragment,Be),Ma=y(Be," on the other hand move the left and the right lines to the bottom or top respectively."),Be.forEach(r),Xt=h(e),T(pe.$$.fragment,e),Jt=h(e),he=z(e,"P",{});var He=M(he);La=y(He,`We can combine scaling and sheering to achieve interesting transformations.
    The matrix
    `),T(we.$$.fragment,He),Na=y(He," for example rotates the data."),He.forEach(r),Ut=h(e),T(ce.$$.fragment,e),jt=h(e),Ke=z(e,"P",{});var xt=M(Ke);Oa=y(xt,"Next let's look at the visual interpretation of the bias."),xt.forEach(r),Kt=h(e),T(de.$$.fragment,e),Qt=h(e),ge=z(e,"P",{});var Ve=M(ge);Fa=y(Ve,`A bias allows us to translate the data. That means that each point is
    equally moved. The vector
    `),T(_e.$$.fragment,Ve),Ca=y(Ve," would move all points in the x direction by 1."),Ve.forEach(r),Zt=h(e),T(be.$$.fragment,e),ea=h(e),ve=z(e,"P",{});var Ye=M(ve);Ba=y(Ye,`The vector
    `),T(ye.$$.fragment,Ye),Ha=y(Ye," on the other hand, moves all points by 1 in the y direction."),Ye.forEach(r),ta=h(e),T(xe.$$.fragment,e),aa=h(e),Qe=z(e,"P",{});var kt=M(Qe);Va=y(kt,`A translation is not a linear transformation. If we apply a linear
    transformation, that induces rotation, the zero point remains intact after
    the transformation.`),kt.forEach(r),na=h(e),T(Me.$$.fragment,e),ra=h(e),Ze=z(e,"P",{});var Tt=M(Ze);Ya=y(Tt,"A translation on the other hand moves the origin."),Tt.forEach(r),ia=h(e),T(Le.$$.fragment,e),sa=h(e),ke=z(e,"P",{});var Ge=M(ke);Ga=y(Ge,`A neural network combines a liner transformation with a translation. In
    linear algebra this transformation combination is called `),T(Te.$$.fragment,Ge),Ra=y(Ge,"."),Ge.forEach(r),oa=h(e),et=z(e,"P",{});var Et=M(et);Xa=y(Et,"Let's finally move to activation functions."),Et.forEach(r),la=h(e),T(Ee.$$.fragment,e),fa=h(e),tt=z(e,"P",{});var Dt=M(tt);Ja=y(Dt,`We can imagine the nonlinear transformations as some sort of "squishing",
    where the activation function limits the data to a certain range. The
    sigmoid that we have utilized so far pushes the vectors into a 1 by 1 box.`),Dt.forEach(r),ma=h(e),T(De.$$.fragment,e),ua=h(e),at=z(e,"P",{});var St=M(at);Ua=y(St,`The ReLU activation function is even wilder. The function turns negative
    numbers into zeros and leaves positive numbers untouched. With a ReLU
    parallel lines do not necessarily stay parallel.`),St.forEach(r),$a=h(e),T(Se.$$.fragment,e),pa=h(e),nt=z(e,"P",{});var Wt=M(nt);ja=y(Wt,`There is a final remark we would like to make in regards with linear
    transformations. So far we have used a 2x2 weight matrix for our linear
    transformations. We made this in order to keep the number of dimensions
    constant. We took in two features and produced two neurons. That way we
    could visualize the results in a 2d plane. If on the other hand we used a
    2x3 matrix, the transformation would have pushed the features into 3d space.
    In deep learning we change the amount of dimensions all the time by changing
    the number of neurons from layer to layer. Sometimes the network can find a
    better solution in a different dimension.`),Wt.forEach(r),ha=h(e),rt=z(e,"P",{});var Pt=M(rt);Ka=y(Pt,`So what exactly does a neural network try to achieve throught those
    transformations? We are going to use a slightly different architecture to
    solve our circular data problem. The architecture below was not picked
    randomly, but to show some magic that is hidden under the hood of a neural
    network.`),Pt.forEach(r),wa=h(e),T(Ne.$$.fragment,e),ca=h(e),it=z(e,"P",{});var At=M(it);Qa=y(At,`Let us remember that logistic regression is able to deal with classification
    problems, but only if the data is linearly separable. The last layer of the
    above neural network looks exacly like logistic regression with two input
    features. That must mean, that the neural network is somehow able to extract
    features through linear and nonlinear transformations, that are linearly
    separable.`),At.forEach(r),da=h(e),st=z(e,"P",{});var It=M(st);Za=y(It,`The example below shows how the neural network learns those transformations.
    On one side you can see the original inputs with the learned decision
    boundary, on the other side are the two extracted features that are used as
    inputs into the output layer. When the neural network has learned to
    separate the two circles, that means that the two features from the last
    hidden layer are linearly separable. Start the example and observe the
    learning process. At the beginning the hidden features are clustered
    together, but after a while you will notice that you could separate the
    different colored circles by a single line.`),It.forEach(r),ga=h(e),T(We.$$.fragment,e),_a=h(e),Pe=z(e,"SPAN",{});var Re=M(Pe);en=y(Re,"Cross-Entropy: "),ba=y(Re,ft),B=z(Re,"SPAN",{});var H=M(B);Ae=z(H,"DIV",{class:!0});var Xe=M(Ae);T(Ie.$$.fragment,Xe),tn=h(Xe),T(ze.$$.fragment,Xe),Xe.forEach(r),an=h(H),mt=z(H,"P",{});var zt=M(mt);nn=y(zt,`It is not always clear how the neural network does those
        transformations, but we could use the example above to get some
        intuition for the process. If you look at the original circular data
        again you might notice something peculiar. Imagine the data is actually
        located in 3d space and you are looking at the data from above. Now
        imagine that the blue and the red dots are located on different heights
        (z-axis). Wouldn't that mean that you could construct a 2d plane in 3d
        space to linearly separate the data? Yes it would. The first hidden
        layer of our neural network transforms the 2d data into 4d data.
        Afterwards we move the processed features back into 2d space.`),zt.forEach(r),rn=h(H),ut=z(H,"P",{});var qt=M(ut);sn=y(qt,`Modern neural networks have hundreds or thousands of dimensions and
        hidden layers and we can not visualize the hidden features to get a
        better feel for what the neural network does. But generally speaking we
        can state the folllowing.`),qt.forEach(r),on=h(H),T(qe.$$.fragment,H),ln=h(H),$t=z(H,"P",{});var bn=M($t);fn=y(bn,`Try to keep this intuition in mind while you move forward with your
        studies. It is easy to forget.`),bn.forEach(r),mn=h(H),pt=z(H,"DIV",{class:!0}),M(pt).forEach(r),H.forEach(r),Re.forEach(r),this.h()},h(){ot(m,"class","flex justify-center"),ot(Ae,"class","flex flex-col md:flex-row"),ot(pt,"class","separator")},m(e,i){s(e,n,i),S(n,a),E(t,n,null),s(e,o,i),s(e,u,i),S(u,w),s(e,x,i),E(d,e,i),s(e,A,i),s(e,g,i),S(g,f),s(e,b,i),s(e,m,i),E(q,m,null),s(e,$,i),s(e,P,i),S(P,N),s(e,F,i),E(W,e,i),s(e,O,i),s(e,C,i),S(C,K),s(e,L,i),E(G,e,i),s(e,U,i),s(e,j,i),S(j,lt),s(e,Je,i),s(e,V,i),S(V,ya),E(Q,V,null),S(V,xa),E(Z,V,null),S(V,ka),E(ee,V,null),S(V,Ta),s(e,Nt,i),s(e,Ue,i),S(Ue,Ea),s(e,Ot,i),E(te,e,i),s(e,Ft,i),s(e,je,i),S(je,Da),s(e,Ct,i),s(e,ae,i),S(ae,Sa),E(ne,ae,null),S(ae,Wa),s(e,Bt,i),E(re,e,i),s(e,Ht,i),s(e,ie,i),S(ie,Pa),E(se,ie,null),S(ie,Aa),s(e,Vt,i),E(oe,e,i),s(e,Yt,i),s(e,le,i),S(le,Ia),E(fe,le,null),S(le,za),s(e,Gt,i),E(me,e,i),s(e,Rt,i),s(e,ue,i),S(ue,qa),E($e,ue,null),S(ue,Ma),s(e,Xt,i),E(pe,e,i),s(e,Jt,i),s(e,he,i),S(he,La),E(we,he,null),S(he,Na),s(e,Ut,i),E(ce,e,i),s(e,jt,i),s(e,Ke,i),S(Ke,Oa),s(e,Kt,i),E(de,e,i),s(e,Qt,i),s(e,ge,i),S(ge,Fa),E(_e,ge,null),S(ge,Ca),s(e,Zt,i),E(be,e,i),s(e,ea,i),s(e,ve,i),S(ve,Ba),E(ye,ve,null),S(ve,Ha),s(e,ta,i),E(xe,e,i),s(e,aa,i),s(e,Qe,i),S(Qe,Va),s(e,na,i),E(Me,e,i),s(e,ra,i),s(e,Ze,i),S(Ze,Ya),s(e,ia,i),E(Le,e,i),s(e,sa,i),s(e,ke,i),S(ke,Ga),E(Te,ke,null),S(ke,Ra),s(e,oa,i),s(e,et,i),S(et,Xa),s(e,la,i),E(Ee,e,i),s(e,fa,i),s(e,tt,i),S(tt,Ja),s(e,ma,i),E(De,e,i),s(e,ua,i),s(e,at,i),S(at,Ua),s(e,$a,i),E(Se,e,i),s(e,pa,i),s(e,nt,i),S(nt,ja),s(e,ha,i),s(e,rt,i),S(rt,Ka),s(e,wa,i),E(Ne,e,i),s(e,ca,i),s(e,it,i),S(it,Qa),s(e,da,i),s(e,st,i),S(st,Za),s(e,ga,i),E(We,e,i),s(e,_a,i),s(e,Pe,i),S(Pe,en),S(Pe,ba),S(Pe,B),S(B,Ae),E(Ie,Ae,null),S(Ae,tn),E(ze,Ae,null),S(B,an),S(B,mt),S(mt,nn),S(B,rn),S(B,ut),S(ut,sn),S(B,on),E(qe,B,null),S(B,ln),S(B,$t),S($t,fn),S(B,mn),S(B,pt),ht=!0},p(e,i){const wt={};i&8192&&(wt.$$scope={dirty:i,ctx:e}),t.$set(wt);const ct={};i&8192&&(ct.$$scope={dirty:i,ctx:e}),d.$set(ct);const dt={};i&8192&&(dt.$$scope={dirty:i,ctx:e}),q.$set(dt);const gt={};i&1&&(gt.inputData=e[0]),W.$set(gt);const _t={};i&8192&&(_t.$$scope={dirty:i,ctx:e}),G.$set(_t);const bt={};i&8192&&(bt.$$scope={dirty:i,ctx:e}),Q.$set(bt);const J={};i&8192&&(J.$$scope={dirty:i,ctx:e}),Z.$set(J);const vt={};i&8192&&(vt.$$scope={dirty:i,ctx:e}),ee.$set(vt);const yt={};i&8192&&(yt.$$scope={dirty:i,ctx:e}),te.$set(yt);const Oe={};i&8192&&(Oe.$$scope={dirty:i,ctx:e}),ne.$set(Oe);const Fe={};i&1&&(Fe.inputData=e[0]),re.$set(Fe);const Ce={};i&8192&&(Ce.$$scope={dirty:i,ctx:e}),se.$set(Ce);const Be={};i&1&&(Be.inputData=e[0]),oe.$set(Be);const He={};i&8192&&(He.$$scope={dirty:i,ctx:e}),fe.$set(He);const xt={};i&1&&(xt.inputData=e[0]),me.$set(xt);const Ve={};i&8192&&(Ve.$$scope={dirty:i,ctx:e}),$e.$set(Ve);const Ye={};i&1&&(Ye.inputData=e[0]),pe.$set(Ye);const kt={};i&8192&&(kt.$$scope={dirty:i,ctx:e}),we.$set(kt);const Tt={};i&1&&(Tt.inputData=e[0]),ce.$set(Tt);const Ge={};i&8192&&(Ge.$$scope={dirty:i,ctx:e}),de.$set(Ge);const Et={};i&8192&&(Et.$$scope={dirty:i,ctx:e}),_e.$set(Et);const Dt={};i&1&&(Dt.inputData=e[0]),be.$set(Dt);const St={};i&8192&&(St.$$scope={dirty:i,ctx:e}),ye.$set(St);const Wt={};i&1&&(Wt.inputData=e[0]),xe.$set(Wt);const Pt={};i&8192&&(Pt.$$scope={dirty:i,ctx:e}),Te.$set(Pt);const At={};i&8192&&(At.$$scope={dirty:i,ctx:e}),Ee.$set(At);const It={};i&2&&(It.inputData=e[1]),De.$set(It);const Re={};i&2&&(Re.inputData=e[1]),Se.$set(Re);const H={};i&8192&&(H.$$scope={dirty:i,ctx:e}),We.$set(H),(!ht||i&16)&&ft!==(ft=e[4].toFixed(6)+"")&&Mn(ba,ft);const Xe={};i&8196&&(Xe.$$scope={dirty:i,ctx:e}),Ie.$set(Xe);const zt={};i&8200&&(zt.$$scope={dirty:i,ctx:e}),ze.$set(zt);const qt={};i&8192&&(qt.$$scope={dirty:i,ctx:e}),qe.$set(qt)},i(e){ht||(c(t.$$.fragment,e),c(d.$$.fragment,e),c(q.$$.fragment,e),c(W.$$.fragment,e),c(G.$$.fragment,e),c(Q.$$.fragment,e),c(Z.$$.fragment,e),c(ee.$$.fragment,e),c(te.$$.fragment,e),c(ne.$$.fragment,e),c(re.$$.fragment,e),c(se.$$.fragment,e),c(oe.$$.fragment,e),c(fe.$$.fragment,e),c(me.$$.fragment,e),c($e.$$.fragment,e),c(pe.$$.fragment,e),c(we.$$.fragment,e),c(ce.$$.fragment,e),c(de.$$.fragment,e),c(_e.$$.fragment,e),c(be.$$.fragment,e),c(ye.$$.fragment,e),c(xe.$$.fragment,e),c(Me.$$.fragment,e),c(Le.$$.fragment,e),c(Te.$$.fragment,e),c(Ee.$$.fragment,e),c(De.$$.fragment,e),c(Se.$$.fragment,e),c(Ne.$$.fragment,e),c(We.$$.fragment,e),c(Ie.$$.fragment,e),c(ze.$$.fragment,e),c(qe.$$.fragment,e),ht=!0)},o(e){_(t.$$.fragment,e),_(d.$$.fragment,e),_(q.$$.fragment,e),_(W.$$.fragment,e),_(G.$$.fragment,e),_(Q.$$.fragment,e),_(Z.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(re.$$.fragment,e),_(se.$$.fragment,e),_(oe.$$.fragment,e),_(fe.$$.fragment,e),_(me.$$.fragment,e),_($e.$$.fragment,e),_(pe.$$.fragment,e),_(we.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_(ye.$$.fragment,e),_(xe.$$.fragment,e),_(Me.$$.fragment,e),_(Le.$$.fragment,e),_(Te.$$.fragment,e),_(Ee.$$.fragment,e),_(De.$$.fragment,e),_(Se.$$.fragment,e),_(Ne.$$.fragment,e),_(We.$$.fragment,e),_(Ie.$$.fragment,e),_(ze.$$.fragment,e),_(qe.$$.fragment,e),ht=!1},d(e){e&&r(n),D(t),e&&r(o),e&&r(u),e&&r(x),D(d,e),e&&r(A),e&&r(g),e&&r(b),e&&r(m),D(q),e&&r($),e&&r(P),e&&r(F),D(W,e),e&&r(O),e&&r(C),e&&r(L),D(G,e),e&&r(U),e&&r(j),e&&r(Je),e&&r(V),D(Q),D(Z),D(ee),e&&r(Nt),e&&r(Ue),e&&r(Ot),D(te,e),e&&r(Ft),e&&r(je),e&&r(Ct),e&&r(ae),D(ne),e&&r(Bt),D(re,e),e&&r(Ht),e&&r(ie),D(se),e&&r(Vt),D(oe,e),e&&r(Yt),e&&r(le),D(fe),e&&r(Gt),D(me,e),e&&r(Rt),e&&r(ue),D($e),e&&r(Xt),D(pe,e),e&&r(Jt),e&&r(he),D(we),e&&r(Ut),D(ce,e),e&&r(jt),e&&r(Ke),e&&r(Kt),D(de,e),e&&r(Qt),e&&r(ge),D(_e),e&&r(Zt),D(be,e),e&&r(ea),e&&r(ve),D(ye),e&&r(ta),D(xe,e),e&&r(aa),e&&r(Qe),e&&r(na),D(Me,e),e&&r(ra),e&&r(Ze),e&&r(ia),D(Le,e),e&&r(sa),e&&r(ke),D(Te),e&&r(oa),e&&r(et),e&&r(la),D(Ee,e),e&&r(fa),e&&r(tt),e&&r(ma),D(De,e),e&&r(ua),e&&r(at),e&&r($a),D(Se,e),e&&r(pa),e&&r(nt),e&&r(ha),e&&r(rt),e&&r(wa),D(Ne,e),e&&r(ca),e&&r(it),e&&r(da),e&&r(st),e&&r(ga),D(We,e),e&&r(_a),e&&r(Pe),D(Ie),D(ze),D(qe)}}}function gr(l){let n,a,t,o,u,w,x,d,A;return d=new Ln({props:{$$slots:{default:[dr]},$$scope:{ctx:l}}}),{c(){n=I("meta"),a=p(),t=I("h1"),o=v("Geometric Interpretation"),u=p(),w=I("div"),x=p(),k(d.$$.fragment),this.h()},l(g){const f=qn("svelte-1v0uspb",document.head);n=z(f,"META",{name:!0,content:!0}),f.forEach(r),a=h(g),t=z(g,"H1",{});var b=M(t);o=y(b,"Geometric Interpretation"),b.forEach(r),u=h(g),w=z(g,"DIV",{class:!0}),M(w).forEach(r),x=h(g),T(d.$$.fragment,g),this.h()},h(){document.title="Deep Learning Geometric Interpretation - World4AI",ot(n,"name","description"),ot(n,"content","Neural networks transform the inputs by scaling, translating and rotating the data with matrix multiplication. Additionally matrix multiplications can move the hidden features between different dimensions to better solve the task. Activation functions squish the data to provide solutions for non linear problesm. The last layer is linearly separable."),ot(w,"class","separator")},m(g,f){S(document.head,n),s(g,a,f),s(g,t,f),S(t,o),s(g,u,f),s(g,w,f),s(g,x,f),E(d,g,f),A=!0},p(g,[f]){const b={};f&8223&&(b.$$scope={dirty:f,ctx:g}),d.$set(b)},i(g){A||(c(d.$$.fragment,g),A=!0)},o(g){_(d.$$.fragment,g),A=!1},d(g){r(n),g&&r(a),g&&r(t),g&&r(u),g&&r(w),g&&r(x),D(d,g)}}}let _r=.5,br=.5,vr=200,va=50;const pn=1;function yr(l,n,a){let{inputData:t=[{x:-1,y:1},{x:-1,y:-1},{x:1,y:-1},{x:1,y:1}]}=n,{inputData2:o=[{x:-1,y:1},{x:-2,y:0},{x:-1,y:-1},{x:1,y:-1},{x:2,y:0},{x:1,y:1}]}=n;const u=[{title:"Features",nodes:[{value:"x_1",class:"fill-gray-200"},{value:"x_2",class:"fill-gray-200"}]},{title:"Hidden Layer 1",nodes:[{value:"a_{11}",class:"fill-blue-400"},{value:"a_{12}",class:"fill-blue-400"},{value:"a_{13}",class:"fill-blue-400"},{value:"a_{14}",class:"fill-blue-400"}]},{title:"Hidden Layer 2",nodes:[{value:"a_{21}",class:"fill-yellow-400"},{value:"a_{22}",class:"fill-yellow-400"}]},{title:"Output",nodes:[{value:"o",class:"fill-blue-400"}]}];let w=[[],[]],x=[.45,.25],d=[],A=[];for(let P=0;P<x.length;P++)for(let N=0;N<vr;N++){let F=2*Math.PI*Math.random(),W=x[P],O=W*Math.cos(F)+_r,C=W*Math.sin(F)+br;w[P].push({x:O,y:C}),d.push([O,C]),A.push(P)}let g=[];for(let P=0;P<va;P++)for(let N=0;N<va;N++){let F=P/va,W=N/va,O=[];O.push(F),O.push(W),g.push(O)}let f=[[],[]],b=[[],[]],m=0;function q(){let P=new un(2,4),N=new un(4,2),F=new un(2,1),W=new $n(0);function O(){a(3,b=[[],[]]),P.zeroGrad(),N.zeroGrad(),F.zeroGrad();for(let L=0;L<d.length;L++){let G=P.forward(d[L]),U=N.forward(G),j=F.forward(U);if(A[L]===0){let V=new $n(1);W=W.add(V.sub(j).log())}else A[L]===1&&(W=W.add(j.log()));let lt=U[0].data,Je=U[1].data;b[A[L]].push({x:lt,y:Je})}W=W.neg().div(d.length),a(4,m=W.data),W.backward(),P.parameters().forEach(L=>{L.data-=pn*L.grad}),N.parameters().forEach(L=>{L.data-=pn*L.grad}),F.parameters().forEach(L=>{L.data-=pn*L.grad}),W=new $n(0);let C=[],K=[];g.forEach(L=>{let G=P.forward(L),U=N.forward(G);F.forward(U).data<.5?C.push({x:L[0],y:L[1]}):K.push({x:L[0],y:L[1]})}),a(2,f=[]),f.push(C),f.push(K)}return O}let $=q();return l.$$set=P=>{"inputData"in P&&a(0,t=P.inputData),"inputData2"in P&&a(1,o=P.inputData2)},[t,o,f,b,m,u,w,$]}class Cr extends Sn{constructor(n){super(),Wn(this,n,yr,gr,Pn,{inputData:0,inputData2:1})}}export{Cr as default};
