import{S as Kt,i as Jt,s as Qt,k as T,a as p,q as y,y as g,W as Xt,l as L,h as s,c as m,m as A,r as b,z as v,n as de,N as x,b as c,A as w,g as d,d as _,B as E,L as Zt,e as nt,v as Tt,f as Lt,P as Vt,C as Ut}from"../chunks/index.4d92b023.js";import{C as ea}from"../chunks/Container.b0705c7b.js";import{F as ta,I as aa}from"../chunks/InternalLink.7deb899c.js";import{P as z}from"../chunks/PythonCode.212ba7a6.js";import{H as oa}from"../chunks/Highlight.b7c1de53.js";import{T as na,a as sa,b as ra,R as Yt,H as la,D as ia}from"../chunks/HeaderEntry.2b6e8f51.js";const ca=""+new URL("../assets/cifar_10.730dd3fc.webp",import.meta.url).href;function Ot(h,o,i){const t=h.slice();return t[3]=o[i],t}function qt(h,o,i){const t=h.slice();return t[6]=o[i],t}function Bt(h,o,i){const t=h.slice();return t[9]=o[i],t}function fa(h){let o=h[9]+"",i;return{c(){i=y(o)},l(t){i=b(t,o)},m(t,a){c(t,i,a)},p:Ut,d(t){t&&s(i)}}}function Gt(h){let o,i;return o=new la({props:{$$slots:{default:[fa]},$$scope:{ctx:h}}}),{c(){g(o.$$.fragment)},l(t){v(o.$$.fragment,t)},m(t,a){w(o,t,a),i=!0},p(t,a){const u={};a&4096&&(u.$$scope={dirty:a,ctx:t}),o.$set(u)},i(t){i||(d(o.$$.fragment,t),i=!0)},o(t){_(o.$$.fragment,t),i=!1},d(t){E(o,t)}}}function ua(h){let o,i,t=h[0],a=[];for(let n=0;n<t.length;n+=1)a[n]=Gt(Bt(h,t,n));const u=n=>_(a[n],1,1,()=>{a[n]=null});return{c(){for(let n=0;n<a.length;n+=1)a[n].c();o=nt()},l(n){for(let f=0;f<a.length;f+=1)a[f].l(n);o=nt()},m(n,f){for(let l=0;l<a.length;l+=1)a[l]&&a[l].m(n,f);c(n,o,f),i=!0},p(n,f){if(f&1){t=n[0];let l;for(l=0;l<t.length;l+=1){const V=Bt(n,t,l);a[l]?(a[l].p(V,f),d(a[l],1)):(a[l]=Gt(V),a[l].c(),d(a[l],1),a[l].m(o.parentNode,o))}for(Tt(),l=t.length;l<a.length;l+=1)u(l);Lt()}},i(n){if(!i){for(let f=0;f<t.length;f+=1)d(a[f]);i=!0}},o(n){a=a.filter(Boolean);for(let f=0;f<a.length;f+=1)_(a[f]);i=!1},d(n){Vt(a,n),n&&s(o)}}}function pa(h){let o,i;return o=new Yt({props:{$$slots:{default:[ua]},$$scope:{ctx:h}}}),{c(){g(o.$$.fragment)},l(t){v(o.$$.fragment,t)},m(t,a){w(o,t,a),i=!0},p(t,a){const u={};a&4096&&(u.$$scope={dirty:a,ctx:t}),o.$set(u)},i(t){i||(d(o.$$.fragment,t),i=!0)},o(t){_(o.$$.fragment,t),i=!1},d(t){E(o,t)}}}function ma(h){let o=h[6]+"",i;return{c(){i=y(o)},l(t){i=b(t,o)},m(t,a){c(t,i,a)},p:Ut,d(t){t&&s(i)}}}function jt(h){let o,i;return o=new ia({props:{$$slots:{default:[ma]},$$scope:{ctx:h}}}),{c(){g(o.$$.fragment)},l(t){v(o.$$.fragment,t)},m(t,a){w(o,t,a),i=!0},p(t,a){const u={};a&4096&&(u.$$scope={dirty:a,ctx:t}),o.$set(u)},i(t){i||(d(o.$$.fragment,t),i=!0)},o(t){_(o.$$.fragment,t),i=!1},d(t){E(o,t)}}}function da(h){let o,i,t=h[3],a=[];for(let n=0;n<t.length;n+=1)a[n]=jt(qt(h,t,n));const u=n=>_(a[n],1,1,()=>{a[n]=null});return{c(){for(let n=0;n<a.length;n+=1)a[n].c();o=p()},l(n){for(let f=0;f<a.length;f+=1)a[f].l(n);o=m(n)},m(n,f){for(let l=0;l<a.length;l+=1)a[l]&&a[l].m(n,f);c(n,o,f),i=!0},p(n,f){if(f&2){t=n[3];let l;for(l=0;l<t.length;l+=1){const V=qt(n,t,l);a[l]?(a[l].p(V,f),d(a[l],1)):(a[l]=jt(V),a[l].c(),d(a[l],1),a[l].m(o.parentNode,o))}for(Tt(),l=t.length;l<a.length;l+=1)u(l);Lt()}},i(n){if(!i){for(let f=0;f<t.length;f+=1)d(a[f]);i=!0}},o(n){a=a.filter(Boolean);for(let f=0;f<a.length;f+=1)_(a[f]);i=!1},d(n){Vt(a,n),n&&s(o)}}}function Ht(h){let o,i;return o=new Yt({props:{$$slots:{default:[da]},$$scope:{ctx:h}}}),{c(){g(o.$$.fragment)},l(t){v(o.$$.fragment,t)},m(t,a){w(o,t,a),i=!0},p(t,a){const u={};a&4096&&(u.$$scope={dirty:a,ctx:t}),o.$set(u)},i(t){i||(d(o.$$.fragment,t),i=!0)},o(t){_(o.$$.fragment,t),i=!1},d(t){E(o,t)}}}function ha(h){let o,i,t=h[1],a=[];for(let n=0;n<t.length;n+=1)a[n]=Ht(Ot(h,t,n));const u=n=>_(a[n],1,1,()=>{a[n]=null});return{c(){for(let n=0;n<a.length;n+=1)a[n].c();o=nt()},l(n){for(let f=0;f<a.length;f+=1)a[f].l(n);o=nt()},m(n,f){for(let l=0;l<a.length;l+=1)a[l]&&a[l].m(n,f);c(n,o,f),i=!0},p(n,f){if(f&2){t=n[1];let l;for(l=0;l<t.length;l+=1){const V=Ot(n,t,l);a[l]?(a[l].p(V,f),d(a[l],1)):(a[l]=Ht(V),a[l].c(),d(a[l],1),a[l].m(o.parentNode,o))}for(Tt(),l=t.length;l<a.length;l+=1)u(l);Lt()}},i(n){if(!i){for(let f=0;f<t.length;f+=1)d(a[f]);i=!0}},o(n){a=a.filter(Boolean);for(let f=0;f<a.length;f+=1)_(a[f]);i=!1},d(n){Vt(a,n),n&&s(o)}}}function $a(h){let o,i,t,a;return o=new sa({props:{$$slots:{default:[pa]},$$scope:{ctx:h}}}),t=new ra({props:{$$slots:{default:[ha]},$$scope:{ctx:h}}}),{c(){g(o.$$.fragment),i=p(),g(t.$$.fragment)},l(u){v(o.$$.fragment,u),i=m(u),v(t.$$.fragment,u)},m(u,n){w(o,u,n),c(u,i,n),w(t,u,n),a=!0},p(u,n){const f={};n&4096&&(f.$$scope={dirty:n,ctx:u}),o.$set(f);const l={};n&4096&&(l.$$scope={dirty:n,ctx:u}),t.$set(l)},i(u){a||(d(o.$$.fragment,u),d(t.$$.fragment,u),a=!0)},o(u){_(o.$$.fragment,u),_(t.$$.fragment,u),a=!1},d(u){E(o,u),u&&s(i),E(t,u)}}}function _a(h){let o;return{c(){o=y("accuracy of roughly 60%")},l(i){o=b(i,"accuracy of roughly 60%")},m(i,t){c(i,o,t)},d(i){i&&s(o)}}}function ga(h){let o,i,t,a,u,n,f,l,V,S,C,$,k,D,we,W,Ee,se,st,xe,O,ye,q,be,B,Te,re,rt,Le,G,Ve,le,lt,Ae,j,ze,H,ke,ie,it,De,U,Se,Y,Ce,K,Ne,J,Pe,ce,ct,Ie,fe,ft,Me,Q,Fe,ue,ut,Re,X,We,N,At,Oe,pe,pt,qe,P,mt,he,dt,ht,Be,Z,Ge,I,$t,$e,_t,gt,je,ee,He,te,Ue,M,vt,_e,wt,Et,Ye,me,xt,Ke,ae,Je,oe,Qe,ne,Xe,F,yt,R,bt,Ze;return t=new aa({props:{type:"reference",id:1}}),V=new na({props:{$$slots:{default:[$a]},$$scope:{ctx:h}}}),D=new z({props:{code:`from sklearn.model_selection import train_test_split
import time
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, Subset
from torchvision.datasets.mnist import MNIST
from torchvision import transforms as T

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`}}),W=new z({props:{code:`# -----------------------------------
# DATASETS and DATALOADERS
# -----------------------------------

# get MNIST data
train_val_dataset = MNIST(
    root="../datasets", download=True, train=True, transform=T.ToTensor()
)
test_dataset = MNIST(
    root="../datasets", download=False, train=False, transform=T.ToTensor()
)

# split dataset into train and validate
indices = list(range(len(train_val_dataset)))
train_idxs, val_idxs = train_test_split(
    indices, test_size=0.1, stratify=train_val_dataset.targets.numpy()
)

train_dataset = Subset(train_val_dataset, train_idxs)
val_dataset = Subset(train_val_dataset, val_idxs)

batch_size = 32
train_dataloader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4,
    drop_last=True,
)
val_dataloader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=4,
    drop_last=False,
)
test_dataloader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=4,
    drop_last=False,
)`}}),O=new z({props:{code:`# -----------------------------------
# LeNet-5 Model
# -----------------------------------

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),
            nn.Tanh(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0),
            nn.Tanh(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, padding=0),
            nn.Tanh(),
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=120, out_features=84),
            nn.Tanh(),
            nn.Linear(in_features=84, out_features=10),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        logits = self.classifier(features)
        return logits`}}),q=new z({props:{code:`# -----------------------------------
# CALCULATE PERFORMANCE
# -----------------------------------
def track_performance(dataloader, model, criterion):
    # switch to evaluation mode
    model.eval()
    num_samples = 0
    num_correct = 0
    loss_sum = 0

    # no need to calculate gradients
    with torch.inference_mode():
        for _, (features, labels) in enumerate(dataloader):
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                features = features.to(device)
                labels = labels.to(device)
                logits = model(features)

                predictions = logits.max(dim=1)[1]
                num_correct += (predictions == labels).sum().item()

                loss = criterion(logits, labels)
                loss_sum += loss.cpu().item()
                num_samples += len(features)

    # we return the average loss and the accuracy
    return loss_sum / num_samples, num_correct / num_samples`}}),B=new z({props:{code:`# -----------------------------------
# TRAIN
# -----------------------------------

def train(
    num_epochs,
    train_dataloader,
    val_dataloader,
    model,
    criterion,
    optimizer,
    scheduler=None,
):
    model.to(device)
    scaler = torch.cuda.amp.GradScaler()
    for epoch in range(num_epochs):
        start_time = time.time()
        for _, (features, labels) in enumerate(train_dataloader):
            model.train()
            features = features.to(device)
            labels = labels.to(device)

            # Empty the gradients
            optimizer.zero_grad()

            with torch.autocast(device_type="cuda", dtype=torch.float16):
                # Forward Pass
                logits = model(features)
                # Calculate Loss
                loss = criterion(logits, labels)

            # Backward Pass
            scaler.scale(loss).backward()

            # Gradient Descent
            scaler.step(optimizer)
            scaler.update()

        val_loss, val_acc = track_performance(val_dataloader, model, criterion)
        end_time = time.time()

        s = (
            f"Epoch: {epoch+1:>2}/{num_epochs} | "
            f"Epoch Duration: {end_time - start_time:.3f} sec | "
            f"Val Loss: {val_loss:.5f} | "
            f"Val Acc: {val_acc:.3f} |"
        )
        print(s)

        if scheduler:
            scheduler.step(val_loss)`}}),G=new z({props:{code:`model = Model()
optimizer = optim.SGD(params=model.parameters(), lr=0.01)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, factor=0.1, patience=2, verbose=True
)
criterion = nn.CrossEntropyLoss(reduction="sum")`}}),j=new z({props:{code:`train(
    num_epochs=10,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
)`}}),H=new z({props:{isOutput:!0,code:`Epoch:  1/10 | Epoch Duration: 8.358 sec | Val Loss: 0.07504 | Val Acc: 0.978 |
Epoch:  2/10 | Epoch Duration: 7.312 sec | Val Loss: 0.05162 | Val Acc: 0.984 |
Epoch:  3/10 | Epoch Duration: 7.049 sec | Val Loss: 0.04810 | Val Acc: 0.984 |
Epoch:  4/10 | Epoch Duration: 7.019 sec | Val Loss: 0.04933 | Val Acc: 0.985 |
Epoch:  5/10 | Epoch Duration: 7.180 sec | Val Loss: 0.04957 | Val Acc: 0.987 |
Epoch:  6/10 | Epoch Duration: 7.215 sec | Val Loss: 0.04996 | Val Acc: 0.988 |
Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.
Epoch:  7/10 | Epoch Duration: 7.212 sec | Val Loss: 0.03751 | Val Acc: 0.990 |
Epoch:  8/10 | Epoch Duration: 7.243 sec | Val Loss: 0.03633 | Val Acc: 0.989 |
Epoch:  9/10 | Epoch Duration: 7.464 sec | Val Loss: 0.03620 | Val Acc: 0.989 |
Epoch: 10/10 | Epoch Duration: 7.253 sec | Val Loss: 0.03639 | Val Acc: 0.989 |`}}),U=new z({props:{code:"from torchvision.datasets.cifar import CIFAR10"}}),Y=new z({props:{code:`train_val_dataset = CIFAR10(root='../datasets', download=True, train=True, transform=T.ToTensor())
test_dataset = CIFAR10(root='../datasets', download=False, train=False, transform=T.ToTensor())`}}),K=new z({props:{code:`# split dataset into train and validate
indices = list(range(len(train_val_dataset)))
train_idxs, val_idxs = train_test_split(
    indices, test_size=0.1, stratify=train_val_dataset.targets
)

train_dataset = Subset(train_val_dataset, train_idxs)
val_dataset = Subset(train_val_dataset, val_idxs)`}}),J=new z({props:{code:`batch_size = 32
train_dataloader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4,
    drop_last=True,
)
val_dataloader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=4,
    drop_last=False,
)
test_dataloader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=4,
    drop_last=False,
)`}}),Q=new z({props:{code:`classes = ('plane', 'car', 'bird', 'cat','deer', 
            'dog', 'frog', 'horse', 'ship', 'truck')`}}),X=new z({props:{code:`fig = plt.figure(figsize=(6, 8))
columns = 4
rows = 5

for i in range(1, columns*rows +1):
    img, cls = train_val_dataset[i]
    fig.add_subplot(rows, columns, i)
    plt.imshow(img.permute(1, 2, 0).numpy())
    plt.title(classes[cls])
    plt.axis('off')
plt.show()`}}),Z=new z({props:{code:`# -----------------------------------
# LeNet-5 Model
# -----------------------------------

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            # inputut channels equals to 3
            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2),
            nn.Tanh(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0),
            nn.Tanh(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, padding=0),
            nn.Tanh(),
        )
        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=120, out_features=84),
            nn.Tanh(),
            nn.Linear(in_features=84, out_features=10),
        )

    def forward(self, x):
        x = self.feature_extractor(x)
        x = self.avgpool(x)
        x = self.classifier(x)
        return x`}}),ee=new z({props:{code:`with torch.inference_mode():
    x = torch.randn(1, 3, 32, 32).to(device)
    x = model.feature_extractor(x)
    print(x.shape)`}}),te=new z({props:{code:"torch.Size([1, 120, 2, 2])",isOutput:!0}}),ae=new z({props:{code:`model = Model()
optimizer = optim.SGD(params=model.parameters(), lr=0.01)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, factor=0.1, patience=5, verbose=True
)
criterion = nn.CrossEntropyLoss(reduction="sum")`}}),oe=new z({props:{code:`train(
    num_epochs=30,
    train_dataloader=train_dataloader,
    val_dataloader=val_dataloader,
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
)`}}),ne=new z({props:{isOutput:!0,code:`Epoch:  1/30 | Epoch Duration: 4.378 sec | Val Loss: 1.55044 | Val Acc: 0.432 |
Epoch:  2/30 | Epoch Duration: 4.689 sec | Val Loss: 1.39773 | Val Acc: 0.495 |
Epoch:  3/30 | Epoch Duration: 4.552 sec | Val Loss: 1.31610 | Val Acc: 0.529 |
Epoch:  4/30 | Epoch Duration: 4.434 sec | Val Loss: 1.35375 | Val Acc: 0.525 |
Epoch:  5/30 | Epoch Duration: 4.521 sec | Val Loss: 1.27864 | Val Acc: 0.546 |
Epoch:  6/30 | Epoch Duration: 4.576 sec | Val Loss: 1.26108 | Val Acc: 0.554 |
Epoch:  7/30 | Epoch Duration: 4.523 sec | Val Loss: 1.35097 | Val Acc: 0.521 |
Epoch:  8/30 | Epoch Duration: 4.492 sec | Val Loss: 1.25061 | Val Acc: 0.566 |
Epoch:  9/30 | Epoch Duration: 4.675 sec | Val Loss: 1.27933 | Val Acc: 0.549 |
Epoch: 10/30 | Epoch Duration: 4.565 sec | Val Loss: 1.25053 | Val Acc: 0.557 |
Epoch: 11/30 | Epoch Duration: 4.760 sec | Val Loss: 1.25920 | Val Acc: 0.571 |
Epoch: 12/30 | Epoch Duration: 4.596 sec | Val Loss: 1.32838 | Val Acc: 0.540 |
Epoch: 13/30 | Epoch Duration: 4.581 sec | Val Loss: 1.31666 | Val Acc: 0.546 |
Epoch: 14/30 | Epoch Duration: 4.478 sec | Val Loss: 1.27574 | Val Acc: 0.564 |
Epoch 00014: reducing learning rate of group 0 to 1.0000e-03.
Epoch: 15/30 | Epoch Duration: 4.667 sec | Val Loss: 1.17661 | Val Acc: 0.594 |
Epoch: 16/30 | Epoch Duration: 4.651 sec | Val Loss: 1.18592 | Val Acc: 0.599 |
Epoch: 17/30 | Epoch Duration: 4.693 sec | Val Loss: 1.20123 | Val Acc: 0.598 |
Epoch: 18/30 | Epoch Duration: 4.533 sec | Val Loss: 1.21307 | Val Acc: 0.595 |
Epoch: 19/30 | Epoch Duration: 4.688 sec | Val Loss: 1.23074 | Val Acc: 0.597 |
Epoch: 20/30 | Epoch Duration: 4.423 sec | Val Loss: 1.23757 | Val Acc: 0.596 |
Epoch: 21/30 | Epoch Duration: 4.837 sec | Val Loss: 1.26319 | Val Acc: 0.593 |
Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.
Epoch: 22/30 | Epoch Duration: 4.638 sec | Val Loss: 1.25713 | Val Acc: 0.595 |
Epoch: 23/30 | Epoch Duration: 4.472 sec | Val Loss: 1.25976 | Val Acc: 0.593 |
Epoch: 24/30 | Epoch Duration: 4.450 sec | Val Loss: 1.26443 | Val Acc: 0.596 |
Epoch: 25/30 | Epoch Duration: 5.288 sec | Val Loss: 1.26758 | Val Acc: 0.597 |
Epoch: 26/30 | Epoch Duration: 4.750 sec | Val Loss: 1.27124 | Val Acc: 0.596 |
Epoch: 27/30 | Epoch Duration: 4.771 sec | Val Loss: 1.27477 | Val Acc: 0.596 |
Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.
Epoch: 28/30 | Epoch Duration: 4.605 sec | Val Loss: 1.27532 | Val Acc: 0.596 |
Epoch: 29/30 | Epoch Duration: 5.403 sec | Val Loss: 1.27567 | Val Acc: 0.596 |
Epoch: 30/30 | Epoch Duration: 4.882 sec | Val Loss: 1.27617 | Val Acc: 0.596 |`}}),R=new oa({props:{$$slots:{default:[_a]},$$scope:{ctx:h}}}),{c(){o=T("p"),i=y("LeNet-5"),g(t.$$.fragment),a=y(` is simultaneouly one of the
    oldest and the most well known convolutional neural network architecture. The
    name LeNet-5 is a reference to the inventor of the network, Yann LeCun, considered
    to be one of the grandfathers of deep learning. The network was designed for
    image recognition and was extensively used with the MNIST dataset. We can not
    expect this architecture to produce state of the art results, but this is a good
    exercise and a great starting point in our study of cnn architectures.`),u=p(),n=T("p"),f=y(`The table below depicts the architecure of the LeCun-5 network. First we
    apply 3 convolutional layers and 2 average pooling layers. After the third
    convolutional layer we flatten the feature maps and use two fully connected
    layers. We use the tanh activation function for all convolutional and fully
    connected layers, only the last fully connected layer uses the softmax
    activation function.`),l=p(),g(V.$$.fragment),S=p(),C=T("p"),$=y(`Below we implement LeNet-5 in PyTorch. There are no new pieces in the code
    below, all the piece swere already covered in previous sections.`),k=p(),g(D.$$.fragment),we=p(),g(W.$$.fragment),Ee=p(),se=T("p"),st=y("The model is an exact implementation of the table above."),xe=p(),g(O.$$.fragment),ye=p(),g(q.$$.fragment),be=p(),g(B.$$.fragment),Te=p(),re=T("p"),rt=y(`We use the vanilla gradient descent optimizer, as was done in the original
    paper.`),Le=p(),g(G.$$.fragment),Ve=p(),le=T("p"),lt=y(`Using a LeNet-5 ConvNet allows us to achieve the best performance so far. We
    are close to 99% accuracy on the validation dataset. We could theoretically
    squeeze out a little more performance by for example utilizing data
    augmentation, but this is good enough for MNIST. We consider this task as
    solved and will focus on harder datasets in the next sections to demonstrate
    the usefulnes of more modern convolutional architectures.`),Ae=p(),g(j.$$.fragment),ze=p(),g(H.$$.fragment),ke=p(),ie=T("p"),it=y(`In the next couple of sections we are going to focus on the so called
    CIFAR-10 dataset. Torchvision provides the dataset out of the box, so let's
    get the data and prepare the dataloaders.`),De=p(),g(U.$$.fragment),Se=p(),g(Y.$$.fragment),Ce=p(),g(K.$$.fragment),Ne=p(),g(J.$$.fragment),Pe=p(),ce=T("p"),ct=y(`This dataset consists of 45,000 training images, 5,000 validation images and
    10,000 testing images. The images are of size 32x32 pixels and are colored,
    so unlike MNIST we are dealing with a 3-channel input.`),Ie=p(),fe=T("p"),ft=y(`Once again we are dealing with a classification problem with 10 distinct
    labels.`),Me=p(),g(Q.$$.fragment),Fe=p(),ue=T("p"),ut=y(`While the image size is fairly similar to MNIST, the task is significantly
    more complex than MNIST. Look at the images below. Unlike MNIST, CIFAR-10
    consits of real-life images. The dataset is much more diverse. Often you see
    the objects from different angles or distances. The objects often have
    different colors. Moreover the images contain different types of
    backgrounds. Getting a good accuracy is going to be a fairly challenging
    task.`),Re=p(),g(X.$$.fragment),We=p(),N=T("img"),Oe=p(),pe=T("p"),pt=y(`Let's use the LeCun-5 architecture, in order to create a simple baseline,
    that we should try to beat with more modern architectures in the next
    sections.`),qe=p(),P=T("p"),mt=y(`Our model is very similar, but not identical. First, the input channels for
    the very first convolutional layer were increased in order to account for
    the colored images. Second, we introduced the `),he=T("code"),dt=y("AdaptiveAvgPool2d"),ht=y(`
    layer with the output size of (1,1). This layer applies an average pooling, such
    that the width and height of the image are equal to a given size, in our case
    we reduce the image to just 1x1 pixel. We do that, because the parameters we
    have chosen below correspond to an 28x28 MNIST image and when we input a 32x32
    image, we are left with more than 120 parameters. This layer is often use to
    make one single architecture compatible with different image sizes.`),Be=p(),g(Z.$$.fragment),Ge=p(),I=T("p"),$t=y("Let's pretend for a second, that we did'n include the "),$e=T("code"),_t=y("AdaptiveAvgPool2d"),gt=y(" module. In that case the feature extractor produces an image of size 2x2."),je=p(),g(ee.$$.fragment),He=p(),g(te.$$.fragment),Ue=p(),M=T("p"),vt=y(`After flattening the image, we would end up with 120x2x2 features, while the
    linear layer expects exactly 120. The `),_e=T("code"),wt=y("avgpool"),Et=y(` on the other hand
    always reduces the image to a size of 1x1, no matter if the input is 2x2, 3x3
    or of any other dimension.`),Ye=p(),me=T("p"),xt=y(`As usual, we create our model, optimizer, scheduler and criterion and train
    the model.`),Ke=p(),g(ae.$$.fragment),Je=p(),g(oe.$$.fragment),Qe=p(),g(ne.$$.fragment),Xe=p(),F=T("p"),yt=y("After 30 epochs we reach an "),g(R.$$.fragment),bt=y(` This
    is the number we have to beat.`),this.h()},l(e){o=L(e,"P",{});var r=A(o);i=b(r,"LeNet-5"),v(t.$$.fragment,r),a=b(r,` is simultaneouly one of the
    oldest and the most well known convolutional neural network architecture. The
    name LeNet-5 is a reference to the inventor of the network, Yann LeCun, considered
    to be one of the grandfathers of deep learning. The network was designed for
    image recognition and was extensively used with the MNIST dataset. We can not
    expect this architecture to produce state of the art results, but this is a good
    exercise and a great starting point in our study of cnn architectures.`),r.forEach(s),u=m(e),n=L(e,"P",{});var ge=A(n);f=b(ge,`The table below depicts the architecure of the LeCun-5 network. First we
    apply 3 convolutional layers and 2 average pooling layers. After the third
    convolutional layer we flatten the feature maps and use two fully connected
    layers. We use the tanh activation function for all convolutional and fully
    connected layers, only the last fully connected layer uses the softmax
    activation function.`),ge.forEach(s),l=m(e),v(V.$$.fragment,e),S=m(e),C=L(e,"P",{});var ve=A(C);$=b(ve,`Below we implement LeNet-5 in PyTorch. There are no new pieces in the code
    below, all the piece swere already covered in previous sections.`),ve.forEach(s),k=m(e),v(D.$$.fragment,e),we=m(e),v(W.$$.fragment,e),Ee=m(e),se=L(e,"P",{});var zt=A(se);st=b(zt,"The model is an exact implementation of the table above."),zt.forEach(s),xe=m(e),v(O.$$.fragment,e),ye=m(e),v(q.$$.fragment,e),be=m(e),v(B.$$.fragment,e),Te=m(e),re=L(e,"P",{});var kt=A(re);rt=b(kt,`We use the vanilla gradient descent optimizer, as was done in the original
    paper.`),kt.forEach(s),Le=m(e),v(G.$$.fragment,e),Ve=m(e),le=L(e,"P",{});var Dt=A(le);lt=b(Dt,`Using a LeNet-5 ConvNet allows us to achieve the best performance so far. We
    are close to 99% accuracy on the validation dataset. We could theoretically
    squeeze out a little more performance by for example utilizing data
    augmentation, but this is good enough for MNIST. We consider this task as
    solved and will focus on harder datasets in the next sections to demonstrate
    the usefulnes of more modern convolutional architectures.`),Dt.forEach(s),Ae=m(e),v(j.$$.fragment,e),ze=m(e),v(H.$$.fragment,e),ke=m(e),ie=L(e,"P",{});var St=A(ie);it=b(St,`In the next couple of sections we are going to focus on the so called
    CIFAR-10 dataset. Torchvision provides the dataset out of the box, so let's
    get the data and prepare the dataloaders.`),St.forEach(s),De=m(e),v(U.$$.fragment,e),Se=m(e),v(Y.$$.fragment,e),Ce=m(e),v(K.$$.fragment,e),Ne=m(e),v(J.$$.fragment,e),Pe=m(e),ce=L(e,"P",{});var Ct=A(ce);ct=b(Ct,`This dataset consists of 45,000 training images, 5,000 validation images and
    10,000 testing images. The images are of size 32x32 pixels and are colored,
    so unlike MNIST we are dealing with a 3-channel input.`),Ct.forEach(s),Ie=m(e),fe=L(e,"P",{});var Nt=A(fe);ft=b(Nt,`Once again we are dealing with a classification problem with 10 distinct
    labels.`),Nt.forEach(s),Me=m(e),v(Q.$$.fragment,e),Fe=m(e),ue=L(e,"P",{});var Pt=A(ue);ut=b(Pt,`While the image size is fairly similar to MNIST, the task is significantly
    more complex than MNIST. Look at the images below. Unlike MNIST, CIFAR-10
    consits of real-life images. The dataset is much more diverse. Often you see
    the objects from different angles or distances. The objects often have
    different colors. Moreover the images contain different types of
    backgrounds. Getting a good accuracy is going to be a fairly challenging
    task.`),Pt.forEach(s),Re=m(e),v(X.$$.fragment,e),We=m(e),N=L(e,"IMG",{class:!0,src:!0,alt:!0}),Oe=m(e),pe=L(e,"P",{});var It=A(pe);pt=b(It,`Let's use the LeCun-5 architecture, in order to create a simple baseline,
    that we should try to beat with more modern architectures in the next
    sections.`),It.forEach(s),qe=m(e),P=L(e,"P",{});var et=A(P);mt=b(et,`Our model is very similar, but not identical. First, the input channels for
    the very first convolutional layer were increased in order to account for
    the colored images. Second, we introduced the `),he=L(et,"CODE",{});var Mt=A(he);dt=b(Mt,"AdaptiveAvgPool2d"),Mt.forEach(s),ht=b(et,`
    layer with the output size of (1,1). This layer applies an average pooling, such
    that the width and height of the image are equal to a given size, in our case
    we reduce the image to just 1x1 pixel. We do that, because the parameters we
    have chosen below correspond to an 28x28 MNIST image and when we input a 32x32
    image, we are left with more than 120 parameters. This layer is often use to
    make one single architecture compatible with different image sizes.`),et.forEach(s),Be=m(e),v(Z.$$.fragment,e),Ge=m(e),I=L(e,"P",{});var tt=A(I);$t=b(tt,"Let's pretend for a second, that we did'n include the "),$e=L(tt,"CODE",{});var Ft=A($e);_t=b(Ft,"AdaptiveAvgPool2d"),Ft.forEach(s),gt=b(tt," module. In that case the feature extractor produces an image of size 2x2."),tt.forEach(s),je=m(e),v(ee.$$.fragment,e),He=m(e),v(te.$$.fragment,e),Ue=m(e),M=L(e,"P",{});var at=A(M);vt=b(at,`After flattening the image, we would end up with 120x2x2 features, while the
    linear layer expects exactly 120. The `),_e=L(at,"CODE",{});var Rt=A(_e);wt=b(Rt,"avgpool"),Rt.forEach(s),Et=b(at,` on the other hand
    always reduces the image to a size of 1x1, no matter if the input is 2x2, 3x3
    or of any other dimension.`),at.forEach(s),Ye=m(e),me=L(e,"P",{});var Wt=A(me);xt=b(Wt,`As usual, we create our model, optimizer, scheduler and criterion and train
    the model.`),Wt.forEach(s),Ke=m(e),v(ae.$$.fragment,e),Je=m(e),v(oe.$$.fragment,e),Qe=m(e),v(ne.$$.fragment,e),Xe=m(e),F=L(e,"P",{});var ot=A(F);yt=b(ot,"After 30 epochs we reach an "),v(R.$$.fragment,ot),bt=b(ot,` This
    is the number we have to beat.`),ot.forEach(s),this.h()},h(){de(N,"class","mx-auto"),Zt(N.src,At=ca)||de(N,"src",At),de(N,"alt","A collection of cifar-10 images")},m(e,r){c(e,o,r),x(o,i),w(t,o,null),x(o,a),c(e,u,r),c(e,n,r),x(n,f),c(e,l,r),w(V,e,r),c(e,S,r),c(e,C,r),x(C,$),c(e,k,r),w(D,e,r),c(e,we,r),w(W,e,r),c(e,Ee,r),c(e,se,r),x(se,st),c(e,xe,r),w(O,e,r),c(e,ye,r),w(q,e,r),c(e,be,r),w(B,e,r),c(e,Te,r),c(e,re,r),x(re,rt),c(e,Le,r),w(G,e,r),c(e,Ve,r),c(e,le,r),x(le,lt),c(e,Ae,r),w(j,e,r),c(e,ze,r),w(H,e,r),c(e,ke,r),c(e,ie,r),x(ie,it),c(e,De,r),w(U,e,r),c(e,Se,r),w(Y,e,r),c(e,Ce,r),w(K,e,r),c(e,Ne,r),w(J,e,r),c(e,Pe,r),c(e,ce,r),x(ce,ct),c(e,Ie,r),c(e,fe,r),x(fe,ft),c(e,Me,r),w(Q,e,r),c(e,Fe,r),c(e,ue,r),x(ue,ut),c(e,Re,r),w(X,e,r),c(e,We,r),c(e,N,r),c(e,Oe,r),c(e,pe,r),x(pe,pt),c(e,qe,r),c(e,P,r),x(P,mt),x(P,he),x(he,dt),x(P,ht),c(e,Be,r),w(Z,e,r),c(e,Ge,r),c(e,I,r),x(I,$t),x(I,$e),x($e,_t),x(I,gt),c(e,je,r),w(ee,e,r),c(e,He,r),w(te,e,r),c(e,Ue,r),c(e,M,r),x(M,vt),x(M,_e),x(_e,wt),x(M,Et),c(e,Ye,r),c(e,me,r),x(me,xt),c(e,Ke,r),w(ae,e,r),c(e,Je,r),w(oe,e,r),c(e,Qe,r),w(ne,e,r),c(e,Xe,r),c(e,F,r),x(F,yt),w(R,F,null),x(F,bt),Ze=!0},p(e,r){const ge={};r&4096&&(ge.$$scope={dirty:r,ctx:e}),V.$set(ge);const ve={};r&4096&&(ve.$$scope={dirty:r,ctx:e}),R.$set(ve)},i(e){Ze||(d(t.$$.fragment,e),d(V.$$.fragment,e),d(D.$$.fragment,e),d(W.$$.fragment,e),d(O.$$.fragment,e),d(q.$$.fragment,e),d(B.$$.fragment,e),d(G.$$.fragment,e),d(j.$$.fragment,e),d(H.$$.fragment,e),d(U.$$.fragment,e),d(Y.$$.fragment,e),d(K.$$.fragment,e),d(J.$$.fragment,e),d(Q.$$.fragment,e),d(X.$$.fragment,e),d(Z.$$.fragment,e),d(ee.$$.fragment,e),d(te.$$.fragment,e),d(ae.$$.fragment,e),d(oe.$$.fragment,e),d(ne.$$.fragment,e),d(R.$$.fragment,e),Ze=!0)},o(e){_(t.$$.fragment,e),_(V.$$.fragment,e),_(D.$$.fragment,e),_(W.$$.fragment,e),_(O.$$.fragment,e),_(q.$$.fragment,e),_(B.$$.fragment,e),_(G.$$.fragment,e),_(j.$$.fragment,e),_(H.$$.fragment,e),_(U.$$.fragment,e),_(Y.$$.fragment,e),_(K.$$.fragment,e),_(J.$$.fragment,e),_(Q.$$.fragment,e),_(X.$$.fragment,e),_(Z.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(R.$$.fragment,e),Ze=!1},d(e){e&&s(o),E(t),e&&s(u),e&&s(n),e&&s(l),E(V,e),e&&s(S),e&&s(C),e&&s(k),E(D,e),e&&s(we),E(W,e),e&&s(Ee),e&&s(se),e&&s(xe),E(O,e),e&&s(ye),E(q,e),e&&s(be),E(B,e),e&&s(Te),e&&s(re),e&&s(Le),E(G,e),e&&s(Ve),e&&s(le),e&&s(Ae),E(j,e),e&&s(ze),E(H,e),e&&s(ke),e&&s(ie),e&&s(De),E(U,e),e&&s(Se),E(Y,e),e&&s(Ce),E(K,e),e&&s(Ne),E(J,e),e&&s(Pe),e&&s(ce),e&&s(Ie),e&&s(fe),e&&s(Me),E(Q,e),e&&s(Fe),e&&s(ue),e&&s(Re),E(X,e),e&&s(We),e&&s(N),e&&s(Oe),e&&s(pe),e&&s(qe),e&&s(P),e&&s(Be),E(Z,e),e&&s(Ge),e&&s(I),e&&s(je),E(ee,e),e&&s(He),E(te,e),e&&s(Ue),e&&s(M),e&&s(Ye),e&&s(me),e&&s(Ke),E(ae,e),e&&s(Je),E(oe,e),e&&s(Qe),E(ne,e),e&&s(Xe),e&&s(F),E(R)}}}function va(h){let o,i,t,a,u,n,f,l,V,S,C;return l=new ea({props:{$$slots:{default:[ga]},$$scope:{ctx:h}}}),S=new ta({props:{references:h[2]}}),{c(){o=T("meta"),i=p(),t=T("h1"),a=y("LeNet-5"),u=p(),n=T("div"),f=p(),g(l.$$.fragment),V=p(),g(S.$$.fragment),this.h()},l($){const k=Xt("svelte-1v5eo8n",document.head);o=L(k,"META",{name:!0,content:!0}),k.forEach(s),i=m($),t=L($,"H1",{});var D=A(t);a=b(D,"LeNet-5"),D.forEach(s),u=m($),n=L($,"DIV",{class:!0}),A(n).forEach(s),f=m($),v(l.$$.fragment,$),V=m($),v(S.$$.fragment,$),this.h()},h(){document.title="LeNet-5 - World4AI",de(o,"name","description"),de(o,"content","LeNet-5 is one of the oldest and at the same time one of most well known convolutional neural networks architectures. This architecture was developed by Yann LeCun in 1998 and is still used even to these days to learn the basics of convolutional neural networks."),de(n,"class","separator")},m($,k){x(document.head,o),c($,i,k),c($,t,k),x(t,a),c($,u,k),c($,n,k),c($,f,k),w(l,$,k),c($,V,k),w(S,$,k),C=!0},p($,[k]){const D={};k&4096&&(D.$$scope={dirty:k,ctx:$}),l.$set(D)},i($){C||(d(l.$$.fragment,$),d(S.$$.fragment,$),C=!0)},o($){_(l.$$.fragment,$),_(S.$$.fragment,$),C=!1},d($){s(o),$&&s(i),$&&s(t),$&&s(u),$&&s(n),$&&s(f),E(l,$),$&&s(V),E(S,$)}}}function wa(h){return[["Layer","Input Size","Kernel Size","Stride","Padding","Feature Maps","Output Size"],[["Conv","28x28x1","5x5","1","2","6","28x28x6"],["Tanh","-","-","-","-","-","-"],["Avg. Pooling","28x28x6","2x2","2","0","-","14x14x6"],["Conv","14x14x6","5x5","1","0",16,"10x10x16"],["Tanh","-","-","-","-","-","-"],["Avg. Pooling","10x10x16","2x2","2","0","-","5x5x16"],["Conv","5x5x6","5x5","1","0","120","1x1x120"],["Tanh","-","-","-","-","-","-"],["FC","120","-","-","-","-","84"],["Tanh","-","-","-","-","-","-"],["FC","84","-","-","-","-","10"],["Softmax","10","-","-","-","-","10"]],[{author:"Y. Lecun, L. Bottou, Y. Bengio and P. Haffner",title:"Gradient-based learning applied to document recognition",journal:"Proceedings of the IEEE",year:"1998",pages:"2278-2324",volume:"86",issue:"11"}]]}class Va extends Kt{constructor(o){super(),Jt(this,o,wa,va,Qt,{})}}export{Va as default};
