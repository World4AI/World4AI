import{S as ps,i as ms,s as hs,y as E,z as k,A as T,g as I,d as P,B as W,Q as O,q as m,R as L,m as j,h as i,r as h,n as w,b as o,N as y,u as ct,w as Pi,a9 as Wi,a as B,c as S,aa as Bi,k as A,l as F,e as ci,P as un,W as Fi,C as V}from"../chunks/index.4d92b023.js";import{C as zi}from"../chunks/Container.b0705c7b.js";import{L as z}from"../chunks/Latex.e0b308c0.js";import{F as Oi,I as fs}from"../chunks/InternalLink.7deb899c.js";import{S as Si}from"../chunks/SvgContainer.f70b5745.js";import{P as ji,T as qi}from"../chunks/Ticks.45eca5c5.js";import{C as Li}from"../chunks/Circle.f281e92b.js";import{X as Hi,Y as Ci}from"../chunks/YLabel.182e66a3.js";import{P as Di}from"../chunks/Path.7e6df014.js";import{T as mi}from"../chunks/Text.b1e2b624.js";import{S as Ai}from"../chunks/Slider.93409d64.js";import{l as gi}from"../chunks/linear.f56dae1f.js";import{H as At}from"../chunks/Highlight.b7c1de53.js";import{A as ue}from"../chunks/Alert.25a852b3.js";function bi(a){let t,n,s,f,c,u,p,$;return{c(){t=O("rect"),n=O("rect"),s=O("text"),f=m("Bits:"),c=O("text"),u=m(a[3]),p=O("text"),$=m(a[4]),this.h()},l(r){t=L(r,"rect",{class:!0,x:!0,y:!0,width:!0,height:!0}),j(t).forEach(i),n=L(r,"rect",{class:!0,x:!0,y:!0,width:!0,height:!0}),j(n).forEach(i),s=L(r,"text",{x:!0,y:!0,class:!0});var x=j(s);f=h(x,"Bits:"),x.forEach(i),c=L(r,"text",{x:!0,y:!0,class:!0});var q=j(c);u=h(q,a[3]),q.forEach(i),p=L(r,"text",{x:!0,y:!0,class:!0});var g=j(p);$=h(g,a[4]),g.forEach(i),this.h()},h(){w(t,"class","encoding svelte-17n82b1"),w(t,"x","20"),w(t,"y","50"),w(t,"width","20"),w(t,"height","10"),w(n,"class","encoding svelte-17n82b1"),w(n,"x","60"),w(n,"y","50"),w(n,"width","20"),w(n,"height","10"),w(s,"x","9"),w(s,"y","55.5"),w(s,"class","svelte-17n82b1"),w(c,"x","30"),w(c,"y","55.5"),w(c,"class","svelte-17n82b1"),w(p,"x","70"),w(p,"y","55.5"),w(p,"class","svelte-17n82b1")},m(r,x){o(r,t,x),o(r,n,x),o(r,s,x),y(s,f),o(r,c,x),y(c,u),o(r,p,x),y(p,$)},p(r,x){x&8&&ct(u,r[3]),x&16&&ct($,r[4])},d(r){r&&i(t),r&&i(n),r&&i(s),r&&i(c),r&&i(p)}}}function Mi(a){let t,n,s,f,c,u,p,$,r,x,q,g,v,_,H,d,b=a[2]&&bi(a);return{c(){t=O("svg"),n=O("circle"),s=O("circle"),f=O("text"),c=m("H"),u=O("text"),p=m("T"),$=O("rect"),r=O("rect"),x=O("text"),q=m("Pr:"),g=O("text"),v=m(a[0]),_=O("text"),H=m(a[1]),b&&b.c(),this.h()},l(D){t=L(D,"svg",{viewBox:!0});var C=j(t);n=L(C,"circle",{cx:!0,cy:!0,r:!0,class:!0}),j(n).forEach(i),s=L(C,"circle",{cx:!0,cy:!0,r:!0,class:!0}),j(s).forEach(i),f=L(C,"text",{x:!0,y:!0,class:!0});var X=j(f);c=h(X,"H"),X.forEach(i),u=L(C,"text",{x:!0,y:!0,class:!0});var re=j(u);p=h(re,"T"),re.forEach(i),$=L(C,"rect",{class:!0,x:!0,y:!0,width:!0,height:!0}),j($).forEach(i),r=L(C,"rect",{class:!0,x:!0,y:!0,width:!0,height:!0}),j(r).forEach(i),x=L(C,"text",{x:!0,y:!0,class:!0});var te=j(x);q=h(te,"Pr:"),te.forEach(i),g=L(C,"text",{x:!0,y:!0,class:!0});var K=j(g);v=h(K,a[0]),K.forEach(i),_=L(C,"text",{x:!0,y:!0,class:!0});var Xe=j(_);H=h(Xe,a[1]),Xe.forEach(i),b&&b.l(C),C.forEach(i),this.h()},h(){w(n,"cx","30"),w(n,"cy","15"),w(n,"r","10"),w(n,"class","svelte-17n82b1"),w(s,"cx","70"),w(s,"cy","15"),w(s,"r","10"),w(s,"class","svelte-17n82b1"),w(f,"x","30"),w(f,"y","15.5"),w(f,"class","svelte-17n82b1"),w(u,"x","70"),w(u,"y","15.5"),w(u,"class","svelte-17n82b1"),w($,"class","probability svelte-17n82b1"),w($,"x","20"),w($,"y","30"),w($,"width","20"),w($,"height","10"),w(r,"class","probability svelte-17n82b1"),w(r,"x","60"),w(r,"y","30"),w(r,"width","20"),w(r,"height","10"),w(x,"x","6"),w(x,"y","35.5"),w(x,"class","svelte-17n82b1"),w(g,"x","30"),w(g,"y","35.5"),w(g,"class","svelte-17n82b1"),w(_,"x","70"),w(_,"y","35.5"),w(_,"class","svelte-17n82b1"),w(t,"viewBox",d="0 0 100 "+a[5])},m(D,C){o(D,t,C),y(t,n),y(t,s),y(t,f),y(f,c),y(t,u),y(u,p),y(t,$),y(t,r),y(t,x),y(x,q),y(t,g),y(g,v),y(t,_),y(_,H),b&&b.m(t,null)},p(D,C){C&1&&ct(v,D[0]),C&2&&ct(H,D[1]),D[2]?b?b.p(D,C):(b=bi(D),b.c(),b.m(t,null)):b&&(b.d(1),b=null),C&32&&d!==(d="0 0 100 "+D[5])&&w(t,"viewBox",d)},d(D){D&&i(t),b&&b.d()}}}function Ni(a){let t,n;return t=new Si({props:{maxWidth:"300px",$$slots:{default:[Mi]},$$scope:{ctx:a}}}),{c(){E(t.$$.fragment)},l(s){k(t.$$.fragment,s)},m(s,f){T(t,s,f),n=!0},p(s,[f]){const c={};f&127&&(c.$$scope={dirty:f,ctx:s}),t.$set(c)},i(s){n||(I(t.$$.fragment,s),n=!0)},o(s){P(t.$$.fragment,s),n=!1},d(s){W(t,s)}}}function Vi(a,t,n){let{probHead:s=.5}=t,{probTail:f=.5}=t,{showBits:c=!1}=t,u;c===!0?u=70:u=45;let{bitsHeads:p="1 0"}=t,{bitsTails:$="1 0 1"}=t;return a.$$set=r=>{"probHead"in r&&n(0,s=r.probHead),"probTail"in r&&n(1,f=r.probTail),"showBits"in r&&n(2,c=r.showBits),"bitsHeads"in r&&n(3,p=r.bitsHeads),"bitsTails"in r&&n(4,$=r.bitsTails)},[s,f,c,p,$,u]}class $s extends ps{constructor(t){super(),ms(this,t,Vi,Ni,hs,{probHead:0,probTail:1,showBits:2,bitsHeads:3,bitsTails:4})}}function Yi(a){let t,n,s,f,c,u,p,$,r,x,q,g,v,_,H,d;return t=new qi({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],xOffset:-15,yOffset:15}}),s=new Hi({props:{text:"Probability",fontSize:15}}),c=new Ci({props:{text:"Entropy",fontSize:15}}),p=new Di({props:{data:a[3]}}),r=new Li({props:{data:a[2]}}),q=new mi({props:{text:"Probability Heads: "+a[0].toFixed(3),x:"0.2",y:"0.6"}}),v=new mi({props:{text:"Probability Tails: "+(1-a[0]).toFixed(3),x:"0.2",y:"0.5"}}),H=new mi({props:{text:"Entropy: "+a[1].toFixed(5),x:"0.2",y:"0.4"}}),{c(){E(t.$$.fragment),n=B(),E(s.$$.fragment),f=B(),E(c.$$.fragment),u=B(),E(p.$$.fragment),$=B(),E(r.$$.fragment),x=B(),E(q.$$.fragment),g=B(),E(v.$$.fragment),_=B(),E(H.$$.fragment)},l(b){k(t.$$.fragment,b),n=S(b),k(s.$$.fragment,b),f=S(b),k(c.$$.fragment,b),u=S(b),k(p.$$.fragment,b),$=S(b),k(r.$$.fragment,b),x=S(b),k(q.$$.fragment,b),g=S(b),k(v.$$.fragment,b),_=S(b),k(H.$$.fragment,b)},m(b,D){T(t,b,D),o(b,n,D),T(s,b,D),o(b,f,D),T(c,b,D),o(b,u,D),T(p,b,D),o(b,$,D),T(r,b,D),o(b,x,D),T(q,b,D),o(b,g,D),T(v,b,D),o(b,_,D),T(H,b,D),d=!0},p(b,D){const C={};D&4&&(C.data=b[2]),r.$set(C);const X={};D&1&&(X.text="Probability Heads: "+b[0].toFixed(3)),q.$set(X);const re={};D&1&&(re.text="Probability Tails: "+(1-b[0]).toFixed(3)),v.$set(re);const te={};D&2&&(te.text="Entropy: "+b[1].toFixed(5)),H.$set(te)},i(b){d||(I(t.$$.fragment,b),I(s.$$.fragment,b),I(c.$$.fragment,b),I(p.$$.fragment,b),I(r.$$.fragment,b),I(q.$$.fragment,b),I(v.$$.fragment,b),I(H.$$.fragment,b),d=!0)},o(b){P(t.$$.fragment,b),P(s.$$.fragment,b),P(c.$$.fragment,b),P(p.$$.fragment,b),P(r.$$.fragment,b),P(q.$$.fragment,b),P(v.$$.fragment,b),P(H.$$.fragment,b),d=!1},d(b){W(t,b),b&&i(n),W(s,b),b&&i(f),W(c,b),b&&i(u),W(p,b),b&&i($),W(r,b),b&&i(x),W(q,b),b&&i(g),W(v,b),b&&i(_),W(H,b)}}}function Ji(a){let t,n,s,f,c;t=new ji({props:{width:500,height:250,maxWidth:800,domain:[0,1],range:[0,1],$$slots:{default:[Yi]},$$scope:{ctx:a}}});function u($){a[4]($)}let p={min:0,max:1,step:.001};return a[0]!==void 0&&(p.value=a[0]),s=new Ai({props:p}),Pi.push(()=>Wi(s,"value",u)),{c(){E(t.$$.fragment),n=B(),E(s.$$.fragment)},l($){k(t.$$.fragment,$),n=S($),k(s.$$.fragment,$)},m($,r){T(t,$,r),o($,n,r),T(s,$,r),c=!0},p($,[r]){const x={};r&39&&(x.$$scope={dirty:r,ctx:$}),t.$set(x);const q={};!f&&r&1&&(f=!0,q.value=$[0],Bi(()=>f=!1)),s.$set(q)},i($){c||(I(t.$$.fragment,$),I(s.$$.fragment,$),c=!0)},o($){P(t.$$.fragment,$),P(s.$$.fragment,$),c=!1},d($){W(t,$),$&&i(n),W(s,$)}}}function Ri(a,t,n){let s,{p:f=.5}=t,c=0,u=[];for(let $=.001;$<1;$+=.001){let r=$,x=-r*Math.log2(r)-(1-r)*Math.log2(1-r),q={x:r,y:x};u.push(q)}function p($){f=$,n(0,f)}return a.$$set=$=>{"p"in $&&n(0,f=$.p)},a.$$.update=()=>{a.$$.dirty&1&&(f===0||f===1?n(1,c=0):n(1,c=-f*Math.log2(f)-(1-f)*Math.log2(1-f))),a.$$.dirty&3&&n(2,s=[{x:f,y:c}])},[f,c,s,u,p]}class Gi extends ps{constructor(t){super(),ms(this,t,Ri,Ji,hs,{p:0})}}function _i(a,t,n){const s=a.slice();return s[15]=t[n],s[17]=n,s}function wi(a,t,n){const s=a.slice();return s[15]=t[n],s[17]=n,s}function di(a,t,n){const s=a.slice();return s[15]=t[n],s[17]=n,s}function vi(a,t,n){const s=a.slice();return s[20]=t[n],s}function yi(a){let t,n,s,f=a[20]+"",c,u,p;return{c(){t=O("g"),n=O("line"),s=O("text"),c=m(f),this.h()},l($){t=L($,"g",{class:!0,transform:!0});var r=j(t);n=L(r,"line",{x2:!0,class:!0}),j(n).forEach(i),s=L(r,"text",{y:!0,class:!0});var x=j(s);c=h(x,f),x.forEach(i),r.forEach(i),this.h()},h(){w(n,"x2","100%"),w(n,"class","svelte-1jxxma6"),w(s,"y","-4"),w(s,"class","svelte-1jxxma6"),w(t,"class",u="tick tick-"+a[20]+" svelte-1jxxma6"),w(t,"transform",p="translate(0, "+a[7](a[20])+")")},m($,r){o($,t,r),y(t,n),y(t,s),y(s,c)},p($,r){r&4&&f!==(f=$[20]+"")&&ct(c,f),r&4&&u!==(u="tick tick-"+$[20]+" svelte-1jxxma6")&&w(t,"class",u),r&132&&p!==(p="translate(0, "+$[7]($[20])+")")&&w(t,"transform",p)},d($){$&&i(t)}}}function xi(a){let t,n,s=a[15].event+"",f,c,u;return{c(){t=O("g"),n=O("text"),f=m(s),this.h()},l(p){t=L(p,"g",{class:!0,transform:!0});var $=j(t);n=L($,"text",{x:!0,y:!0,class:!0});var r=j(n);f=h(r,s),r.forEach(i),$.forEach(i),this.h()},h(){w(n,"x",c=a[6]/2+20),w(n,"y","-4"),w(n,"class","svelte-1jxxma6"),w(t,"class","tick svelte-1jxxma6"),w(t,"transform",u="translate("+a[8](a[17])+","+us+")")},m(p,$){o(p,t,$),y(t,n),y(n,f)},p(p,$){$&2&&s!==(s=p[15].event+"")&&ct(f,s),$&64&&c!==(c=p[6]/2+20)&&w(n,"x",c),$&256&&u!==(u="translate("+p[8](p[17])+","+us+")")&&w(t,"transform",u)},d(p){p&&i(t)}}}function Ei(a){let t,n,s,f,c;return{c(){t=O("rect"),this.h()},l(u){t=L(u,"rect",{class:!0,x:!0,y:!0,width:!0,height:!0}),j(t).forEach(i),this.h()},h(){w(t,"class","rect-1 svelte-1jxxma6"),w(t,"x",n=a[8](a[17])),w(t,"y",s=a[7](a[15].percentage)),w(t,"width",f=a[6]-4),w(t,"height",c=a[7](0)-a[7](a[15].percentage))},m(u,p){o(u,t,p)},p(u,p){p&256&&n!==(n=u[8](u[17]))&&w(t,"x",n),p&130&&s!==(s=u[7](u[15].percentage))&&w(t,"y",s),p&64&&f!==(f=u[6]-4)&&w(t,"width",f),p&130&&c!==(c=u[7](0)-u[7](u[15].percentage))&&w(t,"height",c)},d(u){u&&i(t)}}}function ki(a){let t,n,s,f,c;return{c(){t=O("rect"),this.h()},l(u){t=L(u,"rect",{class:!0,x:!0,y:!0,width:!0,height:!0}),j(t).forEach(i),this.h()},h(){w(t,"class","rect-2 svelte-1jxxma6"),w(t,"x",n=a[8](a[17])+a[6]),w(t,"y",s=a[7](a[15].percentage)),w(t,"width",f=a[6]-4),w(t,"height",c=a[7](0)-a[7](a[15].percentage))},m(u,p){o(u,t,p)},p(u,p){p&320&&n!==(n=u[8](u[17])+u[6])&&w(t,"x",n),p&144&&s!==(s=u[7](u[15].percentage))&&w(t,"y",s),p&64&&f!==(f=u[6]-4)&&w(t,"width",f),p&144&&c!==(c=u[7](0)-u[7](u[15].percentage))&&w(t,"height",c)},d(u){u&&i(t)}}}function Ui(a){let t,n,s,f,c,u=a[2],p=[];for(let _=0;_<u.length;_+=1)p[_]=yi(vi(a,u,_));let $=a[1],r=[];for(let _=0;_<$.length;_+=1)r[_]=xi(di(a,$,_));let x=a[1],q=[];for(let _=0;_<x.length;_+=1)q[_]=Ei(wi(a,x,_));let g=a[4],v=[];for(let _=0;_<g.length;_+=1)v[_]=ki(_i(a,g,_));return{c(){t=O("svg"),n=O("g");for(let _=0;_<p.length;_+=1)p[_].c();s=O("g");for(let _=0;_<r.length;_+=1)r[_].c();f=O("g");for(let _=0;_<q.length;_+=1)q[_].c();c=ci();for(let _=0;_<v.length;_+=1)v[_].c();this.h()},l(_){t=L(_,"svg",{viewBox:!0});var H=j(t);n=L(H,"g",{class:!0});var d=j(n);for(let C=0;C<p.length;C+=1)p[C].l(d);d.forEach(i),s=L(H,"g",{class:!0});var b=j(s);for(let C=0;C<r.length;C+=1)r[C].l(b);b.forEach(i),f=L(H,"g",{class:!0});var D=j(f);for(let C=0;C<q.length;C+=1)q[C].l(D);c=ci();for(let C=0;C<v.length;C+=1)v[C].l(D);D.forEach(i),H.forEach(i),this.h()},h(){w(n,"class","axis y-axis"),w(s,"class","axis x-axis svelte-1jxxma6"),w(f,"class","bars svelte-1jxxma6"),w(t,"viewBox","0 0 "+a[9]+" "+us)},m(_,H){o(_,t,H),y(t,n);for(let d=0;d<p.length;d+=1)p[d]&&p[d].m(n,null);y(t,s);for(let d=0;d<r.length;d+=1)r[d]&&r[d].m(s,null);y(t,f);for(let d=0;d<q.length;d+=1)q[d]&&q[d].m(f,null);y(f,c);for(let d=0;d<v.length;d+=1)v[d]&&v[d].m(f,null)},p(_,H){if(H&132){u=_[2];let d;for(d=0;d<u.length;d+=1){const b=vi(_,u,d);p[d]?p[d].p(b,H):(p[d]=yi(b),p[d].c(),p[d].m(n,null))}for(;d<p.length;d+=1)p[d].d(1);p.length=u.length}if(H&322){$=_[1];let d;for(d=0;d<$.length;d+=1){const b=di(_,$,d);r[d]?r[d].p(b,H):(r[d]=xi(b),r[d].c(),r[d].m(s,null))}for(;d<r.length;d+=1)r[d].d(1);r.length=$.length}if(H&450){x=_[1];let d;for(d=0;d<x.length;d+=1){const b=wi(_,x,d);q[d]?q[d].p(b,H):(q[d]=Ei(b),q[d].c(),q[d].m(f,c))}for(;d<q.length;d+=1)q[d].d(1);q.length=x.length}if(H&464){g=_[4];let d;for(d=0;d<g.length;d+=1){const b=_i(_,g,d);v[d]?v[d].p(b,H):(v[d]=ki(b),v[d].c(),v[d].m(f,null))}for(;d<v.length;d+=1)v[d].d(1);v.length=g.length}},d(_){_&&i(t),un(p,_),un(r,_),un(q,_),un(v,_)}}}function Xi(a){let t,n,s,f,c,u=a[5].toFixed(2)+"",p,$,r,x,q;t=new Si({props:{maxWidth:a[0],$$slots:{default:[Ui]},$$scope:{ctx:a}}});function g(_){a[12](_)}let v={labelId:"crossentropy",min:"0",max:"1",step:"0.01"};return a[3]!==void 0&&(v.value=a[3]),r=new Ai({props:v}),Pi.push(()=>Wi(r,"value",g)),{c(){E(t.$$.fragment),n=B(),s=A("div"),f=m("CrossEntropy: "),c=A("span"),p=m(u),$=B(),E(r.$$.fragment),this.h()},l(_){k(t.$$.fragment,_),n=S(_),s=F(_,"DIV",{});var H=j(s);f=h(H,"CrossEntropy: "),c=F(H,"SPAN",{class:!0});var d=j(c);p=h(d,u),d.forEach(i),H.forEach(i),$=S(_),k(r.$$.fragment,_),this.h()},h(){w(c,"class","inline-block rounded-lg px-2 bg-red-200")},m(_,H){T(t,_,H),o(_,n,H),o(_,s,H),y(s,f),y(s,c),y(c,p),o(_,$,H),T(r,_,H),q=!0},p(_,[H]){const d={};H&1&&(d.maxWidth=_[0]),H&8389078&&(d.$$scope={dirty:H,ctx:_}),t.$set(d),(!q||H&32)&&u!==(u=_[5].toFixed(2)+"")&&ct(p,u);const b={};!x&&H&8&&(x=!0,b.value=_[3],Bi(()=>x=!1)),r.$set(b)},i(_){q||(I(t.$$.fragment,_),I(r.$$.fragment,_),q=!0)},o(_){P(t.$$.fragment,_),P(r.$$.fragment,_),q=!1},d(_){W(t,_),_&&i(n),_&&i(s),_&&i($),W(r,_)}}}let us=200;function Qi(a,t,n){let s,f,c,u,{maxWidth:p="800px"}=t,{points1:$=[{event:"x1",percentage:.2},{event:"x2",percentage:.2},{event:"x3",percentage:.2},{event:"x4",percentage:.2},{event:"x5",percentage:.2}]}=t,{startingPoints:r=[{event:"x1",percentage:.4},{event:"x2",percentage:.4},{event:"x3",percentage:.05},{event:"x4",percentage:.05},{event:"x5",percentage:.1}]}=t,{yTicks:x=[0,.1,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]}=t,q,g,v=0;function _(D){n(4,q=JSON.parse(JSON.stringify(r)));for(let C=0;C<$.length;C++)n(4,q[C].percentage=r[C].percentage+($[C].percentage-r[C].percentage)*D,q);n(5,g=0);for(let C=0;C<$.length;C++)q[C].percentage!=0&&n(5,g+=-$[C].percentage*Math.log2(q[C].percentage))}const H={top:20,right:0,bottom:20,left:35};let d=$.length*100;function b(D){v=D,n(3,v)}return a.$$set=D=>{"maxWidth"in D&&n(0,p=D.maxWidth),"points1"in D&&n(1,$=D.points1),"startingPoints"in D&&n(10,r=D.startingPoints),"yTicks"in D&&n(2,x=D.yTicks)},a.$$.update=()=>{a.$$.dirty&8&&_(v),a.$$.dirty&2&&n(8,s=gi().domain([0,$.length]).range([H.left,d-H.right])),a.$$.dirty&4&&n(7,f=gi().domain([0,Math.max.apply(null,x)]).range([us-H.bottom,H.top])),a.$$.dirty&2050&&n(6,u=c/$.length/2.2)},n(11,c=d-(H.left+H.right)),[p,$,x,v,q,g,u,f,s,d,r,c,b]}class hi extends ps{constructor(t){super(),ms(this,t,Qi,Xi,hs,{maxWidth:0,points1:1,startingPoints:10,yTicks:2})}}function Ti(a,t,n){const s=a.slice();return s[9]=t[n],s}function Ki(a){let t;return{c(){t=m("cross-entropy")},l(n){t=h(n,"cross-entropy")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Zi(a){let t;return{c(){t=m("information theory")},l(n){t=h(n,"information theory")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ea(a){let t;return{c(){t=m(`In order to understand the cross-entropy loss it is essential to understand
    information theory!`)},l(n){t=h(n,`In order to understand the cross-entropy loss it is essential to understand
    information theory!`)},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ta(a){let t;return{c(){t=m("bit")},l(n){t=h(n,"bit")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Ii(a){let t,n=a[9]+"",s,f;return{c(){t=A("div"),s=m(n),f=B(),this.h()},l(c){t=F(c,"DIV",{class:!0});var u=j(t);s=h(u,n),f=S(u),u.forEach(i),this.h()},h(){w(t,"class","border border-black py-1 px-2 md:px-3 bg-blue-100")},m(c,u){o(c,t,u),y(t,s),y(t,f)},p:V,d(c){c&&i(t)}}}function na(a){let t;return{c(){t=m("Information is inversely related to probability.")},l(n){t=h(n,"Information is inversely related to probability.")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function sa(a){let t;return{c(){t=m("p")},l(n){t=h(n,"p")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ia(a){let t;return{c(){t=m("x")},l(n){t=h(n,"x")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function aa(a){let t;return{c(){t=m("I")},l(n){t=h(n,"I")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ra(a){let t=String.raw`\Big(\dfrac{1}{2}\Big)^I = p(x)`+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function la(a){let t,n,s,f,c,u,p,$,r,x,q;return n=new z({props:{$$slots:{default:[sa]},$$scope:{ctx:a}}}),f=new z({props:{$$slots:{default:[ia]},$$scope:{ctx:a}}}),u=new z({props:{$$slots:{default:[aa]},$$scope:{ctx:a}}}),r=new z({props:{$$slots:{default:[ra]},$$scope:{ctx:a}}}),{c(){t=m("We can convert probability "),E(n.$$.fragment),s=m(" of an event "),E(f.$$.fragment),c=m(` into
    bits of information `),E(u.$$.fragment),p=m(` using the following equation.
    `),$=A("div"),E(r.$$.fragment),x=m(`
    If the probability is 50%, the information content is exactly 1 bit. If the probability
    of an event is 25%, the uncertainty is divided by 4 when this event occurs and
    the information content is 2 bits.`),this.h()},l(g){t=h(g,"We can convert probability "),k(n.$$.fragment,g),s=h(g," of an event "),k(f.$$.fragment,g),c=h(g,` into
    bits of information `),k(u.$$.fragment,g),p=h(g,` using the following equation.
    `),$=F(g,"DIV",{class:!0});var v=j($);k(r.$$.fragment,v),v.forEach(i),x=h(g,`
    If the probability is 50%, the information content is exactly 1 bit. If the probability
    of an event is 25%, the uncertainty is divided by 4 when this event occurs and
    the information content is 2 bits.`),this.h()},h(){w($,"class","flex justify-center")},m(g,v){o(g,t,v),T(n,g,v),o(g,s,v),T(f,g,v),o(g,c,v),T(u,g,v),o(g,p,v),o(g,$,v),T(r,$,null),o(g,x,v),q=!0},p(g,v){const _={};v&4096&&(_.$$scope={dirty:v,ctx:g}),n.$set(_);const H={};v&4096&&(H.$$scope={dirty:v,ctx:g}),f.$set(H);const d={};v&4096&&(d.$$scope={dirty:v,ctx:g}),u.$set(d);const b={};v&4096&&(b.$$scope={dirty:v,ctx:g}),r.$set(b)},i(g){q||(I(n.$$.fragment,g),I(f.$$.fragment,g),I(u.$$.fragment,g),I(r.$$.fragment,g),q=!0)},o(g){P(n.$$.fragment,g),P(f.$$.fragment,g),P(u.$$.fragment,g),P(r.$$.fragment,g),q=!1},d(g){g&&i(t),W(n,g),g&&i(s),W(f,g),g&&i(c),W(u,g),g&&i(p),g&&i($),W(r),g&&i(x)}}}function oa(a){let t=String.raw`\Big(\dfrac{1}{2}\Big)^I = p(x)`+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function fa(a){let t;return{c(){t=m("I")},l(n){t=h(n,"I")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function $a(a){let t=String.raw`
    \begin{aligned}
      &\Big(\frac{1}{2}\Big)^I = p(x) \\
      & 2^I = \frac{1}{p(x)} \\
      & I = \log_2\Big(\frac{1}{p(x)}\Big) \\
      & I = \log_2(1) - \log_2(p(x)) \\
      & I = 0 - \log_2(p(x)) \\
    \end{aligned} \\
    \boxed{I= -\log_2(p(x))}
  `+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function ua(a){let t,n;return t=new z({props:{$$slots:{default:[$a]},$$scope:{ctx:a}}}),{c(){E(t.$$.fragment)},l(s){k(t.$$.fragment,s)},m(s,f){T(t,s,f),n=!0},p(s,f){const c={};f&4096&&(c.$$scope={dirty:f,ctx:s}),t.$set(c)},i(s){n||(I(t.$$.fragment,s),n=!0)},o(s){P(t.$$.fragment,s),n=!1},d(s){W(t,s)}}}function pa(a){let t;return{c(){t=m("p")},l(n){t=h(n,"p")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ma(a){let t;return{c(){t=m("x")},l(n){t=h(n,"x")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ha(a){let t;return{c(){t=m("-\\log_2(p(x))")},l(n){t=h(n,"-\\log_2(p(x))")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ca(a){let t,n,s,f,c,u,p,$;return t=new qi({props:{xTicks:[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1],yTicks:[0,1,2,3,4,5,6,7,8,9,10],xOffset:-15,yOffset:15}}),s=new Hi({props:{text:"Probability",fontSize:15}}),c=new Ci({props:{text:"Number Of Bits",fontSize:15}}),p=new Di({props:{data:a[3]}}),{c(){E(t.$$.fragment),n=B(),E(s.$$.fragment),f=B(),E(c.$$.fragment),u=B(),E(p.$$.fragment)},l(r){k(t.$$.fragment,r),n=S(r),k(s.$$.fragment,r),f=S(r),k(c.$$.fragment,r),u=S(r),k(p.$$.fragment,r)},m(r,x){T(t,r,x),o(r,n,x),T(s,r,x),o(r,f,x),T(c,r,x),o(r,u,x),T(p,r,x),$=!0},p:V,i(r){$||(I(t.$$.fragment,r),I(s.$$.fragment,r),I(c.$$.fragment,r),I(p.$$.fragment,r),$=!0)},o(r){P(t.$$.fragment,r),P(s.$$.fragment,r),P(c.$$.fragment,r),P(p.$$.fragment,r),$=!1},d(r){W(t,r),r&&i(n),W(s,r),r&&i(f),W(c,r),r&&i(u),W(p,r)}}}function ga(a){let t;return{c(){t=m("The lower the probability of an event, the higher the information.")},l(n){t=h(n,"The lower the probability of an event, the higher the information.")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ba(a){let t;return{c(){t=m("p")},l(n){t=h(n,"p")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function _a(a){let t;return{c(){t=m("entropy")},l(n){t=h(n,"entropy")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function wa(a){let t;return{c(){t=m("H(p)")},l(n){t=h(n,"H(p)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function da(a){let t;return{c(){t=m("p")},l(n){t=h(n,"p")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function va(a){let t=String.raw`
    H(p) = -\sum_x p(x) * log_2(p(x))
  `+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function ya(a){let t,n,s,f,c,u,p,$;return n=new z({props:{$$slots:{default:[wa]},$$scope:{ctx:a}}}),f=new z({props:{$$slots:{default:[da]},$$scope:{ctx:a}}}),p=new z({props:{$$slots:{default:[va]},$$scope:{ctx:a}}}),{c(){t=m("The entropy "),E(n.$$.fragment),s=m(" of the probability distrubution "),E(f.$$.fragment),c=m(` is defined as the expected level of information or the expected number of bits.
    `),u=A("div"),E(p.$$.fragment),this.h()},l(r){t=h(r,"The entropy "),k(n.$$.fragment,r),s=h(r," of the probability distrubution "),k(f.$$.fragment,r),c=h(r,` is defined as the expected level of information or the expected number of bits.
    `),u=F(r,"DIV",{class:!0});var x=j(u);k(p.$$.fragment,x),x.forEach(i),this.h()},h(){w(u,"class","flex justify-center mt-1")},m(r,x){o(r,t,x),T(n,r,x),o(r,s,x),T(f,r,x),o(r,c,x),o(r,u,x),T(p,u,null),$=!0},p(r,x){const q={};x&4096&&(q.$$scope={dirty:x,ctx:r}),n.$set(q);const g={};x&4096&&(g.$$scope={dirty:x,ctx:r}),f.$set(g);const v={};x&4096&&(v.$$scope={dirty:x,ctx:r}),p.$set(v)},i(r){$||(I(n.$$.fragment,r),I(f.$$.fragment,r),I(p.$$.fragment,r),$=!0)},o(r){P(n.$$.fragment,r),P(f.$$.fragment,r),P(p.$$.fragment,r),$=!1},d(r){r&&i(t),W(n,r),r&&i(s),W(f,r),r&&i(c),r&&i(u),W(p)}}}function xa(a){let t,n=-(.4*Math.log2(.4)+.6*Math.log2(.6)).toFixed(2)+"",s;return{c(){t=m("-(0.4*\\log_2(0.4) + 0.6*\\log_2(0.6)) = "),s=m(n)},l(f){t=h(f,"-(0.4*\\log_2(0.4) + 0.6*\\log_2(0.6)) = "),s=h(f,n)},m(f,c){o(f,t,c),o(f,s,c)},p:V,d(f){f&&i(t),f&&i(s)}}}function Ea(a){let t;return{c(){t=m("cross-entropy")},l(n){t=h(n,"cross-entropy")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ka(a){let t,n=2 .toFixed(2)+"",s;return{c(){t=m("0.4*2 + 0.6*2 = "),s=m(n)},l(f){t=h(f,"0.4*2 + 0.6*2 = "),s=h(f,n)},m(f,c){o(f,t,c),o(f,s,c)},p:V,d(f){f&&i(t),f&&i(s)}}}function Ta(a){let t;return{c(){t=m("p(x)")},l(n){t=h(n,"p(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Ia(a){let t;return{c(){t=m("q(x)")},l(n){t=h(n,"q(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Pa(a){let t;return{c(){t=m("H(p, q)")},l(n){t=h(n,"H(p, q)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Wa(a){let t=String.raw`
    H(p, q) = - \mathbb{E}_p[\log q(x)] = - \sum_x p(x) \log q(x)
  `+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function Ba(a){let t,n,s,f,c,u,p,$,r,x,q;return n=new z({props:{$$slots:{default:[Ta]},$$scope:{ctx:a}}}),f=new z({props:{$$slots:{default:[Ia]},$$scope:{ctx:a}}}),u=new z({props:{$$slots:{default:[Pa]},$$scope:{ctx:a}}}),r=new z({props:{$$slots:{default:[Wa]},$$scope:{ctx:a}}}),{c(){t=m(`The cross-entropy is defined as the average message length. Given two
    distributions `),E(n.$$.fragment),s=m(" and "),E(f.$$.fragment),c=m(` we can calculate the
    cross-entropy `),E(u.$$.fragment),p=m(`.
    `),$=A("div"),E(r.$$.fragment),x=m("."),this.h()},l(g){t=h(g,`The cross-entropy is defined as the average message length. Given two
    distributions `),k(n.$$.fragment,g),s=h(g," and "),k(f.$$.fragment,g),c=h(g,` we can calculate the
    cross-entropy `),k(u.$$.fragment,g),p=h(g,`.
    `),$=F(g,"DIV",{class:!0});var v=j($);k(r.$$.fragment,v),x=h(v,"."),v.forEach(i),this.h()},h(){w($,"class","flex justify-center mt-1")},m(g,v){o(g,t,v),T(n,g,v),o(g,s,v),T(f,g,v),o(g,c,v),T(u,g,v),o(g,p,v),o(g,$,v),T(r,$,null),y($,x),q=!0},p(g,v){const _={};v&4096&&(_.$$scope={dirty:v,ctx:g}),n.$set(_);const H={};v&4096&&(H.$$scope={dirty:v,ctx:g}),f.$set(H);const d={};v&4096&&(d.$$scope={dirty:v,ctx:g}),u.$set(d);const b={};v&4096&&(b.$$scope={dirty:v,ctx:g}),r.$set(b)},i(g){q||(I(n.$$.fragment,g),I(f.$$.fragment,g),I(u.$$.fragment,g),I(r.$$.fragment,g),q=!0)},o(g){P(n.$$.fragment,g),P(f.$$.fragment,g),P(u.$$.fragment,g),P(r.$$.fragment,g),q=!1},d(g){g&&i(t),W(n,g),g&&i(s),W(f,g),g&&i(c),W(u,g),g&&i(p),g&&i($),W(r)}}}function Sa(a){let t;return{c(){t=m("p(x)")},l(n){t=h(n,"p(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function ja(a){let t;return{c(){t=m("q(x)")},l(n){t=h(n,"q(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function qa(a){let t;return{c(){t=m("q(x)")},l(n){t=h(n,"q(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Ha(a){let t;return{c(){t=m("p(x)")},l(n){t=h(n,"p(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Ca(a){let t;return{c(){t=m("p(x)")},l(n){t=h(n,"p(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Da(a){let t=String.raw`
  \text{cat} = 
  \begin{bmatrix}
  1 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  \end{bmatrix}
  \text{dog} = 
  \begin{bmatrix}
  0 \\ 
  1 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  \end{bmatrix}
  \text{pig} = 
  \begin{bmatrix}
  0 \\ 
  0 \\ 
  1 \\ 
  0 \\ 
  0 \\ 
  \end{bmatrix}
  \text{bear} = 
  \begin{bmatrix}
  0 \\ 
  0 \\ 
  0 \\ 
  1 \\ 
  0 \\ 
  \end{bmatrix}
  \text{monkey} = 
  \begin{bmatrix}
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  1 \\ 
  \end{bmatrix}
    `+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function Aa(a){let t;return{c(){t=m("q(x)")},l(n){t=h(n,"q(x)")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Fa(a){let t=String.raw`

  \begin{bmatrix}
  0.05 \\ 
  0.4 \\ 
  0.05 \\ 
  0.4 \\ 
  0.1 \\ 
  \end{bmatrix}
    `+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function za(a){let t=String.raw`\hat{y} = \dfrac{1}{1 + e^{-(\mathbf{w^Tx}+b)}}`+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function Oa(a){let t=String.raw`\hat{y}`+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function La(a){let t=String.raw`1 - \hat{y}`+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function Ma(a){let t;return{c(){t=m("y")},l(n){t=h(n,"y")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Na(a){let t;return{c(){t=m("binary cross-entropy")},l(n){t=h(n,"binary cross-entropy")},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Va(a){let t=String.raw`
    H(p, q) = -\Big[y \log (\hat{y}) + (1 - y) \log ( 1 - \hat{y})\Big]
  `+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function Ya(a){let t;return{c(){t=m(`When we are dealing with a classification problem we use the cross-entropy
    as the loss function. We use the binary cross-entropy when we have just 2
    categories.`)},l(n){t=h(n,`When we are dealing with a classification problem we use the cross-entropy
    as the loss function. We use the binary cross-entropy when we have just 2
    categories.`)},m(n,s){o(n,t,s)},d(n){n&&i(t)}}}function Ja(a){let t=String.raw`
    H(p, q) = -\dfrac{1}{n}\sum_i y^{(i)} \log (\hat{y}^{(i)}) + (1 - y^{(i)}) \log ( 1 - \hat{y}^{(i)})
  `+"",n;return{c(){n=m(t)},l(s){n=h(s,t)},m(s,f){o(s,n,f)},p:V,d(s){s&&i(n)}}}function Ra(a){let t,n;return t=new z({props:{$$slots:{default:[Ja]},$$scope:{ctx:a}}}),{c(){E(t.$$.fragment)},l(s){k(t.$$.fragment,s)},m(s,f){T(t,s,f),n=!0},p(s,f){const c={};f&4096&&(c.$$scope={dirty:f,ctx:s}),t.$set(c)},i(s){n||(I(t.$$.fragment,s),n=!0)},o(s){P(t.$$.fragment,s),n=!1},d(s){W(t,s)}}}function Ga(a){let t,n,s,f,c,u,p,$,r,x,q,g,v,_,H,d,b,D,C,X,re,te,K,Xe,Z,cs,pe,Qe,gs,Ke,bs,pn,me,mn,gt,_s,hn,bt,ws,cn,Ze,gn,ne,ds,_t,vs,ys,wt,xs,Es,bn,dt,ks,_n,et,wn,vt,Ts,dn,yt,Is,vn,he,yn,xt,Ps,xn,ce,En,Ft,kn,se,Ws,ge,Bs,be,Ss,Tn,_e,In,Q,js,we,qs,de,Hs,ve,Cs,Pn,ye,Wn,xe,Bn,ie,Ds,Ee,As,ke,Fs,Sn,Te,jn,Et,zs,qn,tt,Hn,kt,Os,Cn,Tt,Ls,Dn,nt,An,It,Ms,Fn,st,zn,Pt,Ns,On,it,Ie,Ln,Pe,Vs,We,Ys,Mn,at,Be,Nn,Wt,Js,Vn,Se,Yn,R,Rs,je,Gs,qe,Us,He,Xs,Ce,Qs,Jn,rt,Rn,Bt,Ks,Gn,De,Zs,Ae,ei,Un,lt,Fe,Xn,ze,ti,Oe,ni,Qn,ot,Le,Kn,St,si,Zn,jt,ii,es,ft,ts,N,ai,Me,ri,Ne,li,Ve,oi,Ye,fi,Je,$i,ns,$t,Re,ss,Ge,is,qt,ui,as,ut,rs,Ht,pi,ls,Ue,os;$=new fs({props:{type:"note",id:1}}),x=new At({props:{$$slots:{default:[Ki]},$$scope:{ctx:a}}}),H=new fs({props:{type:"reference",id:1}}),b=new At({props:{$$slots:{default:[Zi]},$$scope:{ctx:a}}}),K=new ue({props:{type:"info",$$slots:{default:[ea]},$$scope:{ctx:a}}}),pe=new At({props:{$$slots:{default:[ta]},$$scope:{ctx:a}}}),Qe=new fs({props:{id:2,type:"note"}}),Ke=new fs({props:{id:3,type:"note"}});let Ct=a[2],M=[];for(let e=0;e<Ct.length;e+=1)M[e]=Ii(Ti(a,Ct,e));return Ze=new $s({props:{probHead:.5,probTail:.5}}),et=new $s({props:{probHead:1,probTail:0}}),he=new ue({props:{type:"info",$$slots:{default:[na]},$$scope:{ctx:a}}}),ce=new ue({props:{type:"info",$$slots:{default:[la]},$$scope:{ctx:a}}}),ge=new z({props:{$$slots:{default:[oa]},$$scope:{ctx:a}}}),be=new z({props:{$$slots:{default:[fa]},$$scope:{ctx:a}}}),_e=new ue({props:{type:"info",$$slots:{default:[ua]},$$scope:{ctx:a}}}),we=new z({props:{$$slots:{default:[pa]},$$scope:{ctx:a}}}),de=new z({props:{$$slots:{default:[ma]},$$scope:{ctx:a}}}),ve=new z({props:{$$slots:{default:[ha]},$$scope:{ctx:a}}}),ye=new ji({props:{width:500,height:250,maxWidth:800,domain:[0,1],range:[0,10],padding:{top:10,right:10,bottom:40,left:40},$$slots:{default:[ca]},$$scope:{ctx:a}}}),xe=new ue({props:{type:"info",$$slots:{default:[ga]},$$scope:{ctx:a}}}),Ee=new z({props:{$$slots:{default:[ba]},$$scope:{ctx:a}}}),ke=new At({props:{$$slots:{default:[_a]},$$scope:{ctx:a}}}),Te=new ue({props:{type:"info",$$slots:{default:[ya]},$$scope:{ctx:a}}}),tt=new Gi({}),nt=new $s({props:{bitsHeads:"1",bitsTails:"0",showBits:!0}}),st=new $s({props:{probHead:.4,probTail:.6,bitsHeads:"1 0",bitsTails:"0 1",showBits:!0}}),Ie=new z({props:{$$slots:{default:[xa]},$$scope:{ctx:a}}}),We=new At({props:{$$slots:{default:[Ea]},$$scope:{ctx:a}}}),Be=new z({props:{$$slots:{default:[ka]},$$scope:{ctx:a}}}),Se=new ue({props:{type:"info",$$slots:{default:[Ba]},$$scope:{ctx:a}}}),je=new z({props:{$$slots:{default:[Sa]},$$scope:{ctx:a}}}),qe=new z({props:{$$slots:{default:[ja]},$$scope:{ctx:a}}}),He=new z({props:{$$slots:{default:[qa]},$$scope:{ctx:a}}}),Ce=new z({props:{$$slots:{default:[Ha]},$$scope:{ctx:a}}}),rt=new hi({props:{yTicks:[0,.1,.2,.3,.4]}}),Ae=new z({props:{$$slots:{default:[Ca]},$$scope:{ctx:a}}}),Fe=new z({props:{$$slots:{default:[Da]},$$scope:{ctx:a}}}),Oe=new z({props:{$$slots:{default:[Aa]},$$scope:{ctx:a}}}),Le=new z({props:{$$slots:{default:[Fa]},$$scope:{ctx:a}}}),ft=new hi({props:{points1:[{event:"Cat",percentage:1},{event:"Dog",percentage:0},{event:"Pig",percentage:0},{event:"Bear",percentage:0},{event:"Monkey",percentage:0}],startingPoints:[{event:"x1",percentage:.05},{event:"x2",percentage:.4},{event:"x3",percentage:.05},{event:"x4",percentage:.4},{event:"x5",percentage:.1}]}}),Me=new z({props:{$$slots:{default:[za]},$$scope:{ctx:a}}}),Ne=new z({props:{$$slots:{default:[Oa]},$$scope:{ctx:a}}}),Ve=new z({props:{$$slots:{default:[La]},$$scope:{ctx:a}}}),Ye=new z({props:{$$slots:{default:[Ma]},$$scope:{ctx:a}}}),Je=new At({props:{$$slots:{default:[Na]},$$scope:{ctx:a}}}),Re=new z({props:{$$slots:{default:[Va]},$$scope:{ctx:a}}}),Ge=new ue({props:{type:"info",$$slots:{default:[Ya]},$$scope:{ctx:a}}}),ut=new hi({props:{maxWidth:"400px",points1:[{event:"Cat",percentage:1},{event:"Dog",percentage:0}],startingPoints:[{event:"x1",percentage:.25},{event:"x2",percentage:.75}]}}),Ue=new ue({props:{type:"info",$$slots:{default:[Ra]},$$scope:{ctx:a}}}),{c(){t=A("h1"),n=m("Cross-Entropy Loss"),s=B(),f=A("div"),c=B(),u=A("p"),p=m(`The mean squared error loss tends to be problematic, when used as the loss
    function for classification tasks`),E($.$$.fragment),r=m(`. The
    loss that is usually used in classification tasks is called the `),E(x.$$.fragment),q=m(" (or the negative log likelihood loss)."),g=B(),v=A("p"),_=m(`In 1948 Claude Shanon published an article called "A Mathematical Theory of
    Communication"`),E(H.$$.fragment),d=m(`. This paper
    introduced a theoretical foundation for a field that has become known as
    `),E(b.$$.fragment),D=m("."),C=B(),X=A("p"),re=m(`At first glance it might look like we are about to go on a tangent here,
    because information theory and the loss function for classification tasks
    should't have a lot in common. Yet the opposite is the case.`),te=B(),E(K.$$.fragment),Xe=B(),Z=A("p"),cs=m(`We measure information using specific information units. The most common
    unit of information is the so called `),E(pe.$$.fragment),E(Qe.$$.fragment),gs=B(),E(Ke.$$.fragment),bs=m(`, which takes a value of either 0 or 1.
    Below for example we use 8 bits to encode and send some information.`),pn=B(),me=A("div");for(let e=0;e<M.length;e+=1)M[e].c();mn=B(),gt=A("p"),_s=m(`While we use 8 bits to send a message, we do not actually know how much of
    that information is useful. To get an intuition regarding that statement let
    us look at a simple toss coin example.`),hn=B(),bt=A("p"),ws=m(`Let us first imagine, that we are dealing with a fair coin, which means that
    the probability to get either heads or tails is exactly 50%.`),cn=B(),E(Ze.$$.fragment),gn=B(),ne=A("p"),ds=m(`To send a message regarding the outcome of the fair coin toss we need 1 bit.
    We could for example define heads as `),_t=A("span"),vs=m("1"),ys=m(` and tails as
    `),wt=A("span"),xs=m("0"),Es=m(`. The recepient of the message can remove the
    uncertainty regarding the coin toss outcome by simply looking at the value
    of the bit.`),bn=B(),dt=A("p"),ks=m(`But what if we deal with an unfair coin where heads comes up with a
    probability of 1.`),_n=B(),E(et.$$.fragment),wn=B(),vt=A("p"),Ts=m(`We could still send 1 bit, but there would be no useful information
    contained in the message, because the recepient has no uncertainty regarding
    the outcome of the toss coin. Sending a bit in such a manner would be a
    waste of resources.`),dn=B(),yt=A("p"),Is=m("Let's try to formalize the ideas we described above."),vn=B(),E(he.$$.fragment),yn=B(),xt=A("p"),Ps=m(`We expect less likely events to provide more information than more likely
    events. In fact an event with a probability of 50% provides exactly 1 bit of
    information. Or to put it differently, one useful bit reduces uncertainty by
    exactly 2. Two bits of useful information reduce uncertainty by 4, three
    bits by 8 and so on.`),xn=B(),E(ce.$$.fragment),En=B(),Ft=A("p"),kn=B(),se=A("p"),Ws=m(`We can use basic math to solve
    `),E(ge.$$.fragment),Bs=m(`
    for information in bits `),E(be.$$.fragment),Ss=m("."),Tn=B(),E(_e.$$.fragment),In=B(),Q=A("p"),js=m("We can plot the relationship between the probability "),E(we.$$.fragment),qs=m(` of an event
    `),E(de.$$.fragment),Hs=m(" and the information measured in bits "),E(ve.$$.fragment),Cs=m("."),Pn=B(),E(ye.$$.fragment),Wn=B(),E(xe.$$.fragment),Bn=B(),ie=A("p"),Ds=m(`Often we are not only interested in the amount of bits that is provided by a
    particular event. Instead we are interested in the expected value of
    information (the expected number of bits), that is contained in the whole
    probability distribution `),E(Ee.$$.fragment),As=m(". This measure is called "),E(ke.$$.fragment),Fs=m("."),Sn=B(),E(Te.$$.fragment),jn=B(),Et=A("p"),zs=m(`Below you can use an interactive example of a binomial distribution where
    you can change the probability of heads and tails. When you have a fair coin
    entropy amounts to exactly 1. When the probability starts getting uneven,
    entropy reduces until it reaches a value of 0.`),qn=B(),E(tt.$$.fragment),Hn=B(),kt=A("p"),Os=m(`Intuitively speaking, the entropy is a measure of order of a probability
    distribution. Entropy is highest, when all the possible events have the same
    probability and entropy is 0 when one of the events has the probability 1
    while all other events have a probability of 0.`),Cn=B(),Tt=A("p"),Ls=m(`Now let us return to the fair coin toss example. Using the equation and the
    example above, we know that the entropy is 1. We should therefore try and
    send the message with the result of the coin toss using 1 bit of
    information.`),Dn=B(),E(nt.$$.fragment),An=B(),It=A("p"),Ms=m(`In the example below on other hand we use an inefficient encoding. We always
    send 2 bits of information when we get heads and 2 bits when we get tails.`),Fn=B(),E(st.$$.fragment),zn=B(),Pt=A("p"),Ns=m("The entropy of the probability distribution is just 0.97 bits."),On=B(),it=A("div"),E(Ie.$$.fragment),Ln=B(),Pe=A("p"),Vs=m("Yet the average message length, also known as "),E(We.$$.fragment),Ys=m(" is 2 bits."),Mn=B(),at=A("div"),E(Be.$$.fragment),Nn=B(),Wt=A("p"),Js=m(`By using 2 bits to encode the message, we implicitly assume a different
    probability distribution, than the one that produced the coin toss. Remember
    that 2 bits would for example correspond to a distribution with 4 likely
    events, each occuring with a probability of 25%. In a way we can say, that
    the cross-entropy allows us to measure the difference between two
    distributions. Only when the distribution that produced the event and the
    distribution we use to encode the message are identical, does the
    cross-entropy reach its minimum value. In that case the cross-entropy and
    the entropy are identical.`),Vn=B(),E(Se.$$.fragment),Yn=B(),R=A("p"),Rs=m("In the below example the red distribution is "),E(je.$$.fragment),Gs=m(` and the yellow
    distribution is `),E(qe.$$.fragment),Us=m(". When you move the slider to the right "),E(He.$$.fragment),Xs=m(" starts moving towards "),E(Ce.$$.fragment),Qs=m(` and you can observe that the cross-entropy
    gets lower and lower until its' minimal value is reached. In that case the two
    distributions are identical and the cross-entropy is equal to the entropy.`),Jn=B(),E(rt.$$.fragment),Rn=B(),Bt=A("p"),Ks=m(`Now it is time to come full circle and to relate the calculation of the
    cross-entropy to our initial task: find a loss function that is suited for
    classification tasks.`),Gn=B(),De=A("p"),Zs=m(`Let us assume that we are dealing with a problem, where we have to classify
    an animal based on certain features in one of the five categories: cat, dog,
    pig, bear or monkey. The cross-entropy deals with probability distributions,
    so we need to put the label into a format that equals a probability
    distribution. For example if we deal with a sample that depicts a cat, the
    true probability distribution would be 100% for the category cat and 0% for
    all other categories. This distribution is put in a so called "one-hot"
    vector. A vector that contains a one for the relevant category and 0
    otherwise. So that we have the following distributions, `),E(Ae.$$.fragment),ei=m("."),Un=B(),lt=A("div"),E(Fe.$$.fragment),Xn=B(),ze=A("p"),ti=m(`The distributions that are produced by the sigmoid or the softmax functions
    on the other hand are just estimations. We designate this distribution `),E(Oe.$$.fragment),ni=m("."),Qn=B(),ot=A("div"),E(Le.$$.fragment),Kn=B(),St=A("p"),si=m(`Now we have everything to calculate the cross-entropy. The closer the one
    hot distribution and the distribution produced by the logistic regression or
    the neural network get, the lower the cross-entropy gets. Because all the
    weight of the one hot vector is on just one event, the entropy corresponds
    to exactly 0, which means the cross-entropy could theoretically also reach
    0. Our goal in a classification task is to minimize the cross-entropy to get
    the two distributions as close as possible.`),Zn=B(),jt=A("p"),ii=m(`Below is an interactive example where the true label corresponds to the
    category cat. The estimated probabilities are far from the ground truth,
    which results in a relatively high cross-entropy. When you move the slider,
    the estimated probabilities start moving towards the ground truth, which
    pushes the cross-entropy down, until it reaches a value of 0.`),es=B(),E(ft.$$.fragment),ts=B(),N=A("p"),ai=m(`In logistic regression we utilize the sigmoid activation function
    `),E(Me.$$.fragment),ri=m(`
    , which produces values between 0 and 1. The sigmoid function can be used to
    differentiate between 2 categories . The sigmoid function produces the probability
    `),E(Ne.$$.fragment),li=m(` to belong to the first category (e.g. cat),
    therefore `),E(Ve.$$.fragment),oi=m(` returns the probability to
    belong to the second category (e.g. dog). If we additionally define that the
    label `),E(Ye.$$.fragment),fi=m(` is 1 when the sample is a cat and 0 when the sample is
    a dog, the expression reduces the cross-entropy to the so called `),E(Je.$$.fragment),$i=m("."),ns=B(),$t=A("div"),E(Re.$$.fragment),ss=B(),E(Ge.$$.fragment),is=B(),qt=A("p"),ui=m(`When we shift the weights and the bias of the sigmoid function, we can move
    the probability to belong to a certain category closer to the ground truth
    in order to reduce the cross-entropy. In the next section we will
    demonstrate how we can we can utilize gradient descent for that purpose. For
    now you can play with the interactive example below.`),as=B(),E(ut.$$.fragment),rs=B(),Ht=A("p"),pi=m(`In practice we always deal with with a dataset, therefore the cross-entropy
    loss that we are going to optimize is going to be the average over the whole
    dataset.`),ls=B(),E(Ue.$$.fragment),this.h()},l(e){t=F(e,"H1",{});var l=j(t);n=h(l,"Cross-Entropy Loss"),l.forEach(i),s=S(e),f=F(e,"DIV",{class:!0}),j(f).forEach(i),c=S(e),u=F(e,"P",{});var G=j(u);p=h(G,`The mean squared error loss tends to be problematic, when used as the loss
    function for classification tasks`),k($.$$.fragment,G),r=h(G,`. The
    loss that is usually used in classification tasks is called the `),k(x.$$.fragment,G),q=h(G," (or the negative log likelihood loss)."),G.forEach(i),g=S(e),v=F(e,"P",{});var le=j(v);_=h(le,`In 1948 Claude Shanon published an article called "A Mathematical Theory of
    Communication"`),k(H.$$.fragment,le),d=h(le,`. This paper
    introduced a theoretical foundation for a field that has become known as
    `),k(b.$$.fragment,le),D=h(le,"."),le.forEach(i),C=S(e),X=F(e,"P",{});var zt=j(X);re=h(zt,`At first glance it might look like we are about to go on a tangent here,
    because information theory and the loss function for classification tasks
    should't have a lot in common. Yet the opposite is the case.`),zt.forEach(i),te=S(e),k(K.$$.fragment,e),Xe=S(e),Z=F(e,"P",{});var ae=j(Z);cs=h(ae,`We measure information using specific information units. The most common
    unit of information is the so called `),k(pe.$$.fragment,ae),k(Qe.$$.fragment,ae),gs=S(ae),k(Ke.$$.fragment,ae),bs=h(ae,`, which takes a value of either 0 or 1.
    Below for example we use 8 bits to encode and send some information.`),ae.forEach(i),pn=S(e),me=F(e,"DIV",{class:!0});var Ot=j(me);for(let Dt=0;Dt<M.length;Dt+=1)M[Dt].l(Ot);Ot.forEach(i),mn=S(e),gt=F(e,"P",{});var Lt=j(gt);_s=h(Lt,`While we use 8 bits to send a message, we do not actually know how much of
    that information is useful. To get an intuition regarding that statement let
    us look at a simple toss coin example.`),Lt.forEach(i),hn=S(e),bt=F(e,"P",{});var Mt=j(bt);ws=h(Mt,`Let us first imagine, that we are dealing with a fair coin, which means that
    the probability to get either heads or tails is exactly 50%.`),Mt.forEach(i),cn=S(e),k(Ze.$$.fragment,e),gn=S(e),ne=F(e,"P",{});var oe=j(ne);ds=h(oe,`To send a message regarding the outcome of the fair coin toss we need 1 bit.
    We could for example define heads as `),_t=F(oe,"SPAN",{class:!0});var Nt=j(_t);vs=h(Nt,"1"),Nt.forEach(i),ys=h(oe,` and tails as
    `),wt=F(oe,"SPAN",{class:!0});var Vt=j(wt);xs=h(Vt,"0"),Vt.forEach(i),Es=h(oe,`. The recepient of the message can remove the
    uncertainty regarding the coin toss outcome by simply looking at the value
    of the bit.`),oe.forEach(i),bn=S(e),dt=F(e,"P",{});var Yt=j(dt);ks=h(Yt,`But what if we deal with an unfair coin where heads comes up with a
    probability of 1.`),Yt.forEach(i),_n=S(e),k(et.$$.fragment,e),wn=S(e),vt=F(e,"P",{});var Jt=j(vt);Ts=h(Jt,`We could still send 1 bit, but there would be no useful information
    contained in the message, because the recepient has no uncertainty regarding
    the outcome of the toss coin. Sending a bit in such a manner would be a
    waste of resources.`),Jt.forEach(i),dn=S(e),yt=F(e,"P",{});var Rt=j(yt);Is=h(Rt,"Let's try to formalize the ideas we described above."),Rt.forEach(i),vn=S(e),k(he.$$.fragment,e),yn=S(e),xt=F(e,"P",{});var Gt=j(xt);Ps=h(Gt,`We expect less likely events to provide more information than more likely
    events. In fact an event with a probability of 50% provides exactly 1 bit of
    information. Or to put it differently, one useful bit reduces uncertainty by
    exactly 2. Two bits of useful information reduce uncertainty by 4, three
    bits by 8 and so on.`),Gt.forEach(i),xn=S(e),k(ce.$$.fragment,e),En=S(e),Ft=F(e,"P",{}),j(Ft).forEach(i),kn=S(e),se=F(e,"P",{});var fe=j(se);Ws=h(fe,`We can use basic math to solve
    `),k(ge.$$.fragment,fe),Bs=h(fe,`
    for information in bits `),k(be.$$.fragment,fe),Ss=h(fe,"."),fe.forEach(i),Tn=S(e),k(_e.$$.fragment,e),In=S(e),Q=F(e,"P",{});var ee=j(Q);js=h(ee,"We can plot the relationship between the probability "),k(we.$$.fragment,ee),qs=h(ee,` of an event
    `),k(de.$$.fragment,ee),Hs=h(ee," and the information measured in bits "),k(ve.$$.fragment,ee),Cs=h(ee,"."),ee.forEach(i),Pn=S(e),k(ye.$$.fragment,e),Wn=S(e),k(xe.$$.fragment,e),Bn=S(e),ie=F(e,"P",{});var $e=j(ie);Ds=h($e,`Often we are not only interested in the amount of bits that is provided by a
    particular event. Instead we are interested in the expected value of
    information (the expected number of bits), that is contained in the whole
    probability distribution `),k(Ee.$$.fragment,$e),As=h($e,". This measure is called "),k(ke.$$.fragment,$e),Fs=h($e,"."),$e.forEach(i),Sn=S(e),k(Te.$$.fragment,e),jn=S(e),Et=F(e,"P",{});var Ut=j(Et);zs=h(Ut,`Below you can use an interactive example of a binomial distribution where
    you can change the probability of heads and tails. When you have a fair coin
    entropy amounts to exactly 1. When the probability starts getting uneven,
    entropy reduces until it reaches a value of 0.`),Ut.forEach(i),qn=S(e),k(tt.$$.fragment,e),Hn=S(e),kt=F(e,"P",{});var Xt=j(kt);Os=h(Xt,`Intuitively speaking, the entropy is a measure of order of a probability
    distribution. Entropy is highest, when all the possible events have the same
    probability and entropy is 0 when one of the events has the probability 1
    while all other events have a probability of 0.`),Xt.forEach(i),Cn=S(e),Tt=F(e,"P",{});var Qt=j(Tt);Ls=h(Qt,`Now let us return to the fair coin toss example. Using the equation and the
    example above, we know that the entropy is 1. We should therefore try and
    send the message with the result of the coin toss using 1 bit of
    information.`),Qt.forEach(i),Dn=S(e),k(nt.$$.fragment,e),An=S(e),It=F(e,"P",{});var Kt=j(It);Ms=h(Kt,`In the example below on other hand we use an inefficient encoding. We always
    send 2 bits of information when we get heads and 2 bits when we get tails.`),Kt.forEach(i),Fn=S(e),k(st.$$.fragment,e),zn=S(e),Pt=F(e,"P",{});var Zt=j(Pt);Ns=h(Zt,"The entropy of the probability distribution is just 0.97 bits."),Zt.forEach(i),On=S(e),it=F(e,"DIV",{class:!0});var en=j(it);k(Ie.$$.fragment,en),en.forEach(i),Ln=S(e),Pe=F(e,"P",{});var pt=j(Pe);Vs=h(pt,"Yet the average message length, also known as "),k(We.$$.fragment,pt),Ys=h(pt," is 2 bits."),pt.forEach(i),Mn=S(e),at=F(e,"DIV",{class:!0});var tn=j(at);k(Be.$$.fragment,tn),tn.forEach(i),Nn=S(e),Wt=F(e,"P",{});var nn=j(Wt);Js=h(nn,`By using 2 bits to encode the message, we implicitly assume a different
    probability distribution, than the one that produced the coin toss. Remember
    that 2 bits would for example correspond to a distribution with 4 likely
    events, each occuring with a probability of 25%. In a way we can say, that
    the cross-entropy allows us to measure the difference between two
    distributions. Only when the distribution that produced the event and the
    distribution we use to encode the message are identical, does the
    cross-entropy reach its minimum value. In that case the cross-entropy and
    the entropy are identical.`),nn.forEach(i),Vn=S(e),k(Se.$$.fragment,e),Yn=S(e),R=F(e,"P",{});var U=j(R);Rs=h(U,"In the below example the red distribution is "),k(je.$$.fragment,U),Gs=h(U,` and the yellow
    distribution is `),k(qe.$$.fragment,U),Us=h(U,". When you move the slider to the right "),k(He.$$.fragment,U),Xs=h(U," starts moving towards "),k(Ce.$$.fragment,U),Qs=h(U,` and you can observe that the cross-entropy
    gets lower and lower until its' minimal value is reached. In that case the two
    distributions are identical and the cross-entropy is equal to the entropy.`),U.forEach(i),Jn=S(e),k(rt.$$.fragment,e),Rn=S(e),Bt=F(e,"P",{});var sn=j(Bt);Ks=h(sn,`Now it is time to come full circle and to relate the calculation of the
    cross-entropy to our initial task: find a loss function that is suited for
    classification tasks.`),sn.forEach(i),Gn=S(e),De=F(e,"P",{});var mt=j(De);Zs=h(mt,`Let us assume that we are dealing with a problem, where we have to classify
    an animal based on certain features in one of the five categories: cat, dog,
    pig, bear or monkey. The cross-entropy deals with probability distributions,
    so we need to put the label into a format that equals a probability
    distribution. For example if we deal with a sample that depicts a cat, the
    true probability distribution would be 100% for the category cat and 0% for
    all other categories. This distribution is put in a so called "one-hot"
    vector. A vector that contains a one for the relevant category and 0
    otherwise. So that we have the following distributions, `),k(Ae.$$.fragment,mt),ei=h(mt,"."),mt.forEach(i),Un=S(e),lt=F(e,"DIV",{class:!0});var an=j(lt);k(Fe.$$.fragment,an),an.forEach(i),Xn=S(e),ze=F(e,"P",{});var ht=j(ze);ti=h(ht,`The distributions that are produced by the sigmoid or the softmax functions
    on the other hand are just estimations. We designate this distribution `),k(Oe.$$.fragment,ht),ni=h(ht,"."),ht.forEach(i),Qn=S(e),ot=F(e,"DIV",{class:!0});var rn=j(ot);k(Le.$$.fragment,rn),rn.forEach(i),Kn=S(e),St=F(e,"P",{});var ln=j(St);si=h(ln,`Now we have everything to calculate the cross-entropy. The closer the one
    hot distribution and the distribution produced by the logistic regression or
    the neural network get, the lower the cross-entropy gets. Because all the
    weight of the one hot vector is on just one event, the entropy corresponds
    to exactly 0, which means the cross-entropy could theoretically also reach
    0. Our goal in a classification task is to minimize the cross-entropy to get
    the two distributions as close as possible.`),ln.forEach(i),Zn=S(e),jt=F(e,"P",{});var on=j(jt);ii=h(on,`Below is an interactive example where the true label corresponds to the
    category cat. The estimated probabilities are far from the ground truth,
    which results in a relatively high cross-entropy. When you move the slider,
    the estimated probabilities start moving towards the ground truth, which
    pushes the cross-entropy down, until it reaches a value of 0.`),on.forEach(i),es=S(e),k(ft.$$.fragment,e),ts=S(e),N=F(e,"P",{});var Y=j(N);ai=h(Y,`In logistic regression we utilize the sigmoid activation function
    `),k(Me.$$.fragment,Y),ri=h(Y,`
    , which produces values between 0 and 1. The sigmoid function can be used to
    differentiate between 2 categories . The sigmoid function produces the probability
    `),k(Ne.$$.fragment,Y),li=h(Y,` to belong to the first category (e.g. cat),
    therefore `),k(Ve.$$.fragment,Y),oi=h(Y,` returns the probability to
    belong to the second category (e.g. dog). If we additionally define that the
    label `),k(Ye.$$.fragment,Y),fi=h(Y,` is 1 when the sample is a cat and 0 when the sample is
    a dog, the expression reduces the cross-entropy to the so called `),k(Je.$$.fragment,Y),$i=h(Y,"."),Y.forEach(i),ns=S(e),$t=F(e,"DIV",{class:!0});var fn=j($t);k(Re.$$.fragment,fn),fn.forEach(i),ss=S(e),k(Ge.$$.fragment,e),is=S(e),qt=F(e,"P",{});var $n=j(qt);ui=h($n,`When we shift the weights and the bias of the sigmoid function, we can move
    the probability to belong to a certain category closer to the ground truth
    in order to reduce the cross-entropy. In the next section we will
    demonstrate how we can we can utilize gradient descent for that purpose. For
    now you can play with the interactive example below.`),$n.forEach(i),as=S(e),k(ut.$$.fragment,e),rs=S(e),Ht=F(e,"P",{});var J=j(Ht);pi=h(J,`In practice we always deal with with a dataset, therefore the cross-entropy
    loss that we are going to optimize is going to be the average over the whole
    dataset.`),J.forEach(i),ls=S(e),k(Ue.$$.fragment,e),this.h()},h(){w(f,"class","separator"),w(me,"class","flex justify-center items-center gap-1"),w(_t,"class","bit"),w(wt,"class","bit"),w(it,"class","flex justify-center"),w(at,"class","flex justify-center"),w(lt,"class","flex justify-center"),w(ot,"class","flex justify-center"),w($t,"class","flex justify-center")},m(e,l){o(e,t,l),y(t,n),o(e,s,l),o(e,f,l),o(e,c,l),o(e,u,l),y(u,p),T($,u,null),y(u,r),T(x,u,null),y(u,q),o(e,g,l),o(e,v,l),y(v,_),T(H,v,null),y(v,d),T(b,v,null),y(v,D),o(e,C,l),o(e,X,l),y(X,re),o(e,te,l),T(K,e,l),o(e,Xe,l),o(e,Z,l),y(Z,cs),T(pe,Z,null),T(Qe,Z,null),y(Z,gs),T(Ke,Z,null),y(Z,bs),o(e,pn,l),o(e,me,l);for(let G=0;G<M.length;G+=1)M[G]&&M[G].m(me,null);o(e,mn,l),o(e,gt,l),y(gt,_s),o(e,hn,l),o(e,bt,l),y(bt,ws),o(e,cn,l),T(Ze,e,l),o(e,gn,l),o(e,ne,l),y(ne,ds),y(ne,_t),y(_t,vs),y(ne,ys),y(ne,wt),y(wt,xs),y(ne,Es),o(e,bn,l),o(e,dt,l),y(dt,ks),o(e,_n,l),T(et,e,l),o(e,wn,l),o(e,vt,l),y(vt,Ts),o(e,dn,l),o(e,yt,l),y(yt,Is),o(e,vn,l),T(he,e,l),o(e,yn,l),o(e,xt,l),y(xt,Ps),o(e,xn,l),T(ce,e,l),o(e,En,l),o(e,Ft,l),o(e,kn,l),o(e,se,l),y(se,Ws),T(ge,se,null),y(se,Bs),T(be,se,null),y(se,Ss),o(e,Tn,l),T(_e,e,l),o(e,In,l),o(e,Q,l),y(Q,js),T(we,Q,null),y(Q,qs),T(de,Q,null),y(Q,Hs),T(ve,Q,null),y(Q,Cs),o(e,Pn,l),T(ye,e,l),o(e,Wn,l),T(xe,e,l),o(e,Bn,l),o(e,ie,l),y(ie,Ds),T(Ee,ie,null),y(ie,As),T(ke,ie,null),y(ie,Fs),o(e,Sn,l),T(Te,e,l),o(e,jn,l),o(e,Et,l),y(Et,zs),o(e,qn,l),T(tt,e,l),o(e,Hn,l),o(e,kt,l),y(kt,Os),o(e,Cn,l),o(e,Tt,l),y(Tt,Ls),o(e,Dn,l),T(nt,e,l),o(e,An,l),o(e,It,l),y(It,Ms),o(e,Fn,l),T(st,e,l),o(e,zn,l),o(e,Pt,l),y(Pt,Ns),o(e,On,l),o(e,it,l),T(Ie,it,null),o(e,Ln,l),o(e,Pe,l),y(Pe,Vs),T(We,Pe,null),y(Pe,Ys),o(e,Mn,l),o(e,at,l),T(Be,at,null),o(e,Nn,l),o(e,Wt,l),y(Wt,Js),o(e,Vn,l),T(Se,e,l),o(e,Yn,l),o(e,R,l),y(R,Rs),T(je,R,null),y(R,Gs),T(qe,R,null),y(R,Us),T(He,R,null),y(R,Xs),T(Ce,R,null),y(R,Qs),o(e,Jn,l),T(rt,e,l),o(e,Rn,l),o(e,Bt,l),y(Bt,Ks),o(e,Gn,l),o(e,De,l),y(De,Zs),T(Ae,De,null),y(De,ei),o(e,Un,l),o(e,lt,l),T(Fe,lt,null),o(e,Xn,l),o(e,ze,l),y(ze,ti),T(Oe,ze,null),y(ze,ni),o(e,Qn,l),o(e,ot,l),T(Le,ot,null),o(e,Kn,l),o(e,St,l),y(St,si),o(e,Zn,l),o(e,jt,l),y(jt,ii),o(e,es,l),T(ft,e,l),o(e,ts,l),o(e,N,l),y(N,ai),T(Me,N,null),y(N,ri),T(Ne,N,null),y(N,li),T(Ve,N,null),y(N,oi),T(Ye,N,null),y(N,fi),T(Je,N,null),y(N,$i),o(e,ns,l),o(e,$t,l),T(Re,$t,null),o(e,ss,l),T(Ge,e,l),o(e,is,l),o(e,qt,l),y(qt,ui),o(e,as,l),T(ut,e,l),o(e,rs,l),o(e,Ht,l),y(Ht,pi),o(e,ls,l),T(Ue,e,l),os=!0},p(e,l){const G={};l&4096&&(G.$$scope={dirty:l,ctx:e}),x.$set(G);const le={};l&4096&&(le.$$scope={dirty:l,ctx:e}),b.$set(le);const zt={};l&4096&&(zt.$$scope={dirty:l,ctx:e}),K.$set(zt);const ae={};if(l&4096&&(ae.$$scope={dirty:l,ctx:e}),pe.$set(ae),l&4){Ct=e[2];let J;for(J=0;J<Ct.length;J+=1){const Dt=Ti(e,Ct,J);M[J]?M[J].p(Dt,l):(M[J]=Ii(Dt),M[J].c(),M[J].m(me,null))}for(;J<M.length;J+=1)M[J].d(1);M.length=Ct.length}const Ot={};l&4096&&(Ot.$$scope={dirty:l,ctx:e}),he.$set(Ot);const Lt={};l&4096&&(Lt.$$scope={dirty:l,ctx:e}),ce.$set(Lt);const Mt={};l&4096&&(Mt.$$scope={dirty:l,ctx:e}),ge.$set(Mt);const oe={};l&4096&&(oe.$$scope={dirty:l,ctx:e}),be.$set(oe);const Nt={};l&4096&&(Nt.$$scope={dirty:l,ctx:e}),_e.$set(Nt);const Vt={};l&4096&&(Vt.$$scope={dirty:l,ctx:e}),we.$set(Vt);const Yt={};l&4096&&(Yt.$$scope={dirty:l,ctx:e}),de.$set(Yt);const Jt={};l&4096&&(Jt.$$scope={dirty:l,ctx:e}),ve.$set(Jt);const Rt={};l&4096&&(Rt.$$scope={dirty:l,ctx:e}),ye.$set(Rt);const Gt={};l&4096&&(Gt.$$scope={dirty:l,ctx:e}),xe.$set(Gt);const fe={};l&4096&&(fe.$$scope={dirty:l,ctx:e}),Ee.$set(fe);const ee={};l&4096&&(ee.$$scope={dirty:l,ctx:e}),ke.$set(ee);const $e={};l&4096&&($e.$$scope={dirty:l,ctx:e}),Te.$set($e);const Ut={};l&4096&&(Ut.$$scope={dirty:l,ctx:e}),Ie.$set(Ut);const Xt={};l&4096&&(Xt.$$scope={dirty:l,ctx:e}),We.$set(Xt);const Qt={};l&4096&&(Qt.$$scope={dirty:l,ctx:e}),Be.$set(Qt);const Kt={};l&4096&&(Kt.$$scope={dirty:l,ctx:e}),Se.$set(Kt);const Zt={};l&4096&&(Zt.$$scope={dirty:l,ctx:e}),je.$set(Zt);const en={};l&4096&&(en.$$scope={dirty:l,ctx:e}),qe.$set(en);const pt={};l&4096&&(pt.$$scope={dirty:l,ctx:e}),He.$set(pt);const tn={};l&4096&&(tn.$$scope={dirty:l,ctx:e}),Ce.$set(tn);const nn={};l&4096&&(nn.$$scope={dirty:l,ctx:e}),Ae.$set(nn);const U={};l&4096&&(U.$$scope={dirty:l,ctx:e}),Fe.$set(U);const sn={};l&4096&&(sn.$$scope={dirty:l,ctx:e}),Oe.$set(sn);const mt={};l&4096&&(mt.$$scope={dirty:l,ctx:e}),Le.$set(mt);const an={};l&4096&&(an.$$scope={dirty:l,ctx:e}),Me.$set(an);const ht={};l&4096&&(ht.$$scope={dirty:l,ctx:e}),Ne.$set(ht);const rn={};l&4096&&(rn.$$scope={dirty:l,ctx:e}),Ve.$set(rn);const ln={};l&4096&&(ln.$$scope={dirty:l,ctx:e}),Ye.$set(ln);const on={};l&4096&&(on.$$scope={dirty:l,ctx:e}),Je.$set(on);const Y={};l&4096&&(Y.$$scope={dirty:l,ctx:e}),Re.$set(Y);const fn={};l&4096&&(fn.$$scope={dirty:l,ctx:e}),Ge.$set(fn);const $n={};l&4096&&($n.$$scope={dirty:l,ctx:e}),Ue.$set($n)},i(e){os||(I($.$$.fragment,e),I(x.$$.fragment,e),I(H.$$.fragment,e),I(b.$$.fragment,e),I(K.$$.fragment,e),I(pe.$$.fragment,e),I(Qe.$$.fragment,e),I(Ke.$$.fragment,e),I(Ze.$$.fragment,e),I(et.$$.fragment,e),I(he.$$.fragment,e),I(ce.$$.fragment,e),I(ge.$$.fragment,e),I(be.$$.fragment,e),I(_e.$$.fragment,e),I(we.$$.fragment,e),I(de.$$.fragment,e),I(ve.$$.fragment,e),I(ye.$$.fragment,e),I(xe.$$.fragment,e),I(Ee.$$.fragment,e),I(ke.$$.fragment,e),I(Te.$$.fragment,e),I(tt.$$.fragment,e),I(nt.$$.fragment,e),I(st.$$.fragment,e),I(Ie.$$.fragment,e),I(We.$$.fragment,e),I(Be.$$.fragment,e),I(Se.$$.fragment,e),I(je.$$.fragment,e),I(qe.$$.fragment,e),I(He.$$.fragment,e),I(Ce.$$.fragment,e),I(rt.$$.fragment,e),I(Ae.$$.fragment,e),I(Fe.$$.fragment,e),I(Oe.$$.fragment,e),I(Le.$$.fragment,e),I(ft.$$.fragment,e),I(Me.$$.fragment,e),I(Ne.$$.fragment,e),I(Ve.$$.fragment,e),I(Ye.$$.fragment,e),I(Je.$$.fragment,e),I(Re.$$.fragment,e),I(Ge.$$.fragment,e),I(ut.$$.fragment,e),I(Ue.$$.fragment,e),os=!0)},o(e){P($.$$.fragment,e),P(x.$$.fragment,e),P(H.$$.fragment,e),P(b.$$.fragment,e),P(K.$$.fragment,e),P(pe.$$.fragment,e),P(Qe.$$.fragment,e),P(Ke.$$.fragment,e),P(Ze.$$.fragment,e),P(et.$$.fragment,e),P(he.$$.fragment,e),P(ce.$$.fragment,e),P(ge.$$.fragment,e),P(be.$$.fragment,e),P(_e.$$.fragment,e),P(we.$$.fragment,e),P(de.$$.fragment,e),P(ve.$$.fragment,e),P(ye.$$.fragment,e),P(xe.$$.fragment,e),P(Ee.$$.fragment,e),P(ke.$$.fragment,e),P(Te.$$.fragment,e),P(tt.$$.fragment,e),P(nt.$$.fragment,e),P(st.$$.fragment,e),P(Ie.$$.fragment,e),P(We.$$.fragment,e),P(Be.$$.fragment,e),P(Se.$$.fragment,e),P(je.$$.fragment,e),P(qe.$$.fragment,e),P(He.$$.fragment,e),P(Ce.$$.fragment,e),P(rt.$$.fragment,e),P(Ae.$$.fragment,e),P(Fe.$$.fragment,e),P(Oe.$$.fragment,e),P(Le.$$.fragment,e),P(ft.$$.fragment,e),P(Me.$$.fragment,e),P(Ne.$$.fragment,e),P(Ve.$$.fragment,e),P(Ye.$$.fragment,e),P(Je.$$.fragment,e),P(Re.$$.fragment,e),P(Ge.$$.fragment,e),P(ut.$$.fragment,e),P(Ue.$$.fragment,e),os=!1},d(e){e&&i(t),e&&i(s),e&&i(f),e&&i(c),e&&i(u),W($),W(x),e&&i(g),e&&i(v),W(H),W(b),e&&i(C),e&&i(X),e&&i(te),W(K,e),e&&i(Xe),e&&i(Z),W(pe),W(Qe),W(Ke),e&&i(pn),e&&i(me),un(M,e),e&&i(mn),e&&i(gt),e&&i(hn),e&&i(bt),e&&i(cn),W(Ze,e),e&&i(gn),e&&i(ne),e&&i(bn),e&&i(dt),e&&i(_n),W(et,e),e&&i(wn),e&&i(vt),e&&i(dn),e&&i(yt),e&&i(vn),W(he,e),e&&i(yn),e&&i(xt),e&&i(xn),W(ce,e),e&&i(En),e&&i(Ft),e&&i(kn),e&&i(se),W(ge),W(be),e&&i(Tn),W(_e,e),e&&i(In),e&&i(Q),W(we),W(de),W(ve),e&&i(Pn),W(ye,e),e&&i(Wn),W(xe,e),e&&i(Bn),e&&i(ie),W(Ee),W(ke),e&&i(Sn),W(Te,e),e&&i(jn),e&&i(Et),e&&i(qn),W(tt,e),e&&i(Hn),e&&i(kt),e&&i(Cn),e&&i(Tt),e&&i(Dn),W(nt,e),e&&i(An),e&&i(It),e&&i(Fn),W(st,e),e&&i(zn),e&&i(Pt),e&&i(On),e&&i(it),W(Ie),e&&i(Ln),e&&i(Pe),W(We),e&&i(Mn),e&&i(at),W(Be),e&&i(Nn),e&&i(Wt),e&&i(Vn),W(Se,e),e&&i(Yn),e&&i(R),W(je),W(qe),W(He),W(Ce),e&&i(Jn),W(rt,e),e&&i(Rn),e&&i(Bt),e&&i(Gn),e&&i(De),W(Ae),e&&i(Un),e&&i(lt),W(Fe),e&&i(Xn),e&&i(ze),W(Oe),e&&i(Qn),e&&i(ot),W(Le),e&&i(Kn),e&&i(St),e&&i(Zn),e&&i(jt),e&&i(es),W(ft,e),e&&i(ts),e&&i(N),W(Me),W(Ne),W(Ve),W(Ye),W(Je),e&&i(ns),e&&i($t),W(Re),e&&i(ss),W(Ge,e),e&&i(is),e&&i(qt),e&&i(as),W(ut,e),e&&i(rs),e&&i(Ht),e&&i(ls),W(Ue,e)}}}function Ua(a){let t,n,s,f,c,u;return s=new zi({props:{$$slots:{default:[Ga]},$$scope:{ctx:a}}}),c=new Oi({props:{notes:a[0],references:a[1]}}),{c(){t=A("meta"),n=B(),E(s.$$.fragment),f=B(),E(c.$$.fragment),this.h()},l(p){const $=Fi("svelte-vfr19e",document.head);t=F($,"META",{name:!0,content:!0}),$.forEach(i),n=S(p),k(s.$$.fragment,p),f=S(p),k(c.$$.fragment,p),this.h()},h(){document.title="Cross Entropy Loss - World4AI",w(t,"name","description"),w(t,"content","Cross-entropy, also called negative log likelihood, is the loss function that is used in classification tasks.")},m(p,$){y(document.head,t),o(p,n,$),T(s,p,$),o(p,f,$),T(c,p,$),u=!0},p(p,[$]){const r={};$&4096&&(r.$$scope={dirty:$,ctx:p}),s.$set(r)},i(p){u||(I(s.$$.fragment,p),I(c.$$.fragment,p),u=!0)},o(p){P(s.$$.fragment,p),P(c.$$.fragment,p),u=!1},d(p){i(t),p&&i(n),W(s,p),p&&i(f),W(c,p)}}}function Xa(a){const t=["The properties that make the mean squared error a bad choice for classification tasks are discussed in the next section.","A bit is a binary unit of of information. This unit is also referred to as a Shanon.","In machine learning we often use nats for convenience (natural units), which use the base e instead of bits with the base of 2. This does not pose a problem, as we can simply convert from bits to nats by changing the base from 2 to e."],n=[{author:"Shannon C. E.",title:"A Mathematical Theory of Communication",journal:"Bell System Technical Journal",year:"1948",pages:"379-423",volume:"27",issue:"3"}],s=[1,1,0,1,1,0,0,1],f=[];function c(){for(let u=.001;u<=1;u+=.001){let p=u,$=-Math.log2(u),r={x:p,y:$};f.push(r)}}return c(),[t,n,s,f]}class ur extends ps{constructor(t){super(),ms(this,t,Xa,Ua,hs,{})}}export{ur as default};
