import{S as Xn,i as Yn,s as Zn,k as T,a as B,y as v,W as er,l as x,h as i,c as P,z as b,n as A,N as p,b as m,A as y,g as d,d as g,B as E,q as _,m as k,r as w,Q as Me,R as Oe,C as L,P as oe,e as z,v as Lt,f as Kt}from"../chunks/index.4d92b023.js";import{C as tr}from"../chunks/Container.b0705c7b.js";import{H as D}from"../chunks/Highlight.b7c1de53.js";import{F as nr,I as rr}from"../chunks/InternalLink.7deb899c.js";import{P as sr}from"../chunks/PythonCode.212ba7a6.js";import{S as Ce}from"../chunks/SvgContainer.f70b5745.js";import{B as S}from"../chunks/Block.059eddcd.js";import{A as $e}from"../chunks/Arrow.ae91874c.js";function Hn(c,t,n){const r=c.slice();return r[3]=t[n],r[5]=n,r}function Nn(c,t,n){const r=c.slice();return r[3]=t[n],r[5]=n,r}function Vn(c,t,n){const r=c.slice();return r[7]=t[n],r[5]=n,r}function ar(c,t,n){const r=c.slice();return r[7]=t[n],r[10]=n,r}function Gn(c,t,n){const r=c.slice();return r[7]=t[n],r[5]=n,r}function or(c,t,n){const r=c.slice();return r[7]=t[n],r[10]=n,r}function lr(c,t,n){const r=c.slice();return r[7]=t[n],r[5]=n,r}function ir(c,t,n){const r=c.slice();return r[7]=t[n],r[5]=n,r}function fr(c){let t;return{c(){t=_("BERT")},l(n){t=w(n,"BERT")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function hr(c){let t;return{c(){t=_("B")},l(n){t=w(n,"B")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function cr(c){let t;return{c(){t=_("E")},l(n){t=w(n,"E")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function ur(c){let t;return{c(){t=_("R")},l(n){t=w(n,"R")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function mr(c){let t;return{c(){t=_("T")},l(n){t=w(n,"T")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function $r(c){let t,n;return t=new S({props:{x:100,y:30+c[5]*50,width:160,height:30,fontSize:15,class:"fill-green-100",text:"Encoder Layer"}}),{c(){v(t.$$.fragment)},l(r){b(t.$$.fragment,r)},m(r,u){y(t,r,u),n=!0},p:L,i(r){n||(d(t.$$.fragment,r),n=!0)},o(r){g(t.$$.fragment,r),n=!1},d(r){E(t,r)}}}function dr(c){let t,n,r;n=new $e({props:{data:[{x:100,y:300},{x:100,y:5}],strokeWidth:1.6,dashed:!0,strokeDashArray:"4 4",moving:!0}});let u=Array(6),l=[];for(let a=0;a<u.length;a+=1)l[a]=$r(ir(c,u,a));return{c(){t=Me("svg"),v(n.$$.fragment);for(let a=0;a<l.length;a+=1)l[a].c();this.h()},l(a){t=Oe(a,"svg",{viewBox:!0});var s=k(t);b(n.$$.fragment,s);for(let o=0;o<l.length;o+=1)l[o].l(s);s.forEach(i),this.h()},h(){A(t,"viewBox","0 0 200 300")},m(a,s){m(a,t,s),y(n,t,null);for(let o=0;o<l.length;o+=1)l[o]&&l[o].m(t,null);r=!0},p:L,i(a){if(!r){d(n.$$.fragment,a);for(let s=0;s<u.length;s+=1)d(l[s]);r=!0}},o(a){g(n.$$.fragment,a),l=l.filter(Boolean);for(let s=0;s<l.length;s+=1)g(l[s]);r=!1},d(a){a&&i(t),E(n),oe(l,a)}}}function pr(c){let t;return{c(){t=_("BERT-Base")},l(n){t=w(n,"BERT-Base")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function gr(c){let t;return{c(){t=_("BERT-Large")},l(n){t=w(n,"BERT-Large")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function _r(c){let t,n,r,u,l;return t=new S({props:{x:30+c[5]*35,y:10,width:10,height:10,class:"fill-yellow-100"}}),n=new $e({props:{data:[{x:30+c[5]*35,y:55},{x:30+c[5]*35,y:25}],strokeWidth:1.5,dashed:!0,strokeDashArray:"4 4",moving:!0}}),r=new S({props:{x:30+c[5]*35,y:140,width:10,height:10,class:"fill-red-300"}}),u=new $e({props:{data:[{x:30+c[5]*35,y:130},{x:30+c[5]*35,y:100}],strokeWidth:1.5,dashed:!0,strokeDashArray:"4 4",moving:!0}}),{c(){v(t.$$.fragment),v(n.$$.fragment),v(r.$$.fragment),v(u.$$.fragment)},l(a){b(t.$$.fragment,a),b(n.$$.fragment,a),b(r.$$.fragment,a),b(u.$$.fragment,a)},m(a,s){y(t,a,s),y(n,a,s),y(r,a,s),y(u,a,s),l=!0},p:L,i(a){l||(d(t.$$.fragment,a),d(n.$$.fragment,a),d(r.$$.fragment,a),d(u.$$.fragment,a),l=!0)},o(a){g(t.$$.fragment,a),g(n.$$.fragment,a),g(r.$$.fragment,a),g(u.$$.fragment,a),l=!1},d(a){E(t,a),E(n,a),E(r,a),E(u,a)}}}function wr(c){let t,n,r,u,l=Array(5),a=[];for(let s=0;s<l.length;s+=1)a[s]=_r(lr(c,l,s));return r=new S({props:{x:100,y:75,width:160,height:30,fontSize:15,class:"fill-green-100",text:"Encoder Layer"}}),{c(){t=Me("svg");for(let s=0;s<a.length;s+=1)a[s].c();n=z(),v(r.$$.fragment),this.h()},l(s){t=Oe(s,"svg",{viewBox:!0});var o=k(t);for(let h=0;h<a.length;h+=1)a[h].l(o);n=z(),b(r.$$.fragment,o),o.forEach(i),this.h()},h(){A(t,"viewBox","0 0 200 150")},m(s,o){m(s,t,o);for(let h=0;h<a.length;h+=1)a[h]&&a[h].m(t,null);p(t,n),y(r,t,null),u=!0},p:L,i(s){if(!u){for(let o=0;o<l.length;o+=1)d(a[o]);d(r.$$.fragment,s),u=!0}},o(s){a=a.filter(Boolean);for(let o=0;o<a.length;o+=1)g(a[o]);g(r.$$.fragment,s),u=!1},d(s){s&&i(t),oe(a,s),E(r)}}}function kr(c){let t,n;return t=new $e({props:{data:[{x:25+c[5]*50,y:120},{x:25+c[10]*50,y:45}],showMarker:!1,dashed:!0,moving:!0,speed:50}}),{c(){v(t.$$.fragment)},l(r){b(t.$$.fragment,r)},m(r,u){y(t,r,u),n=!0},p:L,i(r){n||(d(t.$$.fragment,r),n=!0)},o(r){g(t.$$.fragment,r),n=!1},d(r){E(t,r)}}}function jn(c){let t,n,r,u;t=new S({props:{x:25+c[5]*50,y:130,width:20,height:20,text:"T_"+(c[5]+1),class:"fill-red-300"}}),n=new S({props:{x:25+c[5]*50,y:35,width:20,height:20,text:"T_"+(c[5]+1),class:"fill-yellow-100"}});let l=Array(6),a=[];for(let s=0;s<l.length;s+=1)a[s]=kr(or(c,l,s));return{c(){v(t.$$.fragment),v(n.$$.fragment);for(let s=0;s<a.length;s+=1)a[s].c();r=z()},l(s){b(t.$$.fragment,s),b(n.$$.fragment,s);for(let o=0;o<a.length;o+=1)a[o].l(s);r=z()},m(s,o){y(t,s,o),y(n,s,o);for(let h=0;h<a.length;h+=1)a[h]&&a[h].m(s,o);m(s,r,o),u=!0},p:L,i(s){if(!u){d(t.$$.fragment,s),d(n.$$.fragment,s);for(let o=0;o<l.length;o+=1)d(a[o]);u=!0}},o(s){g(t.$$.fragment,s),g(n.$$.fragment,s),a=a.filter(Boolean);for(let o=0;o<a.length;o+=1)g(a[o]);u=!1},d(s){E(t,s),E(n,s),oe(a,s),s&&i(r)}}}function vr(c){let t,n,r;n=new S({props:{x:40,y:10,width:50,height:12,text:"BERT Family"}});let u=Array(6),l=[];for(let s=0;s<u.length;s+=1)l[s]=jn(Gn(c,u,s));const a=s=>g(l[s],1,1,()=>{l[s]=null});return{c(){t=Me("svg"),v(n.$$.fragment);for(let s=0;s<l.length;s+=1)l[s].c();this.h()},l(s){t=Oe(s,"svg",{viewBox:!0});var o=k(t);b(n.$$.fragment,o);for(let h=0;h<l.length;h+=1)l[h].l(o);o.forEach(i),this.h()},h(){A(t,"viewBox","0 0 300 150")},m(s,o){m(s,t,o),y(n,t,null);for(let h=0;h<l.length;h+=1)l[h]&&l[h].m(t,null);r=!0},p(s,o){if(o&0){u=Array(6);let h;for(h=0;h<u.length;h+=1){const $=Gn(s,u,h);l[h]?(l[h].p($,o),d(l[h],1)):(l[h]=jn($),l[h].c(),d(l[h],1),l[h].m(t,null))}for(Lt(),h=u.length;h<l.length;h+=1)a(h);Kt()}},i(s){if(!r){d(n.$$.fragment,s);for(let o=0;o<u.length;o+=1)d(l[o]);r=!0}},o(s){g(n.$$.fragment,s),l=l.filter(Boolean);for(let o=0;o<l.length;o+=1)g(l[o]);r=!1},d(s){s&&i(t),E(n),oe(l,s)}}}function br(c){let t,n;return t=new $e({props:{data:[{x:25+c[5]*50,y:120},{x:25+c[10]*50,y:45}],showMarker:!1,dashed:!0,moving:!0,speed:50}}),{c(){v(t.$$.fragment)},l(r){b(t.$$.fragment,r)},m(r,u){y(t,r,u),n=!0},i(r){n||(d(t.$$.fragment,r),n=!0)},o(r){g(t.$$.fragment,r),n=!1},d(r){E(t,r)}}}function yr(c){let t,n,r=c[10]>=c[5]&&br(c);return{c(){r&&r.c(),t=z()},l(u){r&&r.l(u),t=z()},m(u,l){r&&r.m(u,l),m(u,t,l),n=!0},p:L,i(u){n||(d(r),n=!0)},o(u){g(r),n=!1},d(u){r&&r.d(u),u&&i(t)}}}function Jn(c){let t,n,r,u;t=new S({props:{x:25+c[5]*50,y:130,width:20,height:20,text:"T_"+(c[5]+1),class:"fill-red-300"}}),n=new S({props:{x:25+c[5]*50,y:35,width:20,height:20,text:"T_"+(c[5]+1),class:"fill-yellow-100"}});let l=Array(6),a=[];for(let s=0;s<l.length;s+=1)a[s]=yr(ar(c,l,s));return{c(){v(t.$$.fragment),v(n.$$.fragment);for(let s=0;s<a.length;s+=1)a[s].c();r=z()},l(s){b(t.$$.fragment,s),b(n.$$.fragment,s);for(let o=0;o<a.length;o+=1)a[o].l(s);r=z()},m(s,o){y(t,s,o),y(n,s,o);for(let h=0;h<a.length;h+=1)a[h]&&a[h].m(s,o);m(s,r,o),u=!0},p:L,i(s){if(!u){d(t.$$.fragment,s),d(n.$$.fragment,s);for(let o=0;o<l.length;o+=1)d(a[o]);u=!0}},o(s){g(t.$$.fragment,s),g(n.$$.fragment,s),a=a.filter(Boolean);for(let o=0;o<a.length;o+=1)g(a[o]);u=!1},d(s){E(t,s),E(n,s),oe(a,s),s&&i(r)}}}function Er(c){let t,n,r;n=new S({props:{x:40,y:10,width:50,height:12,text:"GPT Family"}});let u=Array(6),l=[];for(let s=0;s<u.length;s+=1)l[s]=Jn(Vn(c,u,s));const a=s=>g(l[s],1,1,()=>{l[s]=null});return{c(){t=Me("svg"),v(n.$$.fragment);for(let s=0;s<l.length;s+=1)l[s].c();this.h()},l(s){t=Oe(s,"svg",{viewBox:!0});var o=k(t);b(n.$$.fragment,o);for(let h=0;h<l.length;h+=1)l[h].l(o);o.forEach(i),this.h()},h(){A(t,"viewBox","0 0 300 150")},m(s,o){m(s,t,o),y(n,t,null);for(let h=0;h<l.length;h+=1)l[h]&&l[h].m(t,null);r=!0},p(s,o){if(o&0){u=Array(6);let h;for(h=0;h<u.length;h+=1){const $=Vn(s,u,h);l[h]?(l[h].p($,o),d(l[h],1)):(l[h]=Jn($),l[h].c(),d(l[h],1),l[h].m(t,null))}for(Lt(),h=u.length;h<l.length;h+=1)a(h);Kt()}},i(s){if(!r){d(n.$$.fragment,s);for(let o=0;o<u.length;o+=1)d(l[o]);r=!0}},o(s){g(n.$$.fragment,s),l=l.filter(Boolean);for(let o=0;o<l.length;o+=1)g(l[o]);r=!1},d(s){s&&i(t),E(n),oe(l,s)}}}function Tr(c){let t;return{c(){t=_("masked language model")},l(n){t=w(n,"masked language model")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function xr(c){let t;return{c(){t=_("next section prediction")},l(n){t=w(n,"next section prediction")},m(n,r){m(n,t,r)},d(n){n&&i(t)}}}function Qn(c){let t,n,r,u;return t=new S({props:{x:22+c[5]*51,y:90,width:38,height:16,text:c[3],fontSize:10,class:"fill-red-300"}}),n=new S({props:{x:22+c[5]*51,y:10,width:38,height:16,text:c[3],fontSize:10,class:"fill-yellow-100"}}),r=new $e({props:{data:[{x:22+c[5]*51,y:80},{x:22+c[5]*51,y:25}],strokeWidth:1.5}}),{c(){v(t.$$.fragment),v(n.$$.fragment),v(r.$$.fragment)},l(l){b(t.$$.fragment,l),b(n.$$.fragment,l),b(r.$$.fragment,l)},m(l,a){y(t,l,a),y(n,l,a),y(r,l,a),u=!0},p:L,i(l){u||(d(t.$$.fragment,l),d(n.$$.fragment,l),d(r.$$.fragment,l),u=!0)},o(l){g(t.$$.fragment,l),g(n.$$.fragment,l),g(r.$$.fragment,l),u=!1},d(l){E(t,l),E(n,l),E(r,l)}}}function Br(c){let t,n,r,u,l=c[0],a=[];for(let o=0;o<l.length;o+=1)a[o]=Qn(Nn(c,l,o));const s=o=>g(a[o],1,1,()=>{a[o]=null});return r=new S({props:{x:175,y:50,width:345,height:20,text:"Encoder Layers",class:"fill-green-100",fontSize:12}}),{c(){t=Me("svg");for(let o=0;o<a.length;o+=1)a[o].c();n=z(),v(r.$$.fragment),this.h()},l(o){t=Oe(o,"svg",{viewBox:!0});var h=k(t);for(let $=0;$<a.length;$+=1)a[$].l(h);n=z(),b(r.$$.fragment,h),h.forEach(i),this.h()},h(){A(t,"viewBox","0 0 350 100")},m(o,h){m(o,t,h);for(let $=0;$<a.length;$+=1)a[$]&&a[$].m(t,null);p(t,n),y(r,t,null),u=!0},p(o,h){if(h&1){l=o[0];let $;for($=0;$<l.length;$+=1){const K=Nn(o,l,$);a[$]?(a[$].p(K,h),d(a[$],1)):(a[$]=Qn(K),a[$].c(),d(a[$],1),a[$].m(t,n))}for(Lt(),$=l.length;$<a.length;$+=1)s($);Kt()}},i(o){if(!u){for(let h=0;h<l.length;h+=1)d(a[h]);d(r.$$.fragment,o),u=!0}},o(o){a=a.filter(Boolean);for(let h=0;h<a.length;h+=1)g(a[h]);g(r.$$.fragment,o),u=!1},d(o){o&&i(t),oe(a,o),E(r)}}}function Un(c){let t,n,r,u;return t=new S({props:{x:22+c[5]*51,y:90,width:38,height:16,text:c[3],fontSize:10,class:"fill-red-300"}}),n=new S({props:{x:22+c[5]*51,y:10,width:38,height:16,text:c[3],fontSize:10,class:c[5]===0?"fill-yellow-100":"fill-black"}}),r=new $e({props:{data:[{x:22+c[5]*51,y:80},{x:22+c[5]*51,y:25}],strokeWidth:1.5}}),{c(){v(t.$$.fragment),v(n.$$.fragment),v(r.$$.fragment)},l(l){b(t.$$.fragment,l),b(n.$$.fragment,l),b(r.$$.fragment,l)},m(l,a){y(t,l,a),y(n,l,a),y(r,l,a),u=!0},p:L,i(l){u||(d(t.$$.fragment,l),d(n.$$.fragment,l),d(r.$$.fragment,l),u=!0)},o(l){g(t.$$.fragment,l),g(n.$$.fragment,l),g(r.$$.fragment,l),u=!1},d(l){E(t,l),E(n,l),E(r,l)}}}function Pr(c){let t,n,r,u,l=c[1],a=[];for(let o=0;o<l.length;o+=1)a[o]=Un(Hn(c,l,o));const s=o=>g(a[o],1,1,()=>{a[o]=null});return r=new S({props:{x:175,y:50,width:345,height:20,text:"Encoder Layers",class:"fill-green-100",fontSize:12}}),{c(){t=Me("svg");for(let o=0;o<a.length;o+=1)a[o].c();n=z(),v(r.$$.fragment),this.h()},l(o){t=Oe(o,"svg",{viewBox:!0});var h=k(t);for(let $=0;$<a.length;$+=1)a[$].l(h);n=z(),b(r.$$.fragment,h),h.forEach(i),this.h()},h(){A(t,"viewBox","0 0 350 100")},m(o,h){m(o,t,h);for(let $=0;$<a.length;$+=1)a[$]&&a[$].m(t,null);p(t,n),y(r,t,null),u=!0},p(o,h){if(h&2){l=o[1];let $;for($=0;$<l.length;$+=1){const K=Hn(o,l,$);a[$]?(a[$].p(K,h),d(a[$],1)):(a[$]=Un(K),a[$].c(),d(a[$],1),a[$].m(t,n))}for(Lt(),$=l.length;$<a.length;$+=1)s($);Kt()}},i(o){if(!u){for(let h=0;h<l.length;h+=1)d(a[h]);d(r.$$.fragment,o),u=!0}},o(o){a=a.filter(Boolean);for(let h=0;h<a.length;h+=1)g(a[h]);g(r.$$.fragment,o),u=!1},d(o){o&&i(t),oe(a,o),E(r)}}}function Ar(c){let t,n,r,u,l,a,s,o,h,$,K,Ye,de,Ze,pe,Ft,et,R,Ct,H,Mt,N,Ot,V,Dt,G,Ht,tt,j,nt,q,Nt,J,Vt,Q,Gt,rt,ge,jt,st,U,at,X,Jt,De,Qt,Ut,ot,Y,lt,_e,Xt,it,Z,ft,we,ht,ke,Yt,ct,I,Zt,ee,en,te,tn,ut,ve,nn,mt,F,rn,be,sn,an,$t,ye,on,dt,Ee,ln,pt,Te,fn,gt,le,xe,hn,_t,ie,Be,cn,wt,Pe,un,kt,fe,Ae,mn,vt,he,Se,$n,bt,Re,dn,yt,We,pn,Et,ne,Tt,ze,gn,xt,qe,_n,Bt,Ie,Pt,Le,wn,At,Ke,kn,St,Fe,vn,Rt,re,Wt,se,bn,ae,yn,En,zt,ce,qt;return o=new D({props:{$$slots:{default:[fr]},$$scope:{ctx:c}}}),$=new rr({props:{id:1,type:"reference"}}),H=new D({props:{$$slots:{default:[hr]},$$scope:{ctx:c}}}),N=new D({props:{$$slots:{default:[cr]},$$scope:{ctx:c}}}),V=new D({props:{$$slots:{default:[ur]},$$scope:{ctx:c}}}),G=new D({props:{$$slots:{default:[mr]},$$scope:{ctx:c}}}),j=new Ce({props:{maxWidth:"300px",$$slots:{default:[dr]},$$scope:{ctx:c}}}),J=new D({props:{$$slots:{default:[pr]},$$scope:{ctx:c}}}),Q=new D({props:{$$slots:{default:[gr]},$$scope:{ctx:c}}}),U=new Ce({props:{maxWidth:"300px",$$slots:{default:[wr]},$$scope:{ctx:c}}}),Y=new Ce({props:{maxWidth:"600px",$$slots:{default:[vr]},$$scope:{ctx:c}}}),Z=new Ce({props:{maxWidth:"600px",$$slots:{default:[Er]},$$scope:{ctx:c}}}),ee=new D({props:{$$slots:{default:[Tr]},$$scope:{ctx:c}}}),te=new D({props:{$$slots:{default:[xr]},$$scope:{ctx:c}}}),ne=new Ce({props:{maxWidth:"500px",$$slots:{default:[Br]},$$scope:{ctx:c}}}),re=new Ce({props:{maxWidth:"500px",$$slots:{default:[Pr]},$$scope:{ctx:c}}}),ce=new sr({props:{code:`import numpy as np
import torch
from transformers import pipeline
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments
import evaluate

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
batch_size = 64
model_ckpt = "bert-base-uncased"
dataset_name = "sst2"

dataset = load_dataset(dataset_name)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
tokenize = lambda batch: tokenizer(batch["sentence"], padding=True, truncation=True)
tokenized_dataset = dataset.map(tokenize, batched=True, batch_size=None)

model = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to(device)

metric_name = "accuracy"
metric = evaluate.load(metric_name)


def compute_metrics(pred):
    logits, labels = pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


training_args = TrainingArguments(
    output_dir="bert",
    num_train_epochs=1,
    learning_rate=1e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    evaluation_strategy="epoch",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()`}}),{c(){t=T("h1"),n=_("BERT"),r=B(),u=T("div"),l=B(),a=T("p"),s=_(`When the original transformer paper was released in the year 2017, it was
    not clear what tremendous impact that architecture would have on deep
    learning. Slowly but surely researchers from different research labs begun
    to release transformer-based architectures and the more time passed, the
    more areas were conquered by transformers. One of the first models that
    garnered a lot of attention was `),v(o.$$.fragment),h=_(" from Google"),v($.$$.fragment),K=_(`. BERT is a pre-trained language model that allowes practitioners to
    fine-tune the model to their specific needs. Nowadays BERT (and its
    relatives) is de facto standard tool that is used for transfer learning in
    the area of natural language processing.`),Ye=B(),de=T("div"),Ze=B(),pe=T("h2"),Ft=_("Architecture"),et=B(),R=T("p"),Ct=_("BERT is short for "),v(H.$$.fragment),Mt=_("iderectional "),v(N.$$.fragment),Ot=_("ncoder "),v(V.$$.fragment),Dt=_("epresentation from "),v(G.$$.fragment),Ht=_(`ransformers. We can infer from the name, that the model architecture
    consists solely from a stack of transformer encoders, without a decoder.`),tt=B(),v(j.$$.fragment),nt=B(),q=T("p"),Nt=_("The original BERT paper introduced two models. The "),v(J.$$.fragment),Vt=_(` model consists of 12 encoder layers, each with 12 attention heads and 768 hidden
    units for each token. `),v(Q.$$.fragment),Gt=_(` on the other hand uses
    24 layers with 16 heads and 1024 hidden units.`),rt=B(),ge=T("p"),jt=_(`Each layer takes a certain amount of tokens and outputs the same number of
    tokens. The number of tokens never changes between the encoder layers.`),st=B(),v(U.$$.fragment),at=B(),X=T("p"),Jt=_("The "),De=T("em"),Qt=_("biderectional"),Ut=_(` part means, that the encoder can pay attention to
    all tokens contained in the sequence. For that reason BERT is primarily used
    for tasks, that have access to the full sequence at inference time. For example
    in a classification task we process the whole sequence in order to classify the
    sentence.`),ot=B(),v(Y.$$.fragment),lt=B(),_e=T("p"),Xt=_(`In the next section we will additionally encouter the GPT family of models,
    that are created by stacking transformer decoders. A GPT like decoder
    creates output tokens, based on all current or previous input
    embeddings/tokens.`),it=B(),v(Z.$$.fragment),ft=B(),we=T("div"),ht=B(),ke=T("h2"),Yt=_("Pre-Training"),ct=B(),I=T("p"),Zt=_("BERT was pre-trained jointly on two different objectives: "),v(ee.$$.fragment),en=_(" and "),v(te.$$.fragment),tn=_("."),ut=B(),ve=T("p"),nn=_(`In a masked language model, we replace some parts of the original sequence
    randomly with a special [MASK] token and the model has to predict the
    missing word. Let's for example look at the below sentence to understand,
    why this task matters for pretraining.`),mt=B(),F=T("p"),rn=_("today i went to a "),be=T("span"),sn=_("[MASK]"),an=_(" to get my hair done"),$t=B(),ye=T("p"),on=_(`While you probably have a couple of options to fill out the masked word,
    your options are still limited by logic and the rules of the english
    language. The word "hairdresser" is probably the most likely option, but the
    word "salon" or even "friend" are also valid options. When a model learns to
    replace [MASK] by a valid word, that shows that in the very least, the model
    learned the basic statistics that govern the english language. And those
    statistics are useful not only for the task at hand, but also for many other
    tasks that require natural language understanding.`),dt=B(),Ee=T("p"),ln=_(`In the next section prediction task, the model faces two sequences and has
    to predict if the second sequence is the logical continuation of the first
    sequence.`),pt=B(),Te=T("p"),fn=_(`The two sentences below seem to have a logical connection, so we expect the
    model to return true.`),gt=B(),le=T("div"),xe=T("span"),hn=_("a man went to the store"),_t=B(),ie=T("div"),Be=T("span"),cn=_("he bought a gallon of milk"),wt=B(),Pe=T("p"),un=_(`The next two sentences on the other hand are unrelated and the model should
    return false.`),kt=B(),fe=T("div"),Ae=T("span"),mn=_("a man went to the store"),vt=B(),he=T("div"),Se=T("span"),$n=_("penguins are flightless birds"),bt=B(),Re=T("p"),dn=_(`Similarly to the masked model, solving this task is related to understanding
    the english language. Once a model is competent at solving this task, we can
    assume that it has learned some important statistics of the language and
    those statistics can be used for downstream tasks.`),yt=B(),We=T("p"),pn=_(`Let's for a second assume, that we are training on two sentences, each
    consisting of 2 words (2 tokens).`),Et=B(),v(ne.$$.fragment),Tt=B(),ze=T("p"),gn=_(`We first prepend the two sentences with the [CLS] token. Once this token is
    processed by a stack of encoders, it is used for the binary classification
    to determine if the second sentence should follow the first sentence: the
    next section prediction task. We separate the two sentences by the [SEP]
    token and additionally append this token at the end of the sentence. We mask
    out some of the words for the masked language model. Those masked tokens are
    also processed by layers of encoders and are used to predict the correct
    token that was masked out. Both losses are aggregated for the gradient
    descent step.`),xt=B(),qe=T("p"),_n=_(`It is important to mention that both those tasks are trained using
    self-supervised learning. We do not require anyone to collect and label a
    dataset. We can for example use Wikipedia articles and pick out random
    consecutive sentences and mask out some of the tokens. This gives us a huge
    dataset for pre-training.`),Bt=B(),Ie=T("div"),Pt=B(),Le=T("h2"),wn=_("Fine-Tuning"),At=B(),Ke=T("p"),kn=_(`When we have a labeled language dataset, we can use BERT for fine-tuning.
    BERT can be used How the pre-trained BERT model can be used for fine-tuning
    depends on the task at hand.`),St=B(),Fe=T("p"),vn=_(`Let's assume we are dealing with a classification task, like sentiment
    analysis. The first token, CLS, is the start token and is generally used for
    classification tasks. We can use the embedding for the class token from the
    last encoder layer as an input into a classification layer and ignore the
    rest.`),Rt=B(),v(re.$$.fragment),Wt=B(),se=T("p"),bn=_(`BERT is designed for fine-tuning, so it makes no sense to train the model
    from scratch. Instead we will use the pre-trained BERT weights to solve our
    task at hand. Nowadays the most efficient way to use BERT is with the help
    of the `),ae=T("a"),yn=_("🤗HuggingFace"),En=_(`. HuggingFace includes models, pretrained weights, datasets and much more.
    We will make heavy use of it in future, and not only for natural language
    processing.`),zt=B(),v(ce.$$.fragment),this.h()},l(e){t=x(e,"H1",{});var f=k(t);n=w(f,"BERT"),f.forEach(i),r=P(e),u=x(e,"DIV",{class:!0}),k(u).forEach(i),l=P(e),a=x(e,"P",{});var C=k(a);s=w(C,`When the original transformer paper was released in the year 2017, it was
    not clear what tremendous impact that architecture would have on deep
    learning. Slowly but surely researchers from different research labs begun
    to release transformer-based architectures and the more time passed, the
    more areas were conquered by transformers. One of the first models that
    garnered a lot of attention was `),b(o.$$.fragment,C),h=w(C," from Google"),b($.$$.fragment,C),K=w(C,`. BERT is a pre-trained language model that allowes practitioners to
    fine-tune the model to their specific needs. Nowadays BERT (and its
    relatives) is de facto standard tool that is used for transfer learning in
    the area of natural language processing.`),C.forEach(i),Ye=P(e),de=x(e,"DIV",{class:!0}),k(de).forEach(i),Ze=P(e),pe=x(e,"H2",{});var He=k(pe);Ft=w(He,"Architecture"),He.forEach(i),et=P(e),R=x(e,"P",{});var W=k(R);Ct=w(W,"BERT is short for "),b(H.$$.fragment,W),Mt=w(W,"iderectional "),b(N.$$.fragment,W),Ot=w(W,"ncoder "),b(V.$$.fragment,W),Dt=w(W,"epresentation from "),b(G.$$.fragment,W),Ht=w(W,`ransformers. We can infer from the name, that the model architecture
    consists solely from a stack of transformer encoders, without a decoder.`),W.forEach(i),tt=P(e),b(j.$$.fragment,e),nt=P(e),q=x(e,"P",{});var M=k(q);Nt=w(M,"The original BERT paper introduced two models. The "),b(J.$$.fragment,M),Vt=w(M,` model consists of 12 encoder layers, each with 12 attention heads and 768 hidden
    units for each token. `),b(Q.$$.fragment,M),Gt=w(M,` on the other hand uses
    24 layers with 16 heads and 1024 hidden units.`),M.forEach(i),rt=P(e),ge=x(e,"P",{});var Ne=k(ge);jt=w(Ne,`Each layer takes a certain amount of tokens and outputs the same number of
    tokens. The number of tokens never changes between the encoder layers.`),Ne.forEach(i),st=P(e),b(U.$$.fragment,e),at=P(e),X=x(e,"P",{});var ue=k(X);Jt=w(ue,"The "),De=x(ue,"EM",{});var Ve=k(De);Qt=w(Ve,"biderectional"),Ve.forEach(i),Ut=w(ue,` part means, that the encoder can pay attention to
    all tokens contained in the sequence. For that reason BERT is primarily used
    for tasks, that have access to the full sequence at inference time. For example
    in a classification task we process the whole sequence in order to classify the
    sentence.`),ue.forEach(i),ot=P(e),b(Y.$$.fragment,e),lt=P(e),_e=x(e,"P",{});var Ge=k(_e);Xt=w(Ge,`In the next section we will additionally encouter the GPT family of models,
    that are created by stacking transformer decoders. A GPT like decoder
    creates output tokens, based on all current or previous input
    embeddings/tokens.`),Ge.forEach(i),it=P(e),b(Z.$$.fragment,e),ft=P(e),we=x(e,"DIV",{class:!0}),k(we).forEach(i),ht=P(e),ke=x(e,"H2",{});var je=k(ke);Yt=w(je,"Pre-Training"),je.forEach(i),ct=P(e),I=x(e,"P",{});var O=k(I);Zt=w(O,"BERT was pre-trained jointly on two different objectives: "),b(ee.$$.fragment,O),en=w(O," and "),b(te.$$.fragment,O),tn=w(O,"."),O.forEach(i),ut=P(e),ve=x(e,"P",{});var Je=k(ve);nn=w(Je,`In a masked language model, we replace some parts of the original sequence
    randomly with a special [MASK] token and the model has to predict the
    missing word. Let's for example look at the below sentence to understand,
    why this task matters for pretraining.`),Je.forEach(i),mt=P(e),F=x(e,"P",{class:!0});var me=k(F);rn=w(me,"today i went to a "),be=x(me,"SPAN",{class:!0});var Qe=k(be);sn=w(Qe,"[MASK]"),Qe.forEach(i),an=w(me," to get my hair done"),me.forEach(i),$t=P(e),ye=x(e,"P",{});var Ue=k(ye);on=w(Ue,`While you probably have a couple of options to fill out the masked word,
    your options are still limited by logic and the rules of the english
    language. The word "hairdresser" is probably the most likely option, but the
    word "salon" or even "friend" are also valid options. When a model learns to
    replace [MASK] by a valid word, that shows that in the very least, the model
    learned the basic statistics that govern the english language. And those
    statistics are useful not only for the task at hand, but also for many other
    tasks that require natural language understanding.`),Ue.forEach(i),dt=P(e),Ee=x(e,"P",{});var Xe=k(Ee);ln=w(Xe,`In the next section prediction task, the model faces two sequences and has
    to predict if the second sequence is the logical continuation of the first
    sequence.`),Xe.forEach(i),pt=P(e),Te=x(e,"P",{});var Tn=k(Te);fn=w(Tn,`The two sentences below seem to have a logical connection, so we expect the
    model to return true.`),Tn.forEach(i),gt=P(e),le=x(e,"DIV",{class:!0});var xn=k(le);xe=x(xn,"SPAN",{class:!0});var Bn=k(xe);hn=w(Bn,"a man went to the store"),Bn.forEach(i),xn.forEach(i),_t=P(e),ie=x(e,"DIV",{class:!0});var Pn=k(ie);Be=x(Pn,"SPAN",{class:!0});var An=k(Be);cn=w(An,"he bought a gallon of milk"),An.forEach(i),Pn.forEach(i),wt=P(e),Pe=x(e,"P",{});var Sn=k(Pe);un=w(Sn,`The next two sentences on the other hand are unrelated and the model should
    return false.`),Sn.forEach(i),kt=P(e),fe=x(e,"DIV",{class:!0});var Rn=k(fe);Ae=x(Rn,"SPAN",{class:!0});var Wn=k(Ae);mn=w(Wn,"a man went to the store"),Wn.forEach(i),Rn.forEach(i),vt=P(e),he=x(e,"DIV",{class:!0});var zn=k(he);Se=x(zn,"SPAN",{class:!0});var qn=k(Se);$n=w(qn,"penguins are flightless birds"),qn.forEach(i),zn.forEach(i),bt=P(e),Re=x(e,"P",{});var In=k(Re);dn=w(In,`Similarly to the masked model, solving this task is related to understanding
    the english language. Once a model is competent at solving this task, we can
    assume that it has learned some important statistics of the language and
    those statistics can be used for downstream tasks.`),In.forEach(i),yt=P(e),We=x(e,"P",{});var Ln=k(We);pn=w(Ln,`Let's for a second assume, that we are training on two sentences, each
    consisting of 2 words (2 tokens).`),Ln.forEach(i),Et=P(e),b(ne.$$.fragment,e),Tt=P(e),ze=x(e,"P",{});var Kn=k(ze);gn=w(Kn,`We first prepend the two sentences with the [CLS] token. Once this token is
    processed by a stack of encoders, it is used for the binary classification
    to determine if the second sentence should follow the first sentence: the
    next section prediction task. We separate the two sentences by the [SEP]
    token and additionally append this token at the end of the sentence. We mask
    out some of the words for the masked language model. Those masked tokens are
    also processed by layers of encoders and are used to predict the correct
    token that was masked out. Both losses are aggregated for the gradient
    descent step.`),Kn.forEach(i),xt=P(e),qe=x(e,"P",{});var Fn=k(qe);_n=w(Fn,`It is important to mention that both those tasks are trained using
    self-supervised learning. We do not require anyone to collect and label a
    dataset. We can for example use Wikipedia articles and pick out random
    consecutive sentences and mask out some of the tokens. This gives us a huge
    dataset for pre-training.`),Fn.forEach(i),Bt=P(e),Ie=x(e,"DIV",{class:!0}),k(Ie).forEach(i),Pt=P(e),Le=x(e,"H2",{});var Cn=k(Le);wn=w(Cn,"Fine-Tuning"),Cn.forEach(i),At=P(e),Ke=x(e,"P",{});var Mn=k(Ke);kn=w(Mn,`When we have a labeled language dataset, we can use BERT for fine-tuning.
    BERT can be used How the pre-trained BERT model can be used for fine-tuning
    depends on the task at hand.`),Mn.forEach(i),St=P(e),Fe=x(e,"P",{});var On=k(Fe);vn=w(On,`Let's assume we are dealing with a classification task, like sentiment
    analysis. The first token, CLS, is the start token and is generally used for
    classification tasks. We can use the embedding for the class token from the
    last encoder layer as an input into a classification layer and ignore the
    rest.`),On.forEach(i),Rt=P(e),b(re.$$.fragment,e),Wt=P(e),se=x(e,"P",{});var It=k(se);bn=w(It,`BERT is designed for fine-tuning, so it makes no sense to train the model
    from scratch. Instead we will use the pre-trained BERT weights to solve our
    task at hand. Nowadays the most efficient way to use BERT is with the help
    of the `),ae=x(It,"A",{href:!0,target:!0,rel:!0});var Dn=k(ae);yn=w(Dn,"🤗HuggingFace"),Dn.forEach(i),En=w(It,`. HuggingFace includes models, pretrained weights, datasets and much more.
    We will make heavy use of it in future, and not only for natural language
    processing.`),It.forEach(i),zt=P(e),b(ce.$$.fragment,e),this.h()},h(){A(u,"class","separator"),A(de,"class","separator"),A(we,"class","separator"),A(be,"class","inline-block py-1 px-2 bg-lime-100 font-bold border border-black"),A(F,"class","text-center"),A(xe,"class","inline-text py-1 px-2 bg-lime-100 border border-black"),A(le,"class","text-center font-bold mb-4"),A(Be,"class","inline-text py-1 px-2 bg-lime-100 border border-black"),A(ie,"class","text-center font-bold"),A(Ae,"class","inline-text py-1 px-2 bg-lime-100 border border-black"),A(fe,"class","text-center font-bold mb-4"),A(Se,"class","inline-text py-1 px-2 bg-red-100 border border-black"),A(he,"class","text-center font-bold"),A(Ie,"class","separator"),A(ae,"href","https://huggingface.co/"),A(ae,"target","_blank"),A(ae,"rel","noreferrer")},m(e,f){m(e,t,f),p(t,n),m(e,r,f),m(e,u,f),m(e,l,f),m(e,a,f),p(a,s),y(o,a,null),p(a,h),y($,a,null),p(a,K),m(e,Ye,f),m(e,de,f),m(e,Ze,f),m(e,pe,f),p(pe,Ft),m(e,et,f),m(e,R,f),p(R,Ct),y(H,R,null),p(R,Mt),y(N,R,null),p(R,Ot),y(V,R,null),p(R,Dt),y(G,R,null),p(R,Ht),m(e,tt,f),y(j,e,f),m(e,nt,f),m(e,q,f),p(q,Nt),y(J,q,null),p(q,Vt),y(Q,q,null),p(q,Gt),m(e,rt,f),m(e,ge,f),p(ge,jt),m(e,st,f),y(U,e,f),m(e,at,f),m(e,X,f),p(X,Jt),p(X,De),p(De,Qt),p(X,Ut),m(e,ot,f),y(Y,e,f),m(e,lt,f),m(e,_e,f),p(_e,Xt),m(e,it,f),y(Z,e,f),m(e,ft,f),m(e,we,f),m(e,ht,f),m(e,ke,f),p(ke,Yt),m(e,ct,f),m(e,I,f),p(I,Zt),y(ee,I,null),p(I,en),y(te,I,null),p(I,tn),m(e,ut,f),m(e,ve,f),p(ve,nn),m(e,mt,f),m(e,F,f),p(F,rn),p(F,be),p(be,sn),p(F,an),m(e,$t,f),m(e,ye,f),p(ye,on),m(e,dt,f),m(e,Ee,f),p(Ee,ln),m(e,pt,f),m(e,Te,f),p(Te,fn),m(e,gt,f),m(e,le,f),p(le,xe),p(xe,hn),m(e,_t,f),m(e,ie,f),p(ie,Be),p(Be,cn),m(e,wt,f),m(e,Pe,f),p(Pe,un),m(e,kt,f),m(e,fe,f),p(fe,Ae),p(Ae,mn),m(e,vt,f),m(e,he,f),p(he,Se),p(Se,$n),m(e,bt,f),m(e,Re,f),p(Re,dn),m(e,yt,f),m(e,We,f),p(We,pn),m(e,Et,f),y(ne,e,f),m(e,Tt,f),m(e,ze,f),p(ze,gn),m(e,xt,f),m(e,qe,f),p(qe,_n),m(e,Bt,f),m(e,Ie,f),m(e,Pt,f),m(e,Le,f),p(Le,wn),m(e,At,f),m(e,Ke,f),p(Ke,kn),m(e,St,f),m(e,Fe,f),p(Fe,vn),m(e,Rt,f),y(re,e,f),m(e,Wt,f),m(e,se,f),p(se,bn),p(se,ae),p(ae,yn),p(se,En),m(e,zt,f),y(ce,e,f),qt=!0},p(e,f){const C={};f&32768&&(C.$$scope={dirty:f,ctx:e}),o.$set(C);const He={};f&32768&&(He.$$scope={dirty:f,ctx:e}),H.$set(He);const W={};f&32768&&(W.$$scope={dirty:f,ctx:e}),N.$set(W);const M={};f&32768&&(M.$$scope={dirty:f,ctx:e}),V.$set(M);const Ne={};f&32768&&(Ne.$$scope={dirty:f,ctx:e}),G.$set(Ne);const ue={};f&32768&&(ue.$$scope={dirty:f,ctx:e}),j.$set(ue);const Ve={};f&32768&&(Ve.$$scope={dirty:f,ctx:e}),J.$set(Ve);const Ge={};f&32768&&(Ge.$$scope={dirty:f,ctx:e}),Q.$set(Ge);const je={};f&32768&&(je.$$scope={dirty:f,ctx:e}),U.$set(je);const O={};f&32768&&(O.$$scope={dirty:f,ctx:e}),Y.$set(O);const Je={};f&32768&&(Je.$$scope={dirty:f,ctx:e}),Z.$set(Je);const me={};f&32768&&(me.$$scope={dirty:f,ctx:e}),ee.$set(me);const Qe={};f&32768&&(Qe.$$scope={dirty:f,ctx:e}),te.$set(Qe);const Ue={};f&32768&&(Ue.$$scope={dirty:f,ctx:e}),ne.$set(Ue);const Xe={};f&32768&&(Xe.$$scope={dirty:f,ctx:e}),re.$set(Xe)},i(e){qt||(d(o.$$.fragment,e),d($.$$.fragment,e),d(H.$$.fragment,e),d(N.$$.fragment,e),d(V.$$.fragment,e),d(G.$$.fragment,e),d(j.$$.fragment,e),d(J.$$.fragment,e),d(Q.$$.fragment,e),d(U.$$.fragment,e),d(Y.$$.fragment,e),d(Z.$$.fragment,e),d(ee.$$.fragment,e),d(te.$$.fragment,e),d(ne.$$.fragment,e),d(re.$$.fragment,e),d(ce.$$.fragment,e),qt=!0)},o(e){g(o.$$.fragment,e),g($.$$.fragment,e),g(H.$$.fragment,e),g(N.$$.fragment,e),g(V.$$.fragment,e),g(G.$$.fragment,e),g(j.$$.fragment,e),g(J.$$.fragment,e),g(Q.$$.fragment,e),g(U.$$.fragment,e),g(Y.$$.fragment,e),g(Z.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(ne.$$.fragment,e),g(re.$$.fragment,e),g(ce.$$.fragment,e),qt=!1},d(e){e&&i(t),e&&i(r),e&&i(u),e&&i(l),e&&i(a),E(o),E($),e&&i(Ye),e&&i(de),e&&i(Ze),e&&i(pe),e&&i(et),e&&i(R),E(H),E(N),E(V),E(G),e&&i(tt),E(j,e),e&&i(nt),e&&i(q),E(J),E(Q),e&&i(rt),e&&i(ge),e&&i(st),E(U,e),e&&i(at),e&&i(X),e&&i(ot),E(Y,e),e&&i(lt),e&&i(_e),e&&i(it),E(Z,e),e&&i(ft),e&&i(we),e&&i(ht),e&&i(ke),e&&i(ct),e&&i(I),E(ee),E(te),e&&i(ut),e&&i(ve),e&&i(mt),e&&i(F),e&&i($t),e&&i(ye),e&&i(dt),e&&i(Ee),e&&i(pt),e&&i(Te),e&&i(gt),e&&i(le),e&&i(_t),e&&i(ie),e&&i(wt),e&&i(Pe),e&&i(kt),e&&i(fe),e&&i(vt),e&&i(he),e&&i(bt),e&&i(Re),e&&i(yt),e&&i(We),e&&i(Et),E(ne,e),e&&i(Tt),e&&i(ze),e&&i(xt),e&&i(qe),e&&i(Bt),e&&i(Ie),e&&i(Pt),e&&i(Le),e&&i(At),e&&i(Ke),e&&i(St),e&&i(Fe),e&&i(Rt),E(re,e),e&&i(Wt),e&&i(se),e&&i(zt),E(ce,e)}}}function Sr(c){let t,n,r,u,l,a;return r=new tr({props:{$$slots:{default:[Ar]},$$scope:{ctx:c}}}),l=new nr({props:{references:c[2]}}),{c(){t=T("meta"),n=B(),v(r.$$.fragment),u=B(),v(l.$$.fragment),this.h()},l(s){const o=er("svelte-1c05h5m",document.head);t=x(o,"META",{name:!0,content:!0}),o.forEach(i),n=P(s),b(r.$$.fragment,s),u=P(s),b(l.$$.fragment,s),this.h()},h(){document.title="BERT - World4AI",A(t,"name","description"),A(t,"content","BERT (short for biderectional encoder representation from transformers) is a pretrained language model, that can used for different fine-tuning tasks, like text summarization, sentiment analysis and much more. BERT (and its relatives) has become the de facto standard transfer learning tool for natural language processing.")},m(s,o){p(document.head,t),m(s,n,o),y(r,s,o),m(s,u,o),y(l,s,o),a=!0},p(s,[o]){const h={};o&32768&&(h.$$scope={dirty:o,ctx:s}),r.$set(h)},i(s){a||(d(r.$$.fragment,s),d(l.$$.fragment,s),a=!0)},o(s){g(r.$$.fragment,s),g(l.$$.fragment,s),a=!1},d(s){i(t),s&&i(n),E(r,s),s&&i(u),E(l,s)}}}function Rr(c){return[["[CLS]","[MASK]","TOK 2","[SEP]","TOK 1","TOK 2","[SEP]"],["[CLS]","TOK 1","TOK 2","TOK 3","TOK 4","TOK 5","[SEP]"],[{author:"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",title:"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",year:"2018"}]]}class Mr extends Xn{constructor(t){super(),Yn(this,t,Rr,Sr,Zn,{})}}export{Mr as default};
