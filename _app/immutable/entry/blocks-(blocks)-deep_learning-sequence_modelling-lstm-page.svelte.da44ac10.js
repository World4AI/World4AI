import{S as Fr,i as Rr,s as Vr,k as S,a as x,q as f,y as u,W as Or,l as E,h as a,c as k,m as T,r as i,z as p,n as N,N as v,b as l,A as g,g as m,d as h,B as c,C as A,Q as nn,R as rn,P as Hr}from"../chunks/index.4d92b023.js";import{C as Gr}from"../chunks/Container.b0705c7b.js";import{H as Xe}from"../chunks/Highlight.b7c1de53.js";import{L as I}from"../chunks/Latex.e0b308c0.js";import{F as Ur,I as Yr}from"../chunks/InternalLink.7deb899c.js";import{P as tn}from"../chunks/PythonCode.212ba7a6.js";import{S as On}from"../chunks/SvgContainer.f70b5745.js";import{B}from"../chunks/Block.059eddcd.js";import{A as H}from"../chunks/Arrow.ae91874c.js";import{P as Jr}from"../chunks/Plus.fc904b16.js";import{M as qr}from"../chunks/Multiply.cec66028.js";import{C as Qr}from"../chunks/Circle.6709e74f.js";function Kr($,n,t){const r=$.slice();return r[1]=n[t],r[3]=t,r}function Xr($){let n;return{c(){n=f("Computers")},l(t){n=i(t,"Computers")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function Zr($){let n;return{c(){n=f("-----------")},l(t){n=i(t,"-----------")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function ea($){let n;return{c(){n=f("long term dependencies")},l(t){n=i(t,"long term dependencies")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function ta($){let n;return{c(){n=f("LSTM")},l(t){n=i(t,"LSTM")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function na($){let n=String.raw`\mathbf{h}_t`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function ra($){let n=String.raw`\mathbf{c_t}`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function aa($){let n,t,r,o,y,w,b,W,P,L,j;return t=new H({props:{strokeWidth:"2",data:[{x:31,y:45},{x:76,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),r=new H({props:{strokeWidth:"2",data:[{x:120,y:45},{x:164,y:45}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),o=new H({props:{strokeWidth:"2",data:[{x:87,y:62},{x:87,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),y=new H({props:{strokeWidth:"2",data:[{x:112,y:62},{x:112,y:140}],dashed:!0,moving:!0,strokeDashArray:"4 4"}}),w=new B({props:{x:"100",y:"45",width:"30",height:"30",class:"fill-slate-500"}}),b=new B({props:{text:"x_"+($[3]+1),type:"latex",fontSize:12,x:"15",y:"45",width:"25",height:"25",class:"fill-blue-100"}}),W=new B({props:{type:"latex",text:"y_"+($[3]+1),fontSize:12,x:"185",y:"45",width:"25",height:"25",class:"fill-blue-100"}}),P=new B({props:{type:"latex",text:"h_"+($[3]+1),fontSize:12,x:"80",y:"100",width:"25",height:"25",class:"fill-yellow-100"}}),L=new B({props:{type:"latex",text:"c_"+($[3]+1),fontSize:12,x:"115",y:"100",width:"25",height:"25",class:"fill-red-100"}}),{c(){n=nn("g"),u(t.$$.fragment),u(r.$$.fragment),u(o.$$.fragment),u(y.$$.fragment),u(w.$$.fragment),u(b.$$.fragment),u(W.$$.fragment),u(P.$$.fragment),u(L.$$.fragment),this.h()},l(d){n=rn(d,"g",{transform:!0});var M=T(n);p(t.$$.fragment,M),p(r.$$.fragment,M),p(o.$$.fragment,M),p(y.$$.fragment,M),p(w.$$.fragment,M),p(b.$$.fragment,M),p(W.$$.fragment,M),p(P.$$.fragment,M),p(L.$$.fragment,M),M.forEach(a),this.h()},h(){N(n,"transform","translate(0, "+($[3]*120-20)+")")},m(d,M){l(d,n,M),g(t,n,null),g(r,n,null),g(o,n,null),g(y,n,null),g(w,n,null),g(b,n,null),g(W,n,null),g(P,n,null),g(L,n,null),j=!0},p:A,i(d){j||(m(t.$$.fragment,d),m(r.$$.fragment,d),m(o.$$.fragment,d),m(y.$$.fragment,d),m(w.$$.fragment,d),m(b.$$.fragment,d),m(W.$$.fragment,d),m(P.$$.fragment,d),m(L.$$.fragment,d),j=!0)},o(d){h(t.$$.fragment,d),h(r.$$.fragment,d),h(o.$$.fragment,d),h(y.$$.fragment,d),h(w.$$.fragment,d),h(b.$$.fragment,d),h(W.$$.fragment,d),h(P.$$.fragment,d),h(L.$$.fragment,d),j=!1},d(d){d&&a(n),c(t),c(r),c(o),c(y),c(w),c(b),c(W),c(P),c(L)}}}function sa($){let n,t,r=Array(4),o=[];for(let y=0;y<r.length;y+=1)o[y]=aa(Kr($,r,y));return{c(){n=nn("svg");for(let y=0;y<o.length;y+=1)o[y].c();this.h()},l(y){n=rn(y,"svg",{viewBox:!0});var w=T(n);for(let b=0;b<o.length;b+=1)o[b].l(w);w.forEach(a),this.h()},h(){N(n,"viewBox","0 0 200 470")},m(y,w){l(y,n,w);for(let b=0;b<o.length;b+=1)o[b]&&o[b].m(n,null);t=!0},p:A,i(y){if(!t){for(let w=0;w<r.length;w+=1)m(o[w]);t=!0}},o(y){o=o.filter(Boolean);for(let w=0;w<o.length;w+=1)h(o[w]);t=!1},d(y){y&&a(n),Hr(o,y)}}}function la($){let n;return{c(){n=f("gates")},l(t){n=i(t,"gates")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function oa($){let n,t,r,o,y;return t=new H({props:{data:[{x:0,y:50},{x:230,y:50}],strokeWidth:"4",dashed:"true",strokeDashArray:"8 8",moving:"true",speed:20}}),r=new B({props:{x:"250",y:"25",width:"10",height:"50",class:"fill-slate-400"}}),o=new B({props:{x:"250",y:"75",width:"10",height:"50",class:"fill-slate-400"}}),{c(){n=nn("svg"),u(t.$$.fragment),u(r.$$.fragment),u(o.$$.fragment),this.h()},l(w){n=rn(w,"svg",{viewBox:!0});var b=T(n);p(t.$$.fragment,b),p(r.$$.fragment,b),p(o.$$.fragment,b),b.forEach(a),this.h()},h(){N(n,"viewBox","0 0 500 100")},m(w,b){l(w,n,b),g(t,n,null),g(r,n,null),g(o,n,null),y=!0},p:A,i(w){y||(m(t.$$.fragment,w),m(r.$$.fragment,w),m(o.$$.fragment,w),y=!0)},o(w){h(t.$$.fragment,w),h(r.$$.fragment,w),h(o.$$.fragment,w),y=!1},d(w){w&&a(n),c(t),c(r),c(o)}}}function fa($){let n,t,r,o,y;return t=new H({props:{data:[{x:0,y:50},{x:480,y:50}],strokeWidth:"4",dashed:"true",strokeDashArray:"8 8",moving:"true",speed:20}}),r=new B({props:{x:"250",y:"15",width:"10",height:"30",class:"fill-slate-400"}}),o=new B({props:{x:"250",y:"85",width:"10",height:"30",class:"fill-slate-400"}}),{c(){n=nn("svg"),u(t.$$.fragment),u(r.$$.fragment),u(o.$$.fragment),this.h()},l(w){n=rn(w,"svg",{viewBox:!0});var b=T(n);p(t.$$.fragment,b),p(r.$$.fragment,b),p(o.$$.fragment,b),b.forEach(a),this.h()},h(){N(n,"viewBox","0 0 500 100")},m(w,b){l(w,n,b),g(t,n,null),g(r,n,null),g(o,n,null),y=!0},p:A,i(w){y||(m(t.$$.fragment,w),m(r.$$.fragment,w),m(o.$$.fragment,w),y=!0)},o(w){h(t.$$.fragment,w),h(r.$$.fragment,w),h(o.$$.fragment,w),y=!1},d(w){w&&a(n),c(t),c(r),c(o)}}}function ia($){let n=String.raw`
        \text{Data} = 
        \begin{bmatrix}
          2 \\
          5 \\ 
        \end{bmatrix}
      `+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function $a($){let n=String.raw`
        \text{Gate} = 
        \begin{bmatrix}
          1 \\
          0 \\ 
        \end{bmatrix}
      `+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function ma($){let n=String.raw`
        \begin{bmatrix}
          2 \\
          5 \\ 
        \end{bmatrix} \odot
        \begin{bmatrix}
          1 \\
          0 \\ 
        \end{bmatrix} = 
        \begin{bmatrix}
          2 \\
          0 \\ 
        \end{bmatrix}
      `+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function ha($){let n,t,r,o,y,w,b,W,P,L,j,d,M,C,Y,X,Z,R,ee,K,F,te,V,O,ne,le;return t=new B({props:{fontSize:12,x:280,y:20,width:30,height:30,text:"\\mathbf{c}_{t-1}",type:"latex",class:"fill-green-100"}}),r=new B({props:{fontSize:12,x:80,y:20,width:30,height:30,text:"\\mathbf{h}_{t-1}",type:"latex",class:"fill-green-100"}}),o=new B({props:{fontSize:12,x:20,y:60,width:30,height:30,text:"\\mathbf{x}_{t}",type:"latex",class:"fill-green-100"}}),y=new H({props:{data:[{x:40,y:60},{x:80,y:60}],showMarker:!1,dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),w=new H({props:{data:[{x:280,y:40},{x:280,y:430}],dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),b=new qr({props:{x:280,y:90,radius:10,class:"fill-blue-200"}}),W=new H({props:{data:[{x:80,y:40},{x:80,y:430}],dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),P=new Qr({props:{x:80,y:60,r:5,class:"fill-black"}}),L=new H({props:{data:[{x:80,y:90},{x:260,y:90}],dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),j=new B({props:{fontSize:16,x:180,y:90,width:30,height:30,text:"f",type:"latex",class:"fill-red-400"}}),d=new Jr({props:{x:280,y:180,radius:10,offset:4,class:"fill-blue-200"}}),M=new H({props:{data:[{x:80,y:160},{x:200,y:160},{x:200,y:180}],showMarker:!1,dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),C=new B({props:{fontSize:16,x:140,y:160,width:30,height:30,text:"g",type:"latex",class:"fill-violet-200"}}),Y=new H({props:{data:[{x:80,y:210},{x:200,y:210},{x:200,y:180}],showMarker:!1,dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),X=new B({props:{fontSize:16,x:140,y:210,width:30,height:30,text:"i",type:"latex",class:"fill-red-400"}}),Z=new qr({props:{x:200,y:180,radius:10,class:"fill-blue-200"}}),R=new H({props:{data:[{x:210,y:180},{x:260,y:180}],dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),ee=new B({props:{fontSize:16,x:80,y:280,width:30,height:30,text:"o",type:"latex",class:"fill-red-400"}}),K=new qr({props:{x:80,y:350,radius:10,class:"fill-blue-200"}}),F=new H({props:{data:[{x:210,y:180},{x:260,y:180}],dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),te=new H({props:{data:[{x:280,y:350},{x:100,y:350}],dashed:!0,strokeDashArray:"5 5",moving:!0,strokeWidth:2}}),V=new B({props:{fontSize:13,x:180,y:350,width:50,height:30,text:"\\\\tanh",type:"latex",class:"fill-gray-100"}}),O=new B({props:{fontSize:12,x:280,y:400,width:30,height:30,text:"\\mathbf{c}_{t}",type:"latex",class:"fill-green-100"}}),ne=new B({props:{fontSize:12,x:80,y:400,width:30,height:30,text:"\\mathbf{h}_{t}",type:"latex",class:"fill-green-100"}}),{c(){n=nn("svg"),u(t.$$.fragment),u(r.$$.fragment),u(o.$$.fragment),u(y.$$.fragment),u(w.$$.fragment),u(b.$$.fragment),u(W.$$.fragment),u(P.$$.fragment),u(L.$$.fragment),u(j.$$.fragment),u(d.$$.fragment),u(M.$$.fragment),u(C.$$.fragment),u(Y.$$.fragment),u(X.$$.fragment),u(Z.$$.fragment),u(R.$$.fragment),u(ee.$$.fragment),u(K.$$.fragment),u(F.$$.fragment),u(te.$$.fragment),u(V.$$.fragment),u(O.$$.fragment),u(ne.$$.fragment),this.h()},l(_){n=rn(_,"svg",{viewBox:!0});var z=T(n);p(t.$$.fragment,z),p(r.$$.fragment,z),p(o.$$.fragment,z),p(y.$$.fragment,z),p(w.$$.fragment,z),p(b.$$.fragment,z),p(W.$$.fragment,z),p(P.$$.fragment,z),p(L.$$.fragment,z),p(j.$$.fragment,z),p(d.$$.fragment,z),p(M.$$.fragment,z),p(C.$$.fragment,z),p(Y.$$.fragment,z),p(X.$$.fragment,z),p(Z.$$.fragment,z),p(R.$$.fragment,z),p(ee.$$.fragment,z),p(K.$$.fragment,z),p(F.$$.fragment,z),p(te.$$.fragment,z),p(V.$$.fragment,z),p(O.$$.fragment,z),p(ne.$$.fragment,z),z.forEach(a),this.h()},h(){N(n,"viewBox","0 0 300 450")},m(_,z){l(_,n,z),g(t,n,null),g(r,n,null),g(o,n,null),g(y,n,null),g(w,n,null),g(b,n,null),g(W,n,null),g(P,n,null),g(L,n,null),g(j,n,null),g(d,n,null),g(M,n,null),g(C,n,null),g(Y,n,null),g(X,n,null),g(Z,n,null),g(R,n,null),g(ee,n,null),g(K,n,null),g(F,n,null),g(te,n,null),g(V,n,null),g(O,n,null),g(ne,n,null),le=!0},p:A,i(_){le||(m(t.$$.fragment,_),m(r.$$.fragment,_),m(o.$$.fragment,_),m(y.$$.fragment,_),m(w.$$.fragment,_),m(b.$$.fragment,_),m(W.$$.fragment,_),m(P.$$.fragment,_),m(L.$$.fragment,_),m(j.$$.fragment,_),m(d.$$.fragment,_),m(M.$$.fragment,_),m(C.$$.fragment,_),m(Y.$$.fragment,_),m(X.$$.fragment,_),m(Z.$$.fragment,_),m(R.$$.fragment,_),m(ee.$$.fragment,_),m(K.$$.fragment,_),m(F.$$.fragment,_),m(te.$$.fragment,_),m(V.$$.fragment,_),m(O.$$.fragment,_),m(ne.$$.fragment,_),le=!0)},o(_){h(t.$$.fragment,_),h(r.$$.fragment,_),h(o.$$.fragment,_),h(y.$$.fragment,_),h(w.$$.fragment,_),h(b.$$.fragment,_),h(W.$$.fragment,_),h(P.$$.fragment,_),h(L.$$.fragment,_),h(j.$$.fragment,_),h(d.$$.fragment,_),h(M.$$.fragment,_),h(C.$$.fragment,_),h(Y.$$.fragment,_),h(X.$$.fragment,_),h(Z.$$.fragment,_),h(R.$$.fragment,_),h(ee.$$.fragment,_),h(K.$$.fragment,_),h(F.$$.fragment,_),h(te.$$.fragment,_),h(V.$$.fragment,_),h(O.$$.fragment,_),h(ne.$$.fragment,_),le=!1},d(_){_&&a(n),c(t),c(r),c(o),c(y),c(w),c(b),c(W),c(P),c(L),c(j),c(d),c(M),c(C),c(Y),c(X),c(Z),c(R),c(ee),c(K),c(F),c(te),c(V),c(O),c(ne)}}}function ua($){let n=String.raw`\mathbf{c}_t`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function pa($){let n=String.raw`\mathbf{h}_t`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function ga($){let n;return{c(){n=f("f")},l(t){n=i(t,"f")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function ca($){let n;return{c(){n=f("i")},l(t){n=i(t,"i")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function da($){let n;return{c(){n=f("o")},l(t){n=i(t,"o")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function wa($){let n;return{c(){n=f("g")},l(t){n=i(t,"g")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function _a($){let n=String.raw`\mathbf{h}_{t-1}`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function va($){let n=String.raw`\mathbf{x}_{t}`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function ya($){let n=String.raw`\mathbf{c}`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function ba($){let n;return{c(){n=f("f")},l(t){n=i(t,"f")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function xa($){let n;return{c(){n=f("forget gate")},l(t){n=i(t,"forget gate")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function ka($){let n=String.raw`\mathbf{c}_{t-1}`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Ta($){let n;return{c(){n=f("f")},l(t){n=i(t,"f")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function Sa($){let n=String.raw`\mathbf{f_t} = \sigma(\mathbf{h_{t-1}}\mathbf{W}_{hf}^T + \mathbf{x_t} \mathbf{W}_{xf}^T + \mathbf{b}_f)`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Ea($){let n=String.raw`g`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Ma($){let n=String.raw`\mathbf{g_t} = \tanh(\mathbf{h_{t-1}}\mathbf{W}_{hg}^T + \mathbf{x_t} \mathbf{W}_{xg}^T + \mathbf{b}_g)`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function La($){let n=String.raw`i`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function za($){let n;return{c(){n=f("input gate")},l(t){n=i(t,"input gate")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function Wa($){let n=String.raw`\mathbf{i_t} = \sigma(\mathbf{h_{t-1}}\mathbf{W}_{hi}^T + \mathbf{x_t} \mathbf{W}_{xi}^T + \mathbf{b}_i)`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Ia($){let n=String.raw`\mathbf{c_t} = \mathbf{f_t} \odot \mathbf{c}_{t-1} + \mathbf{i}_{t} \odot \mathbf{g}_t `+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Aa($){let n;return{c(){n=f("o")},l(t){n=i(t,"o")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function Pa($){let n=String.raw`\mathbf{h}_t`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Da($){let n;return{c(){n=f("output gate")},l(t){n=i(t,"output gate")},m(t,r){l(t,n,r)},d(t){t&&a(n)}}}function ja($){let n=String.raw`\mathbf{c}_t`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function qa($){let n=String.raw`\mathbf{o_t} = \sigma(\mathbf{h_{t-1}}\mathbf{W}_{ho}^T + \mathbf{x_t} \mathbf{W}_{xo}^T + \mathbf{b}_o)`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Na($){let n=String.raw`\mathbf{h_t} = \mathbf{o_t} \odot \tanh(\mathbf{c}_{t})`+"",t;return{c(){t=f(n)},l(r){t=i(r,n)},m(r,o){l(r,t,o)},p:A,d(r){r&&a(t)}}}function Ba($){let n,t,r,o,y,w,b,W,P,L,j,d,M,C,Y,X,Z,R,ee,K,F,te,V,O,ne,le,_,z,$e,Hn,me,Gn,an,he,sn,ue,Un,pe,Yn,ln,ct,Jn,on,ge,fn,dt,Qn,$n,ce,mn,wt,Kn,hn,_t,Xn,un,Ze,de,pn,vt,Zn,gn,et,we,cn,yt,er,dn,tt,_e,wn,bt,tr,_n,xt,nr,vn,kt,rr,yn,ve,bn,D,ar,ye,sr,be,lr,xe,or,ke,fr,Te,ir,Se,$r,Ee,mr,Me,hr,xn,Le,ur,ze,pr,kn,G,gr,We,cr,Ie,dr,Ae,wr,Pe,_r,Tn,nt,De,Sn,je,vr,qe,yr,En,rt,Ne,Mn,ae,br,Be,xr,Ce,kr,Ln,at,Fe,zn,Tt,Tr,Wn,st,Re,In,U,Sr,Ve,Er,Oe,Mr,He,Lr,Ge,zr,An,lt,Ue,Pn,Et,Dn,ot,Ye,jn,se,Wr,Je,Mt,Ir,Ar,Lt,Pr,Dr,qn,ft,Nn,it,Bn,St,jr,Cn,$t,Fn,mt,Rn,ht,Vn;return y=new Xe({props:{$$slots:{default:[Xr]},$$scope:{ctx:$}}}),b=new Xe({props:{$$slots:{default:[Zr]},$$scope:{ctx:$}}}),R=new Xe({props:{$$slots:{default:[ea]},$$scope:{ctx:$}}}),V=new Xe({props:{$$slots:{default:[ta]},$$scope:{ctx:$}}}),O=new Yr({props:{type:"reference",id:1}}),$e=new I({props:{$$slots:{default:[na]},$$scope:{ctx:$}}}),me=new I({props:{$$slots:{default:[ra]},$$scope:{ctx:$}}}),he=new On({props:{maxWidth:"250px",$$slots:{default:[sa]},$$scope:{ctx:$}}}),pe=new Xe({props:{$$slots:{default:[la]},$$scope:{ctx:$}}}),ge=new On({props:{maxWidth:"500px",$$slots:{default:[oa]},$$scope:{ctx:$}}}),ce=new On({props:{maxWidth:"500px",$$slots:{default:[fa]},$$scope:{ctx:$}}}),de=new I({props:{$$slots:{default:[ia]},$$scope:{ctx:$}}}),we=new I({props:{$$slots:{default:[$a]},$$scope:{ctx:$}}}),_e=new I({props:{$$slots:{default:[ma]},$$scope:{ctx:$}}}),ve=new On({props:{maxWidth:"400px",$$slots:{default:[ha]},$$scope:{ctx:$}}}),ye=new I({props:{$$slots:{default:[ua]},$$scope:{ctx:$}}}),be=new I({props:{$$slots:{default:[pa]},$$scope:{ctx:$}}}),xe=new I({props:{$$slots:{default:[ga]},$$scope:{ctx:$}}}),ke=new I({props:{$$slots:{default:[ca]},$$scope:{ctx:$}}}),Te=new I({props:{$$slots:{default:[da]},$$scope:{ctx:$}}}),Se=new I({props:{$$slots:{default:[wa]},$$scope:{ctx:$}}}),Ee=new I({props:{$$slots:{default:[_a]},$$scope:{ctx:$}}}),Me=new I({props:{$$slots:{default:[va]},$$scope:{ctx:$}}}),ze=new I({props:{$$slots:{default:[ya]},$$scope:{ctx:$}}}),We=new I({props:{$$slots:{default:[ba]},$$scope:{ctx:$}}}),Ie=new Xe({props:{$$slots:{default:[xa]},$$scope:{ctx:$}}}),Ae=new I({props:{$$slots:{default:[ka]},$$scope:{ctx:$}}}),Pe=new I({props:{$$slots:{default:[Ta]},$$scope:{ctx:$}}}),De=new I({props:{$$slots:{default:[Sa]},$$scope:{ctx:$}}}),qe=new I({props:{$$slots:{default:[Ea]},$$scope:{ctx:$}}}),Ne=new I({props:{$$slots:{default:[Ma]},$$scope:{ctx:$}}}),Be=new I({props:{$$slots:{default:[La]},$$scope:{ctx:$}}}),Ce=new Xe({props:{$$slots:{default:[za]},$$scope:{ctx:$}}}),Fe=new I({props:{$$slots:{default:[Wa]},$$scope:{ctx:$}}}),Re=new I({props:{$$slots:{default:[Ia]},$$scope:{ctx:$}}}),Ve=new I({props:{$$slots:{default:[Aa]},$$scope:{ctx:$}}}),Oe=new I({props:{$$slots:{default:[Pa]},$$scope:{ctx:$}}}),He=new Xe({props:{$$slots:{default:[Da]},$$scope:{ctx:$}}}),Ge=new I({props:{$$slots:{default:[ja]},$$scope:{ctx:$}}}),Ue=new I({props:{$$slots:{default:[qa]},$$scope:{ctx:$}}}),Ye=new I({props:{$$slots:{default:[Na]},$$scope:{ctx:$}}}),ft=new tn({props:{code:`batch_size=4
sequence_length=5
input_size=6
hidden_size=3
num_layers=2`}}),it=new tn({props:{code:`lstm = nn.LSTM(input_size=input_size, 
               hidden_size=hidden_size, 
               num_layers=num_layers)`}}),$t=new tn({props:{code:`# create inputs to the LSTM
sequence = torch.randn(sequence_length, batch_size, input_size)
h_0 = torch.zeros(num_layers, batch_size, hidden_size)
c_0 = torch.zeros(num_layers, batch_size, hidden_size)`}}),mt=new tn({props:{code:`with torch.inference_mode():
    output, (h_n, c_n) = lstm(sequence, (h_0, c_0))
print(output.shape, h_n.shape, c_n.shape)`}}),ht=new tn({props:{code:"torch.Size([5, 4, 3]) torch.Size([2, 4, 3]) torch.Size([2, 4, 3])",isOutput:!0}}),{c(){n=S("p"),t=f(`Often a recurrent neural network has to process very large sequences. With
    each recurrent step we incorporate more and more information into the hidden
    state while the distance to the start of the sequence increases. Let's take
    the following language task as an example.`),r=x(),o=S("p"),u(y.$$.fragment),w=f(` have been a passion of mine from the very early
    age. I was fascinated by the idea that I could create anything out of nothing
    using only my skills and my imagination. So when the time came to select my major,
    I did not hesitate and picked `),u(b.$$.fragment),W=f("."),P=x(),L=S("p"),j=f(`In this task we need to predict the last word(s). The key to the solution is
    to realize that the very first word in the very first sentence provides the
    necessary context. If that person has been interested in computers for such
    a long time, it is reasonable to assume that `),d=S("em"),M=f("computer science"),C=f(`
    or `),Y=S("em"),X=f("programming"),Z=f(` would be good choces to fill the blank. Yet the
    distance between the first word and the prediction is roughly 50 words. Each
    time the recurrent neural network processes a piece of the sentence, it
    adjusts the hidden state, so that by the end of the sentence hardly any
    information remains from the beginning of the sentence. The network forgets
    the beginning of the sentence long before it is done reading. A recurrent
    neural network struggles with so called `),u(R.$$.fragment),ee=f("."),K=x(),F=S("p"),te=f(`Theoretically speaking there is nothing that prevents a recurrent neural
    network from learning such dependencies. Assuming you had the perfect
    weights for a particular task, an RNN should have the capacity to model long
    term dependencies. It is the learning part that makes recurrent networks
    less attractive. A recurrent neural network shares weights, thus when the
    network encounters a long sequence, gradients will explode or vanish. We can
    use gradient clipping to deal with exploding gradients, but many of the
    techniques that we used with feed-forward neural networks to deal with
    vanishing gradients will not work. Batch normalization for example
    calculates the mean and the standard deviation per feature and layer, but in
    a recurrent neural network the weights are shared and we might theoretically
    need different statistics for a different part of the recurrent loop.
    Specialized techniques were developed to deal with the vanishing gradients
    problem of RNNs and in this section we are gong to cover a new type of a
    recurrent neural network called long short-term memory, or `),u(V.$$.fragment),u(O.$$.fragment),ne=f(`
    for short.`),le=x(),_=S("p"),z=f(`For the most part we do not change the overarching design of a recurrent
    neural network. The LSTM cell produces outputs, that are used as an input in
    the next iteration. Unlike the regular RNN cell, an LSTM cell produces two
    vectors: the short term memory `),u($e.$$.fragment),Hn=f(` (hidden
    value) and the long term memory
    `),u(me.$$.fragment),Gn=f("(cell value)."),an=x(),u(he.$$.fragment),sn=x(),ue=S("p"),Un=f("An LSTM cell makes heavy use of so-called "),u(pe.$$.fragment),Yn=f(`.
    Gates allow information to keep flowing or stop information flow depending
    on the state of the gate.`),ln=x(),ct=S("p"),Jn=f(`When the gate is closed, no information is allowed to flow and data can not
    pass past the gate.`),on=x(),u(ge.$$.fragment),fn=x(),dt=S("p"),Qn=f("When the gate is open, information can flow without interruption."),$n=x(),u(ce.$$.fragment),mn=x(),wt=S("p"),Kn=f(`Essentially an LSTM cell determines which parts of the sequence data should
    be processed and saved for future reference and which parts are irrelevant.`),hn=x(),_t=S("p"),Xn=f(`Let's for example assume that our data is a two dimensional vector, with
    values 2 and 5.`),un=x(),Ze=S("div"),u(de.$$.fragment),pn=x(),vt=S("p"),Zn=f(`The gate is a vector of the same size, that contains values of either 0 or
    1.`),gn=x(),et=S("div"),u(we.$$.fragment),cn=x(),yt=S("p"),er=f(`In order to determine what part of the data is allowed to flow we use
    elementwise multiplication.`),dn=x(),tt=S("div"),u(_e.$$.fragment),wn=x(),bt=S("p"),tr=f(`The parts of the vector that are multiplied by a 0 are essentially erased,
    while those parts that are multiplied by a 1 are allowed to keep flowing. In
    practice LSTM cells do not work in a completely binary fashion, but contain
    continuous values between 0 and 1. This allows the gate to pass just a
    fraction of the information.`),_n=x(),xt=S("p"),nr=f(`The beauty of an LSTM cell is its ability to learn and to calculate the
    values of different gates automatically. That means that an LSTM cell
    decides on the fly which information is important for the future and should
    be saved and which information should be discarded. These gates are
    essentially fully connected neural networks, which use a sigmid activation
    function in order to scale the values between 0 and 1.`),vn=x(),kt=S("p"),rr=f(`Now let's have a look at the inner workings of the LSTM cell. The design
    might look intimidating at first glance, but we will take it one step at a
    time.`),yn=x(),u(ve.$$.fragment),bn=x(),D=S("p"),ar=f("The LSTM cell outputs the long-term memory "),u(ye.$$.fragment),sr=f(` and the short term memory
    `),u(be.$$.fragment),lr=f(`. For that purpose the LSTM cell
    contains four fully connected neural networks. The red networks `),u(xe.$$.fragment),or=f(", "),u(ke.$$.fragment),fr=f(" and "),u(Te.$$.fragment),ir=f(` are networks with a sigmoid activation
    function, that act as gates, while the violet neural network `),u(Se.$$.fragment),$r=f(` applies a tanh activation function and is used to generate values that can
    be used to adjust the long-term memory. All four networks take the same inputs:
    a vector that contains previous hidden state
    `),u(Ee.$$.fragment),mr=f(` and the current piece of the sequence
    `),u(Me.$$.fragment),hr=f("."),xn=x(),Le=S("p"),ur=f("If you look at the flow of the long term memory "),u(ze.$$.fragment),pr=f(` you should notice, that it flows in a straight line from one part of the sequence
    to the next. The general idea is to only adjust that flow if it is warranted.
    That allows the LSTM cell to establish long-term dependencies.`),kn=x(),G=S("p"),gr=f("The neural network "),u(We.$$.fragment),cr=f(` calculates the
    `),u(Ie.$$.fragment),dr=f(`. We multipy each component of the long
    term memory `),u(Ae.$$.fragment),wr=f(` vector by each component
    from the neural network `),u(Pe.$$.fragment),_r=f(`. The gate uses the sigmoid
    activation function and can therefore theoretically reduce or even
    completely erase the long term memory if the LSTM cell deems this necessary.
    The closer the outputs of the fully connected neural network are to 1, the
    more long-term memory is kept.`),Tn=x(),nt=S("div"),u(De.$$.fragment),Sn=x(),je=S("p"),vr=f(`In the second step we decide if we should add anything to the long term
    memory. First we calculate the memories that can be used as potential
    additions to the long-term memory using the fully connected neural network `),u(qe.$$.fragment),yr=f("."),En=x(),rt=S("div"),u(Ne.$$.fragment),Mn=x(),ae=S("p"),br=f("Then we use the neural network "),u(Be.$$.fragment),xr=f(`, which acts as
    a gate for those "potential memories". This gate is called `),u(Ce.$$.fragment),kr=f(`. The elementwise product of the two neural networks outputs is the actual
    adjustment to the long-term state, which are added elementwise to the values
    that were passed through the forget gate.`),Ln=x(),at=S("div"),u(Fe.$$.fragment),zn=x(),Tt=S("p"),Tr=f(`The forget gate, the input gate and the "potential memories" are used to
    calculate the long-term memories for the next timestep of the series.`),Wn=x(),st=S("div"),u(Re.$$.fragment),In=x(),U=S("p"),Sr=f("The final neural network "),u(Ve.$$.fragment),Er=f(` is used to determine which values are
    suitable for the short-term memory `),u(Oe.$$.fragment),Mr=f(". This gate is called the "),u(He.$$.fragment),Lr=f(`. For that
    purpose the long-term memory `),u(Ge.$$.fragment),zr=f(` is copied
    and is preprocessed by the tanh activation function. The result is multiplied
    by the output gate.`),An=x(),lt=S("div"),u(Ue.$$.fragment),Pn=x(),Et=S("p"),Dn=x(),ot=S("div"),u(Ye.$$.fragment),jn=x(),se=S("p"),Wr=f(`If we want to use a LSTM instead of a plain valilla recurrent neural net, we
    have to use the `),Je=S("a"),Mt=S("code"),Ir=f("nn.LSTM"),Ar=f(`
    module instead of the
    `),Lt=S("code"),Pr=f("nn.RNN"),Dr=f("."),qn=x(),u(ft.$$.fragment),Nn=x(),u(it.$$.fragment),Bn=x(),St=S("p"),jr=f(`We need to account for the long term memory, but the rest of the
    implementation is almost identical.`),Cn=x(),u($t.$$.fragment),Fn=x(),u(mt.$$.fragment),Rn=x(),u(ht.$$.fragment),this.h()},l(e){n=E(e,"P",{});var s=T(n);t=i(s,`Often a recurrent neural network has to process very large sequences. With
    each recurrent step we incorporate more and more information into the hidden
    state while the distance to the start of the sequence increases. Let's take
    the following language task as an example.`),s.forEach(a),r=k(e),o=E(e,"P",{class:!0});var Qe=T(o);p(y.$$.fragment,Qe),w=i(Qe,` have been a passion of mine from the very early
    age. I was fascinated by the idea that I could create anything out of nothing
    using only my skills and my imagination. So when the time came to select my major,
    I did not hesitate and picked `),p(b.$$.fragment,Qe),W=i(Qe,"."),Qe.forEach(a),P=k(e),L=E(e,"P",{});var re=T(L);j=i(re,`In this task we need to predict the last word(s). The key to the solution is
    to realize that the very first word in the very first sentence provides the
    necessary context. If that person has been interested in computers for such
    a long time, it is reasonable to assume that `),d=E(re,"EM",{});var zt=T(d);M=i(zt,"computer science"),zt.forEach(a),C=i(re,`
    or `),Y=E(re,"EM",{});var Wt=T(Y);X=i(Wt,"programming"),Wt.forEach(a),Z=i(re,` would be good choces to fill the blank. Yet the
    distance between the first word and the prediction is roughly 50 words. Each
    time the recurrent neural network processes a piece of the sentence, it
    adjusts the hidden state, so that by the end of the sentence hardly any
    information remains from the beginning of the sentence. The network forgets
    the beginning of the sentence long before it is done reading. A recurrent
    neural network struggles with so called `),p(R.$$.fragment,re),ee=i(re,"."),re.forEach(a),K=k(e),F=E(e,"P",{});var Ke=T(F);te=i(Ke,`Theoretically speaking there is nothing that prevents a recurrent neural
    network from learning such dependencies. Assuming you had the perfect
    weights for a particular task, an RNN should have the capacity to model long
    term dependencies. It is the learning part that makes recurrent networks
    less attractive. A recurrent neural network shares weights, thus when the
    network encounters a long sequence, gradients will explode or vanish. We can
    use gradient clipping to deal with exploding gradients, but many of the
    techniques that we used with feed-forward neural networks to deal with
    vanishing gradients will not work. Batch normalization for example
    calculates the mean and the standard deviation per feature and layer, but in
    a recurrent neural network the weights are shared and we might theoretically
    need different statistics for a different part of the recurrent loop.
    Specialized techniques were developed to deal with the vanishing gradients
    problem of RNNs and in this section we are gong to cover a new type of a
    recurrent neural network called long short-term memory, or `),p(V.$$.fragment,Ke),p(O.$$.fragment,Ke),ne=i(Ke,`
    for short.`),Ke.forEach(a),le=k(e),_=E(e,"P",{});var oe=T(_);z=i(oe,`For the most part we do not change the overarching design of a recurrent
    neural network. The LSTM cell produces outputs, that are used as an input in
    the next iteration. Unlike the regular RNN cell, an LSTM cell produces two
    vectors: the short term memory `),p($e.$$.fragment,oe),Hn=i(oe,` (hidden
    value) and the long term memory
    `),p(me.$$.fragment,oe),Gn=i(oe,"(cell value)."),oe.forEach(a),an=k(e),p(he.$$.fragment,e),sn=k(e),ue=E(e,"P",{});var ut=T(ue);Un=i(ut,"An LSTM cell makes heavy use of so-called "),p(pe.$$.fragment,ut),Yn=i(ut,`.
    Gates allow information to keep flowing or stop information flow depending
    on the state of the gate.`),ut.forEach(a),ln=k(e),ct=E(e,"P",{});var It=T(ct);Jn=i(It,`When the gate is closed, no information is allowed to flow and data can not
    pass past the gate.`),It.forEach(a),on=k(e),p(ge.$$.fragment,e),fn=k(e),dt=E(e,"P",{});var At=T(dt);Qn=i(At,"When the gate is open, information can flow without interruption."),At.forEach(a),$n=k(e),p(ce.$$.fragment,e),mn=k(e),wt=E(e,"P",{});var Pt=T(wt);Kn=i(Pt,`Essentially an LSTM cell determines which parts of the sequence data should
    be processed and saved for future reference and which parts are irrelevant.`),Pt.forEach(a),hn=k(e),_t=E(e,"P",{});var Dt=T(_t);Xn=i(Dt,`Let's for example assume that our data is a two dimensional vector, with
    values 2 and 5.`),Dt.forEach(a),un=k(e),Ze=E(e,"DIV",{class:!0});var jt=T(Ze);p(de.$$.fragment,jt),jt.forEach(a),pn=k(e),vt=E(e,"P",{});var qt=T(vt);Zn=i(qt,`The gate is a vector of the same size, that contains values of either 0 or
    1.`),qt.forEach(a),gn=k(e),et=E(e,"DIV",{class:!0});var Nt=T(et);p(we.$$.fragment,Nt),Nt.forEach(a),cn=k(e),yt=E(e,"P",{});var Bt=T(yt);er=i(Bt,`In order to determine what part of the data is allowed to flow we use
    elementwise multiplication.`),Bt.forEach(a),dn=k(e),tt=E(e,"DIV",{class:!0});var Ct=T(tt);p(_e.$$.fragment,Ct),Ct.forEach(a),wn=k(e),bt=E(e,"P",{});var Ft=T(bt);tr=i(Ft,`The parts of the vector that are multiplied by a 0 are essentially erased,
    while those parts that are multiplied by a 1 are allowed to keep flowing. In
    practice LSTM cells do not work in a completely binary fashion, but contain
    continuous values between 0 and 1. This allows the gate to pass just a
    fraction of the information.`),Ft.forEach(a),_n=k(e),xt=E(e,"P",{});var Rt=T(xt);nr=i(Rt,`The beauty of an LSTM cell is its ability to learn and to calculate the
    values of different gates automatically. That means that an LSTM cell
    decides on the fly which information is important for the future and should
    be saved and which information should be discarded. These gates are
    essentially fully connected neural networks, which use a sigmid activation
    function in order to scale the values between 0 and 1.`),Rt.forEach(a),vn=k(e),kt=E(e,"P",{});var Vt=T(kt);rr=i(Vt,`Now let's have a look at the inner workings of the LSTM cell. The design
    might look intimidating at first glance, but we will take it one step at a
    time.`),Vt.forEach(a),yn=k(e),p(ve.$$.fragment,e),bn=k(e),D=E(e,"P",{});var q=T(D);ar=i(q,"The LSTM cell outputs the long-term memory "),p(ye.$$.fragment,q),sr=i(q,` and the short term memory
    `),p(be.$$.fragment,q),lr=i(q,`. For that purpose the LSTM cell
    contains four fully connected neural networks. The red networks `),p(xe.$$.fragment,q),or=i(q,", "),p(ke.$$.fragment,q),fr=i(q," and "),p(Te.$$.fragment,q),ir=i(q,` are networks with a sigmoid activation
    function, that act as gates, while the violet neural network `),p(Se.$$.fragment,q),$r=i(q,` applies a tanh activation function and is used to generate values that can
    be used to adjust the long-term memory. All four networks take the same inputs:
    a vector that contains previous hidden state
    `),p(Ee.$$.fragment,q),mr=i(q,` and the current piece of the sequence
    `),p(Me.$$.fragment,q),hr=i(q,"."),q.forEach(a),xn=k(e),Le=E(e,"P",{});var pt=T(Le);ur=i(pt,"If you look at the flow of the long term memory "),p(ze.$$.fragment,pt),pr=i(pt,` you should notice, that it flows in a straight line from one part of the sequence
    to the next. The general idea is to only adjust that flow if it is warranted.
    That allows the LSTM cell to establish long-term dependencies.`),pt.forEach(a),kn=k(e),G=E(e,"P",{});var J=T(G);gr=i(J,"The neural network "),p(We.$$.fragment,J),cr=i(J,` calculates the
    `),p(Ie.$$.fragment,J),dr=i(J,`. We multipy each component of the long
    term memory `),p(Ae.$$.fragment,J),wr=i(J,` vector by each component
    from the neural network `),p(Pe.$$.fragment,J),_r=i(J,`. The gate uses the sigmoid
    activation function and can therefore theoretically reduce or even
    completely erase the long term memory if the LSTM cell deems this necessary.
    The closer the outputs of the fully connected neural network are to 1, the
    more long-term memory is kept.`),J.forEach(a),Tn=k(e),nt=E(e,"DIV",{class:!0});var Ot=T(nt);p(De.$$.fragment,Ot),Ot.forEach(a),Sn=k(e),je=E(e,"P",{});var gt=T(je);vr=i(gt,`In the second step we decide if we should add anything to the long term
    memory. First we calculate the memories that can be used as potential
    additions to the long-term memory using the fully connected neural network `),p(qe.$$.fragment,gt),yr=i(gt,"."),gt.forEach(a),En=k(e),rt=E(e,"DIV",{class:!0});var Ht=T(rt);p(Ne.$$.fragment,Ht),Ht.forEach(a),Mn=k(e),ae=E(e,"P",{});var fe=T(ae);br=i(fe,"Then we use the neural network "),p(Be.$$.fragment,fe),xr=i(fe,`, which acts as
    a gate for those "potential memories". This gate is called `),p(Ce.$$.fragment,fe),kr=i(fe,`. The elementwise product of the two neural networks outputs is the actual
    adjustment to the long-term state, which are added elementwise to the values
    that were passed through the forget gate.`),fe.forEach(a),Ln=k(e),at=E(e,"DIV",{class:!0});var Gt=T(at);p(Fe.$$.fragment,Gt),Gt.forEach(a),zn=k(e),Tt=E(e,"P",{});var Ut=T(Tt);Tr=i(Ut,`The forget gate, the input gate and the "potential memories" are used to
    calculate the long-term memories for the next timestep of the series.`),Ut.forEach(a),Wn=k(e),st=E(e,"DIV",{class:!0});var Yt=T(st);p(Re.$$.fragment,Yt),Yt.forEach(a),In=k(e),U=E(e,"P",{});var Q=T(U);Sr=i(Q,"The final neural network "),p(Ve.$$.fragment,Q),Er=i(Q,` is used to determine which values are
    suitable for the short-term memory `),p(Oe.$$.fragment,Q),Mr=i(Q,". This gate is called the "),p(He.$$.fragment,Q),Lr=i(Q,`. For that
    purpose the long-term memory `),p(Ge.$$.fragment,Q),zr=i(Q,` is copied
    and is preprocessed by the tanh activation function. The result is multiplied
    by the output gate.`),Q.forEach(a),An=k(e),lt=E(e,"DIV",{class:!0});var Jt=T(lt);p(Ue.$$.fragment,Jt),Jt.forEach(a),Pn=k(e),Et=E(e,"P",{}),T(Et).forEach(a),Dn=k(e),ot=E(e,"DIV",{class:!0});var Qt=T(ot);p(Ye.$$.fragment,Qt),Qt.forEach(a),jn=k(e),se=E(e,"P",{});var ie=T(se);Wr=i(ie,`If we want to use a LSTM instead of a plain valilla recurrent neural net, we
    have to use the `),Je=E(ie,"A",{href:!0,target:!0,rel:!0});var Kt=T(Je);Mt=E(Kt,"CODE",{});var Xt=T(Mt);Ir=i(Xt,"nn.LSTM"),Xt.forEach(a),Kt.forEach(a),Ar=i(ie,`
    module instead of the
    `),Lt=E(ie,"CODE",{});var Zt=T(Lt);Pr=i(Zt,"nn.RNN"),Zt.forEach(a),Dr=i(ie,"."),ie.forEach(a),qn=k(e),p(ft.$$.fragment,e),Nn=k(e),p(it.$$.fragment,e),Bn=k(e),St=E(e,"P",{});var en=T(St);jr=i(en,`We need to account for the long term memory, but the rest of the
    implementation is almost identical.`),en.forEach(a),Cn=k(e),p($t.$$.fragment,e),Fn=k(e),p(mt.$$.fragment,e),Rn=k(e),p(ht.$$.fragment,e),this.h()},h(){N(o,"class","bg-blue-200 px-3 py-2"),N(Ze,"class","flex justify-center"),N(et,"class","flex justify-center"),N(tt,"class","flex justify-center"),N(nt,"class","flex justify-center"),N(rt,"class","flex justify-center"),N(at,"class","flex justify-center"),N(st,"class","flex justify-center"),N(lt,"class","flex justify-center"),N(ot,"class","flex justify-center"),N(Je,"href","https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"),N(Je,"target","_blank"),N(Je,"rel","noreferrer")},m(e,s){l(e,n,s),v(n,t),l(e,r,s),l(e,o,s),g(y,o,null),v(o,w),g(b,o,null),v(o,W),l(e,P,s),l(e,L,s),v(L,j),v(L,d),v(d,M),v(L,C),v(L,Y),v(Y,X),v(L,Z),g(R,L,null),v(L,ee),l(e,K,s),l(e,F,s),v(F,te),g(V,F,null),g(O,F,null),v(F,ne),l(e,le,s),l(e,_,s),v(_,z),g($e,_,null),v(_,Hn),g(me,_,null),v(_,Gn),l(e,an,s),g(he,e,s),l(e,sn,s),l(e,ue,s),v(ue,Un),g(pe,ue,null),v(ue,Yn),l(e,ln,s),l(e,ct,s),v(ct,Jn),l(e,on,s),g(ge,e,s),l(e,fn,s),l(e,dt,s),v(dt,Qn),l(e,$n,s),g(ce,e,s),l(e,mn,s),l(e,wt,s),v(wt,Kn),l(e,hn,s),l(e,_t,s),v(_t,Xn),l(e,un,s),l(e,Ze,s),g(de,Ze,null),l(e,pn,s),l(e,vt,s),v(vt,Zn),l(e,gn,s),l(e,et,s),g(we,et,null),l(e,cn,s),l(e,yt,s),v(yt,er),l(e,dn,s),l(e,tt,s),g(_e,tt,null),l(e,wn,s),l(e,bt,s),v(bt,tr),l(e,_n,s),l(e,xt,s),v(xt,nr),l(e,vn,s),l(e,kt,s),v(kt,rr),l(e,yn,s),g(ve,e,s),l(e,bn,s),l(e,D,s),v(D,ar),g(ye,D,null),v(D,sr),g(be,D,null),v(D,lr),g(xe,D,null),v(D,or),g(ke,D,null),v(D,fr),g(Te,D,null),v(D,ir),g(Se,D,null),v(D,$r),g(Ee,D,null),v(D,mr),g(Me,D,null),v(D,hr),l(e,xn,s),l(e,Le,s),v(Le,ur),g(ze,Le,null),v(Le,pr),l(e,kn,s),l(e,G,s),v(G,gr),g(We,G,null),v(G,cr),g(Ie,G,null),v(G,dr),g(Ae,G,null),v(G,wr),g(Pe,G,null),v(G,_r),l(e,Tn,s),l(e,nt,s),g(De,nt,null),l(e,Sn,s),l(e,je,s),v(je,vr),g(qe,je,null),v(je,yr),l(e,En,s),l(e,rt,s),g(Ne,rt,null),l(e,Mn,s),l(e,ae,s),v(ae,br),g(Be,ae,null),v(ae,xr),g(Ce,ae,null),v(ae,kr),l(e,Ln,s),l(e,at,s),g(Fe,at,null),l(e,zn,s),l(e,Tt,s),v(Tt,Tr),l(e,Wn,s),l(e,st,s),g(Re,st,null),l(e,In,s),l(e,U,s),v(U,Sr),g(Ve,U,null),v(U,Er),g(Oe,U,null),v(U,Mr),g(He,U,null),v(U,Lr),g(Ge,U,null),v(U,zr),l(e,An,s),l(e,lt,s),g(Ue,lt,null),l(e,Pn,s),l(e,Et,s),l(e,Dn,s),l(e,ot,s),g(Ye,ot,null),l(e,jn,s),l(e,se,s),v(se,Wr),v(se,Je),v(Je,Mt),v(Mt,Ir),v(se,Ar),v(se,Lt),v(Lt,Pr),v(se,Dr),l(e,qn,s),g(ft,e,s),l(e,Nn,s),g(it,e,s),l(e,Bn,s),l(e,St,s),v(St,jr),l(e,Cn,s),g($t,e,s),l(e,Fn,s),g(mt,e,s),l(e,Rn,s),g(ht,e,s),Vn=!0},p(e,s){const Qe={};s&16&&(Qe.$$scope={dirty:s,ctx:e}),y.$set(Qe);const re={};s&16&&(re.$$scope={dirty:s,ctx:e}),b.$set(re);const zt={};s&16&&(zt.$$scope={dirty:s,ctx:e}),R.$set(zt);const Wt={};s&16&&(Wt.$$scope={dirty:s,ctx:e}),V.$set(Wt);const Ke={};s&16&&(Ke.$$scope={dirty:s,ctx:e}),$e.$set(Ke);const oe={};s&16&&(oe.$$scope={dirty:s,ctx:e}),me.$set(oe);const ut={};s&16&&(ut.$$scope={dirty:s,ctx:e}),he.$set(ut);const It={};s&16&&(It.$$scope={dirty:s,ctx:e}),pe.$set(It);const At={};s&16&&(At.$$scope={dirty:s,ctx:e}),ge.$set(At);const Pt={};s&16&&(Pt.$$scope={dirty:s,ctx:e}),ce.$set(Pt);const Dt={};s&16&&(Dt.$$scope={dirty:s,ctx:e}),de.$set(Dt);const jt={};s&16&&(jt.$$scope={dirty:s,ctx:e}),we.$set(jt);const qt={};s&16&&(qt.$$scope={dirty:s,ctx:e}),_e.$set(qt);const Nt={};s&16&&(Nt.$$scope={dirty:s,ctx:e}),ve.$set(Nt);const Bt={};s&16&&(Bt.$$scope={dirty:s,ctx:e}),ye.$set(Bt);const Ct={};s&16&&(Ct.$$scope={dirty:s,ctx:e}),be.$set(Ct);const Ft={};s&16&&(Ft.$$scope={dirty:s,ctx:e}),xe.$set(Ft);const Rt={};s&16&&(Rt.$$scope={dirty:s,ctx:e}),ke.$set(Rt);const Vt={};s&16&&(Vt.$$scope={dirty:s,ctx:e}),Te.$set(Vt);const q={};s&16&&(q.$$scope={dirty:s,ctx:e}),Se.$set(q);const pt={};s&16&&(pt.$$scope={dirty:s,ctx:e}),Ee.$set(pt);const J={};s&16&&(J.$$scope={dirty:s,ctx:e}),Me.$set(J);const Ot={};s&16&&(Ot.$$scope={dirty:s,ctx:e}),ze.$set(Ot);const gt={};s&16&&(gt.$$scope={dirty:s,ctx:e}),We.$set(gt);const Ht={};s&16&&(Ht.$$scope={dirty:s,ctx:e}),Ie.$set(Ht);const fe={};s&16&&(fe.$$scope={dirty:s,ctx:e}),Ae.$set(fe);const Gt={};s&16&&(Gt.$$scope={dirty:s,ctx:e}),Pe.$set(Gt);const Ut={};s&16&&(Ut.$$scope={dirty:s,ctx:e}),De.$set(Ut);const Yt={};s&16&&(Yt.$$scope={dirty:s,ctx:e}),qe.$set(Yt);const Q={};s&16&&(Q.$$scope={dirty:s,ctx:e}),Ne.$set(Q);const Jt={};s&16&&(Jt.$$scope={dirty:s,ctx:e}),Be.$set(Jt);const Qt={};s&16&&(Qt.$$scope={dirty:s,ctx:e}),Ce.$set(Qt);const ie={};s&16&&(ie.$$scope={dirty:s,ctx:e}),Fe.$set(ie);const Kt={};s&16&&(Kt.$$scope={dirty:s,ctx:e}),Re.$set(Kt);const Xt={};s&16&&(Xt.$$scope={dirty:s,ctx:e}),Ve.$set(Xt);const Zt={};s&16&&(Zt.$$scope={dirty:s,ctx:e}),Oe.$set(Zt);const en={};s&16&&(en.$$scope={dirty:s,ctx:e}),He.$set(en);const Nr={};s&16&&(Nr.$$scope={dirty:s,ctx:e}),Ge.$set(Nr);const Br={};s&16&&(Br.$$scope={dirty:s,ctx:e}),Ue.$set(Br);const Cr={};s&16&&(Cr.$$scope={dirty:s,ctx:e}),Ye.$set(Cr)},i(e){Vn||(m(y.$$.fragment,e),m(b.$$.fragment,e),m(R.$$.fragment,e),m(V.$$.fragment,e),m(O.$$.fragment,e),m($e.$$.fragment,e),m(me.$$.fragment,e),m(he.$$.fragment,e),m(pe.$$.fragment,e),m(ge.$$.fragment,e),m(ce.$$.fragment,e),m(de.$$.fragment,e),m(we.$$.fragment,e),m(_e.$$.fragment,e),m(ve.$$.fragment,e),m(ye.$$.fragment,e),m(be.$$.fragment,e),m(xe.$$.fragment,e),m(ke.$$.fragment,e),m(Te.$$.fragment,e),m(Se.$$.fragment,e),m(Ee.$$.fragment,e),m(Me.$$.fragment,e),m(ze.$$.fragment,e),m(We.$$.fragment,e),m(Ie.$$.fragment,e),m(Ae.$$.fragment,e),m(Pe.$$.fragment,e),m(De.$$.fragment,e),m(qe.$$.fragment,e),m(Ne.$$.fragment,e),m(Be.$$.fragment,e),m(Ce.$$.fragment,e),m(Fe.$$.fragment,e),m(Re.$$.fragment,e),m(Ve.$$.fragment,e),m(Oe.$$.fragment,e),m(He.$$.fragment,e),m(Ge.$$.fragment,e),m(Ue.$$.fragment,e),m(Ye.$$.fragment,e),m(ft.$$.fragment,e),m(it.$$.fragment,e),m($t.$$.fragment,e),m(mt.$$.fragment,e),m(ht.$$.fragment,e),Vn=!0)},o(e){h(y.$$.fragment,e),h(b.$$.fragment,e),h(R.$$.fragment,e),h(V.$$.fragment,e),h(O.$$.fragment,e),h($e.$$.fragment,e),h(me.$$.fragment,e),h(he.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(ce.$$.fragment,e),h(de.$$.fragment,e),h(we.$$.fragment,e),h(_e.$$.fragment,e),h(ve.$$.fragment,e),h(ye.$$.fragment,e),h(be.$$.fragment,e),h(xe.$$.fragment,e),h(ke.$$.fragment,e),h(Te.$$.fragment,e),h(Se.$$.fragment,e),h(Ee.$$.fragment,e),h(Me.$$.fragment,e),h(ze.$$.fragment,e),h(We.$$.fragment,e),h(Ie.$$.fragment,e),h(Ae.$$.fragment,e),h(Pe.$$.fragment,e),h(De.$$.fragment,e),h(qe.$$.fragment,e),h(Ne.$$.fragment,e),h(Be.$$.fragment,e),h(Ce.$$.fragment,e),h(Fe.$$.fragment,e),h(Re.$$.fragment,e),h(Ve.$$.fragment,e),h(Oe.$$.fragment,e),h(He.$$.fragment,e),h(Ge.$$.fragment,e),h(Ue.$$.fragment,e),h(Ye.$$.fragment,e),h(ft.$$.fragment,e),h(it.$$.fragment,e),h($t.$$.fragment,e),h(mt.$$.fragment,e),h(ht.$$.fragment,e),Vn=!1},d(e){e&&a(n),e&&a(r),e&&a(o),c(y),c(b),e&&a(P),e&&a(L),c(R),e&&a(K),e&&a(F),c(V),c(O),e&&a(le),e&&a(_),c($e),c(me),e&&a(an),c(he,e),e&&a(sn),e&&a(ue),c(pe),e&&a(ln),e&&a(ct),e&&a(on),c(ge,e),e&&a(fn),e&&a(dt),e&&a($n),c(ce,e),e&&a(mn),e&&a(wt),e&&a(hn),e&&a(_t),e&&a(un),e&&a(Ze),c(de),e&&a(pn),e&&a(vt),e&&a(gn),e&&a(et),c(we),e&&a(cn),e&&a(yt),e&&a(dn),e&&a(tt),c(_e),e&&a(wn),e&&a(bt),e&&a(_n),e&&a(xt),e&&a(vn),e&&a(kt),e&&a(yn),c(ve,e),e&&a(bn),e&&a(D),c(ye),c(be),c(xe),c(ke),c(Te),c(Se),c(Ee),c(Me),e&&a(xn),e&&a(Le),c(ze),e&&a(kn),e&&a(G),c(We),c(Ie),c(Ae),c(Pe),e&&a(Tn),e&&a(nt),c(De),e&&a(Sn),e&&a(je),c(qe),e&&a(En),e&&a(rt),c(Ne),e&&a(Mn),e&&a(ae),c(Be),c(Ce),e&&a(Ln),e&&a(at),c(Fe),e&&a(zn),e&&a(Tt),e&&a(Wn),e&&a(st),c(Re),e&&a(In),e&&a(U),c(Ve),c(Oe),c(He),c(Ge),e&&a(An),e&&a(lt),c(Ue),e&&a(Pn),e&&a(Et),e&&a(Dn),e&&a(ot),c(Ye),e&&a(jn),e&&a(se),e&&a(qn),c(ft,e),e&&a(Nn),c(it,e),e&&a(Bn),e&&a(St),e&&a(Cn),c($t,e),e&&a(Fn),c(mt,e),e&&a(Rn),c(ht,e)}}}function Ca($){let n,t,r,o,y,w,b,W,P,L,j;return W=new Gr({props:{$$slots:{default:[Ba]},$$scope:{ctx:$}}}),L=new Ur({props:{references:$[0]}}),{c(){n=S("meta"),t=x(),r=S("h1"),o=f("LSTM"),y=x(),w=S("div"),b=x(),u(W.$$.fragment),P=x(),u(L.$$.fragment),this.h()},l(d){const M=Or("svelte-1ajwx36",document.head);n=E(M,"META",{name:!0,content:!0}),M.forEach(a),t=k(d),r=E(d,"H1",{});var C=T(r);o=i(C,"LSTM"),C.forEach(a),y=k(d),w=E(d,"DIV",{class:!0}),T(w).forEach(a),b=k(d),p(W.$$.fragment,d),P=k(d),p(L.$$.fragment,d),this.h()},h(){document.title="LSTM - World4AI",N(n,"name","description"),N(n,"content","The LSTM (long short-term memory) cell uses a gated architecture in order to learn and preserve long-term dependencies. Unlike the simple RNN neural network, LSTM networks can deal with large sequences."),N(w,"class","separator")},m(d,M){v(document.head,n),l(d,t,M),l(d,r,M),v(r,o),l(d,y,M),l(d,w,M),l(d,b,M),g(W,d,M),l(d,P,M),g(L,d,M),j=!0},p(d,[M]){const C={};M&16&&(C.$$scope={dirty:M,ctx:d}),W.$set(C)},i(d){j||(m(W.$$.fragment,d),m(L.$$.fragment,d),j=!0)},o(d){h(W.$$.fragment,d),h(L.$$.fragment,d),j=!1},d(d){a(n),d&&a(t),d&&a(r),d&&a(y),d&&a(w),d&&a(b),c(W,d),d&&a(P),c(L,d)}}}function Fa($){return[[{author:"S. Hochreiter and J. Schmidhuber",title:"Long Short-Term Memory",journal:"Neural Computation,",year:"1997",pages:"1735-1780",volume:"9",issue:"8"}]]}class es extends Fr{constructor(n){super(),Rr(this,n,Fa,Ca,Vr,{})}}export{es as default};
