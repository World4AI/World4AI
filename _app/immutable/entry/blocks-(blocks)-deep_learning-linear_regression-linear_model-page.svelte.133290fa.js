import{S as Ws,i as Es,s as Ss,k as P,a as _,y as m,W as qs,l as W,h as r,c as b,z as u,n as Qe,N as w,b as o,A as p,g as h,d as c,B as g,w as vs,a9 as xs,q as $,m as E,r as f,aa as ys,C as B}from"../chunks/index.4d92b023.js";import{C as zs}from"../chunks/Container.b0705c7b.js";import{L as y}from"../chunks/Latex.e0b308c0.js";import{H as Mt}from"../chunks/Highlight.b7c1de53.js";import{S as Ts}from"../chunks/Slider.93409d64.js";import{A as Is}from"../chunks/Alert.25a852b3.js";import{P as Lt}from"../chunks/PythonCode.212ba7a6.js";import{P as La,T as Va}from"../chunks/Ticks.45eca5c5.js";import{C as ja}from"../chunks/Circle.f281e92b.js";import{X as Ua,Y as Ya}from"../chunks/YLabel.182e66a3.js";import{P as Na}from"../chunks/Path.7e6df014.js";function Xs(i){let n;return{c(){n=$("linear")},l(t){n=f(t,"linear")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Fs(i){let n;return{c(){n=$("regression")},l(t){n=f(t,"regression")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Os(i){let n,t,a,d,v,T,k,q,S,z;return n=new Va({props:{xTicks:[-100,-80,-60,-40,-20,0,20,40,60,80,100],yTicks:[-500,-400,-300,-200,-100,0,100,200,300,400,500],fontSize:18,xOffset:-25,yOffset:30}}),a=new Ua({props:{text:"Feature",fontSize:30,x:280}}),v=new Ya({props:{text:"Target",fontSize:30,x:15}}),k=new Na({props:{data:[{x:-100,y:-500},{x:100,y:500}]}}),S=new ja({props:{data:i[3],radius:3}}),{c(){m(n.$$.fragment),t=_(),m(a.$$.fragment),d=_(),m(v.$$.fragment),T=_(),m(k.$$.fragment),q=_(),m(S.$$.fragment)},l(l){u(n.$$.fragment,l),t=b(l),u(a.$$.fragment,l),d=b(l),u(v.$$.fragment,l),T=b(l),u(k.$$.fragment,l),q=b(l),u(S.$$.fragment,l)},m(l,x){p(n,l,x),o(l,t,x),p(a,l,x),o(l,d,x),p(v,l,x),o(l,T,x),p(k,l,x),o(l,q,x),p(S,l,x),z=!0},p:B,i(l){z||(h(n.$$.fragment,l),h(a.$$.fragment,l),h(v.$$.fragment,l),h(k.$$.fragment,l),h(S.$$.fragment,l),z=!0)},o(l){c(n.$$.fragment,l),c(a.$$.fragment,l),c(v.$$.fragment,l),c(k.$$.fragment,l),c(S.$$.fragment,l),z=!1},d(l){g(n,l),l&&r(t),g(a,l),l&&r(d),g(v,l),l&&r(T),g(k,l),l&&r(q),g(S,l)}}}function Cs(i){let n,t,a,d,v,T,k,q,S,z;return n=new Va({props:{xTicks:[-100,-80,-60,-40,-20,0,20,40,60,80,100],yTicks:[0,1e3,2e3,3e3,4e3,5e3,6e3,7e3,8e3,9e3,1e4],fontSize:18,xOffset:-25,yOffset:45}}),a=new Ua({props:{text:"Feature",fontSize:30}}),v=new Ya({props:{text:"Target",fontSize:30,x:15}}),k=new ja({props:{data:i[4],radius:3}}),S=new Na({props:{data:[{x:-100,y:1e4},{x:100,y:0}]}}),{c(){m(n.$$.fragment),t=_(),m(a.$$.fragment),d=_(),m(v.$$.fragment),T=_(),m(k.$$.fragment),q=_(),m(S.$$.fragment)},l(l){u(n.$$.fragment,l),t=b(l),u(a.$$.fragment,l),d=b(l),u(v.$$.fragment,l),T=b(l),u(k.$$.fragment,l),q=b(l),u(S.$$.fragment,l)},m(l,x){p(n,l,x),o(l,t,x),p(a,l,x),o(l,d,x),p(v,l,x),o(l,T,x),p(k,l,x),o(l,q,x),p(S,l,x),z=!0},p:B,i(l){z||(h(n.$$.fragment,l),h(a.$$.fragment,l),h(v.$$.fragment,l),h(k.$$.fragment,l),h(S.$$.fragment,l),z=!0)},o(l){c(n.$$.fragment,l),c(a.$$.fragment,l),c(v.$$.fragment,l),c(k.$$.fragment,l),c(S.$$.fragment,l),z=!1},d(l){g(n,l),l&&r(t),g(a,l),l&&r(d),g(v,l),l&&r(T),g(k,l),l&&r(q),g(S,l)}}}function As(i){let n;return{c(){n=$("y = xw + b")},l(t){n=f(t,"y = xw + b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Bs(i){let n;return{c(){n=$("x")},l(t){n=f(t,"x")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Ds(i){let n;return{c(){n=$("y")},l(t){n=f(t,"y")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Ms(i){let n;return{c(){n=$("w")},l(t){n=f(t,"w")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Ls(i){let n;return{c(){n=$("b")},l(t){n=f(t,"b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Vs(i){let n;return{c(){n=$("x")},l(t){n=f(t,"x")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function js(i){let n;return{c(){n=$("y")},l(t){n=f(t,"y")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Us(i){let n;return{c(){n=$("w")},l(t){n=f(t,"w")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Ys(i){let n;return{c(){n=$("b")},l(t){n=f(t,"b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Ns(i){let n;return{c(){n=$("y = xw + b")},l(t){n=f(t,"y = xw + b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Hs(i){let n;return{c(){n=$("y")},l(t){n=f(t,"y")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Gs(i){let n;return{c(){n=$("x")},l(t){n=f(t,"x")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Rs(i){let n;return{c(){n=$("y = 0w + b")},l(t){n=f(t,"y = 0w + b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Js(i){let n;return{c(){n=$("b")},l(t){n=f(t,"b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Ks(i){let n;return{c(){n=$("x")},l(t){n=f(t,"x")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Qs(i){let n;return{c(){n=$("w")},l(t){n=f(t,"w")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Zs(i){let n;return{c(){n=$("y = x*5cm + 50cm")},l(t){n=f(t,"y = x*5cm + 50cm")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function er(i){let n;return{c(){n=$(`When we use a linear regression model, we assume a linear relationship
    between the inputs and the output. If you apply linear regression to data
    that is nonlinear in nature, you might get illogical results.`)},l(t){n=f(t,`When we use a linear regression model, we assume a linear relationship
    between the inputs and the output. If you apply linear regression to data
    that is nonlinear in nature, you might get illogical results.`)},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function tr(i){let n;return{c(){n=$("y = xw + b")},l(t){n=f(t,"y = xw + b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function nr(i){let n;return{c(){n=$("fits")},l(t){n=f(t,"fits")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function ar(i){let n,t,a,d,v,T,k,q,S,z;return n=new Va({props:{xTicks:[-100,-80,-60,-40,-20,0,20,40,60,80,100],yTicks:[-500,-400,-300,-200,-100,0,100,200,300,400,500],fontSize:18,xOffset:-25,yOffset:30}}),a=new Ua({props:{text:"Feature",fontSize:30,x:280}}),v=new Ya({props:{text:"Target",fontSize:30,x:15}}),k=new ja({props:{data:i[3],radius:3}}),S=new Na({props:{data:i[2],stroke:2}}),{c(){m(n.$$.fragment),t=_(),m(a.$$.fragment),d=_(),m(v.$$.fragment),T=_(),m(k.$$.fragment),q=_(),m(S.$$.fragment)},l(l){u(n.$$.fragment,l),t=b(l),u(a.$$.fragment,l),d=b(l),u(v.$$.fragment,l),T=b(l),u(k.$$.fragment,l),q=b(l),u(S.$$.fragment,l)},m(l,x){p(n,l,x),o(l,t,x),p(a,l,x),o(l,d,x),p(v,l,x),o(l,T,x),p(k,l,x),o(l,q,x),p(S,l,x),z=!0},p(l,x){const M={};x&4&&(M.data=l[2]),S.$set(M)},i(l){z||(h(n.$$.fragment,l),h(a.$$.fragment,l),h(v.$$.fragment,l),h(k.$$.fragment,l),h(S.$$.fragment,l),z=!0)},o(l){c(n.$$.fragment,l),c(a.$$.fragment,l),c(v.$$.fragment,l),c(k.$$.fragment,l),c(S.$$.fragment,l),z=!1},d(l){g(n,l),l&&r(t),g(a,l),l&&r(d),g(v,l),l&&r(T),g(k,l),l&&r(q),g(S,l)}}}function sr(i){let n;return{c(){n=$("w")},l(t){n=f(t,"w")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function rr(i){let n;return{c(){n=$("b")},l(t){n=f(t,"b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function lr(i){let n;return{c(){n=$("fits")},l(t){n=f(t,"fits")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function or(i){let n;return{c(){n=$("y = x_1w_1 + x_2w_2 + ... + x_mw_m+ b")},l(t){n=f(t,"y = x_1w_1 + x_2w_2 + ... + x_mw_m+ b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function ir(i){let n=String.raw`
    y = \mathbf{x} \mathbf{w}^T + b \\
    \mathbf{x} = 
    \begin{bmatrix}
       x_1 & x_2 & \cdots & x_n
    \end{bmatrix} \\
    \mathbf{w} = 
    \begin{bmatrix}
      w_1 & 
      w_2 & 
      \cdots &
      w_m
    \end{bmatrix}
`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function $r(i){let n;return{c(){n=$("y")},l(t){n=f(t,"y")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function fr(i){let n=String.raw`\hat{y} = \mathbf{x} \mathbf{w}^T + b`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function mr(i){let n;return{c(){n=$("y")},l(t){n=f(t,"y")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function ur(i){let n=String.raw`\mathbf{x}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function pr(i){let n=String.raw`\mathbf{X}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function hr(i){let n=String.raw`
      \mathbf{X} =
      \begin{bmatrix}
      x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & \cdots & x_m^{(1)} \\
      x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & \cdots & x_m^{(2)} \\
      x_1^{(3)} & x_2^{(3)} & x_3^{(/3)} & \cdots & x_m^{(3)} \\
      \vdots & \vdots & \vdots & \cdots & \vdots \\
      x_1^{(n)} & x_2^{(n)} & x_3^{(n)} & \cdots & x_m^{(n)} \\
      \end{bmatrix}
    `+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function cr(i){let n=String.raw`\mathbf{X}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function gr(i){let n;return{c(){n=$("n \\times m")},l(t){n=f(t,"n \\times m")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function wr(i){let n;return{c(){n=$("n")},l(t){n=f(t,"n")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function dr(i){let n;return{c(){n=$("m")},l(t){n=f(t,"m")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function _r(i){let n=String.raw`\mathbf{X}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function br(i){let n=String.raw`\mathbf{w}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function vr(i){let n;return{c(){n=$("b")},l(t){n=f(t,"b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function xr(i){let n=String.raw`\mathbf{\hat{y}}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function yr(i){let n=String.raw`
      \mathbf{\hat{y}} = \mathbf{X}\mathbf{w}^T + b
    `+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function Tr(i){let n=String.raw`
      \mathbf{\hat{y}} = \mathbf{X}\mathbf{w}^T + b
    `+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function kr(i){let n=String.raw`\mathbf{Xw}^T`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function Pr(i){let n;return{c(){n=$("b")},l(t){n=f(t,"b")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Wr(i){let n;return{c(){n=$("broadcasting")},l(t){n=f(t,"broadcasting")},m(t,a){o(t,n,a)},d(t){t&&r(n)}}}function Er(i){let n=String.raw`\mathbf{Xw}^T`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function Sr(i){let n=String.raw`\mathbf{X}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function qr(i){let n=String.raw`\mathbf{y}`+"",t;return{c(){t=$(n)},l(a){t=f(a,n)},m(a,d){o(a,t,d)},p:B,d(a){a&&r(t)}}}function zr(i){let n,t,a,d,v,T,k,q,S,z,l,x,M,vt,Vt,Z,jt,mt,En,Ut,ee,Yt,I,Sn,te,qn,ne,zn,ae,In,se,Xn,re,Fn,le,On,oe,Cn,ie,An,$e,Bn,Nt,F,Dn,fe,Mn,me,Ln,ue,Vn,pe,jn,he,Un,ce,Yn,ge,Nn,we,Hn,Ht,de,Gt,U,Gn,_e,Rn,be,Jn,Rt,ve,Jt,H,Kn,Kt,G,Qn,Qt,Y,Zn,xe,ea,ye,ta,Zt,Te,na,ke,aa,en,ut,sa,tn,Pe,nn,pt,ra,an,We,sn,ht,la,rn,D,oa,Ee,ia,Se,$a,qe,fa,ze,ma,Ie,ua,ln,Ze,Xe,on,O,Fe,pa,Oe,ha,Ce,ca,Ae,ga,Be,wa,De,da,Me,_a,Le,ba,$n,et,Ve,fn,ct,va,mn,gt,xa,un,tt,je,pn,V,ya,Ue,Ta,Ye,ka,Ne,Pa,He,Wa,hn,wt,Ea,cn,nt,gn,Ge,Sa,xt,qa,za,wn,at,dn,N,Ia,Re,Xa,Je,Fa,_n,st,bn,Ke,Oa,yt,Ca,Aa,vn,rt,xn,dt,Ba,yn,lt,Tn,ot,Da,kn,_t,Ma,Pn,bt,Wn;q=new Mt({props:{$$slots:{default:[Xs]},$$scope:{ctx:i}}}),z=new Mt({props:{$$slots:{default:[Fs]},$$scope:{ctx:i}}}),Z=new La({props:{width:500,height:500,maxWidth:500,domain:[-100,100],range:[-500,500],padding:{top:40,right:15,bottom:65,left:65},$$slots:{default:[Os]},$$scope:{ctx:i}}}),ee=new La({props:{width:500,height:500,maxWidth:500,domain:[-100,100],range:[0,1e4],padding:{top:40,right:14,bottom:65,left:100},$$slots:{default:[Cs]},$$scope:{ctx:i}}}),te=new y({props:{$$slots:{default:[As]},$$scope:{ctx:i}}}),ne=new y({props:{$$slots:{default:[Bs]},$$scope:{ctx:i}}}),ae=new y({props:{$$slots:{default:[Ds]},$$scope:{ctx:i}}}),se=new y({props:{$$slots:{default:[Ms]},$$scope:{ctx:i}}}),re=new y({props:{$$slots:{default:[Ls]},$$scope:{ctx:i}}}),le=new y({props:{$$slots:{default:[Vs]},$$scope:{ctx:i}}}),oe=new y({props:{$$slots:{default:[js]},$$scope:{ctx:i}}}),ie=new y({props:{$$slots:{default:[Us]},$$scope:{ctx:i}}}),$e=new y({props:{$$slots:{default:[Ys]},$$scope:{ctx:i}}}),fe=new y({props:{$$slots:{default:[Ns]},$$scope:{ctx:i}}}),me=new y({props:{$$slots:{default:[Hs]},$$scope:{ctx:i}}}),ue=new y({props:{$$slots:{default:[Gs]},$$scope:{ctx:i}}}),pe=new y({props:{$$slots:{default:[Rs]},$$scope:{ctx:i}}}),he=new y({props:{$$slots:{default:[Js]},$$scope:{ctx:i}}}),ce=new y({props:{$$slots:{default:[Ks]},$$scope:{ctx:i}}}),ge=new y({props:{$$slots:{default:[Qs]},$$scope:{ctx:i}}}),we=new y({props:{$$slots:{default:[Zs]},$$scope:{ctx:i}}}),de=new Is({props:{type:"warning",$$slots:{default:[er]},$$scope:{ctx:i}}}),_e=new y({props:{$$slots:{default:[tr]},$$scope:{ctx:i}}}),be=new Mt({props:{$$slots:{default:[nr]},$$scope:{ctx:i}}}),ve=new La({props:{width:500,height:500,maxWidth:500,domain:[-100,100],range:[-500,500],padding:{top:40,right:15,bottom:65,left:65},$$slots:{default:[ar]},$$scope:{ctx:i}}});function ks(e){i[5](e)}let Ha={label:"Weight",labelId:"weight",showValue:!0,min:-200,max:200};i[1]!==void 0&&(Ha.value=i[1]),H=new Ts({props:Ha}),vs.push(()=>xs(H,"value",ks));function Ps(e){i[6](e)}let Ga={label:"Bias",labelId:"bias",showValue:!0,min:-500,max:500};return i[0]!==void 0&&(Ga.value=i[0]),G=new Ts({props:Ga}),vs.push(()=>xs(G,"value",Ps)),xe=new y({props:{$$slots:{default:[sr]},$$scope:{ctx:i}}}),ye=new y({props:{$$slots:{default:[rr]},$$scope:{ctx:i}}}),ke=new Mt({props:{$$slots:{default:[lr]},$$scope:{ctx:i}}}),Pe=new y({props:{$$slots:{default:[or]},$$scope:{ctx:i}}}),We=new y({props:{$$slots:{default:[ir]},$$scope:{ctx:i}}}),Ee=new y({props:{$$slots:{default:[$r]},$$scope:{ctx:i}}}),Se=new y({props:{$$slots:{default:[fr]},$$scope:{ctx:i}}}),qe=new y({props:{$$slots:{default:[mr]},$$scope:{ctx:i}}}),ze=new y({props:{$$slots:{default:[ur]},$$scope:{ctx:i}}}),Ie=new y({props:{$$slots:{default:[pr]},$$scope:{ctx:i}}}),Xe=new y({props:{$$slots:{default:[hr]},$$scope:{ctx:i}}}),Fe=new y({props:{$$slots:{default:[cr]},$$scope:{ctx:i}}}),Oe=new y({props:{$$slots:{default:[gr]},$$scope:{ctx:i}}}),Ce=new y({props:{$$slots:{default:[wr]},$$scope:{ctx:i}}}),Ae=new y({props:{$$slots:{default:[dr]},$$scope:{ctx:i}}}),Be=new y({props:{$$slots:{default:[_r]},$$scope:{ctx:i}}}),De=new y({props:{$$slots:{default:[br]},$$scope:{ctx:i}}}),Me=new y({props:{$$slots:{default:[vr]},$$scope:{ctx:i}}}),Le=new y({props:{$$slots:{default:[xr]},$$scope:{ctx:i}}}),Ve=new y({props:{$$slots:{default:[yr]},$$scope:{ctx:i}}}),je=new y({props:{$$slots:{default:[Tr]},$$scope:{ctx:i}}}),Ue=new y({props:{$$slots:{default:[kr]},$$scope:{ctx:i}}}),Ye=new y({props:{$$slots:{default:[Pr]},$$scope:{ctx:i}}}),Ne=new Mt({props:{$$slots:{default:[Wr]},$$scope:{ctx:i}}}),He=new y({props:{$$slots:{default:[Er]},$$scope:{ctx:i}}}),nt=new Lt({props:{code:String.raw`import torch
import sklearn.datasets as datasets
`}}),at=new Lt({props:{code:String.raw`X, y = datasets.make_regression(n_samples=100, n_features=2, n_informative=2, noise=0.01)
`}}),Re=new y({props:{$$slots:{default:[Sr]},$$scope:{ctx:i}}}),Je=new y({props:{$$slots:{default:[qr]},$$scope:{ctx:i}}}),st=new Lt({props:{code:String.raw`X = torch.from_numpy(X).to(torch.float32)
y = torch.from_numpy(y).to(torch.float32)
`}}),rt=new Lt({props:{code:String.raw`w = torch.randn(1, 2)
b = torch.randn(1, 1)
`}}),lt=new Lt({props:{code:String.raw`y_hat = X @ w.T + b
y_hat.shape
`}}),{c(){n=P("h1"),t=$("Linear Model"),a=_(),d=P("div"),v=_(),T=P("p"),k=$(`The term "linear regression" consists of two words, that fully describe the
    type of model we are dealing with: `),m(q.$$.fragment),S=$(" and "),m(z.$$.fragment),l=$(`. The "regression" part signifies that our model predicts a numeric target
    variable based on given features and we are not dealing with a
    classification task. The "linear" part suggests that linear regression can
    only model a linear relationship between features and targets. To clarify
    what the words "linear relationship" mean we present two examples below.`),x=_(),M=P("p"),vt=$(`In the first scatterplot we could plot a line that goes from the coordinates
    of (-100, -500) to coordinates (100, 500). While there is some randomness in
    the data, the line would depict the relationship between the feature and the
    target relatively well. When we get new data points we can use the line to
    predict the target and be relatively confident regarding the outcome.`),Vt=_(),m(Z.$$.fragment),jt=_(),mt=P("p"),En=$(`In contrast the data in the following scatterplot represents a nonlinear
    relationship between the feature and the target. Theoretically there is
    nothing that stops us from using linear regression for the below problem,
    but there are better alternatives (like neural networks) for non linear
    problems.`),Ut=_(),m(ee.$$.fragment),Yt=_(),I=P("p"),Sn=$(`From basic math we know, that in the two dimensional space we can draw a
    line using the equation `),m(te.$$.fragment),qn=$(", where "),m(ne.$$.fragment),zn=$(` is
    the only feature, `),m(ae.$$.fragment),In=$(" is the target, "),m(se.$$.fragment),Xn=$(` is the weight
    that we use to scale the feature and `),m(re.$$.fragment),Fn=$(` is the bias. While we can
    easily understand that the feature `),m(le.$$.fragment),On=$(` is the input of our equation
    and the label `),m(oe.$$.fragment),Cn=$(` is the output of the equation, we have a harder
    time imagining what role the weight `),m(ie.$$.fragment),An=$(" and the bias "),m($e.$$.fragment),Bn=$(" play in the equation. Below we present two possible interpretations."),Nt=_(),F=P("p"),Dn=$("When we look at the equation "),m(fe.$$.fragment),Mn=$(` from the arithmetic perspective,
    we should notice two things. First, the output `),m(me.$$.fragment),Ln=$(` equals the bias
    when the input `),m(ue.$$.fragment),Vn=$(" is 0: "),m(pe.$$.fragment),jn=$(`. The bias in
    a way encompasses a starting point for the calculation of the output. If for
    example we tried to model the relationship between age and height, even at
    birth (age 0) a human would have some average height, which would be encoded
    in the bias `),m(he.$$.fragment),Un=$(". Second, for each unit of "),m(ce.$$.fragment),Yn=$(`, the
    output increases by exactly `),m(ge.$$.fragment),Nn=$(". The equation "),m(we.$$.fragment),Hn=$(` would indicate that on average a human grows by 5cm for each year in life.
    At this point you would hopefully interject that this relation is out of touch
    with reality. For once the equation does not reflect that a human being grows
    up to a certain length or that a child grows at a higher rate, than a young adult.
    At a certain age people even start to shrink. While all these points are valid,
    we make specific assumtions, when we model the world using linear regression.`),Ht=_(),m(de.$$.fragment),Gt=_(),U=P("p"),Gn=$("When on the other hand we look at the equation "),m(_e.$$.fragment),Rn=$(` from
    the geometric perspective, we should realize, that weight determines the rotation
    (slope) of the line while the bias determines the horizontal position. Below
    we present an interactive example to demonstrate the impact of the weight and
    the bias on the the regression line. You can move the two sliders to change the
    weight and the bias. Observe what we mean when we say rotation and position.
    Try to position the line, such that it `),m(be.$$.fragment),Jn=$(` the data as
    good as possible.`),Rt=_(),m(ve.$$.fragment),Jt=_(),m(H.$$.fragment),Kt=_(),m(G.$$.fragment),Qt=_(),Y=P("p"),Zn=$("We used the weight "),m(xe.$$.fragment),ea=$(" of 5 and the bias "),m(ye.$$.fragment),ta=$(` of 0 plus
    some randomness to generate the data above. When you played with sliders you
    should have come relatively close.`),Zt=_(),Te=P("p"),na=$(`The weight and the bias are learnable parameters. The linear regression
    algorithm provides us with a way to find those parameters. You can imagine
    that the algorithm rotates and moves the line, until the line `),m(ke.$$.fragment),aa=$(" the data. This process is called data or curve fitting."),en=_(),ut=P("p"),sa=$(`In practice we rarely deal with a dataset where we only have one feature. In
    that case our equation looks as follows.`),tn=_(),m(Pe.$$.fragment),nn=_(),pt=P("p"),ra=$("We can also use a more compact form and write the equation in vector form."),an=_(),m(We.$$.fragment),sn=_(),ht=P("p"),la=$(`In a three dimensional space we calculate a two dimensional plane that
    divides the coordinate system into two regions. This procedure is harder to
    imagine for more than 3 dimensions, but we still create a plane (a so called
    hyperplane) in space. The weights are used to rotate the hyperplane while
    the bias moves the plane.`),rn=_(),D=P("p"),oa=$(`When we use linear regression to make predictions based on features, we draw
    a "hat" over the `),m(Ee.$$.fragment),ia=$(` value to indicate that we are dealing with
    a prediction from a model,
    `),m(Se.$$.fragment),$a=$(". The "),m(qe.$$.fragment),fa=$(` value on the other hand represents the actual target from the dataset, the
    so called ground truth. Usually we want to create predictions not for a single
    sample `),m(ze.$$.fragment),ma=$(`, but for a whole dataset
    `),m(Ie.$$.fragment),ua=$("."),ln=_(),Ze=P("div"),m(Xe.$$.fragment),on=_(),O=P("p"),m(Fe.$$.fragment),pa=$(" is an "),m(Oe.$$.fragment),ha=$(` matrix,
    where `),m(Ce.$$.fragment),ca=$(" (rows) is the number of samples and "),m(Ae.$$.fragment),ga=$(` (columns)
    is the number of input features. We can multiply the dataset matrix `),m(Be.$$.fragment),wa=$(" with the transposed weight vector"),m(De.$$.fragment),da=$(` and
    add the bias `),m(Me.$$.fragment),_a=$(" to generate a prediction vector "),m(Le.$$.fragment),ba=$("."),$n=_(),et=P("div"),m(Ve.$$.fragment),fn=_(),ct=P("p"),va=$(`The advantage of the above procedure is not only due to a more compact
    representation, but has also practical implications. Matrix operations in
    all modern deep learning frameworks can be parallelized. Therefore when you
    utilize matrix notation in your code, you actually make use of that
    parallelism and can speed up your code tremendously. Think about it. Each
    row of the dataset can be multiplied with the weight vector independently.
    By outsourcing the calculations to different CPU or GPU cores, a lot of
    computation time can be saved.`),mn=_(),gt=P("p"),xa=$(`By this point you might have noticed, that there is something fishy about
    the expression.`),un=_(),tt=P("div"),m(je.$$.fragment),pn=_(),V=P("p"),ya=$("On the one side we have a vector that results from "),m(Ue.$$.fragment),Ta=$(", on the other side we have a scalar "),m(Ye.$$.fragment),ka=$(`. From a mathematical
    standpoint adding a scalar to a vector is techincally not allowed. From the
    programming standpoint this procedure is valid, because NumPy and all deep
    leanring frameworks utilize a technique called `),m(Ne.$$.fragment),Pa=$(`. We will have a closer look at broadcasting in our practical sessions, for
    now it is sufficient to know, that broadcasting expands scalars, vectors and
    matrices in order for the calculations to make sense. In our example above
    for example, the scalar would be expanded into a vector, which would be of
    the same size as the vector that results from `),m(He.$$.fragment),Wa=$(`. We will often include notation that incorporates broadcasting in order to
    make the notation more similar to our Python code.`),hn=_(),wt=P("p"),Ea=$("Now let's see how we can impelemnt this idea of a linear model in PyTorch."),cn=_(),m(nt.$$.fragment),gn=_(),Ge=P("p"),Sa=$("We make use of the "),xt=P("code"),qa=$("make_regression()"),za=$(` function from the sklearn library
    to make a dataset with 100 samples and 2 features.`),wn=_(),m(at.$$.fragment),dn=_(),N=P("p"),Ia=$("The above function returns numpy arrays "),m(Re.$$.fragment),Xa=$(" and "),m(Je.$$.fragment),Fa=$(` and we transform those into PyTorch
    tensors.`),_n=_(),m(st.$$.fragment),bn=_(),Ke=P("p"),Oa=$("We initialze the two weights and the bias randomly, using the "),yt=P("code"),Ca=$("torch.randn()"),Aa=$(` function. This function returns random variables, that are drawn from the standard
    normal distribution.`),vn=_(),m(rt.$$.fragment),xn=_(),dt=P("p"),Ba=$("The actual model predictions can be calculated using a one liner."),yn=_(),m(lt.$$.fragment),Tn=_(),ot=P("pre"),Da=$("torch.Size([100, 1])"),kn=_(),_t=P("p"),Ma=$(`While it is relatively easy to use a linear model in PyTorch, we have still
    not encountered any methods to generate predictions that are as close to the
    true labels in the dataset as possible. In the next sections we are going to
    cover how the learning procedure actually works.`),Pn=_(),bt=P("div"),this.h()},l(e){n=W(e,"H1",{});var s=E(n);t=f(s,"Linear Model"),s.forEach(r),a=b(e),d=W(e,"DIV",{class:!0}),E(d).forEach(r),v=b(e),T=W(e,"P",{});var R=E(T);k=f(R,`The term "linear regression" consists of two words, that fully describe the
    type of model we are dealing with: `),u(q.$$.fragment,R),S=f(R," and "),u(z.$$.fragment,R),l=f(R,`. The "regression" part signifies that our model predicts a numeric target
    variable based on given features and we are not dealing with a
    classification task. The "linear" part suggests that linear regression can
    only model a linear relationship between features and targets. To clarify
    what the words "linear relationship" mean we present two examples below.`),R.forEach(r),x=b(e),M=W(e,"P",{});var Tt=E(M);vt=f(Tt,`In the first scatterplot we could plot a line that goes from the coordinates
    of (-100, -500) to coordinates (100, 500). While there is some randomness in
    the data, the line would depict the relationship between the feature and the
    target relatively well. When we get new data points we can use the line to
    predict the target and be relatively confident regarding the outcome.`),Tt.forEach(r),Vt=b(e),u(Z.$$.fragment,e),jt=b(e),mt=W(e,"P",{});var kt=E(mt);En=f(kt,`In contrast the data in the following scatterplot represents a nonlinear
    relationship between the feature and the target. Theoretically there is
    nothing that stops us from using linear regression for the below problem,
    but there are better alternatives (like neural networks) for non linear
    problems.`),kt.forEach(r),Ut=b(e),u(ee.$$.fragment,e),Yt=b(e),I=W(e,"P",{});var X=E(I);Sn=f(X,`From basic math we know, that in the two dimensional space we can draw a
    line using the equation `),u(te.$$.fragment,X),qn=f(X,", where "),u(ne.$$.fragment,X),zn=f(X,` is
    the only feature, `),u(ae.$$.fragment,X),In=f(X," is the target, "),u(se.$$.fragment,X),Xn=f(X,` is the weight
    that we use to scale the feature and `),u(re.$$.fragment,X),Fn=f(X,` is the bias. While we can
    easily understand that the feature `),u(le.$$.fragment,X),On=f(X,` is the input of our equation
    and the label `),u(oe.$$.fragment,X),Cn=f(X,` is the output of the equation, we have a harder
    time imagining what role the weight `),u(ie.$$.fragment,X),An=f(X," and the bias "),u($e.$$.fragment,X),Bn=f(X," play in the equation. Below we present two possible interpretations."),X.forEach(r),Nt=b(e),F=W(e,"P",{});var C=E(F);Dn=f(C,"When we look at the equation "),u(fe.$$.fragment,C),Mn=f(C,` from the arithmetic perspective,
    we should notice two things. First, the output `),u(me.$$.fragment,C),Ln=f(C,` equals the bias
    when the input `),u(ue.$$.fragment,C),Vn=f(C," is 0: "),u(pe.$$.fragment,C),jn=f(C,`. The bias in
    a way encompasses a starting point for the calculation of the output. If for
    example we tried to model the relationship between age and height, even at
    birth (age 0) a human would have some average height, which would be encoded
    in the bias `),u(he.$$.fragment,C),Un=f(C,". Second, for each unit of "),u(ce.$$.fragment,C),Yn=f(C,`, the
    output increases by exactly `),u(ge.$$.fragment,C),Nn=f(C,". The equation "),u(we.$$.fragment,C),Hn=f(C,` would indicate that on average a human grows by 5cm for each year in life.
    At this point you would hopefully interject that this relation is out of touch
    with reality. For once the equation does not reflect that a human being grows
    up to a certain length or that a child grows at a higher rate, than a young adult.
    At a certain age people even start to shrink. While all these points are valid,
    we make specific assumtions, when we model the world using linear regression.`),C.forEach(r),Ht=b(e),u(de.$$.fragment,e),Gt=b(e),U=W(e,"P",{});var J=E(U);Gn=f(J,"When on the other hand we look at the equation "),u(_e.$$.fragment,J),Rn=f(J,` from
    the geometric perspective, we should realize, that weight determines the rotation
    (slope) of the line while the bias determines the horizontal position. Below
    we present an interactive example to demonstrate the impact of the weight and
    the bias on the the regression line. You can move the two sliders to change the
    weight and the bias. Observe what we mean when we say rotation and position.
    Try to position the line, such that it `),u(be.$$.fragment,J),Jn=f(J,` the data as
    good as possible.`),J.forEach(r),Rt=b(e),u(ve.$$.fragment,e),Jt=b(e),u(H.$$.fragment,e),Kt=b(e),u(G.$$.fragment,e),Qt=b(e),Y=W(e,"P",{});var K=E(Y);Zn=f(K,"We used the weight "),u(xe.$$.fragment,K),ea=f(K," of 5 and the bias "),u(ye.$$.fragment,K),ta=f(K,` of 0 plus
    some randomness to generate the data above. When you played with sliders you
    should have come relatively close.`),K.forEach(r),Zt=b(e),Te=W(e,"P",{});var it=E(Te);na=f(it,`The weight and the bias are learnable parameters. The linear regression
    algorithm provides us with a way to find those parameters. You can imagine
    that the algorithm rotates and moves the line, until the line `),u(ke.$$.fragment,it),aa=f(it," the data. This process is called data or curve fitting."),it.forEach(r),en=b(e),ut=W(e,"P",{});var Pt=E(ut);sa=f(Pt,`In practice we rarely deal with a dataset where we only have one feature. In
    that case our equation looks as follows.`),Pt.forEach(r),tn=b(e),u(Pe.$$.fragment,e),nn=b(e),pt=W(e,"P",{});var Wt=E(pt);ra=f(Wt,"We can also use a more compact form and write the equation in vector form."),Wt.forEach(r),an=b(e),u(We.$$.fragment,e),sn=b(e),ht=W(e,"P",{});var Et=E(ht);la=f(Et,`In a three dimensional space we calculate a two dimensional plane that
    divides the coordinate system into two regions. This procedure is harder to
    imagine for more than 3 dimensions, but we still create a plane (a so called
    hyperplane) in space. The weights are used to rotate the hyperplane while
    the bias moves the plane.`),Et.forEach(r),rn=b(e),D=W(e,"P",{});var L=E(D);oa=f(L,`When we use linear regression to make predictions based on features, we draw
    a "hat" over the `),u(Ee.$$.fragment,L),ia=f(L,` value to indicate that we are dealing with
    a prediction from a model,
    `),u(Se.$$.fragment,L),$a=f(L,". The "),u(qe.$$.fragment,L),fa=f(L,` value on the other hand represents the actual target from the dataset, the
    so called ground truth. Usually we want to create predictions not for a single
    sample `),u(ze.$$.fragment,L),ma=f(L,`, but for a whole dataset
    `),u(Ie.$$.fragment,L),ua=f(L,"."),L.forEach(r),ln=b(e),Ze=W(e,"DIV",{class:!0});var St=E(Ze);u(Xe.$$.fragment,St),St.forEach(r),on=b(e),O=W(e,"P",{});var A=E(O);u(Fe.$$.fragment,A),pa=f(A," is an "),u(Oe.$$.fragment,A),ha=f(A,` matrix,
    where `),u(Ce.$$.fragment,A),ca=f(A," (rows) is the number of samples and "),u(Ae.$$.fragment,A),ga=f(A,` (columns)
    is the number of input features. We can multiply the dataset matrix `),u(Be.$$.fragment,A),wa=f(A," with the transposed weight vector"),u(De.$$.fragment,A),da=f(A,` and
    add the bias `),u(Me.$$.fragment,A),_a=f(A," to generate a prediction vector "),u(Le.$$.fragment,A),ba=f(A,"."),A.forEach(r),$n=b(e),et=W(e,"DIV",{class:!0});var qt=E(et);u(Ve.$$.fragment,qt),qt.forEach(r),fn=b(e),ct=W(e,"P",{});var zt=E(ct);va=f(zt,`The advantage of the above procedure is not only due to a more compact
    representation, but has also practical implications. Matrix operations in
    all modern deep learning frameworks can be parallelized. Therefore when you
    utilize matrix notation in your code, you actually make use of that
    parallelism and can speed up your code tremendously. Think about it. Each
    row of the dataset can be multiplied with the weight vector independently.
    By outsourcing the calculations to different CPU or GPU cores, a lot of
    computation time can be saved.`),zt.forEach(r),mn=b(e),gt=W(e,"P",{});var It=E(gt);xa=f(It,`By this point you might have noticed, that there is something fishy about
    the expression.`),It.forEach(r),un=b(e),tt=W(e,"DIV",{class:!0});var Xt=E(tt);u(je.$$.fragment,Xt),Xt.forEach(r),pn=b(e),V=W(e,"P",{});var j=E(V);ya=f(j,"On the one side we have a vector that results from "),u(Ue.$$.fragment,j),Ta=f(j,", on the other side we have a scalar "),u(Ye.$$.fragment,j),ka=f(j,`. From a mathematical
    standpoint adding a scalar to a vector is techincally not allowed. From the
    programming standpoint this procedure is valid, because NumPy and all deep
    leanring frameworks utilize a technique called `),u(Ne.$$.fragment,j),Pa=f(j,`. We will have a closer look at broadcasting in our practical sessions, for
    now it is sufficient to know, that broadcasting expands scalars, vectors and
    matrices in order for the calculations to make sense. In our example above
    for example, the scalar would be expanded into a vector, which would be of
    the same size as the vector that results from `),u(He.$$.fragment,j),Wa=f(j,`. We will often include notation that incorporates broadcasting in order to
    make the notation more similar to our Python code.`),j.forEach(r),hn=b(e),wt=W(e,"P",{});var Ft=E(wt);Ea=f(Ft,"Now let's see how we can impelemnt this idea of a linear model in PyTorch."),Ft.forEach(r),cn=b(e),u(nt.$$.fragment,e),gn=b(e),Ge=W(e,"P",{});var $t=E(Ge);Sa=f($t,"We make use of the "),xt=W($t,"CODE",{});var Ot=E(xt);qa=f(Ot,"make_regression()"),Ot.forEach(r),za=f($t,` function from the sklearn library
    to make a dataset with 100 samples and 2 features.`),$t.forEach(r),wn=b(e),u(at.$$.fragment,e),dn=b(e),N=W(e,"P",{});var Q=E(N);Ia=f(Q,"The above function returns numpy arrays "),u(Re.$$.fragment,Q),Xa=f(Q," and "),u(Je.$$.fragment,Q),Fa=f(Q,` and we transform those into PyTorch
    tensors.`),Q.forEach(r),_n=b(e),u(st.$$.fragment,e),bn=b(e),Ke=W(e,"P",{});var ft=E(Ke);Oa=f(ft,"We initialze the two weights and the bias randomly, using the "),yt=W(ft,"CODE",{});var Ct=E(yt);Ca=f(Ct,"torch.randn()"),Ct.forEach(r),Aa=f(ft,` function. This function returns random variables, that are drawn from the standard
    normal distribution.`),ft.forEach(r),vn=b(e),u(rt.$$.fragment,e),xn=b(e),dt=W(e,"P",{});var At=E(dt);Ba=f(At,"The actual model predictions can be calculated using a one liner."),At.forEach(r),yn=b(e),u(lt.$$.fragment,e),Tn=b(e),ot=W(e,"PRE",{class:!0});var Bt=E(ot);Da=f(Bt,"torch.Size([100, 1])"),Bt.forEach(r),kn=b(e),_t=W(e,"P",{});var Dt=E(_t);Ma=f(Dt,`While it is relatively easy to use a linear model in PyTorch, we have still
    not encountered any methods to generate predictions that are as close to the
    true labels in the dataset as possible. In the next sections we are going to
    cover how the learning procedure actually works.`),Dt.forEach(r),Pn=b(e),bt=W(e,"DIV",{class:!0}),E(bt).forEach(r),this.h()},h(){Qe(d,"class","separator"),Qe(Ze,"class","flex justify-center"),Qe(et,"class","flex justify-center"),Qe(tt,"class","flex justify-center"),Qe(ot,"class","text-sm"),Qe(bt,"class","separator")},m(e,s){o(e,n,s),w(n,t),o(e,a,s),o(e,d,s),o(e,v,s),o(e,T,s),w(T,k),p(q,T,null),w(T,S),p(z,T,null),w(T,l),o(e,x,s),o(e,M,s),w(M,vt),o(e,Vt,s),p(Z,e,s),o(e,jt,s),o(e,mt,s),w(mt,En),o(e,Ut,s),p(ee,e,s),o(e,Yt,s),o(e,I,s),w(I,Sn),p(te,I,null),w(I,qn),p(ne,I,null),w(I,zn),p(ae,I,null),w(I,In),p(se,I,null),w(I,Xn),p(re,I,null),w(I,Fn),p(le,I,null),w(I,On),p(oe,I,null),w(I,Cn),p(ie,I,null),w(I,An),p($e,I,null),w(I,Bn),o(e,Nt,s),o(e,F,s),w(F,Dn),p(fe,F,null),w(F,Mn),p(me,F,null),w(F,Ln),p(ue,F,null),w(F,Vn),p(pe,F,null),w(F,jn),p(he,F,null),w(F,Un),p(ce,F,null),w(F,Yn),p(ge,F,null),w(F,Nn),p(we,F,null),w(F,Hn),o(e,Ht,s),p(de,e,s),o(e,Gt,s),o(e,U,s),w(U,Gn),p(_e,U,null),w(U,Rn),p(be,U,null),w(U,Jn),o(e,Rt,s),p(ve,e,s),o(e,Jt,s),p(H,e,s),o(e,Kt,s),p(G,e,s),o(e,Qt,s),o(e,Y,s),w(Y,Zn),p(xe,Y,null),w(Y,ea),p(ye,Y,null),w(Y,ta),o(e,Zt,s),o(e,Te,s),w(Te,na),p(ke,Te,null),w(Te,aa),o(e,en,s),o(e,ut,s),w(ut,sa),o(e,tn,s),p(Pe,e,s),o(e,nn,s),o(e,pt,s),w(pt,ra),o(e,an,s),p(We,e,s),o(e,sn,s),o(e,ht,s),w(ht,la),o(e,rn,s),o(e,D,s),w(D,oa),p(Ee,D,null),w(D,ia),p(Se,D,null),w(D,$a),p(qe,D,null),w(D,fa),p(ze,D,null),w(D,ma),p(Ie,D,null),w(D,ua),o(e,ln,s),o(e,Ze,s),p(Xe,Ze,null),o(e,on,s),o(e,O,s),p(Fe,O,null),w(O,pa),p(Oe,O,null),w(O,ha),p(Ce,O,null),w(O,ca),p(Ae,O,null),w(O,ga),p(Be,O,null),w(O,wa),p(De,O,null),w(O,da),p(Me,O,null),w(O,_a),p(Le,O,null),w(O,ba),o(e,$n,s),o(e,et,s),p(Ve,et,null),o(e,fn,s),o(e,ct,s),w(ct,va),o(e,mn,s),o(e,gt,s),w(gt,xa),o(e,un,s),o(e,tt,s),p(je,tt,null),o(e,pn,s),o(e,V,s),w(V,ya),p(Ue,V,null),w(V,Ta),p(Ye,V,null),w(V,ka),p(Ne,V,null),w(V,Pa),p(He,V,null),w(V,Wa),o(e,hn,s),o(e,wt,s),w(wt,Ea),o(e,cn,s),p(nt,e,s),o(e,gn,s),o(e,Ge,s),w(Ge,Sa),w(Ge,xt),w(xt,qa),w(Ge,za),o(e,wn,s),p(at,e,s),o(e,dn,s),o(e,N,s),w(N,Ia),p(Re,N,null),w(N,Xa),p(Je,N,null),w(N,Fa),o(e,_n,s),p(st,e,s),o(e,bn,s),o(e,Ke,s),w(Ke,Oa),w(Ke,yt),w(yt,Ca),w(Ke,Aa),o(e,vn,s),p(rt,e,s),o(e,xn,s),o(e,dt,s),w(dt,Ba),o(e,yn,s),p(lt,e,s),o(e,Tn,s),o(e,ot,s),w(ot,Da),o(e,kn,s),o(e,_t,s),w(_t,Ma),o(e,Pn,s),o(e,bt,s),Wn=!0},p(e,s){const R={};s&256&&(R.$$scope={dirty:s,ctx:e}),q.$set(R);const Tt={};s&256&&(Tt.$$scope={dirty:s,ctx:e}),z.$set(Tt);const kt={};s&256&&(kt.$$scope={dirty:s,ctx:e}),Z.$set(kt);const X={};s&256&&(X.$$scope={dirty:s,ctx:e}),ee.$set(X);const C={};s&256&&(C.$$scope={dirty:s,ctx:e}),te.$set(C);const J={};s&256&&(J.$$scope={dirty:s,ctx:e}),ne.$set(J);const K={};s&256&&(K.$$scope={dirty:s,ctx:e}),ae.$set(K);const it={};s&256&&(it.$$scope={dirty:s,ctx:e}),se.$set(it);const Pt={};s&256&&(Pt.$$scope={dirty:s,ctx:e}),re.$set(Pt);const Wt={};s&256&&(Wt.$$scope={dirty:s,ctx:e}),le.$set(Wt);const Et={};s&256&&(Et.$$scope={dirty:s,ctx:e}),oe.$set(Et);const L={};s&256&&(L.$$scope={dirty:s,ctx:e}),ie.$set(L);const St={};s&256&&(St.$$scope={dirty:s,ctx:e}),$e.$set(St);const A={};s&256&&(A.$$scope={dirty:s,ctx:e}),fe.$set(A);const qt={};s&256&&(qt.$$scope={dirty:s,ctx:e}),me.$set(qt);const zt={};s&256&&(zt.$$scope={dirty:s,ctx:e}),ue.$set(zt);const It={};s&256&&(It.$$scope={dirty:s,ctx:e}),pe.$set(It);const Xt={};s&256&&(Xt.$$scope={dirty:s,ctx:e}),he.$set(Xt);const j={};s&256&&(j.$$scope={dirty:s,ctx:e}),ce.$set(j);const Ft={};s&256&&(Ft.$$scope={dirty:s,ctx:e}),ge.$set(Ft);const $t={};s&256&&($t.$$scope={dirty:s,ctx:e}),we.$set($t);const Ot={};s&256&&(Ot.$$scope={dirty:s,ctx:e}),de.$set(Ot);const Q={};s&256&&(Q.$$scope={dirty:s,ctx:e}),_e.$set(Q);const ft={};s&256&&(ft.$$scope={dirty:s,ctx:e}),be.$set(ft);const Ct={};s&260&&(Ct.$$scope={dirty:s,ctx:e}),ve.$set(Ct);const At={};!Kn&&s&2&&(Kn=!0,At.value=e[1],ys(()=>Kn=!1)),H.$set(At);const Bt={};!Qn&&s&1&&(Qn=!0,Bt.value=e[0],ys(()=>Qn=!1)),G.$set(Bt);const Dt={};s&256&&(Dt.$$scope={dirty:s,ctx:e}),xe.$set(Dt);const Ra={};s&256&&(Ra.$$scope={dirty:s,ctx:e}),ye.$set(Ra);const Ja={};s&256&&(Ja.$$scope={dirty:s,ctx:e}),ke.$set(Ja);const Ka={};s&256&&(Ka.$$scope={dirty:s,ctx:e}),Pe.$set(Ka);const Qa={};s&256&&(Qa.$$scope={dirty:s,ctx:e}),We.$set(Qa);const Za={};s&256&&(Za.$$scope={dirty:s,ctx:e}),Ee.$set(Za);const es={};s&256&&(es.$$scope={dirty:s,ctx:e}),Se.$set(es);const ts={};s&256&&(ts.$$scope={dirty:s,ctx:e}),qe.$set(ts);const ns={};s&256&&(ns.$$scope={dirty:s,ctx:e}),ze.$set(ns);const as={};s&256&&(as.$$scope={dirty:s,ctx:e}),Ie.$set(as);const ss={};s&256&&(ss.$$scope={dirty:s,ctx:e}),Xe.$set(ss);const rs={};s&256&&(rs.$$scope={dirty:s,ctx:e}),Fe.$set(rs);const ls={};s&256&&(ls.$$scope={dirty:s,ctx:e}),Oe.$set(ls);const os={};s&256&&(os.$$scope={dirty:s,ctx:e}),Ce.$set(os);const is={};s&256&&(is.$$scope={dirty:s,ctx:e}),Ae.$set(is);const $s={};s&256&&($s.$$scope={dirty:s,ctx:e}),Be.$set($s);const fs={};s&256&&(fs.$$scope={dirty:s,ctx:e}),De.$set(fs);const ms={};s&256&&(ms.$$scope={dirty:s,ctx:e}),Me.$set(ms);const us={};s&256&&(us.$$scope={dirty:s,ctx:e}),Le.$set(us);const ps={};s&256&&(ps.$$scope={dirty:s,ctx:e}),Ve.$set(ps);const hs={};s&256&&(hs.$$scope={dirty:s,ctx:e}),je.$set(hs);const cs={};s&256&&(cs.$$scope={dirty:s,ctx:e}),Ue.$set(cs);const gs={};s&256&&(gs.$$scope={dirty:s,ctx:e}),Ye.$set(gs);const ws={};s&256&&(ws.$$scope={dirty:s,ctx:e}),Ne.$set(ws);const ds={};s&256&&(ds.$$scope={dirty:s,ctx:e}),He.$set(ds);const _s={};s&256&&(_s.$$scope={dirty:s,ctx:e}),Re.$set(_s);const bs={};s&256&&(bs.$$scope={dirty:s,ctx:e}),Je.$set(bs)},i(e){Wn||(h(q.$$.fragment,e),h(z.$$.fragment,e),h(Z.$$.fragment,e),h(ee.$$.fragment,e),h(te.$$.fragment,e),h(ne.$$.fragment,e),h(ae.$$.fragment,e),h(se.$$.fragment,e),h(re.$$.fragment,e),h(le.$$.fragment,e),h(oe.$$.fragment,e),h(ie.$$.fragment,e),h($e.$$.fragment,e),h(fe.$$.fragment,e),h(me.$$.fragment,e),h(ue.$$.fragment,e),h(pe.$$.fragment,e),h(he.$$.fragment,e),h(ce.$$.fragment,e),h(ge.$$.fragment,e),h(we.$$.fragment,e),h(de.$$.fragment,e),h(_e.$$.fragment,e),h(be.$$.fragment,e),h(ve.$$.fragment,e),h(H.$$.fragment,e),h(G.$$.fragment,e),h(xe.$$.fragment,e),h(ye.$$.fragment,e),h(ke.$$.fragment,e),h(Pe.$$.fragment,e),h(We.$$.fragment,e),h(Ee.$$.fragment,e),h(Se.$$.fragment,e),h(qe.$$.fragment,e),h(ze.$$.fragment,e),h(Ie.$$.fragment,e),h(Xe.$$.fragment,e),h(Fe.$$.fragment,e),h(Oe.$$.fragment,e),h(Ce.$$.fragment,e),h(Ae.$$.fragment,e),h(Be.$$.fragment,e),h(De.$$.fragment,e),h(Me.$$.fragment,e),h(Le.$$.fragment,e),h(Ve.$$.fragment,e),h(je.$$.fragment,e),h(Ue.$$.fragment,e),h(Ye.$$.fragment,e),h(Ne.$$.fragment,e),h(He.$$.fragment,e),h(nt.$$.fragment,e),h(at.$$.fragment,e),h(Re.$$.fragment,e),h(Je.$$.fragment,e),h(st.$$.fragment,e),h(rt.$$.fragment,e),h(lt.$$.fragment,e),Wn=!0)},o(e){c(q.$$.fragment,e),c(z.$$.fragment,e),c(Z.$$.fragment,e),c(ee.$$.fragment,e),c(te.$$.fragment,e),c(ne.$$.fragment,e),c(ae.$$.fragment,e),c(se.$$.fragment,e),c(re.$$.fragment,e),c(le.$$.fragment,e),c(oe.$$.fragment,e),c(ie.$$.fragment,e),c($e.$$.fragment,e),c(fe.$$.fragment,e),c(me.$$.fragment,e),c(ue.$$.fragment,e),c(pe.$$.fragment,e),c(he.$$.fragment,e),c(ce.$$.fragment,e),c(ge.$$.fragment,e),c(we.$$.fragment,e),c(de.$$.fragment,e),c(_e.$$.fragment,e),c(be.$$.fragment,e),c(ve.$$.fragment,e),c(H.$$.fragment,e),c(G.$$.fragment,e),c(xe.$$.fragment,e),c(ye.$$.fragment,e),c(ke.$$.fragment,e),c(Pe.$$.fragment,e),c(We.$$.fragment,e),c(Ee.$$.fragment,e),c(Se.$$.fragment,e),c(qe.$$.fragment,e),c(ze.$$.fragment,e),c(Ie.$$.fragment,e),c(Xe.$$.fragment,e),c(Fe.$$.fragment,e),c(Oe.$$.fragment,e),c(Ce.$$.fragment,e),c(Ae.$$.fragment,e),c(Be.$$.fragment,e),c(De.$$.fragment,e),c(Me.$$.fragment,e),c(Le.$$.fragment,e),c(Ve.$$.fragment,e),c(je.$$.fragment,e),c(Ue.$$.fragment,e),c(Ye.$$.fragment,e),c(Ne.$$.fragment,e),c(He.$$.fragment,e),c(nt.$$.fragment,e),c(at.$$.fragment,e),c(Re.$$.fragment,e),c(Je.$$.fragment,e),c(st.$$.fragment,e),c(rt.$$.fragment,e),c(lt.$$.fragment,e),Wn=!1},d(e){e&&r(n),e&&r(a),e&&r(d),e&&r(v),e&&r(T),g(q),g(z),e&&r(x),e&&r(M),e&&r(Vt),g(Z,e),e&&r(jt),e&&r(mt),e&&r(Ut),g(ee,e),e&&r(Yt),e&&r(I),g(te),g(ne),g(ae),g(se),g(re),g(le),g(oe),g(ie),g($e),e&&r(Nt),e&&r(F),g(fe),g(me),g(ue),g(pe),g(he),g(ce),g(ge),g(we),e&&r(Ht),g(de,e),e&&r(Gt),e&&r(U),g(_e),g(be),e&&r(Rt),g(ve,e),e&&r(Jt),g(H,e),e&&r(Kt),g(G,e),e&&r(Qt),e&&r(Y),g(xe),g(ye),e&&r(Zt),e&&r(Te),g(ke),e&&r(en),e&&r(ut),e&&r(tn),g(Pe,e),e&&r(nn),e&&r(pt),e&&r(an),g(We,e),e&&r(sn),e&&r(ht),e&&r(rn),e&&r(D),g(Ee),g(Se),g(qe),g(ze),g(Ie),e&&r(ln),e&&r(Ze),g(Xe),e&&r(on),e&&r(O),g(Fe),g(Oe),g(Ce),g(Ae),g(Be),g(De),g(Me),g(Le),e&&r($n),e&&r(et),g(Ve),e&&r(fn),e&&r(ct),e&&r(mn),e&&r(gt),e&&r(un),e&&r(tt),g(je),e&&r(pn),e&&r(V),g(Ue),g(Ye),g(Ne),g(He),e&&r(hn),e&&r(wt),e&&r(cn),g(nt,e),e&&r(gn),e&&r(Ge),e&&r(wn),g(at,e),e&&r(dn),e&&r(N),g(Re),g(Je),e&&r(_n),g(st,e),e&&r(bn),e&&r(Ke),e&&r(vn),g(rt,e),e&&r(xn),e&&r(dt),e&&r(yn),g(lt,e),e&&r(Tn),e&&r(ot),e&&r(kn),e&&r(_t),e&&r(Pn),e&&r(bt)}}}function Ir(i){let n,t,a,d;return a=new zs({props:{$$slots:{default:[zr]},$$scope:{ctx:i}}}),{c(){n=P("meta"),t=_(),m(a.$$.fragment),this.h()},l(v){const T=qs("svelte-1q68gj5",document.head);n=W(T,"META",{name:!0,content:!0}),T.forEach(r),t=b(v),u(a.$$.fragment,v),this.h()},h(){document.title="Linear Model - World4AI",Qe(n,"name","description"),Qe(n,"content","A linear model allows us to model the data using a line (or a hyperplane) in the coordinate system.")},m(v,T){w(document.head,n),o(v,t,T),p(a,v,T),d=!0},p(v,[T]){const k={};T&263&&(k.$$scope={dirty:T,ctx:v}),a.$set(k)},i(v){d||(h(a.$$.fragment,v),d=!0)},o(v){c(a.$$.fragment,v),d=!1},d(v){r(n),v&&r(t),g(a,v)}}}let Xr=0,Fr=5;function Or(i,n,t){let a=[];for(let l=-100;l<100;l++){let x=l,M=Xr+Fr*(x+Math.random()*20-10);a.push({x,y:M})}let d=[];for(let l=-100;l<100;l++){let x=l+1,M=(x+Math.random()*10-10)**2;d.push({x,y:M})}let v=[{x:-100,y:0},{x:100,y:0}],T=-200,k=-100;function q(l,x){let M=l+x*v[0].x,vt=l+x*v[1].x;t(2,v[0].y=M,v),t(2,v[1].y=vt,v),t(2,v)}function S(l){k=l,t(1,k)}function z(l){T=l,t(0,T)}return i.$$.update=()=>{i.$$.dirty&3&&q(T,k)},[T,k,v,a,d,S,z]}class Hr extends Ws{constructor(n){super(),Es(this,n,Or,Ir,Ss,{})}}export{Hr as default};
