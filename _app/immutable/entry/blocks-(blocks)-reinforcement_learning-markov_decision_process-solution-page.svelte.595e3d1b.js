import{S as Qa,i as Ua,s as Za,k as E,a as x,y as g,W as es,l as S,h as s,c as T,z as h,n as V,N as k,b as r,A as _,g as w,d,B as v,w as ts,a9 as ns,q as i,m as I,r as p,aa as as,C as M}from"../chunks/index.4d92b023.js";import{C as ss}from"../chunks/Container.b0705c7b.js";import{H as Aa}from"../chunks/Highlight.b7c1de53.js";import{L as P}from"../chunks/Latex.e0b308c0.js";import{A as Pa}from"../chunks/Alert.25a852b3.js";import{S as $s}from"../chunks/Slider.93409d64.js";import{P as fs,T as Ja}from"../chunks/Ticks.45eca5c5.js";import{P as ls}from"../chunks/Path.7e6df014.js";import{X as rs,Y as os}from"../chunks/YLabel.182e66a3.js";import{T as is,a as ps,b as us,R as zt,H as Ea,D as O}from"../chunks/HeaderEntry.2b6e8f51.js";import{M as bt,S as ms}from"../chunks/Sequence.ddbdf7ae.js";function cs(f){let n;return{c(){n=i("reward hypothesis")},l(t){n=p(t,"reward hypothesis")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function gs(f){let n;return{c(){n=i("G_t")},l(t){n=p(t,"G_t")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function hs(f){let n;return{c(){n=i("return")},l(t){n=p(t,"return")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function _s(f){let n;return{c(){n=i("t+1")},l(t){n=p(t,"t+1")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function ws(f){let n;return{c(){n=i("T")},l(t){n=p(t,"T")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function ds(f){let n=String.raw`G_t = R_{t+1} + R_{t+2} + R_{t+3} + R_{t+4} + \dots`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function vs(f){let n;return{c(){n=i("G_0")},l(t){n=p(t,"G_0")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function bs(f){let n;return{c(){n=i("\\gamma")},l(t){n=p(t,"\\gamma")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function ys(f){let n;return{c(){n=i("t")},l(t){n=p(t,"t")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function xs(f){let n;return{c(){n=i("G_t")},l(t){n=p(t,"G_t")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ts(f){let n=String.raw`G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function ks(f){let n;return{c(){n=i("G")},l(t){n=p(t,"G")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function As(f){let n;return{c(){n=i("\\gamma")},l(t){n=p(t,"\\gamma")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ps(f){let n,t,a,m,u,b,c,l,o,A;return n=new Ja({props:{xTicks:[0,10,20,30,40,50,60,70,80,90,100],xOffset:-15}}),a=new Ja({props:{yTicks:[0,.2,.4,.6,.8,1],yOffset:15}}),u=new rs({props:{text:"Steps",fontSize:12}}),c=new os({props:{text:"Discount Factor",fontSize:12}}),o=new ls({props:{data:f[1]}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment),m=x(),g(u.$$.fragment),b=x(),g(c.$$.fragment),l=x(),g(o.$$.fragment)},l(y){h(n.$$.fragment,y),t=T(y),h(a.$$.fragment,y),m=T(y),h(u.$$.fragment,y),b=T(y),h(c.$$.fragment,y),l=T(y),h(o.$$.fragment,y)},m(y,D){_(n,y,D),r(y,t,D),_(a,y,D),r(y,m,D),_(u,y,D),r(y,b,D),_(c,y,D),r(y,l,D),_(o,y,D),A=!0},p(y,D){const z={};D&2&&(z.data=y[1]),o.$set(z)},i(y){A||(w(n.$$.fragment,y),w(a.$$.fragment,y),w(u.$$.fragment,y),w(c.$$.fragment,y),w(o.$$.fragment,y),A=!0)},o(y){d(n.$$.fragment,y),d(a.$$.fragment,y),d(u.$$.fragment,y),d(c.$$.fragment,y),d(o.$$.fragment,y),A=!1},d(y){v(n,y),y&&s(t),v(a,y),y&&s(m),v(u,y),y&&s(b),v(c,y),y&&s(l),v(o,y)}}}function Es(f){let n;return{c(){n=i("A policy is a mapping from a state to a probability of an action.")},l(t){n=p(t,"A policy is a mapping from a state to a probability of an action.")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ss(f){let n;return{c(){n=i("\\pi")},l(t){n=p(t,"\\pi")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Is(f){let n;return{c(){n=i("s")},l(t){n=p(t,"s")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ds(f){let n;return{c(){n=i("a")},l(t){n=p(t,"a")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ms(f){let n=String.raw`\pi{(a \mid s)} = Pr[A_t = a \mid S_t = s]`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function Bs(f){let n=String.raw`A_t \sim \pi{(. \mid S_t)}`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function qs(f){let n;return{c(){n=i("State")},l(t){n=p(t,"State")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ws(f){let n;return{c(){n=i("Action")},l(t){n=p(t,"Action")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Gs(f){let n;return{c(){n=i("Probability")},l(t){n=p(t,"Probability")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function zs(f){let n,t,a,m,u,b;return n=new Ea({props:{$$slots:{default:[qs]},$$scope:{ctx:f}}}),a=new Ea({props:{$$slots:{default:[Ws]},$$scope:{ctx:f}}}),u=new Ea({props:{$$slots:{default:[Gs]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment),m=x(),g(u.$$.fragment)},l(c){h(n.$$.fragment,c),t=T(c),h(a.$$.fragment,c),m=T(c),h(u.$$.fragment,c)},m(c,l){_(n,c,l),r(c,t,l),_(a,c,l),r(c,m,l),_(u,c,l),b=!0},p(c,l){const o={};l&16&&(o.$$scope={dirty:l,ctx:c}),n.$set(o);const A={};l&16&&(A.$$scope={dirty:l,ctx:c}),a.$set(A);const y={};l&16&&(y.$$scope={dirty:l,ctx:c}),u.$set(y)},i(c){b||(w(n.$$.fragment,c),w(a.$$.fragment,c),w(u.$$.fragment,c),b=!0)},o(c){d(n.$$.fragment,c),d(a.$$.fragment,c),d(u.$$.fragment,c),b=!1},d(c){v(n,c),c&&s(t),v(a,c),c&&s(m),v(u,c)}}}function Rs(f){let n,t;return n=new zt({props:{$$slots:{default:[zs]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment)},l(a){h(n.$$.fragment,a)},m(a,m){_(n,a,m),t=!0},p(a,m){const u={};m&16&&(u.$$scope={dirty:m,ctx:a}),n.$set(u)},i(a){t||(w(n.$$.fragment,a),t=!0)},o(a){d(n.$$.fragment,a),t=!1},d(a){v(n,a)}}}function Vs(f){let n;return{c(){n=i("0")},l(t){n=p(t,"0")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function js(f){let n;return{c(){n=i("A")},l(t){n=p(t,"A")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Hs(f){let n;return{c(){n=i("0.5")},l(t){n=p(t,"0.5")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Cs(f){let n,t,a,m,u,b,c;return n=new O({props:{$$slots:{default:[Vs]},$$scope:{ctx:f}}}),a=new O({props:{$$slots:{default:[js]},$$scope:{ctx:f}}}),b=new O({props:{$$slots:{default:[Hs]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment),m=x(),u=E("span"),g(b.$$.fragment),this.h()},l(l){h(n.$$.fragment,l),t=T(l),h(a.$$.fragment,l),m=T(l),u=S(l,"SPAN",{class:!0});var o=I(u);h(b.$$.fragment,o),o.forEach(s),this.h()},h(){V(u,"class","bg-blue-200 rounded-full font-bold")},m(l,o){_(n,l,o),r(l,t,o),_(a,l,o),r(l,m,o),r(l,u,o),_(b,u,null),c=!0},p(l,o){const A={};o&16&&(A.$$scope={dirty:o,ctx:l}),n.$set(A);const y={};o&16&&(y.$$scope={dirty:o,ctx:l}),a.$set(y);const D={};o&16&&(D.$$scope={dirty:o,ctx:l}),b.$set(D)},i(l){c||(w(n.$$.fragment,l),w(a.$$.fragment,l),w(b.$$.fragment,l),c=!0)},o(l){d(n.$$.fragment,l),d(a.$$.fragment,l),d(b.$$.fragment,l),c=!1},d(l){v(n,l),l&&s(t),v(a,l),l&&s(m),l&&s(u),v(b)}}}function Fs(f){let n;return{c(){n=i("0")},l(t){n=p(t,"0")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ls(f){let n;return{c(){n=i("B")},l(t){n=p(t,"B")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ns(f){let n;return{c(){n=i("0.5")},l(t){n=p(t,"0.5")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Os(f){let n,t,a,m,u,b,c;return n=new O({props:{$$slots:{default:[Fs]},$$scope:{ctx:f}}}),a=new O({props:{$$slots:{default:[Ls]},$$scope:{ctx:f}}}),b=new O({props:{$$slots:{default:[Ns]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment),m=x(),u=E("span"),g(b.$$.fragment),this.h()},l(l){h(n.$$.fragment,l),t=T(l),h(a.$$.fragment,l),m=T(l),u=S(l,"SPAN",{class:!0});var o=I(u);h(b.$$.fragment,o),o.forEach(s),this.h()},h(){V(u,"class","bg-blue-200 rounded-full font-bold")},m(l,o){_(n,l,o),r(l,t,o),_(a,l,o),r(l,m,o),r(l,u,o),_(b,u,null),c=!0},p(l,o){const A={};o&16&&(A.$$scope={dirty:o,ctx:l}),n.$set(A);const y={};o&16&&(y.$$scope={dirty:o,ctx:l}),a.$set(y);const D={};o&16&&(D.$$scope={dirty:o,ctx:l}),b.$set(D)},i(l){c||(w(n.$$.fragment,l),w(a.$$.fragment,l),w(b.$$.fragment,l),c=!0)},o(l){d(n.$$.fragment,l),d(a.$$.fragment,l),d(b.$$.fragment,l),c=!1},d(l){v(n,l),l&&s(t),v(a,l),l&&s(m),l&&s(u),v(b)}}}function Xs(f){let n;return{c(){n=i("1")},l(t){n=p(t,"1")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ys(f){let n;return{c(){n=i("A")},l(t){n=p(t,"A")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Js(f){let n;return{c(){n=i("0.5")},l(t){n=p(t,"0.5")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Ks(f){let n,t,a,m,u,b,c;return n=new O({props:{$$slots:{default:[Xs]},$$scope:{ctx:f}}}),a=new O({props:{$$slots:{default:[Ys]},$$scope:{ctx:f}}}),b=new O({props:{$$slots:{default:[Js]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment),m=x(),u=E("span"),g(b.$$.fragment),this.h()},l(l){h(n.$$.fragment,l),t=T(l),h(a.$$.fragment,l),m=T(l),u=S(l,"SPAN",{class:!0});var o=I(u);h(b.$$.fragment,o),o.forEach(s),this.h()},h(){V(u,"class","bg-blue-200 rounded-full font-bold")},m(l,o){_(n,l,o),r(l,t,o),_(a,l,o),r(l,m,o),r(l,u,o),_(b,u,null),c=!0},p(l,o){const A={};o&16&&(A.$$scope={dirty:o,ctx:l}),n.$set(A);const y={};o&16&&(y.$$scope={dirty:o,ctx:l}),a.$set(y);const D={};o&16&&(D.$$scope={dirty:o,ctx:l}),b.$set(D)},i(l){c||(w(n.$$.fragment,l),w(a.$$.fragment,l),w(b.$$.fragment,l),c=!0)},o(l){d(n.$$.fragment,l),d(a.$$.fragment,l),d(b.$$.fragment,l),c=!1},d(l){v(n,l),l&&s(t),v(a,l),l&&s(m),l&&s(u),v(b)}}}function Qs(f){let n;return{c(){n=i("1")},l(t){n=p(t,"1")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Us(f){let n;return{c(){n=i("B")},l(t){n=p(t,"B")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function Zs(f){let n;return{c(){n=i("0.5")},l(t){n=p(t,"0.5")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function e$(f){let n,t,a,m,u,b,c;return n=new O({props:{$$slots:{default:[Qs]},$$scope:{ctx:f}}}),a=new O({props:{$$slots:{default:[Us]},$$scope:{ctx:f}}}),b=new O({props:{$$slots:{default:[Zs]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment),m=x(),u=E("span"),g(b.$$.fragment),this.h()},l(l){h(n.$$.fragment,l),t=T(l),h(a.$$.fragment,l),m=T(l),u=S(l,"SPAN",{class:!0});var o=I(u);h(b.$$.fragment,o),o.forEach(s),this.h()},h(){V(u,"class","bg-blue-200 rounded-full font-bold")},m(l,o){_(n,l,o),r(l,t,o),_(a,l,o),r(l,m,o),r(l,u,o),_(b,u,null),c=!0},p(l,o){const A={};o&16&&(A.$$scope={dirty:o,ctx:l}),n.$set(A);const y={};o&16&&(y.$$scope={dirty:o,ctx:l}),a.$set(y);const D={};o&16&&(D.$$scope={dirty:o,ctx:l}),b.$set(D)},i(l){c||(w(n.$$.fragment,l),w(a.$$.fragment,l),w(b.$$.fragment,l),c=!0)},o(l){d(n.$$.fragment,l),d(a.$$.fragment,l),d(b.$$.fragment,l),c=!1},d(l){v(n,l),l&&s(t),v(a,l),l&&s(m),l&&s(u),v(b)}}}function t$(f){let n,t,a,m,u,b,c,l;return n=new zt({props:{$$slots:{default:[Cs]},$$scope:{ctx:f}}}),a=new zt({props:{$$slots:{default:[Os]},$$scope:{ctx:f}}}),u=new zt({props:{$$slots:{default:[Ks]},$$scope:{ctx:f}}}),c=new zt({props:{$$slots:{default:[e$]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment),m=x(),g(u.$$.fragment),b=x(),g(c.$$.fragment)},l(o){h(n.$$.fragment,o),t=T(o),h(a.$$.fragment,o),m=T(o),h(u.$$.fragment,o),b=T(o),h(c.$$.fragment,o)},m(o,A){_(n,o,A),r(o,t,A),_(a,o,A),r(o,m,A),_(u,o,A),r(o,b,A),_(c,o,A),l=!0},p(o,A){const y={};A&16&&(y.$$scope={dirty:A,ctx:o}),n.$set(y);const D={};A&16&&(D.$$scope={dirty:A,ctx:o}),a.$set(D);const z={};A&16&&(z.$$scope={dirty:A,ctx:o}),u.$set(z);const it={};A&16&&(it.$$scope={dirty:A,ctx:o}),c.$set(it)},i(o){l||(w(n.$$.fragment,o),w(a.$$.fragment,o),w(u.$$.fragment,o),w(c.$$.fragment,o),l=!0)},o(o){d(n.$$.fragment,o),d(a.$$.fragment,o),d(u.$$.fragment,o),d(c.$$.fragment,o),l=!1},d(o){v(n,o),o&&s(t),v(a,o),o&&s(m),v(u,o),o&&s(b),v(c,o)}}}function n$(f){let n,t,a,m;return n=new ps({props:{$$slots:{default:[Rs]},$$scope:{ctx:f}}}),a=new us({props:{$$slots:{default:[t$]},$$scope:{ctx:f}}}),{c(){g(n.$$.fragment),t=x(),g(a.$$.fragment)},l(u){h(n.$$.fragment,u),t=T(u),h(a.$$.fragment,u)},m(u,b){_(n,u,b),r(u,t,b),_(a,u,b),m=!0},p(u,b){const c={};b&16&&(c.$$scope={dirty:b,ctx:u}),n.$set(c);const l={};b&16&&(l.$$scope={dirty:b,ctx:u}),a.$set(l)},i(u){m||(w(n.$$.fragment,u),w(a.$$.fragment,u),m=!0)},o(u){d(n.$$.fragment,u),d(a.$$.fragment,u),m=!1},d(u){v(n,u),u&&s(t),v(a,u)}}}function a$(f){let n;return{c(){n=i("G_t")},l(t){n=p(t,"G_t")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function s$(f){let n;return{c(){n=i("\\pi")},l(t){n=p(t,"\\pi")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function $$(f){let n;return{c(){n=i("value functions")},l(t){n=p(t,"value functions")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function f$(f){let n;return{c(){n=i("A value function measures the expected value of returns.")},l(t){n=p(t,"A value function measures the expected value of returns.")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function l$(f){let n=String.raw`v_{\pi}(s)`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function r$(f){let n;return{c(){n=i("s")},l(t){n=p(t,"s")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function o$(f){let n=String.raw`\pi`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function i$(f){let n=String.raw`v_{\pi}(s) = \mathbb{E_{\pi}}[G_t \mid S_t = s]`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function p$(f){let n=String.raw`q_{\pi}(s, a)`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function u$(f){let n;return{c(){n=i("s")},l(t){n=p(t,"s")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function m$(f){let n;return{c(){n=i("a")},l(t){n=p(t,"a")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function c$(f){let n=String.raw`\pi`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function g$(f){let n=String.raw`q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_t \mid S_t = s, A_t = a]`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function h$(f){let n;return{c(){n=i(`For the agent to solve the Markov decision process means to find the optimal
    policy.`)},l(t){n=p(t,`For the agent to solve the Markov decision process means to find the optimal
    policy.`)},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function _$(f){let n=String.raw`\pi`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function w$(f){let n=String.raw`\pi`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function d$(f){let n=String.raw`\pi`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function v$(f){let n=String.raw`\pi'`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function b$(f){let n=String.raw`\mathcal{S}`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function y$(f){let n=String.raw`\pi \geq \pi \iff
   v_{\pi}(s) \geq v_{\pi'}(s) 
    \text{ for all } s \in \mathcal{S}`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function x$(f){let n=String.raw`\pi_*`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function T$(f){let n=String.raw`\pi_* \geq \pi \text{ for all }
     \pi`+"",t;return{c(){t=i(n)},l(a){t=p(a,n)},m(a,m){r(a,t,m)},p:M,d(a){a&&s(t)}}}function k$(f){let n;return{c(){n=i("\\pi_*")},l(t){n=p(t,"\\pi_*")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function A$(f){let n;return{c(){n=i("v_*")},l(t){n=p(t,"v_*")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function P$(f){let n;return{c(){n=i("q_*")},l(t){n=p(t,"q_*")},m(t,a){r(t,n,a)},d(t){t&&s(n)}}}function E$(f){let n,t,a,m,u,b,c,l,o,A,y,D,z,it,U,An,Z,Pn,ee,En,Rt,Oe,te,Vt,pt,Sn,jt,Xe,Ht,Ye,Ct,ne,In,ae,Dn,Ft,Je,Lt,C,Mn,se,Bn,$e,qn,fe,Wn,Nt,Ke,le,Ot,ut,Gn,Xt,re,zn,oe,Rn,Yt,ie,Vn,pe,jn,Jt,ue,Kt,Q,Hn,Qt,mt,Cn,Ut,me,Zt,B,Fn,ce,Ln,ge,Nn,he,On,_e,Xn,we,Yn,en,ct,Jn,tn,de,nn,F,Kn,ve,Qn,be,Un,ye,Zn,an,xe,sn,L,ea,Te,ta,ke,na,Ae,aa,$n,Qe,Pe,fn,R,sa,Ee,$a,Se,fa,Ie,la,De,ra,ln,Ue,Me,rn,gt,oa,on,ht,ia,pn,Ze,un,_t,pa,mn,et,cn,wt,ua,gn,Be,hn,q,ma,qe,ca,We,ga,Ge,ha,ze,_a,Re,wa,_n,tt,Ve,wn,je,da,He,va,dn,nt,Ce,vn,dt,ba,bn,at,yn,st,xn,N,ya,Fe,xa,Le,Ta,Ne,ka,Tn,vt,kn;l=new Aa({props:{$$slots:{default:[cs]},$$scope:{ctx:f}}}),z=new P({props:{$$slots:{default:[gs]},$$scope:{ctx:f}}}),U=new Aa({props:{$$slots:{default:[hs]},$$scope:{ctx:f}}}),Z=new P({props:{$$slots:{default:[_s]},$$scope:{ctx:f}}}),ee=new P({props:{$$slots:{default:[ws]},$$scope:{ctx:f}}}),te=new P({props:{$$slots:{default:[ds]},$$scope:{ctx:f}}}),Xe=new bt({props:{config:{root:"0",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[.5,.5],p:[.7,.3,.2,.8],type:"decision"}}}),Ye=new bt({props:{config:{root:"1",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[.5,.5],p:[.9,.1,.3,.7],type:"decision"}}}),ae=new P({props:{$$slots:{default:[vs]},$$scope:{ctx:f}}}),Je=new ms({props:{f:D$(),showReturn:!0}}),se=new P({props:{$$slots:{default:[bs]},$$scope:{ctx:f}}}),$e=new P({props:{$$slots:{default:[ys]},$$scope:{ctx:f}}}),fe=new P({props:{$$slots:{default:[xs]},$$scope:{ctx:f}}}),le=new P({props:{$$slots:{default:[Ts]},$$scope:{ctx:f}}}),oe=new P({props:{$$slots:{default:[ks]},$$scope:{ctx:f}}}),pe=new P({props:{$$slots:{default:[As]},$$scope:{ctx:f}}}),ue=new fs({props:{maxWidth:700,domain:[0,100],range:[0,1],padding:{top:10,right:10,bottom:40,left:40},$$slots:{default:[Ps]},$$scope:{ctx:f}}});function Ka(e){f[2](e)}let Sa={min:.9,max:.999,step:.001,label:"Discount Factor",showValue:!0};return f[0]!==void 0&&(Sa.value=f[0]),Q=new $s({props:Sa}),ts.push(()=>ns(Q,"value",Ka)),me=new Pa({props:{type:"info",$$slots:{default:[Es]},$$scope:{ctx:f}}}),ce=new P({props:{$$slots:{default:[Ss]},$$scope:{ctx:f}}}),ge=new P({props:{$$slots:{default:[Is]},$$scope:{ctx:f}}}),he=new P({props:{$$slots:{default:[Ds]},$$scope:{ctx:f}}}),_e=new P({props:{$$slots:{default:[Ms]},$$scope:{ctx:f}}}),we=new P({props:{$$slots:{default:[Bs]},$$scope:{ctx:f}}}),de=new is({props:{$$slots:{default:[n$]},$$scope:{ctx:f}}}),ve=new P({props:{$$slots:{default:[a$]},$$scope:{ctx:f}}}),be=new P({props:{$$slots:{default:[s$]},$$scope:{ctx:f}}}),ye=new Aa({props:{$$slots:{default:[$$]},$$scope:{ctx:f}}}),xe=new Pa({props:{type:"info",$$slots:{default:[f$]},$$scope:{ctx:f}}}),Te=new P({props:{$$slots:{default:[l$]},$$scope:{ctx:f}}}),ke=new P({props:{$$slots:{default:[r$]},$$scope:{ctx:f}}}),Ae=new P({props:{$$slots:{default:[o$]},$$scope:{ctx:f}}}),Pe=new P({props:{$$slots:{default:[i$]},$$scope:{ctx:f}}}),Ee=new P({props:{$$slots:{default:[p$]},$$scope:{ctx:f}}}),Se=new P({props:{$$slots:{default:[u$]},$$scope:{ctx:f}}}),Ie=new P({props:{$$slots:{default:[m$]},$$scope:{ctx:f}}}),De=new P({props:{$$slots:{default:[c$]},$$scope:{ctx:f}}}),Me=new P({props:{$$slots:{default:[g$]},$$scope:{ctx:f}}}),Ze=new bt({props:{config:{root:"0",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[.5,.5],p:[.7,.3,.2,.8],type:"decision"}}}),et=new bt({props:{config:{root:"0",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[.2,.8],p:[.7,.3,.2,.8],type:"decision"}}}),Be=new Pa({props:{type:"info",$$slots:{default:[h$]},$$scope:{ctx:f}}}),qe=new P({props:{$$slots:{default:[_$]},$$scope:{ctx:f}}}),We=new P({props:{$$slots:{default:[w$]},$$scope:{ctx:f}}}),Ge=new P({props:{$$slots:{default:[d$]},$$scope:{ctx:f}}}),ze=new P({props:{$$slots:{default:[v$]},$$scope:{ctx:f}}}),Re=new P({props:{$$slots:{default:[b$]},$$scope:{ctx:f}}}),Ve=new P({props:{$$slots:{default:[y$]},$$scope:{ctx:f}}}),He=new P({props:{$$slots:{default:[x$]},$$scope:{ctx:f}}}),Ce=new P({props:{$$slots:{default:[T$]},$$scope:{ctx:f}}}),at=new bt({props:{config:{root:"0",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[0,1],p:[.7,.3,.2,.8],type:"decision"}}}),st=new bt({props:{config:{root:"1",states:["0","1","0","1"],actions:["A","B"],rewards:["-1","5","-1","5"],actionp:[0,1],p:[.9,.1,.3,.7],type:"decision"}}}),Fe=new P({props:{$$slots:{default:[k$]},$$scope:{ctx:f}}}),Le=new P({props:{$$slots:{default:[A$]},$$scope:{ctx:f}}}),Ne=new P({props:{$$slots:{default:[P$]},$$scope:{ctx:f}}}),{c(){n=E("h1"),t=i("Solution to a Markov Decision Process"),a=x(),m=E("div"),u=x(),b=E("p"),c=i("The "),g(l.$$.fragment),o=i(` states that the goal of the environment
    is encoded in the rewards. If you want the agent to learn how to play chess for
    example you give out a reward of +1 when the agent wins, a reward of -1 when
    the agent loses and a reward of 0 whenever there is a draw. The goal of the agent
    is therefore to collect as many rewards as possible.`),A=x(),y=E("p"),D=i("We can express the same idea mathematically by defining the goal "),g(z.$$.fragment),it=i(", also called the "),g(U.$$.fragment),An=i(`, as the sum of rewards from
    the timestep `),g(Z.$$.fragment),Pn=i(` to either the terminal state
    `),g(ee.$$.fragment),En=i(` if we are dealing with episodic tasks or to infinity if we are
    dealing with continuing tasks.`),Rt=x(),Oe=E("div"),g(te.$$.fragment),Vt=x(),pt=E("p"),Sn=i(`To demonstrate this idea with a simple example let's once again take the MDP
    from the previous section. We are dealing with two states and an agent that
    takes random actions. Whenever the agent lands in the state 1 it receives a
    reward of 1, other wise it receives a negative reward of -1.`),jt=x(),g(Xe.$$.fragment),Ht=x(),g(Ye.$$.fragment),Ct=x(),ne=E("p"),In=i(`In the example below we calculate the return from the perspective of the
    initial state `),g(ae.$$.fragment),Dn=i(`. Each time the agent receives a reward we
    add the value to the return value.`),Ft=x(),g(Je.$$.fragment),Lt=x(),C=E("p"),Mn=i(`In order to reduce the value of future rewards we discount each reward, by
    applying a discount factor `),g(se.$$.fragment),Bn=i(`, a value between 0 and 1.
    The farther away the reward is from the current time step `),g($e.$$.fragment),qn=i(`,
    the lower its contribution to the return `),g(fe.$$.fragment),Wn=i("."),Nt=x(),Ke=E("div"),g(le.$$.fragment),Ot=x(),ut=E("p"),Gn=i(`The easiest way to illustrate this concept is by thinking about the time
    value of money. If we have to choose between getting 1000$ now and getting
    1000$ in 10 years, we should definetely choose the 1000$ in the present. The
    money could be invested for a ten year period, such that in 10 years the
    investor gets back an amout that is larger than 1000$. A reward now is
    always more valuable thant a future reward.`),Xt=x(),re=E("p"),zn=i(`Discounting is especially important for continuing tasks. If the episode has
    no natural ending, the return `),g(oe.$$.fragment),Rn=i(` could theoretically become infinite.
    Discounted rewards on the other hand approach 0 when they are far into the future.`),Yt=x(),ie=E("p"),Vn=i(`The interactive example below shows how the discounting rate dependings on
    the value of the `),g(pe.$$.fragment),jn=i(`. The lower the gamma the faster the
    progression towards 0.`),Jt=x(),g(ue.$$.fragment),Kt=x(),g(Q.$$.fragment),Qt=x(),mt=E("p"),Cn=i(`The return is very tighly integrated with the strategy thate the agent is
    following. This strategy is called policy. A better policy will obviously
    generate better returns and vice versa.`),Ut=x(),g(me.$$.fragment),Zt=x(),B=E("p"),Fn=i("A policy "),g(ce.$$.fragment),Ln=i(` as a mapping from a particular state
    `),g(ge.$$.fragment),Nn=i(`
    to a probability of an action `),g(he.$$.fragment),On=i(": "),g(_e.$$.fragment),Xn=i(`. To take an action in a state the agent simply draws an action from the
    policy distribution `),g(we.$$.fragment),Yn=i("."),en=x(),ct=E("p"),Jn=i(`In the above example we are dealing with 2 possible states and 2 possible
    actions. We can therefore simply store the policy in a mapping table. In
    future chapters we will use neural networks for more complex MDPs.`),tn=x(),g(de.$$.fragment),nn=x(),F=E("p"),Kn=i(`The goal of the agent is to find a policy that maximizes returns, but simply
    maximizing the return is tricky, because in reinforcement learning we often
    deal with stochastic policies and environments. That stochasticity produces
    different trajectories and therefore different returns `),g(ve.$$.fragment),Qn=i(`.
    But how can we measure how good it is for the agent to use a certain policy `),g(be.$$.fragment),Un=i(" if the generated returns are not consistent? By utilizing "),g(ye.$$.fragment),Zn=i("."),an=x(),g(xe.$$.fragment),sn=x(),L=E("p"),ea=i("The state-value function "),g(Te.$$.fragment),ta=i(` calculates the
    expected return when the agent is in the state `),g(ke.$$.fragment),na=i(`
    and follows the policy `),g(Ae.$$.fragment),aa=i("."),$n=x(),Qe=E("div"),g(Pe.$$.fragment),fn=x(),R=E("p"),sa=i("The action-value function "),g(Ee.$$.fragment),$a=i(` calculates
    the expected return when the agent is the state `),g(Se.$$.fragment),fa=i(`, takes the
    action `),g(Ie.$$.fragment),la=i(" at first and then keeps following the policy "),g(De.$$.fragment),ra=i("."),ln=x(),Ue=E("div"),g(Me.$$.fragment),rn=x(),gt=E("p"),oa=i(`For the time being we do not have the tools to exactly calculate the value
    function for each state, but we can make an educated guess. Let's take the
    same Markov decision we have been dealing so far, but only focus on the
    state 0.`),on=x(),ht=E("p"),ia=i("The random policy that we have been using so far is clearly not optimal."),pn=x(),g(Ze.$$.fragment),un=x(),_t=E("p"),pa=i(`The below agent implements a policy that tends to take the action B more
    often. This behaviour will generate a higher expected return.`),mn=x(),g(et.$$.fragment),cn=x(),wt=E("p"),ua=i(`A state-value function allows the agent to assign a goodness value to each
    of the states for a given policy. Change the policy and observe if you see
    any improvements. That is where the action-value function comes into play.
    The action-value function allows us to reason about changing a single
    action. We can ask the question what would happen if we generally keep the
    same policy and only change the next action. Would that new policy be more
    benefitial to us? If yes, maybe we should change our policy. And the goal of
    the agent is to find the optimal policy.`),gn=x(),g(Be.$$.fragment),hn=x(),q=E("p"),ma=i(`Optimality implies that there is a way to compare different policies and to
    determine which of the policies is better. In a Markov decision process
    value functions are used as a metric of the goodness of a policy. The policy `),g(qe.$$.fragment),ca=i(`
    is said to be better than the policy `),g(We.$$.fragment),ga=i(` if and
    only if the value function of `),g(Ge.$$.fragment),ha=i(` is larger or equal
    to the value function of policy `),g(ze.$$.fragment),_a=i(` for all states
    in the state set `),g(Re.$$.fragment),wa=i("."),_n=x(),tt=E("div"),g(Ve.$$.fragment),wn=x(),je=E("p"),da=i("The optimal policy "),g(He.$$.fragment),va=i(` is therefore the policy
    that is better (or at least not worse) than any other policy.`),dn=x(),nt=E("div"),g(Ce.$$.fragment),vn=x(),dt=E("p"),ba=i(`It is not hard to imagine the optimal strategy for the two-states MDP. The
    agent needs to always take the action B. This policy increases the
    probability to land in the state 2 and thus increases the probability of
    getting a reward of 5. In other words always taking the actio B would
    maximize the value function.`),bn=x(),g(at.$$.fragment),yn=x(),g(st.$$.fragment),xn=x(),N=E("p"),ya=i("How we can numerically find the optimal policy "),g(Fe.$$.fragment),xa=i(` and the corresponding
    value functions `),g(Le.$$.fragment),Ta=i(", "),g(Ne.$$.fragment),ka=i(` is going to be the topic
    of the rest of the reinforcement learning block. The essence of reinforcement
    learning is to find the optimal policy, given that the environment contains a
    Markov decision process under the hood.`),Tn=x(),vt=E("div"),this.h()},l(e){n=S(e,"H1",{});var $=I(n);t=p($,"Solution to a Markov Decision Process"),$.forEach(s),a=T(e),m=S(e,"DIV",{class:!0}),I(m).forEach(s),u=T(e),b=S(e,"P",{});var $t=I(b);c=p($t,"The "),h(l.$$.fragment,$t),o=p($t,` states that the goal of the environment
    is encoded in the rewards. If you want the agent to learn how to play chess for
    example you give out a reward of +1 when the agent wins, a reward of -1 when
    the agent loses and a reward of 0 whenever there is a draw. The goal of the agent
    is therefore to collect as many rewards as possible.`),$t.forEach(s),A=T(e),y=S(e,"P",{});var j=I(y);D=p(j,"We can express the same idea mathematically by defining the goal "),h(z.$$.fragment,j),it=p(j,", also called the "),h(U.$$.fragment,j),An=p(j,`, as the sum of rewards from
    the timestep `),h(Z.$$.fragment,j),Pn=p(j,` to either the terminal state
    `),h(ee.$$.fragment,j),En=p(j,` if we are dealing with episodic tasks or to infinity if we are
    dealing with continuing tasks.`),j.forEach(s),Rt=T(e),Oe=S(e,"DIV",{class:!0});var yt=I(Oe);h(te.$$.fragment,yt),yt.forEach(s),Vt=T(e),pt=S(e,"P",{});var xt=I(pt);Sn=p(xt,`To demonstrate this idea with a simple example let's once again take the MDP
    from the previous section. We are dealing with two states and an agent that
    takes random actions. Whenever the agent lands in the state 1 it receives a
    reward of 1, other wise it receives a negative reward of -1.`),xt.forEach(s),jt=T(e),h(Xe.$$.fragment,e),Ht=T(e),h(Ye.$$.fragment,e),Ct=T(e),ne=S(e,"P",{});var ft=I(ne);In=p(ft,`In the example below we calculate the return from the perspective of the
    initial state `),h(ae.$$.fragment,ft),Dn=p(ft,`. Each time the agent receives a reward we
    add the value to the return value.`),ft.forEach(s),Ft=T(e),h(Je.$$.fragment,e),Lt=T(e),C=S(e,"P",{});var X=I(C);Mn=p(X,`In order to reduce the value of future rewards we discount each reward, by
    applying a discount factor `),h(se.$$.fragment,X),Bn=p(X,`, a value between 0 and 1.
    The farther away the reward is from the current time step `),h($e.$$.fragment,X),qn=p(X,`,
    the lower its contribution to the return `),h(fe.$$.fragment,X),Wn=p(X,"."),X.forEach(s),Nt=T(e),Ke=S(e,"DIV",{class:!0});var Tt=I(Ke);h(le.$$.fragment,Tt),Tt.forEach(s),Ot=T(e),ut=S(e,"P",{});var kt=I(ut);Gn=p(kt,`The easiest way to illustrate this concept is by thinking about the time
    value of money. If we have to choose between getting 1000$ now and getting
    1000$ in 10 years, we should definetely choose the 1000$ in the present. The
    money could be invested for a ten year period, such that in 10 years the
    investor gets back an amout that is larger than 1000$. A reward now is
    always more valuable thant a future reward.`),kt.forEach(s),Xt=T(e),re=S(e,"P",{});var lt=I(re);zn=p(lt,`Discounting is especially important for continuing tasks. If the episode has
    no natural ending, the return `),h(oe.$$.fragment,lt),Rn=p(lt,` could theoretically become infinite.
    Discounted rewards on the other hand approach 0 when they are far into the future.`),lt.forEach(s),Yt=T(e),ie=S(e,"P",{});var rt=I(ie);Vn=p(rt,`The interactive example below shows how the discounting rate dependings on
    the value of the `),h(pe.$$.fragment,rt),jn=p(rt,`. The lower the gamma the faster the
    progression towards 0.`),rt.forEach(s),Jt=T(e),h(ue.$$.fragment,e),Kt=T(e),h(Q.$$.fragment,e),Qt=T(e),mt=S(e,"P",{});var At=I(mt);Cn=p(At,`The return is very tighly integrated with the strategy thate the agent is
    following. This strategy is called policy. A better policy will obviously
    generate better returns and vice versa.`),At.forEach(s),Ut=T(e),h(me.$$.fragment,e),Zt=T(e),B=S(e,"P",{});var W=I(B);Fn=p(W,"A policy "),h(ce.$$.fragment,W),Ln=p(W,` as a mapping from a particular state
    `),h(ge.$$.fragment,W),Nn=p(W,`
    to a probability of an action `),h(he.$$.fragment,W),On=p(W,": "),h(_e.$$.fragment,W),Xn=p(W,`. To take an action in a state the agent simply draws an action from the
    policy distribution `),h(we.$$.fragment,W),Yn=p(W,"."),W.forEach(s),en=T(e),ct=S(e,"P",{});var Pt=I(ct);Jn=p(Pt,`In the above example we are dealing with 2 possible states and 2 possible
    actions. We can therefore simply store the policy in a mapping table. In
    future chapters we will use neural networks for more complex MDPs.`),Pt.forEach(s),tn=T(e),h(de.$$.fragment,e),nn=T(e),F=S(e,"P",{});var Y=I(F);Kn=p(Y,`The goal of the agent is to find a policy that maximizes returns, but simply
    maximizing the return is tricky, because in reinforcement learning we often
    deal with stochastic policies and environments. That stochasticity produces
    different trajectories and therefore different returns `),h(ve.$$.fragment,Y),Qn=p(Y,`.
    But how can we measure how good it is for the agent to use a certain policy `),h(be.$$.fragment,Y),Un=p(Y," if the generated returns are not consistent? By utilizing "),h(ye.$$.fragment,Y),Zn=p(Y,"."),Y.forEach(s),an=T(e),h(xe.$$.fragment,e),sn=T(e),L=S(e,"P",{});var J=I(L);ea=p(J,"The state-value function "),h(Te.$$.fragment,J),ta=p(J,` calculates the
    expected return when the agent is in the state `),h(ke.$$.fragment,J),na=p(J,`
    and follows the policy `),h(Ae.$$.fragment,J),aa=p(J,"."),J.forEach(s),$n=T(e),Qe=S(e,"DIV",{class:!0});var Et=I(Qe);h(Pe.$$.fragment,Et),Et.forEach(s),fn=T(e),R=S(e,"P",{});var H=I(R);sa=p(H,"The action-value function "),h(Ee.$$.fragment,H),$a=p(H,` calculates
    the expected return when the agent is the state `),h(Se.$$.fragment,H),fa=p(H,`, takes the
    action `),h(Ie.$$.fragment,H),la=p(H," at first and then keeps following the policy "),h(De.$$.fragment,H),ra=p(H,"."),H.forEach(s),ln=T(e),Ue=S(e,"DIV",{class:!0});var St=I(Ue);h(Me.$$.fragment,St),St.forEach(s),rn=T(e),gt=S(e,"P",{});var It=I(gt);oa=p(It,`For the time being we do not have the tools to exactly calculate the value
    function for each state, but we can make an educated guess. Let's take the
    same Markov decision we have been dealing so far, but only focus on the
    state 0.`),It.forEach(s),on=T(e),ht=S(e,"P",{});var Dt=I(ht);ia=p(Dt,"The random policy that we have been using so far is clearly not optimal."),Dt.forEach(s),pn=T(e),h(Ze.$$.fragment,e),un=T(e),_t=S(e,"P",{});var Mt=I(_t);pa=p(Mt,`The below agent implements a policy that tends to take the action B more
    often. This behaviour will generate a higher expected return.`),Mt.forEach(s),mn=T(e),h(et.$$.fragment,e),cn=T(e),wt=S(e,"P",{});var Bt=I(wt);ua=p(Bt,`A state-value function allows the agent to assign a goodness value to each
    of the states for a given policy. Change the policy and observe if you see
    any improvements. That is where the action-value function comes into play.
    The action-value function allows us to reason about changing a single
    action. We can ask the question what would happen if we generally keep the
    same policy and only change the next action. Would that new policy be more
    benefitial to us? If yes, maybe we should change our policy. And the goal of
    the agent is to find the optimal policy.`),Bt.forEach(s),gn=T(e),h(Be.$$.fragment,e),hn=T(e),q=S(e,"P",{});var G=I(q);ma=p(G,`Optimality implies that there is a way to compare different policies and to
    determine which of the policies is better. In a Markov decision process
    value functions are used as a metric of the goodness of a policy. The policy `),h(qe.$$.fragment,G),ca=p(G,`
    is said to be better than the policy `),h(We.$$.fragment,G),ga=p(G,` if and
    only if the value function of `),h(Ge.$$.fragment,G),ha=p(G,` is larger or equal
    to the value function of policy `),h(ze.$$.fragment,G),_a=p(G,` for all states
    in the state set `),h(Re.$$.fragment,G),wa=p(G,"."),G.forEach(s),_n=T(e),tt=S(e,"DIV",{class:!0});var qt=I(tt);h(Ve.$$.fragment,qt),qt.forEach(s),wn=T(e),je=S(e,"P",{});var ot=I(je);da=p(ot,"The optimal policy "),h(He.$$.fragment,ot),va=p(ot,` is therefore the policy
    that is better (or at least not worse) than any other policy.`),ot.forEach(s),dn=T(e),nt=S(e,"DIV",{class:!0});var Wt=I(nt);h(Ce.$$.fragment,Wt),Wt.forEach(s),vn=T(e),dt=S(e,"P",{});var Gt=I(dt);ba=p(Gt,`It is not hard to imagine the optimal strategy for the two-states MDP. The
    agent needs to always take the action B. This policy increases the
    probability to land in the state 2 and thus increases the probability of
    getting a reward of 5. In other words always taking the actio B would
    maximize the value function.`),Gt.forEach(s),bn=T(e),h(at.$$.fragment,e),yn=T(e),h(st.$$.fragment,e),xn=T(e),N=S(e,"P",{});var K=I(N);ya=p(K,"How we can numerically find the optimal policy "),h(Fe.$$.fragment,K),xa=p(K,` and the corresponding
    value functions `),h(Le.$$.fragment,K),Ta=p(K,", "),h(Ne.$$.fragment,K),ka=p(K,` is going to be the topic
    of the rest of the reinforcement learning block. The essence of reinforcement
    learning is to find the optimal policy, given that the environment contains a
    Markov decision process under the hood.`),K.forEach(s),Tn=T(e),vt=S(e,"DIV",{class:!0}),I(vt).forEach(s),this.h()},h(){V(m,"class","separator"),V(Oe,"class","flex justify-center"),V(Ke,"class","flex justify-center"),V(Qe,"class","flex justify-center"),V(Ue,"class","flex justify-center"),V(tt,"class","flex justify-center"),V(nt,"class","flex justify-center"),V(vt,"class","separator")},m(e,$){r(e,n,$),k(n,t),r(e,a,$),r(e,m,$),r(e,u,$),r(e,b,$),k(b,c),_(l,b,null),k(b,o),r(e,A,$),r(e,y,$),k(y,D),_(z,y,null),k(y,it),_(U,y,null),k(y,An),_(Z,y,null),k(y,Pn),_(ee,y,null),k(y,En),r(e,Rt,$),r(e,Oe,$),_(te,Oe,null),r(e,Vt,$),r(e,pt,$),k(pt,Sn),r(e,jt,$),_(Xe,e,$),r(e,Ht,$),_(Ye,e,$),r(e,Ct,$),r(e,ne,$),k(ne,In),_(ae,ne,null),k(ne,Dn),r(e,Ft,$),_(Je,e,$),r(e,Lt,$),r(e,C,$),k(C,Mn),_(se,C,null),k(C,Bn),_($e,C,null),k(C,qn),_(fe,C,null),k(C,Wn),r(e,Nt,$),r(e,Ke,$),_(le,Ke,null),r(e,Ot,$),r(e,ut,$),k(ut,Gn),r(e,Xt,$),r(e,re,$),k(re,zn),_(oe,re,null),k(re,Rn),r(e,Yt,$),r(e,ie,$),k(ie,Vn),_(pe,ie,null),k(ie,jn),r(e,Jt,$),_(ue,e,$),r(e,Kt,$),_(Q,e,$),r(e,Qt,$),r(e,mt,$),k(mt,Cn),r(e,Ut,$),_(me,e,$),r(e,Zt,$),r(e,B,$),k(B,Fn),_(ce,B,null),k(B,Ln),_(ge,B,null),k(B,Nn),_(he,B,null),k(B,On),_(_e,B,null),k(B,Xn),_(we,B,null),k(B,Yn),r(e,en,$),r(e,ct,$),k(ct,Jn),r(e,tn,$),_(de,e,$),r(e,nn,$),r(e,F,$),k(F,Kn),_(ve,F,null),k(F,Qn),_(be,F,null),k(F,Un),_(ye,F,null),k(F,Zn),r(e,an,$),_(xe,e,$),r(e,sn,$),r(e,L,$),k(L,ea),_(Te,L,null),k(L,ta),_(ke,L,null),k(L,na),_(Ae,L,null),k(L,aa),r(e,$n,$),r(e,Qe,$),_(Pe,Qe,null),r(e,fn,$),r(e,R,$),k(R,sa),_(Ee,R,null),k(R,$a),_(Se,R,null),k(R,fa),_(Ie,R,null),k(R,la),_(De,R,null),k(R,ra),r(e,ln,$),r(e,Ue,$),_(Me,Ue,null),r(e,rn,$),r(e,gt,$),k(gt,oa),r(e,on,$),r(e,ht,$),k(ht,ia),r(e,pn,$),_(Ze,e,$),r(e,un,$),r(e,_t,$),k(_t,pa),r(e,mn,$),_(et,e,$),r(e,cn,$),r(e,wt,$),k(wt,ua),r(e,gn,$),_(Be,e,$),r(e,hn,$),r(e,q,$),k(q,ma),_(qe,q,null),k(q,ca),_(We,q,null),k(q,ga),_(Ge,q,null),k(q,ha),_(ze,q,null),k(q,_a),_(Re,q,null),k(q,wa),r(e,_n,$),r(e,tt,$),_(Ve,tt,null),r(e,wn,$),r(e,je,$),k(je,da),_(He,je,null),k(je,va),r(e,dn,$),r(e,nt,$),_(Ce,nt,null),r(e,vn,$),r(e,dt,$),k(dt,ba),r(e,bn,$),_(at,e,$),r(e,yn,$),_(st,e,$),r(e,xn,$),r(e,N,$),k(N,ya),_(Fe,N,null),k(N,xa),_(Le,N,null),k(N,Ta),_(Ne,N,null),k(N,ka),r(e,Tn,$),r(e,vt,$),kn=!0},p(e,$){const $t={};$&16&&($t.$$scope={dirty:$,ctx:e}),l.$set($t);const j={};$&16&&(j.$$scope={dirty:$,ctx:e}),z.$set(j);const yt={};$&16&&(yt.$$scope={dirty:$,ctx:e}),U.$set(yt);const xt={};$&16&&(xt.$$scope={dirty:$,ctx:e}),Z.$set(xt);const ft={};$&16&&(ft.$$scope={dirty:$,ctx:e}),ee.$set(ft);const X={};$&16&&(X.$$scope={dirty:$,ctx:e}),te.$set(X);const Tt={};$&16&&(Tt.$$scope={dirty:$,ctx:e}),ae.$set(Tt);const kt={};$&16&&(kt.$$scope={dirty:$,ctx:e}),se.$set(kt);const lt={};$&16&&(lt.$$scope={dirty:$,ctx:e}),$e.$set(lt);const rt={};$&16&&(rt.$$scope={dirty:$,ctx:e}),fe.$set(rt);const At={};$&16&&(At.$$scope={dirty:$,ctx:e}),le.$set(At);const W={};$&16&&(W.$$scope={dirty:$,ctx:e}),oe.$set(W);const Pt={};$&16&&(Pt.$$scope={dirty:$,ctx:e}),pe.$set(Pt);const Y={};$&18&&(Y.$$scope={dirty:$,ctx:e}),ue.$set(Y);const J={};!Hn&&$&1&&(Hn=!0,J.value=e[0],as(()=>Hn=!1)),Q.$set(J);const Et={};$&16&&(Et.$$scope={dirty:$,ctx:e}),me.$set(Et);const H={};$&16&&(H.$$scope={dirty:$,ctx:e}),ce.$set(H);const St={};$&16&&(St.$$scope={dirty:$,ctx:e}),ge.$set(St);const It={};$&16&&(It.$$scope={dirty:$,ctx:e}),he.$set(It);const Dt={};$&16&&(Dt.$$scope={dirty:$,ctx:e}),_e.$set(Dt);const Mt={};$&16&&(Mt.$$scope={dirty:$,ctx:e}),we.$set(Mt);const Bt={};$&16&&(Bt.$$scope={dirty:$,ctx:e}),de.$set(Bt);const G={};$&16&&(G.$$scope={dirty:$,ctx:e}),ve.$set(G);const qt={};$&16&&(qt.$$scope={dirty:$,ctx:e}),be.$set(qt);const ot={};$&16&&(ot.$$scope={dirty:$,ctx:e}),ye.$set(ot);const Wt={};$&16&&(Wt.$$scope={dirty:$,ctx:e}),xe.$set(Wt);const Gt={};$&16&&(Gt.$$scope={dirty:$,ctx:e}),Te.$set(Gt);const K={};$&16&&(K.$$scope={dirty:$,ctx:e}),ke.$set(K);const Ia={};$&16&&(Ia.$$scope={dirty:$,ctx:e}),Ae.$set(Ia);const Da={};$&16&&(Da.$$scope={dirty:$,ctx:e}),Pe.$set(Da);const Ma={};$&16&&(Ma.$$scope={dirty:$,ctx:e}),Ee.$set(Ma);const Ba={};$&16&&(Ba.$$scope={dirty:$,ctx:e}),Se.$set(Ba);const qa={};$&16&&(qa.$$scope={dirty:$,ctx:e}),Ie.$set(qa);const Wa={};$&16&&(Wa.$$scope={dirty:$,ctx:e}),De.$set(Wa);const Ga={};$&16&&(Ga.$$scope={dirty:$,ctx:e}),Me.$set(Ga);const za={};$&16&&(za.$$scope={dirty:$,ctx:e}),Be.$set(za);const Ra={};$&16&&(Ra.$$scope={dirty:$,ctx:e}),qe.$set(Ra);const Va={};$&16&&(Va.$$scope={dirty:$,ctx:e}),We.$set(Va);const ja={};$&16&&(ja.$$scope={dirty:$,ctx:e}),Ge.$set(ja);const Ha={};$&16&&(Ha.$$scope={dirty:$,ctx:e}),ze.$set(Ha);const Ca={};$&16&&(Ca.$$scope={dirty:$,ctx:e}),Re.$set(Ca);const Fa={};$&16&&(Fa.$$scope={dirty:$,ctx:e}),Ve.$set(Fa);const La={};$&16&&(La.$$scope={dirty:$,ctx:e}),He.$set(La);const Na={};$&16&&(Na.$$scope={dirty:$,ctx:e}),Ce.$set(Na);const Oa={};$&16&&(Oa.$$scope={dirty:$,ctx:e}),Fe.$set(Oa);const Xa={};$&16&&(Xa.$$scope={dirty:$,ctx:e}),Le.$set(Xa);const Ya={};$&16&&(Ya.$$scope={dirty:$,ctx:e}),Ne.$set(Ya)},i(e){kn||(w(l.$$.fragment,e),w(z.$$.fragment,e),w(U.$$.fragment,e),w(Z.$$.fragment,e),w(ee.$$.fragment,e),w(te.$$.fragment,e),w(Xe.$$.fragment,e),w(Ye.$$.fragment,e),w(ae.$$.fragment,e),w(Je.$$.fragment,e),w(se.$$.fragment,e),w($e.$$.fragment,e),w(fe.$$.fragment,e),w(le.$$.fragment,e),w(oe.$$.fragment,e),w(pe.$$.fragment,e),w(ue.$$.fragment,e),w(Q.$$.fragment,e),w(me.$$.fragment,e),w(ce.$$.fragment,e),w(ge.$$.fragment,e),w(he.$$.fragment,e),w(_e.$$.fragment,e),w(we.$$.fragment,e),w(de.$$.fragment,e),w(ve.$$.fragment,e),w(be.$$.fragment,e),w(ye.$$.fragment,e),w(xe.$$.fragment,e),w(Te.$$.fragment,e),w(ke.$$.fragment,e),w(Ae.$$.fragment,e),w(Pe.$$.fragment,e),w(Ee.$$.fragment,e),w(Se.$$.fragment,e),w(Ie.$$.fragment,e),w(De.$$.fragment,e),w(Me.$$.fragment,e),w(Ze.$$.fragment,e),w(et.$$.fragment,e),w(Be.$$.fragment,e),w(qe.$$.fragment,e),w(We.$$.fragment,e),w(Ge.$$.fragment,e),w(ze.$$.fragment,e),w(Re.$$.fragment,e),w(Ve.$$.fragment,e),w(He.$$.fragment,e),w(Ce.$$.fragment,e),w(at.$$.fragment,e),w(st.$$.fragment,e),w(Fe.$$.fragment,e),w(Le.$$.fragment,e),w(Ne.$$.fragment,e),kn=!0)},o(e){d(l.$$.fragment,e),d(z.$$.fragment,e),d(U.$$.fragment,e),d(Z.$$.fragment,e),d(ee.$$.fragment,e),d(te.$$.fragment,e),d(Xe.$$.fragment,e),d(Ye.$$.fragment,e),d(ae.$$.fragment,e),d(Je.$$.fragment,e),d(se.$$.fragment,e),d($e.$$.fragment,e),d(fe.$$.fragment,e),d(le.$$.fragment,e),d(oe.$$.fragment,e),d(pe.$$.fragment,e),d(ue.$$.fragment,e),d(Q.$$.fragment,e),d(me.$$.fragment,e),d(ce.$$.fragment,e),d(ge.$$.fragment,e),d(he.$$.fragment,e),d(_e.$$.fragment,e),d(we.$$.fragment,e),d(de.$$.fragment,e),d(ve.$$.fragment,e),d(be.$$.fragment,e),d(ye.$$.fragment,e),d(xe.$$.fragment,e),d(Te.$$.fragment,e),d(ke.$$.fragment,e),d(Ae.$$.fragment,e),d(Pe.$$.fragment,e),d(Ee.$$.fragment,e),d(Se.$$.fragment,e),d(Ie.$$.fragment,e),d(De.$$.fragment,e),d(Me.$$.fragment,e),d(Ze.$$.fragment,e),d(et.$$.fragment,e),d(Be.$$.fragment,e),d(qe.$$.fragment,e),d(We.$$.fragment,e),d(Ge.$$.fragment,e),d(ze.$$.fragment,e),d(Re.$$.fragment,e),d(Ve.$$.fragment,e),d(He.$$.fragment,e),d(Ce.$$.fragment,e),d(at.$$.fragment,e),d(st.$$.fragment,e),d(Fe.$$.fragment,e),d(Le.$$.fragment,e),d(Ne.$$.fragment,e),kn=!1},d(e){e&&s(n),e&&s(a),e&&s(m),e&&s(u),e&&s(b),v(l),e&&s(A),e&&s(y),v(z),v(U),v(Z),v(ee),e&&s(Rt),e&&s(Oe),v(te),e&&s(Vt),e&&s(pt),e&&s(jt),v(Xe,e),e&&s(Ht),v(Ye,e),e&&s(Ct),e&&s(ne),v(ae),e&&s(Ft),v(Je,e),e&&s(Lt),e&&s(C),v(se),v($e),v(fe),e&&s(Nt),e&&s(Ke),v(le),e&&s(Ot),e&&s(ut),e&&s(Xt),e&&s(re),v(oe),e&&s(Yt),e&&s(ie),v(pe),e&&s(Jt),v(ue,e),e&&s(Kt),v(Q,e),e&&s(Qt),e&&s(mt),e&&s(Ut),v(me,e),e&&s(Zt),e&&s(B),v(ce),v(ge),v(he),v(_e),v(we),e&&s(en),e&&s(ct),e&&s(tn),v(de,e),e&&s(nn),e&&s(F),v(ve),v(be),v(ye),e&&s(an),v(xe,e),e&&s(sn),e&&s(L),v(Te),v(ke),v(Ae),e&&s($n),e&&s(Qe),v(Pe),e&&s(fn),e&&s(R),v(Ee),v(Se),v(Ie),v(De),e&&s(ln),e&&s(Ue),v(Me),e&&s(rn),e&&s(gt),e&&s(on),e&&s(ht),e&&s(pn),v(Ze,e),e&&s(un),e&&s(_t),e&&s(mn),v(et,e),e&&s(cn),e&&s(wt),e&&s(gn),v(Be,e),e&&s(hn),e&&s(q),v(qe),v(We),v(Ge),v(ze),v(Re),e&&s(_n),e&&s(tt),v(Ve),e&&s(wn),e&&s(je),v(He),e&&s(dn),e&&s(nt),v(Ce),e&&s(vn),e&&s(dt),e&&s(bn),v(at,e),e&&s(yn),v(st,e),e&&s(xn),e&&s(N),v(Fe),v(Le),v(Ne),e&&s(Tn),e&&s(vt)}}}function S$(f){let n,t,a,m;return a=new ss({props:{$$slots:{default:[E$]},$$scope:{ctx:f}}}),{c(){n=E("meta"),t=x(),g(a.$$.fragment),this.h()},l(u){const b=es("svelte-16e4egp",document.head);n=S(b,"META",{name:!0,content:!0}),b.forEach(s),t=T(u),h(a.$$.fragment,u),this.h()},h(){document.title="Markov Decision Process Solution - World4AI",V(n,"name","description"),V(n,"content","A Markov decision process is considered to be solved once the agent found the optimal policy and the optimal value function. In essence that means that the agent has to maximize the expected sum of rewards.")},m(u,b){k(document.head,n),r(u,t,b),_(a,u,b),m=!0},p(u,[b]){const c={};b&19&&(c.$$scope={dirty:b,ctx:u}),a.$set(c)},i(u){m||(w(a.$$.fragment,u),m=!0)},o(u){d(a.$$.fragment,u),m=!1},d(u){s(n),u&&s(t),v(a,u)}}}let I$=100;function D$(){let f=0,n="A",t="action";return()=>{if(t==="action")return f===0&&(n==="A"?f=Math.random()<.7?0:1:n==="B"&&(f=Math.random()<.2?0:1)),f===1&&(n==="A"?f=Math.random()<.9?0:1:n==="B"&&(f=Math.random()<.3?0:1)),t="state",{type:"state",value:f};if(t==="reward")return t="action",n=Math.random()<.5?"A":"B",{type:"action",value:n};if(t==="state"){t="reward";let m;return f===0?m=-1:f===1&&(m=5),{type:"reward",value:m}}}}function M$(f,n,t){let a=.95,m=[];function u(){t(1,m=[]);for(let c=0;c<=I$;c++){let l={x:c,y:a**c};m.push(l)}}function b(c){a=c,t(0,a)}return f.$$.update=()=>{f.$$.dirty&1&&a&&u()},[a,m,b]}class L$ extends Qa{constructor(n){super(),Ua(this,n,M$,S$,Za,{})}}export{L$ as default};
