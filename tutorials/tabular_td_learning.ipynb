{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23ec166-6a73-463e-89ba-4bf7c8901870",
   "metadata": {},
   "source": [
    "# Temporal Difference Prediction and Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0d296-8cc6-47e1-957f-0c54ab25ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda4c17-8369-43c4-be9e-40bcc6ae499f",
   "metadata": {},
   "source": [
    "## TD Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451f397-7c57-4991-bbcc-6ba9cbb8c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction(env, policy, obs_space, num_episodes, alpha, gamma):\n",
    "    # v as value function\n",
    "    v = np.zeros(len(obs_space))\n",
    "    \n",
    "    for episode in trange(num_episodes):\n",
    "        # reset variables\n",
    "        done, obs = False, env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            action = policy(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            v[obs] = v[obs] + alpha * (reward + gamma * v[next_obs] - v[obs])\n",
    "            obs = next_obs\n",
    "            \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1fd07-29e9-4a5e-8ad8-7c722a5f9674",
   "metadata": {},
   "source": [
    "## TD-Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8398f-4f11-46fc-aa8e-4b2596f3ff94",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e55958-47c8-4581-975b-dae413520b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, obs_space, action_space, num_episodes, alpha, gamma, epsilon):\n",
    "    # q as action value function\n",
    "    q = np.zeros(shape=(len(obs_space), len(action_space)))\n",
    "                 \n",
    "    # epsilon greedy policy\n",
    "    def policy(obs):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = q[obs].argmax()\n",
    "        return action\n",
    "    \n",
    "    for episode in trange(num_episodes):\n",
    "        # reset variables\n",
    "        done, obs = False, env.reset()\n",
    "        action = policy(obs)\n",
    "        \n",
    "        while not done:\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_action = policy(next_obs)\n",
    "            \n",
    "            q[obs][action] = q[obs][action] + alpha * (reward + gamma * q[next_obs][next_action] * (not done) - q[obs][action])\n",
    "            obs, action = next_obs, next_action\n",
    "    \n",
    "    # greedy policy\n",
    "    policy_mapping = np.argmax(q, axis=1)\n",
    "    policy = lambda x: policy_mapping[x]\n",
    "        \n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbe80b-3e95-4c16-9e75-04b5a16e98d9",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2a976-24eb-4840-b756-df4e957d81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, obs_space, action_space, num_episodes, alpha, gamma, epsilon):\n",
    "    # q as action value function\n",
    "    q = np.zeros(shape=(len(obs_space), len(action_space)))\n",
    "                 \n",
    "    # epsilon greedy policy\n",
    "    def policy(obs):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = q[obs].argmax()\n",
    "        return action\n",
    "    \n",
    "    for episode in trange(num_episodes):\n",
    "        # reset variables\n",
    "        done, obs = False, env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            action = policy(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_action = policy(next_obs)\n",
    "            \n",
    "            q[obs][action] = q[obs][action] + alpha * (reward + gamma * q[next_obs].max() * (not done) - q[obs][action])\n",
    "            obs = next_obs\n",
    "    \n",
    "    # greedy policy\n",
    "    policy_mapping = np.argmax(q, axis=1)\n",
    "    policy = lambda x: policy_mapping[x]\n",
    "        \n",
    "    return policy, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7f5b4-62d9-46ca-881c-4c718e326376",
   "metadata": {},
   "source": [
    "## Testing using FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e16f56-c602-489d-8baf-9c8e9106288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11fd81-319e-4c0a-8fca-604c713ff103",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = {obs for obs in range(env.observation_space.n)}\n",
    "action_space = {action for action in range(env.action_space.n)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047da247-ebca-4d88-b36d-b9310c945635",
   "metadata": {},
   "source": [
    "### TD Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff09139-c041-4e27-9eca-afb52c7bf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    mapping = {\n",
    "            0: 2,\n",
    "            1: 2,\n",
    "            2: 1,\n",
    "            3: 0,\n",
    "            4: 1,\n",
    "            5: 1,\n",
    "            6: 1,\n",
    "            7: 1,\n",
    "            8: 2,\n",
    "            9: 1,\n",
    "            10: 1,\n",
    "            11: 1,\n",
    "            12: 2,\n",
    "            13: 2,\n",
    "            14: 2,\n",
    "            15: 2\n",
    "    }\n",
    "    return mapping[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c5067-c2df-4112-a613-a11283e86749",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_prediction(env=env, policy=policy, obs_space=obs_space, num_episodes=100000, alpha=0.01, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f5d5f-2c98-4d20-81c4-60465ecc2f03",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cddc0-73fb-4e58-95fc-c3ad61a5398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, q = sarsa(env=env, \n",
    "               obs_space=obs_space, \n",
    "               action_space=action_space, \n",
    "               num_episodes=100000, \n",
    "               alpha=0.1, \n",
    "               gamma=0.99, \n",
    "               epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59feeadf-c408-4247-8d64-9fcb8033b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in obs_space:\n",
    "    print(f'Observation: {obs}, q-values: {q[obs]}, action: {policy(obs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988df770-a3ff-4362-a922-01c8dfc94ad9",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a7582-8db8-4d6d-a4d1-17fd1bbc3d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, q = q_learning(env=env, \n",
    "               obs_space=obs_space, \n",
    "               action_space=action_space, \n",
    "               num_episodes=100000, \n",
    "               alpha=0.1, \n",
    "               gamma=0.99, \n",
    "               epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354075f-b3ac-472f-b638-a083904ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in obs_space:\n",
    "    print(f'Observation: {obs}, q-values: {q[obs]}, action: {policy(obs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
