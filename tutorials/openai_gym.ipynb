{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c4b844-eadf-4b55-9e14-130cda40db3b",
   "metadata": {},
   "source": [
    "# OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced74285-81ce-4594-b81e-cb5f7f90e1dc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece71baa-7b64-4680-bc24-e3a1abcafa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import envs\n",
    "from gym import spaces\n",
    "from gym import Wrapper, ObservationWrapper, ActionWrapper, RewardWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9562aac-adfe-4a2c-80ca-a33e8871825a",
   "metadata": {},
   "source": [
    "## Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9596df-d37e-4fcf-93fe-258a69cdcc97",
   "metadata": {
    "tags": []
   },
   "source": [
    "The ```gym.__version__``` attribute shows the version of the used OpenAI Gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a42da8-396b-4fc0-898e-4b4b2f61c1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Version of OpenAI Gym: {gym.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e568a87-039c-4d8c-a7f2-007f1c3d99a2",
   "metadata": {},
   "source": [
    "## Registered Envrironments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f1dc5-8549-47f1-8edd-7b8f5edce435",
   "metadata": {},
   "source": [
    "The ```gym.envs``` module is useful to figure out what environments are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b1b3d-5400-472d-8f05-84fd6caa9cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print all registered environments\n",
    "env_list = [env.id for env in envs.registry.all()]\n",
    "print('Registered environments in OpenAI Gym\\n')\n",
    "print(env_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481fbc0-d809-424d-82a9-82c349431104",
   "metadata": {},
   "source": [
    "## Create an Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ae2ec-aa8e-4b6b-8839-633bcc023a04",
   "metadata": {},
   "source": [
    "The ```gym.make('environment_name')``` function generates and returns an environment that the agent can interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2fd0e-206f-41d9-a6a2-4d0cabc649a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98284e86-e407-47eb-a0f7-d66a87050e71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Agent - Envrionement Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632906e-c6a8-44ba-abc4-6dc224f44597",
   "metadata": {},
   "source": [
    "The ```env.reset()``` method initializes the environment and returns the initial observation. The method is the first that needs to be run after we generate an evironment. Each time the agent encounters a terminal state ```T``` we need to run the method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba48ea6-1ec7-42b1-8b25-1338c5348780",
   "metadata": {},
   "source": [
    "In the second step the agent needs to generate an action based on the current observation. At this point we don't have a trained agent, therefore we use a random action with ```env.action_space.sample()```. The generated action is usually saved in the variable ```action```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d38d1-1af6-4cb0-a449-7ac58af06f88",
   "metadata": {},
   "source": [
    "The method ```env.step(action)``` takes an ```action``` as input and returns a tuple containing ```(next_observation, reward, done, info)```, where ```next_observation``` is the observation the environment transitions into, ```reward``` is the reward that the agent receives based on the action and the previous observation, ```done``` is the boolean value that is ```True``` if the environment transitioned into the terminal state and ```False``` otherwise and ```info``` is additional information, that is primarily intended for debuggin purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8914f78-6e4f-4620-bef7-3749900b09e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "```env.render()``` renders the current observation either directly to the terminal or using a graphical output, like a game engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b2ea2-0684-4077-8b33-99c8c982475f",
   "metadata": {},
   "source": [
    "```env.close()``` is the method that completely closes the environment. That method should be run once you don't need the environmet for training or testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b3fce-61f7-47a5-9c0e-da5ead06f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show interaction for the Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1')\n",
    "obs, done = env.reset(), False\n",
    "# interact with the environment until the terminal state\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()     \n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    obs = next_obs\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbffd72-e1d8-4bb1-bbe2-aef3c85cba56",
   "metadata": {},
   "source": [
    "## State and Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694ea5e-0908-4952-b68b-00410307029e",
   "metadata": {},
   "source": [
    "OpenAI Gym provides several classes that are responsible for defining state and action spaces, all of them are located in the ```gym.spaces``` module. The ```Space``` class is the base class other spaces derive from. The ```Discrete``` class is used for discrete state and action spaces. The ```Box``` class on the other hand is used for continuous state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42d9d9-6b09-4085-83e5-5f607b62db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state and action spaces\n",
    "print(spaces.Space)\n",
    "print(spaces.Discrete)\n",
    "print(spaces.Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bebab-de66-4373-9a56-93fa6c0d2ad4",
   "metadata": {},
   "source": [
    "Generally speaking the ```Discrete``` class only needs a single input ```n``` that determines the number of possible discrete states or actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81558b20-cc2a-46a0-81fc-3eab8dd5f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a discrete space\n",
    "discrete_space = spaces.Discrete(n=10)\n",
    "print(discrete_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56779-89df-4e51-8f15-86b09ee605f6",
   "metadata": {},
   "source": [
    "The ```Box``` class takes takes up to 4 arguments.\n",
    "\n",
    "1. ```low```: the lower bound of the space\n",
    "2. ```high```: the upper bound of the space\n",
    "3. ```shape```: the shape of the space \n",
    "4. ```dtype```: the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db4c58-e6da-4bbe-bd50-09b38a349daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a continuous space\n",
    "continuous_space = spaces.Box(low=-1.0, high=1.0, shape=(2, 2), dtype=np.float32)\n",
    "print(continuous_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f2dd5-0774-4d87-ab0f-e0e8bea59e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# discrete environment example\n",
    "print('FrozenLake-v1')\n",
    "env_discrete = gym.make('FrozenLake-v1')\n",
    "env_discrete.reset()\n",
    "print(f'Observation Space: {env_discrete.observation_space}')\n",
    "print(f'Action Space: {env_discrete.action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a542c06-7638-4cfc-b739-e47ac8e3553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous environment\n",
    "print('MountainCarContinuous-v0')\n",
    "env_continuous = gym.make('MountainCarContinuous-v0')\n",
    "env_continuous.reset()\n",
    "print(f'Observation Space: {env_continuous.observation_space}')\n",
    "print(f'Action Space: {env_continuous.action_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1ea43-454d-4022-a8bf-3f2a7bd245f9",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e58898-7c30-4956-b2f1-018e44b98e85",
   "metadata": {},
   "source": [
    "The main purpose of wrappers is to change or extend the functionality of the original environment in a some way and to train the agent based on the *\"wrapped\"* environment. For example we could prepropress the rgb (red, green, blue) pictures of Atari games to make the training for neural networks easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d65008-45dd-47b9-be9a-e87c01a4b0e7",
   "metadata": {},
   "source": [
    "The ```Wrapper``` class is the base class of all other wrappers. The base class is usially used to extend or change the ```reset``` and ```step``` methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603e7db-cfb2-4e94-9111-80916068dab4",
   "metadata": {},
   "source": [
    "Below we create a ```PrintingWrapper``` which extends the environent by printing the current observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27b62f-5dec-42bd-914d-b6b99f3b94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintingWrapper(Wrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(PrintingWrapper, self).__init__(env)\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        print(f'Observation: {obs}')\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        print(f'Action taken: {action}')\n",
    "        next_obs, reward, done, info = self.env.step(action)\n",
    "        print(f'Observation: {next_obs}, Reward: {reward}, done: {done}')\n",
    "        return next_obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259665e1-0e02-456d-8a65-9a520de2f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = PrintingWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74d56d-ec50-4f85-af60-ec08cb9b3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cdcf7f-52ea-4d33-bdb0-9bf808842697",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs, reward, done, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c9259-7da0-42d2-82d2-4b53fb81295f",
   "metadata": {},
   "source": [
    "The ```ObservationWrapper``` adjust the observation of the environment by using the ```observation``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82d95c-ea61-42ed-b22d-03cdd128284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddHundredObsWrapper(ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(AddHundredObsWrapper, self).__init__(env)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return obs+100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdff7e0-7bb7-4404-8989-946d734294ec",
   "metadata": {},
   "source": [
    "We can stack several wrappers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22683a-897e-4603-a420-cb1aa316e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = AddHundredObsWrapper(env)\n",
    "env = PrintingWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4287106-f6ca-45a3-b956-c78686010ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78f624-f924-4463-b730-453840c776ee",
   "metadata": {},
   "source": [
    "The ```RewardWrapper``` utilizes the ```reward``` method to adjust the rewards of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fce6dc-6caa-4c80-8778-73600a6ba876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTenRewardWrapper(RewardWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(AddTenRewardWrapper, self).__init__(env)\n",
    "        \n",
    "    def reward(self, reward):\n",
    "        return reward+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f6a7b-4ac4-412a-bfbf-0e3ead75c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = AddHundredObsWrapper(env)\n",
    "env = AddTenRewardWrapper(env)\n",
    "env = PrintingWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8feed-f1b3-42ac-a5e1-28ee92ec241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b6e2f-fca1-41fa-8de2-8240e44e428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs, reward, done, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2ddc9-6867-4f4d-95d3-a21fdc9188ed",
   "metadata": {},
   "source": [
    "The ```ActionWrapper``` modifies the action that the agent provides to the environment. The wrapper implements the abstract ```action``` method that needs to be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4797aa3-34f3-4afa-b93f-9d0a68667670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddActionOneWrapper(ActionWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(AddActionOneWrapper, self).__init__(env)\n",
    "        \n",
    "    def action(self, action):\n",
    "        return action+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba5551-e394-4225-b21f-8cdbf7b78f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = PrintingWrapper(env)\n",
    "env = AddHundredObsWrapper(env)\n",
    "env = AddTenRewardWrapper(env)\n",
    "env = AddActionOneWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59d98a-92e9-43f6-a182-ba1057e53666",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce6bd2-0f51-4d1e-8a0a-75b723c8f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs, reward, done, info = env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
