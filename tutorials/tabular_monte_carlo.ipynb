{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a036ab4-f99a-407c-aefc-72cc9ef700b1",
   "metadata": {},
   "source": [
    "# Monte Carlo Prediction and Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7979965-758c-4a9d-9642-3b90eb19e582",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191c011-c789-4ca9-b773-0f5bee4e059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4393f-67ef-4758-aac2-b09b1c04c6da",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e008747-cee9-4771-9b5d-92b7b8ea2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(env, policy, obs_space, num_episodes, alpha, gamma):\n",
    "    # v as value function\n",
    "    v = np.zeros(len(obs_space))\n",
    "    \n",
    "    for episode in trange(num_episodes):\n",
    "        done, obs = False, env.reset()\n",
    "        obs_trajectory = []\n",
    "        reward_trajectory = []\n",
    "\n",
    "        #1: interaction with the environment to generate a trajectory\n",
    "        while not done:\n",
    "            action = policy(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            obs_trajectory.append(obs)\n",
    "            reward_trajectory.append(reward)\n",
    "            obs = next_obs\n",
    "            \n",
    "        #2: calculate value function of the policy\n",
    "        visited = np.zeros(len(obs_space), dtype=np.bool_)\n",
    "        discount_rates = np.array([gamma**i for i in range(len(obs_trajectory))])\n",
    "        for t, obs in enumerate(obs_trajectory):\n",
    "            if visited[obs]:\n",
    "                continue\n",
    "            visited[obs] = True\n",
    "            rewards = np.array(reward_trajectory, dtype=np.float32)\n",
    "            target = np.sum(rewards * discount_rates[:len(rewards)])\n",
    "            v[obs] = v[obs] + alpha * (target - v[obs])\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022205c8-daf5-4e0e-8e48-48063f0fb0d5",
   "metadata": {},
   "source": [
    "## Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09446693-cd9b-4641-b7c4-868fef15ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, obs_space, action_space, num_episodes, alpha, gamma, epsilon):\n",
    "    \n",
    "    # Initialization phase\n",
    "    #------------------------------------------------------------------------------\n",
    "    # v as value function\n",
    "    q = np.zeros(shape=(len(obs_space), len(action_space)))\n",
    "    # epsilon greedy policy\n",
    "    def policy(obs):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = q[obs].argmax()\n",
    "        return action\n",
    "        \n",
    "    # Learning phase\n",
    "    #------------------------------------------------------------------------------\n",
    "    for episode in trange(num_episodes):\n",
    "        done, obs = False, env.reset()\n",
    "        obs_trajectory = []\n",
    "        action_trajectory = []\n",
    "        reward_trajectory = []\n",
    "\n",
    "        #1: interaction with the environment to generate a trajectory\n",
    "        while not done:\n",
    "            action = policy(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            action_trajectory.append(action)\n",
    "            obs_trajectory.append(obs)\n",
    "            reward_trajectory.append(reward)\n",
    "            obs = next_obs\n",
    "            \n",
    "        #2: calculate action value function \n",
    "        visited = np.zeros(len(obs_space), dtype=np.bool_)\n",
    "        discount_rates = np.array([gamma**i for i in range(len(obs_trajectory))])\n",
    "        for t, (obs, action) in enumerate(zip(obs_trajectory, action_trajectory)):\n",
    "            if visited[obs]:\n",
    "                continue\n",
    "            visited[obs] = True\n",
    "            rewards = np.array(reward_trajectory, dtype=np.float32)\n",
    "            target = np.sum(rewards * discount_rates[:len(rewards)])\n",
    "            q[obs][action] = q[obs][action] + alpha * (target - q[obs][action])\n",
    "    \n",
    "    # greedy policy\n",
    "    policy_mapping = np.argmax(q, axis=1)\n",
    "    policy = lambda x: policy_mapping[x]\n",
    "\n",
    "    return policy, q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20bba5-e9e5-44fd-883c-a89111774389",
   "metadata": {},
   "source": [
    "## Test using FrozeLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ed5da-d3f2-47cf-aa4c-e86e416c672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b68c38-f1f0-4c90-8b2c-1550be69748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = {obs for obs in range(env.observation_space.n)}\n",
    "action_space = {action for action in range(env.action_space.n)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03481f66-93c0-4a1b-b7c1-2c7539dce3fb",
   "metadata": {},
   "source": [
    "### FrozenLake Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f78678-a867-4803-85d9-46426b5fb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    mapping = {\n",
    "            0: 2,\n",
    "            1: 2,\n",
    "            2: 1,\n",
    "            3: 0,\n",
    "            4: 1,\n",
    "            5: 1,\n",
    "            6: 1,\n",
    "            7: 1,\n",
    "            8: 2,\n",
    "            9: 1,\n",
    "            10: 1,\n",
    "            11: 1,\n",
    "            12: 2,\n",
    "            13: 2,\n",
    "            14: 2,\n",
    "            15: 2\n",
    "    }\n",
    "    return mapping[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411f79f-2b2c-418e-9eac-2eaeee3f27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_prediction(env=env, policy=policy, obs_space=obs_space, num_episodes=100000, alpha=0.01, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf85acc-9200-477c-b8e0-93496c4bf2ed",
   "metadata": {},
   "source": [
    "### FrozenLake Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe671f6-4587-4886-ada9-f9b85f2f1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, q = mc_control(env=env, \n",
    "                       obs_space=obs_space, \n",
    "                       action_space=action_space, \n",
    "                       num_episodes=100000, \n",
    "                       alpha=0.1, \n",
    "                       gamma=0.99, \n",
    "                       epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89923541-f822-470a-8416-1d0afa4de932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in obs_space:\n",
    "    print(f'Observation: {obs}, q-values: {q[obs]}, action: {policy(obs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
